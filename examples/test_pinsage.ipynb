{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from core import SEED, TEST_K, DATA_CLEAN_PATH, RES_PATH\n",
    "from core.neural_based_methods.pinsage.pinsage_recommender import PinSageRecommender\n",
    "from utils.evaluation import write_results_to_excel, get_test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'random_walk_length': 2,\n",
    "    'random_walk_restart_prob': 0.5,\n",
    "    'num_random_walks': 5,\n",
    "    'num_neighbors': 3,\n",
    "    'num_layers': 2,\n",
    "    'hidden_dims': 32,\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 128,\n",
    "    'device': 'cpu'\n",
    "}\n",
    "\n",
    "rec = PinSageRecommender('adobe_core5', DATA_CLEAN_PATH, use_text_feature=True, use_no_feature=False,\n",
    "                         use_only_text=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning hyperparameters using hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "\n",
    "space = {\n",
    "    'random_walk_length': hp.quniform('random_walk_length', 2, 5, 1),\n",
    "    'random_walk_restart_prob': hp.quniform('random_walk_restart_prob', 0.3, 0.7, 0.1),\n",
    "    'num_random_walks': hp.quniform('num_random_walks', 5, 15, 1),\n",
    "    'num_neighbors': hp.quniform('num_neighbors', 3, 10, 1),\n",
    "    'num_layers': hp.quniform('num_layers', 2, 4, 1),\n",
    "    'hidden_dims': hp.quniform('hidden_dims', 16, 256, 1),\n",
    "    'lr': hp.loguniform('lr', np.log(0.00001), np.log(0.1)), \n",
    "    'batch_size': hp.choice('batch_size', [64, 128, 256, 512])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_pinsage(\n",
    "    data,\n",
    "    use_text_feature=True,\n",
    "    use_no_feature=False,\n",
    "    use_only_text=False, \n",
    "    tune_res_path=os.path.join(RES_PATH, 'pinsage_recommender_tuning_results.txt'),\n",
    "    test_res_path=os.path.join(RES_PATH, 'test_results_PinSageRecommender.xlsx'),\n",
    "    device='cpu',\n",
    "):\n",
    "\n",
    "    rec = PinSageRecommender(data, DATA_CLEAN_PATH, \n",
    "                             use_text_feature=use_text_feature, use_no_feature=use_no_feature,\n",
    "                             use_only_text=use_only_text)\n",
    "    \n",
    "    def objective(params):\n",
    "        rec.train_model(**params, device=device, early_stopping=True, verbose=True)\n",
    "        return {\n",
    "            'loss':-rec.params['best_eval_score'], \n",
    "            'status':STATUS_OK,\n",
    "            'best_epoch':rec.params['best_epoch']\n",
    "        }\n",
    "    \n",
    "    print('Tuning hyperparameters of PinSage Recommender on dataset {}...'.format(data))\n",
    "    print(f'use_text_feature={use_text_feature}, use_no_feature={use_no_feature}, use_only_text={use_only_text}')\n",
    "    trials = Trials()\n",
    "    trials._random_state = np.random.RandomState(SEED)\n",
    "    best = fmin(objective, space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "    # save result\n",
    "    if os.path.exists(tune_res_path):\n",
    "        f = open(tune_res_path, 'a')\n",
    "    else:\n",
    "        f = open(tune_res_path, 'w')\n",
    "\n",
    "    print(f'PinSage Recommender tuning results on dataset {data} with use_text_feature={use_text_feature} and use_no_feature={use_no_feature} and use_only_text={use_only_text}...', \n",
    "        file=f)\n",
    "\n",
    "    # get the best trial information\n",
    "    best_trial_idx = np.argmin([trial_info['result']['loss'] for trial_info in trials.trials])\n",
    "    optimal_params = {\n",
    "        'random_walk_length': trials.trials[best_trial_idx]['misc']['vals']['random_walk_length'][0],\n",
    "        'random_walk_restart_prob': trials.trials[best_trial_idx]['misc']['vals']['random_walk_restart_prob'][0], \n",
    "        'num_random_walks': trials.trials[best_trial_idx]['misc']['vals']['num_random_walks'][0],\n",
    "        'num_neighbors': trials.trials[best_trial_idx]['misc']['vals']['num_neighbors'][0],\n",
    "        'num_layers': trials.trials[best_trial_idx]['misc']['vals']['num_layers'][0],\n",
    "        'hidden_dims': trials.trials[best_trial_idx]['misc']['vals']['hidden_dims'][0],\n",
    "        'lr': trials.trials[best_trial_idx]['misc']['vals']['lr'][0], \n",
    "        'batch_size': [64, 128, 256, 512][trials.trials[best_trial_idx]['misc']['vals']['batch_size'][0]],\n",
    "        'epochs': trials.trials[best_trial_idx]['result']['best_epoch']\n",
    "    }\n",
    "\n",
    "    print('The optimal hyperparamters are: \\nrandom_walk_length={}, random_walk_restart_prob={}, num_random_walks={}, num_neighbors={}, num_layers={}, hidden_dims={}, lr={}, batch_size={}, epochs={}'.format(\n",
    "            optimal_params['random_walk_length'], \n",
    "            optimal_params['random_walk_restart_prob'], \n",
    "            optimal_params['num_random_walks'], \n",
    "            optimal_params['num_neighbors'], \n",
    "            optimal_params['num_layers'], \n",
    "            optimal_params['hidden_dims'], \n",
    "            optimal_params['lr'],\n",
    "            optimal_params['batch_size'],\n",
    "            optimal_params['epochs']\n",
    "        ), file=f\n",
    "    )\n",
    "\n",
    "    best_score = trials.trials[best_trial_idx]['result']['loss']\n",
    "    print('Best validation NDCG@10 = {}'.format(best_score), file=f)\n",
    "\n",
    "    # retrain the model\n",
    "    print('Retraining model using the optimal params...', file=f)\n",
    "    rec.train_model(**optimal_params, device='cpu', early_stopping=False, verbose=False)\n",
    "    print(f'Validation NDCG@10 of the retrained model = {rec.get_validation_ndcg()}', file=f)\n",
    "\n",
    "    # print test metrics\n",
    "    print('Test results of the retrained model:', file=f)\n",
    "    test_res = get_test_results(rec, TEST_K)\n",
    "    print(test_res,file=f)\n",
    "    \n",
    "    sheet_name = data\n",
    "    if use_no_feature:\n",
    "        sheet_name = data+'_nofeature'\n",
    "    elif use_text_feature:\n",
    "        if use_only_text:\n",
    "            sheet_name = data+'_onlytext'\n",
    "        else:\n",
    "            sheet_name = data\n",
    "    else:\n",
    "        sheet_name = data+'_notext'\n",
    "    \n",
    "    write_results_to_excel(test_res, test_res_path, sheet_name)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_pinsage('movielens_100k')\n",
    "# tune_pinsage('adobe_core5')\n",
    "# all feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters of PinSage Recommender on dataset adobe_core5...\n",
      "use_text_feature=False, use_no_feature=False\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengxuan_yan/opt/miniconda3/envs/torch/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 [11.5s]:  training loss=0.7161015868186951    \n",
      "epoch 2 [11.47s]:  training loss=0.9833914041519165   \n",
      "epoch 3 [11.48s]:  training loss=1.1394459009170532   \n",
      "epoch 4 [11.94s]:  training loss=1.20106041431427     \n",
      "epoch 5 [13.24s]: training loss=1.3142961263656616  validation ndcg@10=0.013442241983496864 [0.45s]\n",
      "epoch 6 [12.22s]:  training loss=1.3186707496643066   \n",
      "epoch 7 [12.91s]:  training loss=1.384917974472046    \n",
      "epoch 8 [14.2s]:  training loss=1.4378061294555664    \n",
      "epoch 9 [14.22s]:  training loss=1.4524741172790527   \n",
      "epoch 10 [13.2s]: training loss=1.443228840827942  validation ndcg@10=0.016084149841091747 [0.41s]\n",
      "epoch 11 [14.26s]:  training loss=1.5368672609329224  \n",
      "epoch 12 [13.68s]:  training loss=1.4835575819015503  \n",
      "epoch 13 [14.59s]:  training loss=1.564459204673767   \n",
      "epoch 14 [13.49s]:  training loss=1.5095664262771606  \n",
      "epoch 15 [14.15s]: training loss=1.5592113733291626  validation ndcg@10=0.017449398631320823 [0.43s]\n",
      "epoch 16 [13.31s]:  training loss=1.6797311305999756  \n",
      "epoch 17 [13.91s]:  training loss=1.642861247062683   \n",
      "epoch 18 [13.53s]:  training loss=1.6383171081542969  \n",
      "epoch 19 [14.17s]:  training loss=1.74345064163208    \n",
      "epoch 20 [13.57s]: training loss=1.7401371002197266  validation ndcg@10=0.01720157318801959 [0.48s]\n",
      "epoch 21 [14.09s]:  training loss=1.5751243829727173  \n",
      "epoch 22 [13.25s]:  training loss=1.7344584465026855  \n",
      "epoch 23 [14.98s]:  training loss=1.7835687398910522  \n",
      "epoch 24 [14.57s]:  training loss=1.8585127592086792  \n",
      "epoch 25 [14.68s]: training loss=1.9071625471115112  validation ndcg@10=0.016787305015570964 [0.6s]\n",
      "epoch 26 [13.77s]:  training loss=1.9135565757751465  \n",
      "epoch 27 [14.15s]:  training loss=1.949566125869751   \n",
      "epoch 28 [13.19s]:  training loss=1.8580843210220337  \n",
      "epoch 29 [14.3s]:  training loss=1.9544029235839844   \n",
      "epoch 30 [14.37s]: training loss=1.9590492248535156  validation ndcg@10=0.018065207829068654 [0.47s]\n",
      "epoch 31 [13.2s]:  training loss=1.8720999956130981   \n",
      "epoch 32 [12.51s]:  training loss=1.833270788192749   \n",
      "epoch 33 [12.43s]:  training loss=1.997279167175293   \n",
      "epoch 34 [13.0s]:  training loss=2.0415899753570557   \n",
      "epoch 35 [14.69s]: training loss=2.0157716274261475  validation ndcg@10=0.018192911653388707 [0.5s]\n",
      "epoch 36 [14.66s]:  training loss=1.9626140594482422  \n",
      "epoch 37 [13.69s]:  training loss=2.044870138168335   \n",
      "epoch 38 [13.03s]:  training loss=1.9526615142822266  \n",
      "epoch 39 [12.67s]:  training loss=1.9991146326065063  \n",
      "epoch 40 [13.46s]: training loss=2.0499560832977295  validation ndcg@10=0.01840614164270117 [0.44s]\n",
      "epoch 41 [14.31s]:  training loss=1.8771417140960693  \n",
      "epoch 42 [15.19s]:  training loss=2.0097157955169678  \n",
      "epoch 43 [14.25s]:  training loss=2.1477367877960205  \n",
      "epoch 44 [14.2s]:  training loss=2.036687135696411    \n",
      "epoch 45 [13.2s]: training loss=2.1723837852478027  validation ndcg@10=0.017332833216175104 [0.66s]\n",
      "epoch 46 [12.81s]:  training loss=2.074821710586548   \n",
      "epoch 47 [13.14s]:  training loss=2.2370593547821045  \n",
      "epoch 48 [12.55s]:  training loss=2.1383614540100098  \n",
      "epoch 49 [13.42s]:  training loss=2.0611352920532227  \n",
      "epoch 50 [12.47s]: training loss=2.1635873317718506  validation ndcg@10=0.020293073549508947 [0.4s]\n",
      "epoch 51 [12.98s]:  training loss=2.026913642883301   \n",
      "epoch 52 [12.73s]:  training loss=2.217416763305664   \n",
      "epoch 53 [13.02s]:  training loss=2.376368284225464   \n",
      "epoch 54 [13.4s]:  training loss=2.0896401405334473   \n",
      "epoch 55 [12.96s]: training loss=2.250746488571167  validation ndcg@10=0.019356346949141203 [0.42s]\n",
      "epoch 56 [12.85s]:  training loss=2.1733856201171875  \n",
      "epoch 57 [13.21s]:  training loss=2.2439687252044678  \n",
      "epoch 58 [13.48s]:  training loss=2.254121780395508   \n",
      "epoch 59 [13.39s]:  training loss=2.2284035682678223  \n",
      "epoch 60 [13.64s]: training loss=2.2224531173706055  validation ndcg@10=0.018362013779667286 [0.42s]\n",
      "epoch 61 [13.21s]:  training loss=2.330787420272827   \n",
      "epoch 62 [12.79s]:  training loss=2.1169772148132324  \n",
      "epoch 63 [13.9s]:  training loss=2.3423495292663574   \n",
      "epoch 64 [13.31s]:  training loss=2.394273042678833   \n",
      "epoch 65 [12.75s]: training loss=2.420058250427246  validation ndcg@10=0.018262598955341033 [0.44s]\n",
      "epoch 66 [12.5s]:  training loss=2.259843349456787    \n",
      "epoch 67 [13.31s]:  training loss=2.4064013957977295  \n",
      "epoch 68 [13.65s]:  training loss=2.3714470863342285  \n",
      "epoch 69 [13.4s]:  training loss=2.4295718669891357   \n",
      "epoch 70 [13.36s]: training loss=2.4151079654693604  validation ndcg@10=0.019407917440946748 [0.45s]\n",
      "epoch 71 [12.68s]:  training loss=2.6463963985443115  \n",
      "epoch 72 [13.58s]:  training loss=2.5054824352264404  \n",
      "epoch 73 [13.51s]:  training loss=2.2578940391540527  \n",
      "epoch 74 [12.86s]:  training loss=2.4943504333496094  \n",
      "epoch 75 [12.98s]: training loss=2.335242509841919  validation ndcg@10=0.019478661915733084 [0.43s]\n",
      "epoch 1 [11.82s]:  training loss=0.6153947114944458                                     \n",
      "epoch 2 [11.61s]:  training loss=0.8055941462516785                                     \n",
      "epoch 3 [12.84s]:  training loss=0.9176899194717407                                     \n",
      "epoch 4 [13.03s]:  training loss=0.9881778359413147                                     \n",
      "epoch 5 [12.66s]: training loss=1.027706503868103  validation ndcg@10=0.012951302665091587 [0.41s]\n",
      "epoch 6 [12.5s]:  training loss=1.0660661458969116                                      \n",
      "epoch 7 [12.11s]:  training loss=1.0931212902069092                                     \n",
      "epoch 8 [12.78s]:  training loss=1.1094601154327393                                     \n",
      "epoch 9 [12.73s]:  training loss=1.1548808813095093                                     \n",
      "epoch 10 [12.78s]: training loss=1.127193808555603  validation ndcg@10=0.016065256409081193 [0.49s]\n",
      "epoch 11 [12.23s]:  training loss=1.18718421459198                                      \n",
      "epoch 12 [12.08s]:  training loss=1.1400624513626099                                    \n",
      "epoch 13 [12.64s]:  training loss=1.1827987432479858                                    \n",
      "epoch 14 [12.76s]:  training loss=1.2285692691802979                                    \n",
      "epoch 15 [13.17s]: training loss=1.2186788320541382  validation ndcg@10=0.013875569288057693 [0.65s]\n",
      "epoch 16 [12.75s]:  training loss=1.2413302659988403                                    \n",
      "epoch 17 [12.19s]:  training loss=1.2504802942276                                       \n",
      "epoch 18 [12.3s]:  training loss=1.2564425468444824                                     \n",
      "epoch 19 [13.42s]:  training loss=1.2857298851013184                                    \n",
      "epoch 20 [13.71s]: training loss=1.279944658279419  validation ndcg@10=0.017406254988628005 [0.45s]\n",
      "epoch 21 [12.57s]:  training loss=1.2942615747451782                                    \n",
      "epoch 22 [12.0s]:  training loss=1.3345369100570679                                     \n",
      "epoch 23 [11.79s]:  training loss=1.370561122894287                                     \n",
      "epoch 24 [12.8s]:  training loss=1.3500103950500488                                     \n",
      "epoch 25 [14.02s]: training loss=1.4005457162857056  validation ndcg@10=0.015201127392629275 [0.5s]\n",
      "epoch 26 [12.25s]:  training loss=1.3806242942810059                                    \n",
      "epoch 27 [12.75s]:  training loss=1.4662611484527588                                    \n",
      "epoch 28 [11.78s]:  training loss=1.4894235134124756                                    \n",
      "epoch 29 [12.52s]:  training loss=1.4136309623718262                                    \n",
      "epoch 30 [13.06s]: training loss=1.4086427688598633  validation ndcg@10=0.014385528008532607 [0.48s]\n",
      "epoch 31 [13.66s]:  training loss=1.4536746740341187                                    \n",
      "epoch 32 [13.04s]:  training loss=1.350521206855774                                     \n",
      "epoch 33 [12.37s]:  training loss=1.4479049444198608                                    \n",
      "epoch 34 [12.06s]:  training loss=1.518096923828125                                     \n",
      "epoch 35 [12.89s]: training loss=1.5220792293548584  validation ndcg@10=0.015335151333833239 [0.49s]\n",
      "epoch 36 [14.56s]:  training loss=1.518477439880371                                     \n",
      "epoch 37 [12.51s]:  training loss=1.5105189085006714                                    \n",
      "epoch 38 [12.1s]:  training loss=1.5727115869522095                                     \n",
      "epoch 39 [12.81s]:  training loss=1.5421422719955444                                    \n",
      "epoch 40 [13.54s]: training loss=1.5272797346115112  validation ndcg@10=0.018701125251271055 [0.48s]\n",
      "epoch 41 [16.86s]:  training loss=1.5647234916687012                                    \n",
      "epoch 42 [13.86s]:  training loss=1.5952935218811035                                    \n",
      "epoch 43 [12.43s]:  training loss=1.5896369218826294                                    \n",
      "epoch 44 [12.57s]:  training loss=1.6336075067520142                                    \n",
      "epoch 45 [13.44s]: training loss=1.4829002618789673  validation ndcg@10=0.01601895454468705 [0.47s]\n",
      "epoch 46 [14.47s]:  training loss=1.486633062362671                                     \n",
      "epoch 47 [14.46s]:  training loss=1.5414440631866455                                    \n",
      "epoch 48 [13.03s]:  training loss=1.6851413249969482                                    \n",
      "epoch 49 [12.52s]:  training loss=1.630671739578247                                     \n",
      "epoch 50 [14.38s]: training loss=1.6909277439117432  validation ndcg@10=0.016773451769000143 [0.4s]\n",
      "epoch 51 [14.8s]:  training loss=1.7206456661224365                                     \n",
      "epoch 52 [15.95s]:  training loss=1.6945031881332397                                    \n",
      "epoch 53 [13.13s]:  training loss=1.7375506162643433                                    \n",
      "epoch 54 [12.5s]:  training loss=1.7037957906723022                                     \n",
      "epoch 55 [13.13s]: training loss=1.7173696756362915  validation ndcg@10=0.016923346428891622 [0.4s]\n",
      "epoch 56 [13.71s]:  training loss=1.6639446020126343                                    \n",
      "epoch 57 [17.03s]:  training loss=1.7300162315368652                                    \n",
      "epoch 58 [13.39s]:  training loss=1.7865742444992065                                    \n",
      "epoch 59 [12.39s]:  training loss=1.6563711166381836                                    \n",
      "epoch 60 [12.45s]: training loss=1.68529212474823  validation ndcg@10=0.019048035643446856 [0.44s]\n",
      "epoch 61 [13.45s]:  training loss=1.772749662399292                                     \n",
      "epoch 62 [17.16s]:  training loss=1.675352931022644                                     \n",
      "epoch 63 [14.17s]:  training loss=1.6333249807357788                                    \n",
      "epoch 64 [13.79s]:  training loss=1.7998859882354736                                    \n",
      "epoch 65 [12.5s]: training loss=1.8681612014770508  validation ndcg@10=0.01892620177183329 [0.41s]\n",
      "epoch 66 [13.32s]:  training loss=1.7761743068695068                                    \n",
      "epoch 67 [16.56s]:  training loss=1.994815468788147                                     \n",
      "epoch 68 [16.3s]:  training loss=1.879856824874878                                      \n",
      "epoch 69 [12.94s]:  training loss=1.8647044897079468                                    \n",
      "epoch 70 [11.98s]: training loss=1.7593042850494385  validation ndcg@10=0.018030955780917797 [0.44s]\n",
      "epoch 71 [13.13s]:  training loss=1.7160779237747192                                    \n",
      "epoch 72 [17.11s]:  training loss=1.735276460647583                                     \n",
      "epoch 73 [16.84s]:  training loss=1.8771625757217407                                    \n",
      "epoch 74 [12.91s]:  training loss=1.7408589124679565                                    \n",
      "epoch 75 [12.82s]: training loss=1.9329863786697388  validation ndcg@10=0.019853319519806577 [0.46s]\n",
      "epoch 76 [13.04s]:  training loss=1.8029097318649292                                    \n",
      "epoch 77 [16.26s]:  training loss=1.8413283824920654                                    \n",
      "epoch 78 [17.33s]:  training loss=1.9343781471252441                                    \n",
      "epoch 79 [13.25s]:  training loss=2.0211076736450195                                    \n",
      "epoch 80 [12.74s]: training loss=1.85834538936615  validation ndcg@10=0.01772406658625709 [0.41s]\n",
      "epoch 81 [12.23s]:  training loss=1.8070690631866455                                    \n",
      "epoch 82 [13.83s]:  training loss=1.919374942779541                                     \n",
      "epoch 83 [20.02s]:  training loss=2.0087013244628906                                    \n",
      "epoch 84 [13.17s]:  training loss=1.938165307044983                                     \n",
      "epoch 85 [12.22s]: training loss=1.9856045246124268  validation ndcg@10=0.018934550166876306 [0.45s]\n",
      "epoch 86 [12.2s]:  training loss=1.884922742843628                                      \n",
      "epoch 87 [13.86s]:  training loss=2.0864546298980713                                    \n",
      "epoch 88 [21.0s]:  training loss=1.8676916360855103                                     \n",
      "epoch 89 [13.22s]:  training loss=1.8262584209442139                                    \n",
      "epoch 90 [12.45s]: training loss=1.8835963010787964  validation ndcg@10=0.019557133737600017 [0.43s]\n",
      "epoch 91 [12.57s]:  training loss=2.0113089084625244                                    \n",
      "epoch 92 [13.12s]:  training loss=1.9471060037612915                                    \n",
      "epoch 93 [19.51s]:  training loss=1.9399856328964233                                    \n",
      "epoch 94 [14.47s]:  training loss=1.9161001443862915                                    \n",
      "epoch 95 [12.24s]: training loss=2.0011537075042725  validation ndcg@10=0.01913670153971023 [0.43s]\n",
      "epoch 96 [12.03s]:  training loss=1.9912055730819702                                    \n",
      "epoch 97 [13.0s]:  training loss=1.9379419088363647                                     \n",
      "epoch 98 [18.91s]:  training loss=1.9057422876358032                                    \n",
      "epoch 99 [14.54s]:  training loss=1.975932240486145                                     \n",
      "epoch 100 [12.35s]: training loss=1.870057225227356  validation ndcg@10=0.018803845030088945 [0.41s]\n",
      "epoch 1 [20.73s]:  training loss=0.3400188684463501                                     \n",
      "epoch 2 [28.27s]:  training loss=0.21340949833393097                                    \n",
      "epoch 3 [21.61s]:  training loss=0.17387188971042633                                    \n",
      "epoch 4 [20.54s]:  training loss=0.15480147302150726                                    \n",
      "epoch 5 [23.06s]: training loss=0.13666819036006927  validation ndcg@10=0.021853903906574358 [0.69s]\n",
      "epoch 6 [26.81s]:  training loss=0.12300790846347809                                    \n",
      "epoch 7 [20.72s]:  training loss=0.11319077014923096                                    \n",
      "epoch 8 [21.74s]:  training loss=0.10791248828172684                                    \n",
      "epoch 9 [27.63s]:  training loss=0.10023533552885056                                    \n",
      "epoch 10 [20.94s]: training loss=0.09653403609991074  validation ndcg@10=0.02287810246437465 [0.6s]\n",
      "epoch 11 [21.58s]:  training loss=0.09103630483150482                                   \n",
      "epoch 12 [28.18s]:  training loss=0.08627954870462418                                   \n",
      "epoch 13 [21.34s]:  training loss=0.08780758827924728                                   \n",
      "epoch 14 [21.2s]:  training loss=0.08263072371482849                                    \n",
      "epoch 15 [26.8s]: training loss=0.07864823937416077  validation ndcg@10=0.023951510101084875 [0.87s]\n",
      "epoch 16 [24.49s]:  training loss=0.0759110376238823                                    \n",
      "epoch 17 [21.32s]:  training loss=0.07252354919910431                                   \n",
      "epoch 18 [24.71s]:  training loss=0.07306426018476486                                   \n",
      "epoch 19 [22.44s]:  training loss=0.06980888545513153                                   \n",
      "epoch 20 [21.67s]: training loss=0.06828594952821732  validation ndcg@10=0.02296413654922502 [0.5s]\n",
      "epoch 21 [22.77s]:  training loss=0.06895683705806732                                   \n",
      "epoch 22 [25.62s]:  training loss=0.06438329815864563                                   \n",
      "epoch 23 [20.97s]:  training loss=0.06531742960214615                                   \n",
      "epoch 24 [22.01s]:  training loss=0.06404449790716171                                   \n",
      "epoch 25 [26.16s]: training loss=0.062101833522319794  validation ndcg@10=0.02239203417016642 [0.53s]\n",
      "epoch 26 [22.27s]:  training loss=0.05982128903269768                                   \n",
      "epoch 27 [22.0s]:  training loss=0.05963810160756111                                    \n",
      "epoch 28 [26.72s]:  training loss=0.057727452367544174                                  \n",
      "epoch 29 [22.43s]:  training loss=0.06333015114068985                                   \n",
      "epoch 30 [22.01s]: training loss=0.05795353278517723  validation ndcg@10=0.021554443316585063 [0.54s]\n",
      "epoch 31 [26.28s]:  training loss=0.05520901829004288                                   \n",
      "epoch 32 [21.93s]:  training loss=0.058275751769542694                                  \n",
      "epoch 33 [21.5s]:  training loss=0.054636020213365555                                   \n",
      "epoch 34 [24.79s]:  training loss=0.05694394186139107                                   \n",
      "epoch 35 [23.84s]: training loss=0.05397813022136688  validation ndcg@10=0.02195924831711594 [0.57s]\n",
      "epoch 36 [21.07s]:  training loss=0.05406999960541725                                   \n",
      "epoch 37 [21.79s]:  training loss=0.05240495875477791                                   \n",
      "epoch 38 [21.06s]:  training loss=0.05291324108839035                                   \n",
      "epoch 39 [20.72s]:  training loss=0.05214866250753403                                   \n",
      "epoch 40 [21.65s]: training loss=0.050457846373319626  validation ndcg@10=0.020742938795462615 [0.58s]\n",
      "epoch 1 [11.62s]:  training loss=0.6604458093643188                                     \n",
      "epoch 2 [12.31s]:  training loss=0.6494266390800476                                     \n",
      "epoch 3 [11.91s]:  training loss=0.6372486352920532                                     \n",
      "epoch 4 [11.98s]:  training loss=0.6261722445487976                                     \n",
      "epoch 5 [12.0s]: training loss=0.6191070675849915  validation ndcg@10=0.003696093726747205 [0.49s]\n",
      "epoch 6 [12.8s]:  training loss=0.6088252663612366                                      \n",
      "epoch 7 [12.35s]:  training loss=0.5973889231681824                                     \n",
      "epoch 8 [11.5s]:  training loss=0.5881181955337524                                      \n",
      "epoch 9 [11.42s]:  training loss=0.5826239585876465                                     \n",
      "epoch 10 [12.44s]: training loss=0.5762854814529419  validation ndcg@10=0.004130719927824337 [0.5s]\n",
      "epoch 11 [11.47s]:  training loss=0.569550096988678                                     \n",
      "epoch 12 [12.61s]:  training loss=0.5565029382705688                                    \n",
      "epoch 13 [12.16s]:  training loss=0.5484189987182617                                    \n",
      "epoch 14 [11.56s]:  training loss=0.5464573502540588                                    \n",
      "epoch 15 [11.84s]: training loss=0.533189594745636  validation ndcg@10=0.004375668512116585 [0.45s]\n",
      "epoch 16 [12.31s]:  training loss=0.5251060128211975                                    \n",
      "epoch 17 [12.38s]:  training loss=0.5234593152999878                                    \n",
      "epoch 18 [12.15s]:  training loss=0.5112025737762451                                    \n",
      "epoch 19 [12.24s]:  training loss=0.5056943893432617                                    \n",
      "epoch 20 [11.6s]: training loss=0.49583494663238525  validation ndcg@10=0.0043746930515764105 [0.48s]\n",
      "epoch 21 [11.19s]:  training loss=0.4923456311225891                                    \n",
      "epoch 22 [12.06s]:  training loss=0.4870973527431488                                    \n",
      "epoch 23 [12.76s]:  training loss=0.4814469516277313                                    \n",
      "epoch 24 [12.11s]:  training loss=0.4751584827899933                                    \n",
      "epoch 25 [12.1s]: training loss=0.4679090082645416  validation ndcg@10=0.004583035782487441 [0.57s]\n",
      "epoch 26 [12.02s]:  training loss=0.4647689759731293                                      \n",
      "epoch 27 [11.75s]:  training loss=0.45720019936561584                                     \n",
      "epoch 28 [11.9s]:  training loss=0.451962947845459                                        \n",
      "epoch 29 [11.69s]:  training loss=0.44825029373168945                                     \n",
      "epoch 30 [12.57s]: training loss=0.4390774369239807  validation ndcg@10=0.004964914370704864 [0.55s]\n",
      "epoch 31 [11.95s]:  training loss=0.4417296051979065                                      \n",
      "epoch 32 [11.38s]:  training loss=0.4310931861400604                                      \n",
      "epoch 33 [11.24s]:  training loss=0.4291893243789673                                      \n",
      "epoch 34 [11.47s]:  training loss=0.42987293004989624                                     \n",
      "epoch 35 [11.45s]: training loss=0.4184471070766449  validation ndcg@10=0.005724102930510823 [0.48s]\n",
      "epoch 36 [12.61s]:  training loss=0.4140641987323761                                      \n",
      "epoch 37 [12.02s]:  training loss=0.415622740983963                                       \n",
      "epoch 38 [11.48s]:  training loss=0.40810978412628174                                     \n",
      "epoch 39 [11.91s]:  training loss=0.3991641104221344                                      \n",
      "epoch 40 [12.09s]: training loss=0.3964451551437378  validation ndcg@10=0.006228772522183394 [0.49s]\n",
      "epoch 41 [12.26s]:  training loss=0.3929925858974457                                      \n",
      "epoch 42 [12.27s]:  training loss=0.3873007893562317                                      \n",
      "epoch 43 [12.3s]:  training loss=0.3870459496974945                                       \n",
      "epoch 44 [12.42s]:  training loss=0.3765181601047516                                      \n",
      "epoch 45 [12.21s]: training loss=0.377704381942749  validation ndcg@10=0.007349601569368154 [0.51s]\n",
      "epoch 46 [12.05s]:  training loss=0.37218430638313293                                     \n",
      "epoch 47 [12.83s]:  training loss=0.3707725703716278                                      \n",
      "epoch 48 [11.68s]:  training loss=0.3666391372680664                                      \n",
      "epoch 49 [11.77s]:  training loss=0.36101046204566956                                     \n",
      "epoch 50 [10.96s]: training loss=0.3570067286491394  validation ndcg@10=0.008571981603029818 [0.47s]\n",
      "epoch 51 [11.96s]:  training loss=0.3545721471309662                                      \n",
      "epoch 52 [12.69s]:  training loss=0.3549162447452545                                      \n",
      "epoch 53 [12.47s]:  training loss=0.34812644124031067                                     \n",
      "epoch 54 [12.58s]:  training loss=0.34291085600852966                                     \n",
      "epoch 55 [11.59s]: training loss=0.34417015314102173  validation ndcg@10=0.010118530439276555 [0.62s]\n",
      "epoch 56 [11.71s]:  training loss=0.34199079871177673                                     \n",
      "epoch 57 [12.86s]:  training loss=0.3391589820384979                                      \n",
      "epoch 58 [13.12s]:  training loss=0.33943790197372437                                     \n",
      "epoch 59 [13.06s]:  training loss=0.3323686420917511                                      \n",
      "epoch 60 [11.96s]: training loss=0.33126387000083923  validation ndcg@10=0.012095484815741415 [0.5s]\n",
      "epoch 61 [11.55s]:  training loss=0.3279047906398773                                      \n",
      "epoch 62 [11.56s]:  training loss=0.3213815987110138                                      \n",
      "epoch 63 [12.31s]:  training loss=0.3228967785835266                                      \n",
      "epoch 64 [13.32s]:  training loss=0.3157839775085449                                      \n",
      "epoch 65 [12.47s]: training loss=0.3179199695587158  validation ndcg@10=0.013697496174538202 [0.68s]\n",
      "epoch 66 [11.89s]:  training loss=0.3149505853652954                                      \n",
      "epoch 67 [11.56s]:  training loss=0.3108791410923004                                      \n",
      "epoch 68 [12.49s]:  training loss=0.31372493505477905                                     \n",
      "epoch 69 [12.17s]:  training loss=0.3089572787284851                                      \n",
      "epoch 70 [13.08s]: training loss=0.3057788014411926  validation ndcg@10=0.01512044133553886 [0.51s]\n",
      "epoch 71 [12.52s]:  training loss=0.30450648069381714                                     \n",
      "epoch 72 [12.21s]:  training loss=0.29770442843437195                                     \n",
      "epoch 73 [11.92s]:  training loss=0.2934691607952118                                      \n",
      "epoch 74 [12.8s]:  training loss=0.2951858937740326                                       \n",
      "epoch 75 [13.08s]: training loss=0.29595717787742615  validation ndcg@10=0.015613820952742073 [0.54s]\n",
      "epoch 76 [12.94s]:  training loss=0.29357412457466125                                     \n",
      "epoch 77 [12.22s]:  training loss=0.2913861572742462                                      \n",
      "epoch 78 [12.14s]:  training loss=0.2899782657623291                                      \n",
      "epoch 79 [12.16s]:  training loss=0.2835329473018646                                      \n",
      "epoch 80 [12.67s]: training loss=0.283372700214386  validation ndcg@10=0.016793699167841993 [0.53s]\n",
      "epoch 81 [13.59s]:  training loss=0.2796113193035126                                      \n",
      "epoch 82 [11.61s]:  training loss=0.2841077446937561                                      \n",
      "epoch 83 [12.21s]:  training loss=0.2809860408306122                                      \n",
      "epoch 84 [11.72s]:  training loss=0.2779676616191864                                      \n",
      "epoch 85 [11.39s]: training loss=0.2772858440876007  validation ndcg@10=0.017530778623309713 [0.6s]\n",
      "epoch 86 [11.84s]:  training loss=0.27790457010269165                                     \n",
      "epoch 87 [13.83s]:  training loss=0.2748577296733856                                      \n",
      "epoch 88 [12.65s]:  training loss=0.27598807215690613                                     \n",
      "epoch 89 [12.07s]:  training loss=0.2752101719379425                                      \n",
      "epoch 90 [12.29s]: training loss=0.2729627788066864  validation ndcg@10=0.01820012116546602 [0.56s]\n",
      "epoch 91 [12.54s]:  training loss=0.2709551751613617                                      \n",
      "epoch 92 [12.19s]:  training loss=0.2697976231575012                                      \n",
      "epoch 93 [13.91s]:  training loss=0.26681169867515564                                     \n",
      "epoch 94 [12.06s]:  training loss=0.26453137397766113                                     \n",
      "epoch 95 [11.88s]: training loss=0.26605841517448425  validation ndcg@10=0.018833646879380557 [0.5s]\n",
      "epoch 96 [10.93s]:  training loss=0.26404985785484314                                     \n",
      "epoch 97 [12.43s]:  training loss=0.26018550992012024                                     \n",
      "epoch 98 [13.73s]:  training loss=0.2596763074398041                                      \n",
      "epoch 99 [13.35s]:  training loss=0.25815141201019287                                     \n",
      "epoch 100 [12.34s]: training loss=0.25716447830200195  validation ndcg@10=0.01921409591729592 [0.47s]\n",
      "epoch 101 [11.52s]:  training loss=0.2586909532546997                                     \n",
      "epoch 102 [11.9s]:  training loss=0.25199583172798157                                     \n",
      "epoch 103 [12.2s]:  training loss=0.2556230425834656                                      \n",
      "epoch 104 [14.16s]:  training loss=0.2514694631099701                                     \n",
      "epoch 105 [12.14s]: training loss=0.24983149766921997  validation ndcg@10=0.019534112993962873 [0.49s]\n",
      "epoch 106 [11.39s]:  training loss=0.2530781030654907                                     \n",
      "epoch 107 [11.19s]:  training loss=0.25196191668510437                                    \n",
      "epoch 108 [12.15s]:  training loss=0.24940940737724304                                    \n",
      "epoch 109 [13.6s]:  training loss=0.2460300624370575                                      \n",
      "epoch 110 [13.52s]: training loss=0.24883756041526794  validation ndcg@10=0.019755765898772308 [0.48s]\n",
      "epoch 111 [12.66s]:  training loss=0.2487843781709671                                     \n",
      "epoch 112 [11.64s]:  training loss=0.247679203748703                                      \n",
      "epoch 113 [11.62s]:  training loss=0.24374589323997498                                    \n",
      "epoch 114 [12.87s]:  training loss=0.2446051985025406                                     \n",
      "epoch 115 [14.35s]: training loss=0.24574220180511475  validation ndcg@10=0.01971057868257416 [0.64s]\n",
      "epoch 116 [13.93s]:  training loss=0.24571655690670013                                    \n",
      "epoch 117 [12.21s]:  training loss=0.24802283942699432                                    \n",
      "epoch 118 [11.88s]:  training loss=0.24022497236728668                                    \n",
      "epoch 119 [12.29s]:  training loss=0.23967520892620087                                    \n",
      "epoch 120 [12.81s]: training loss=0.23864416778087616  validation ndcg@10=0.019895380580698407 [0.53s]\n",
      "epoch 121 [15.34s]:  training loss=0.23804235458374023                                    \n",
      "epoch 122 [11.94s]:  training loss=0.24059167504310608                                    \n",
      "epoch 123 [11.76s]:  training loss=0.24116119742393494                                    \n",
      "epoch 124 [11.6s]:  training loss=0.23947498202323914                                     \n",
      "epoch 125 [14.98s]: training loss=0.23410052061080933  validation ndcg@10=0.019802386503775315 [0.47s]\n",
      "epoch 126 [13.13s]:  training loss=0.23859642446041107                                    \n",
      "epoch 127 [11.49s]:  training loss=0.23738819360733032                                    \n",
      "epoch 128 [11.51s]:  training loss=0.23295052349567413                                    \n",
      "epoch 129 [11.46s]:  training loss=0.23517391085624695                                    \n",
      "epoch 130 [11.37s]: training loss=0.23271264135837555  validation ndcg@10=0.020088534472851822 [0.49s]\n",
      "epoch 131 [12.29s]:  training loss=0.23063914477825165                                    \n",
      "epoch 132 [13.72s]:  training loss=0.2322484254837036                                     \n",
      "epoch 133 [12.59s]:  training loss=0.22858363389968872                                    \n",
      "epoch 134 [11.43s]:  training loss=0.23048335313796997                                    \n",
      "epoch 135 [11.47s]: training loss=0.23079894483089447  validation ndcg@10=0.020231688530723383 [0.51s]\n",
      "epoch 136 [12.65s]:  training loss=0.23026295006275177                                    \n",
      "epoch 137 [12.59s]:  training loss=0.228000670671463                                      \n",
      "epoch 138 [13.62s]:  training loss=0.22541819512844086                                    \n",
      "epoch 139 [11.91s]:  training loss=0.22995053231716156                                    \n",
      "epoch 140 [11.8s]: training loss=0.22463414072990417  validation ndcg@10=0.020108573192713454 [0.53s]\n",
      "epoch 141 [11.25s]:  training loss=0.22826534509658813                                    \n",
      "epoch 142 [11.56s]:  training loss=0.22763314843177795                                    \n",
      "epoch 143 [13.72s]:  training loss=0.22895105183124542                                    \n",
      "epoch 144 [13.05s]:  training loss=0.22089794278144836                                    \n",
      "epoch 145 [11.69s]: training loss=0.22299134731292725  validation ndcg@10=0.020363166441225246 [0.51s]\n",
      "epoch 146 [11.27s]:  training loss=0.22449012100696564                                    \n",
      "epoch 147 [11.42s]:  training loss=0.22280555963516235                                    \n",
      "epoch 148 [12.52s]:  training loss=0.22311195731163025                                    \n",
      "epoch 149 [14.15s]:  training loss=0.22410809993743896                                    \n",
      "epoch 150 [12.36s]: training loss=0.21874435245990753  validation ndcg@10=0.02033556640188685 [0.51s]\n",
      "epoch 151 [11.91s]:  training loss=0.22180016338825226                                    \n",
      "epoch 152 [11.64s]:  training loss=0.2217382788658142                                     \n",
      "epoch 153 [11.97s]:  training loss=0.21607254445552826                                    \n",
      "epoch 154 [13.36s]:  training loss=0.21260994672775269                                    \n",
      "epoch 155 [12.89s]: training loss=0.2160448580980301  validation ndcg@10=0.02039786983070287 [0.45s]\n",
      "epoch 156 [11.91s]:  training loss=0.22118082642555237                                    \n",
      "epoch 157 [11.22s]:  training loss=0.21503907442092896                                    \n",
      "epoch 158 [11.57s]:  training loss=0.21923959255218506                                    \n",
      "epoch 159 [12.81s]:  training loss=0.21751466393470764                                    \n",
      "epoch 160 [14.65s]: training loss=0.21618323028087616  validation ndcg@10=0.02029951898433854 [0.5s]\n",
      "epoch 161 [11.84s]:  training loss=0.2176963984966278                                     \n",
      "epoch 162 [11.46s]:  training loss=0.2151319682598114                                     \n",
      "epoch 163 [11.23s]:  training loss=0.21091851592063904                                    \n",
      "epoch 164 [12.19s]:  training loss=0.21549983322620392                                    \n",
      "epoch 165 [13.12s]: training loss=0.21352970600128174  validation ndcg@10=0.020297530217069003 [0.54s]\n",
      "epoch 166 [13.71s]:  training loss=0.21434445679187775                                    \n",
      "epoch 167 [12.3s]:  training loss=0.21184800565242767                                     \n",
      "epoch 168 [11.15s]:  training loss=0.208560511469841                                      \n",
      "epoch 169 [11.81s]:  training loss=0.20785465836524963                                    \n",
      "epoch 170 [11.66s]: training loss=0.20992368459701538  validation ndcg@10=0.020417956841459237 [0.49s]\n",
      "epoch 171 [13.84s]:  training loss=0.2133145034313202                                     \n",
      "epoch 172 [13.56s]:  training loss=0.21097052097320557                                    \n",
      "epoch 173 [12.05s]:  training loss=0.21025149524211884                                    \n",
      "epoch 174 [11.93s]:  training loss=0.20725123584270477                                    \n",
      "epoch 175 [11.65s]: training loss=0.20985157787799835  validation ndcg@10=0.02028169544070056 [0.5s]\n",
      "epoch 176 [12.51s]:  training loss=0.20941607654094696                                    \n",
      "epoch 177 [14.8s]:  training loss=0.20842929184436798                                     \n",
      "epoch 178 [12.42s]:  training loss=0.2103262096643448                                     \n",
      "epoch 179 [11.64s]:  training loss=0.20634996891021729                                    \n",
      "epoch 180 [11.42s]: training loss=0.2039966583251953  validation ndcg@10=0.020651843380390493 [0.45s]\n",
      "epoch 181 [11.67s]:  training loss=0.20815402269363403                                    \n",
      "epoch 182 [13.51s]:  training loss=0.2074710577726364                                     \n",
      "epoch 183 [13.39s]:  training loss=0.20525339245796204                                    \n",
      "epoch 184 [11.96s]:  training loss=0.20138955116271973                                    \n",
      "epoch 185 [11.44s]: training loss=0.2085411250591278  validation ndcg@10=0.020674433833350162 [0.45s]\n",
      "epoch 186 [11.64s]:  training loss=0.2017938792705536                                     \n",
      "epoch 187 [12.21s]:  training loss=0.20108100771903992                                    \n",
      "epoch 188 [18.51s]:  training loss=0.2051997035741806                                     \n",
      "epoch 189 [12.95s]:  training loss=0.20232494175434113                                    \n",
      "epoch 190 [11.51s]: training loss=0.20289941132068634  validation ndcg@10=0.020591247820928985 [0.47s]\n",
      "epoch 191 [11.49s]:  training loss=0.2033577263355255                                     \n",
      "epoch 192 [11.93s]:  training loss=0.2065381407737732                                     \n",
      "epoch 193 [14.03s]:  training loss=0.2026168704032898                                     \n",
      "epoch 194 [14.09s]:  training loss=0.20179767906665802                                    \n",
      "epoch 195 [11.99s]: training loss=0.20114916563034058  validation ndcg@10=0.020536822246439816 [0.47s]\n",
      "epoch 196 [11.32s]:  training loss=0.20043089985847473                                    \n",
      "epoch 197 [11.44s]:  training loss=0.20331771671772003                                    \n",
      "epoch 198 [11.66s]:  training loss=0.20219004154205322                                    \n",
      "epoch 199 [14.02s]:  training loss=0.1995071917772293                                     \n",
      "epoch 200 [13.73s]: training loss=0.20185434818267822  validation ndcg@10=0.02057195039450488 [0.46s]\n",
      "epoch 1 [37.78s]:  training loss=1.6220693588256836                                       \n",
      "epoch 2 [42.77s]:  training loss=2.5007128715515137                                       \n",
      "epoch 3 [35.24s]:  training loss=2.897691249847412                                        \n",
      "epoch 4 [42.73s]:  training loss=3.000220775604248                                        \n",
      "epoch 5 [37.76s]: training loss=3.059697389602661  validation ndcg@10=0.0141899626443279 [1.03s]\n",
      "epoch 6 [40.95s]:  training loss=3.188110828399658                                        \n",
      "epoch 7 [41.81s]:  training loss=3.3663814067840576                                       \n",
      "epoch 8 [38.07s]:  training loss=3.3842179775238037                                       \n",
      "epoch 9 [41.67s]:  training loss=3.4101741313934326                                       \n",
      "epoch 10 [36.89s]: training loss=3.3881499767303467  validation ndcg@10=0.013546455006372641 [0.72s]\n",
      "epoch 11 [42.88s]:  training loss=3.821430206298828                                       \n",
      "epoch 12 [35.76s]:  training loss=3.6269543170928955                                      \n",
      "epoch 13 [42.18s]:  training loss=3.7310471534729004                                      \n",
      "epoch 14 [36.17s]:  training loss=3.7683839797973633                                      \n",
      "epoch 15 [43.74s]: training loss=3.7789626121520996  validation ndcg@10=0.014205225565166695 [0.84s]\n",
      "epoch 16 [43.59s]:  training loss=3.9068174362182617                                      \n",
      "epoch 17 [39.86s]:  training loss=4.1361517906188965                                      \n",
      "epoch 18 [35.67s]:  training loss=3.9918692111968994                                      \n",
      "epoch 19 [36.11s]:  training loss=3.9462931156158447                                      \n",
      "epoch 20 [36.42s]: training loss=4.164834022521973  validation ndcg@10=0.015342459384012122 [0.8s]\n",
      "epoch 21 [35.56s]:  training loss=4.11683464050293                                        \n",
      "epoch 22 [35.62s]:  training loss=4.126034736633301                                       \n",
      "epoch 23 [35.71s]:  training loss=4.337789058685303                                       \n",
      "epoch 24 [35.89s]:  training loss=4.129983425140381                                       \n",
      "epoch 25 [35.96s]: training loss=4.33807373046875  validation ndcg@10=0.01690403822396046 [0.83s]\n",
      "epoch 26 [35.59s]:  training loss=4.187000751495361                                       \n",
      "epoch 27 [35.0s]:  training loss=4.424777507781982                                        \n",
      "epoch 28 [35.58s]:  training loss=4.216601371765137                                       \n",
      "epoch 29 [36.36s]:  training loss=4.453933238983154                                       \n",
      "epoch 30 [36.6s]: training loss=4.583563327789307  validation ndcg@10=0.015303923922331578 [0.86s]\n",
      "epoch 31 [35.82s]:  training loss=4.566664695739746                                       \n",
      "epoch 32 [35.74s]:  training loss=4.68028450012207                                        \n",
      "epoch 33 [35.6s]:  training loss=4.437432765960693                                        \n",
      "epoch 34 [35.29s]:  training loss=4.851397514343262                                       \n",
      "epoch 35 [36.74s]: training loss=4.571665287017822  validation ndcg@10=0.017076534989259196 [0.81s]\n",
      "epoch 36 [35.8s]:  training loss=4.5700764656066895                                       \n",
      "epoch 37 [36.09s]:  training loss=4.559050559997559                                       \n",
      "epoch 38 [36.5s]:  training loss=4.815279006958008                                        \n",
      "epoch 39 [38.98s]:  training loss=4.755216598510742                                       \n",
      "epoch 40 [35.53s]: training loss=4.876521587371826  validation ndcg@10=0.017289869263316417 [0.76s]\n",
      "epoch 41 [36.6s]:  training loss=4.974861145019531                                        \n",
      "epoch 42 [36.48s]:  training loss=4.862961292266846                                       \n",
      "epoch 43 [35.31s]:  training loss=4.8942461013793945                                      \n",
      "epoch 44 [36.53s]:  training loss=4.805948257446289                                       \n",
      "epoch 45 [37.36s]: training loss=5.144379138946533  validation ndcg@10=0.01759253482102995 [0.78s]\n",
      "epoch 46 [35.9s]:  training loss=5.07899808883667                                         \n",
      "epoch 47 [36.95s]:  training loss=5.280249118804932                                       \n",
      "epoch 48 [36.84s]:  training loss=4.983521461486816                                       \n",
      "epoch 49 [35.89s]:  training loss=5.379581928253174                                       \n",
      "epoch 50 [36.93s]: training loss=5.162219524383545  validation ndcg@10=0.017475543389867024 [0.75s]\n",
      "epoch 51 [36.4s]:  training loss=5.403568744659424                                        \n",
      "epoch 52 [36.53s]:  training loss=5.110655307769775                                       \n",
      "epoch 53 [36.42s]:  training loss=4.948422431945801                                       \n",
      "epoch 54 [36.84s]:  training loss=5.955204010009766                                       \n",
      "epoch 55 [36.49s]: training loss=5.372708797454834  validation ndcg@10=0.016943640664612687 [0.74s]\n",
      "epoch 56 [35.57s]:  training loss=5.366703987121582                                       \n",
      "epoch 57 [37.51s]:  training loss=5.676115036010742                                       \n",
      "epoch 58 [36.92s]:  training loss=5.7136616706848145                                      \n",
      "epoch 59 [36.51s]:  training loss=5.593278884887695                                       \n",
      "epoch 60 [38.79s]: training loss=5.365159511566162  validation ndcg@10=0.015828672342125285 [0.8s]\n",
      "epoch 61 [39.08s]:  training loss=5.473544597625732                                       \n",
      "epoch 62 [35.35s]:  training loss=5.956569194793701                                       \n",
      "epoch 63 [36.88s]:  training loss=5.409614086151123                                       \n",
      "epoch 64 [35.8s]:  training loss=5.721166133880615                                        \n",
      "epoch 65 [35.35s]: training loss=5.654362201690674  validation ndcg@10=0.01745168162285283 [0.77s]\n",
      "epoch 66 [35.61s]:  training loss=5.441644191741943                                       \n",
      "epoch 67 [35.28s]:  training loss=5.688079357147217                                       \n",
      "epoch 68 [36.22s]:  training loss=6.138950824737549                                       \n",
      "epoch 69 [37.17s]:  training loss=5.37611198425293                                        \n",
      "epoch 70 [35.97s]: training loss=5.808538436889648  validation ndcg@10=0.016376515125946664 [0.93s]\n",
      "epoch 1 [8.99s]:  training loss=0.6713857054710388                                        \n",
      "epoch 2 [9.34s]:  training loss=0.642011284828186                                         \n",
      "epoch 3 [9.22s]:  training loss=0.6206933259963989                                        \n",
      "epoch 4 [8.62s]:  training loss=0.5870786309242249                                        \n",
      "epoch 5 [8.45s]: training loss=0.5710387229919434  validation ndcg@10=0.0027740952592851266 [0.42s]\n",
      "epoch 6 [8.52s]:  training loss=0.5558239817619324                                        \n",
      "epoch 7 [8.34s]:  training loss=0.5346288084983826                                        \n",
      "epoch 8 [8.63s]:  training loss=0.5216161608695984                                        \n",
      "epoch 9 [8.65s]:  training loss=0.5003469586372375                                        \n",
      "epoch 10 [9.7s]: training loss=0.4874688982963562  validation ndcg@10=0.0036730106155355245 [0.49s]\n",
      "epoch 11 [10.14s]:  training loss=0.474961519241333                                       \n",
      "epoch 12 [10.22s]:  training loss=0.4561693072319031                                      \n",
      "epoch 13 [9.51s]:  training loss=0.44898393750190735                                      \n",
      "epoch 14 [9.27s]:  training loss=0.43186068534851074                                      \n",
      "epoch 15 [8.45s]: training loss=0.4274461567401886  validation ndcg@10=0.00539988454238815 [0.42s]\n",
      "epoch 16 [8.46s]:  training loss=0.4133535623550415                                       \n",
      "epoch 17 [8.76s]:  training loss=0.4023449420928955                                       \n",
      "epoch 18 [9.29s]:  training loss=0.39206454157829285                                      \n",
      "epoch 19 [10.05s]:  training loss=0.3797703981399536                                      \n",
      "epoch 20 [10.52s]: training loss=0.3744374215602875  validation ndcg@10=0.007330143555017558 [0.46s]\n",
      "epoch 21 [10.89s]:  training loss=0.36115872859954834                                     \n",
      "epoch 22 [11.08s]:  training loss=0.3515744209289551                                      \n",
      "epoch 23 [7.98s]:  training loss=0.35183194279670715                                      \n",
      "epoch 24 [8.07s]:  training loss=0.34006786346435547                                      \n",
      "epoch 25 [9.04s]: training loss=0.33165398240089417  validation ndcg@10=0.010656112611954104 [0.45s]\n",
      "epoch 26 [8.37s]:  training loss=0.32521191239356995                                      \n",
      "epoch 27 [8.67s]:  training loss=0.32287701964378357                                      \n",
      "epoch 28 [8.33s]:  training loss=0.31612271070480347                                      \n",
      "epoch 29 [8.87s]:  training loss=0.3075560927391052                                       \n",
      "epoch 30 [8.03s]: training loss=0.30582818388938904  validation ndcg@10=0.013317440981316233 [0.45s]\n",
      "epoch 31 [8.87s]:  training loss=0.30511829257011414                                      \n",
      "epoch 32 [8.58s]:  training loss=0.2918364107608795                                       \n",
      "epoch 33 [8.17s]:  training loss=0.2878320813179016                                       \n",
      "epoch 34 [8.71s]:  training loss=0.2892051339149475                                       \n",
      "epoch 35 [8.44s]: training loss=0.27967584133148193  validation ndcg@10=0.016077295730507434 [0.41s]\n",
      "epoch 36 [8.83s]:  training loss=0.2766609489917755                                       \n",
      "epoch 37 [8.63s]:  training loss=0.2756623923778534                                       \n",
      "epoch 38 [8.46s]:  training loss=0.2702367901802063                                       \n",
      "epoch 39 [8.53s]:  training loss=0.26619890332221985                                      \n",
      "epoch 40 [8.45s]: training loss=0.2664263844490051  validation ndcg@10=0.01787734356201004 [0.8s]\n",
      "epoch 41 [8.56s]:  training loss=0.2590332627296448                                       \n",
      "epoch 42 [8.36s]:  training loss=0.2573757767677307                                       \n",
      "epoch 43 [12.15s]:  training loss=0.2547534704208374                                      \n",
      "epoch 44 [8.36s]:  training loss=0.253124475479126                                        \n",
      "epoch 45 [8.54s]: training loss=0.2518799304962158  validation ndcg@10=0.01949810852692791 [0.43s]\n",
      "epoch 46 [8.32s]:  training loss=0.2503291964530945                                       \n",
      "epoch 47 [8.37s]:  training loss=0.24321524798870087                                      \n",
      "epoch 48 [8.39s]:  training loss=0.24467436969280243                                      \n",
      "epoch 49 [8.41s]:  training loss=0.24373769760131836                                      \n",
      "epoch 50 [8.92s]: training loss=0.24060487747192383  validation ndcg@10=0.019901144428550833 [0.41s]\n",
      "epoch 51 [9.01s]:  training loss=0.2392955869436264                                       \n",
      "epoch 52 [8.67s]:  training loss=0.23558209836483002                                      \n",
      "epoch 53 [8.11s]:  training loss=0.23603704571723938                                      \n",
      "epoch 54 [8.27s]:  training loss=0.23244154453277588                                      \n",
      "epoch 55 [9.19s]: training loss=0.23095320165157318  validation ndcg@10=0.02055036091097088 [0.49s]\n",
      "epoch 56 [8.42s]:  training loss=0.22880198061466217                                      \n",
      "epoch 57 [8.43s]:  training loss=0.22806479036808014                                      \n",
      "epoch 58 [8.82s]:  training loss=0.2269888073205948                                       \n",
      "epoch 59 [8.56s]:  training loss=0.22636926174163818                                      \n",
      "epoch 60 [8.39s]: training loss=0.22354957461357117  validation ndcg@10=0.020945205162809545 [0.4s]\n",
      "epoch 61 [8.91s]:  training loss=0.2236260324716568                                       \n",
      "epoch 62 [8.47s]:  training loss=0.22058914601802826                                      \n",
      "epoch 63 [9.25s]:  training loss=0.21915754675865173                                      \n",
      "epoch 64 [8.41s]:  training loss=0.2199440896511078                                       \n",
      "epoch 65 [8.39s]: training loss=0.2192772924900055  validation ndcg@10=0.020978172663893402 [0.51s]\n",
      "epoch 66 [8.76s]:  training loss=0.2167392075061798                                       \n",
      "epoch 67 [8.77s]:  training loss=0.21518497169017792                                      \n",
      "epoch 68 [8.51s]:  training loss=0.21404480934143066                                      \n",
      "epoch 69 [8.24s]:  training loss=0.21704071760177612                                      \n",
      "epoch 70 [8.78s]: training loss=0.21055243909358978  validation ndcg@10=0.020966870611406507 [0.39s]\n",
      "epoch 71 [8.95s]:  training loss=0.20780305564403534                                      \n",
      "epoch 72 [8.54s]:  training loss=0.20871716737747192                                      \n",
      "epoch 73 [8.54s]:  training loss=0.20962175726890564                                      \n",
      "epoch 74 [8.9s]:  training loss=0.203728586435318                                         \n",
      "epoch 75 [8.63s]: training loss=0.21179139614105225  validation ndcg@10=0.021172846963868026 [0.41s]\n",
      "epoch 76 [8.67s]:  training loss=0.2059730440378189                                       \n",
      "epoch 77 [8.79s]:  training loss=0.2047356218099594                                       \n",
      "epoch 78 [8.85s]:  training loss=0.20286929607391357                                      \n",
      "epoch 79 [9.39s]:  training loss=0.20667611062526703                                      \n",
      "epoch 80 [8.58s]: training loss=0.2031852751970291  validation ndcg@10=0.020802246812223018 [0.63s]\n",
      "epoch 81 [8.69s]:  training loss=0.2024121880531311                                       \n",
      "epoch 82 [8.79s]:  training loss=0.20014825463294983                                      \n",
      "epoch 83 [8.94s]:  training loss=0.19945944845676422                                      \n",
      "epoch 84 [8.83s]:  training loss=0.20082245767116547                                      \n",
      "epoch 85 [8.79s]: training loss=0.20106306672096252  validation ndcg@10=0.020719752433002312 [0.44s]\n",
      "epoch 86 [8.51s]:  training loss=0.1962822824716568                                       \n",
      "epoch 87 [8.7s]:  training loss=0.19611062109470367                                       \n",
      "epoch 88 [8.67s]:  training loss=0.19415856897830963                                      \n",
      "epoch 89 [8.29s]:  training loss=0.19390539824962616                                      \n",
      "epoch 90 [8.68s]: training loss=0.19673076272010803  validation ndcg@10=0.020776375525088972 [0.41s]\n",
      "epoch 91 [8.84s]:  training loss=0.18983213603496552                                      \n",
      "epoch 92 [9.26s]:  training loss=0.195916086435318                                        \n",
      "epoch 93 [8.58s]:  training loss=0.1913127452135086                                       \n",
      "epoch 94 [8.66s]:  training loss=0.18947501480579376                                      \n",
      "epoch 95 [8.31s]: training loss=0.18786320090293884  validation ndcg@10=0.020972035076417338 [0.43s]\n",
      "epoch 96 [9.06s]:  training loss=0.18786819279193878                                      \n",
      "epoch 97 [8.84s]:  training loss=0.19113042950630188                                      \n",
      "epoch 98 [8.73s]:  training loss=0.1864781528711319                                       \n",
      "epoch 99 [8.71s]:  training loss=0.19015894830226898                                      \n",
      "epoch 100 [8.45s]: training loss=0.18804317712783813  validation ndcg@10=0.0209364666696763 [0.49s]\n",
      "epoch 1 [12.32s]:  training loss=0.45445358753204346                                      \n",
      "epoch 2 [12.34s]:  training loss=0.48563331365585327                                      \n",
      "epoch 3 [13.22s]:  training loss=0.5404462218284607                                       \n",
      "epoch 4 [12.53s]:  training loss=0.5934616923332214                                       \n",
      "epoch 5 [11.83s]: training loss=0.6124464869499207  validation ndcg@10=0.01487038111163067 [0.49s]\n",
      "epoch 6 [11.86s]:  training loss=0.6601200699806213                                       \n",
      "epoch 7 [12.68s]:  training loss=0.6884084939956665                                       \n",
      "epoch 8 [13.1s]:  training loss=0.7028266787528992                                        \n",
      "epoch 9 [12.41s]:  training loss=0.6968982219696045                                       \n",
      "epoch 10 [12.17s]: training loss=0.6994010210037231  validation ndcg@10=0.016441519759976875 [0.55s]\n",
      "epoch 11 [12.64s]:  training loss=0.7389792203903198                                      \n",
      "epoch 12 [13.39s]:  training loss=0.7425307631492615                                      \n",
      "epoch 13 [12.65s]:  training loss=0.737256646156311                                       \n",
      "epoch 14 [12.26s]:  training loss=0.7630300521850586                                      \n",
      "epoch 15 [12.24s]: training loss=0.7950287461280823  validation ndcg@10=0.01624731031971916 [0.73s]\n",
      "epoch 16 [13.19s]:  training loss=0.7732619047164917                                      \n",
      "epoch 17 [13.0s]:  training loss=0.7296639084815979                                       \n",
      "epoch 18 [12.71s]:  training loss=0.7772681713104248                                      \n",
      "epoch 19 [12.22s]:  training loss=0.7696657776832581                                      \n",
      "epoch 20 [12.68s]: training loss=0.799074649810791  validation ndcg@10=0.015470766752385305 [0.52s]\n",
      "epoch 21 [14.93s]:  training loss=0.8180754780769348                                      \n",
      "epoch 22 [14.45s]:  training loss=0.8434553146362305                                      \n",
      "epoch 23 [12.14s]:  training loss=0.8553584218025208                                      \n",
      "epoch 24 [11.82s]:  training loss=0.8509198427200317                                      \n",
      "epoch 25 [12.74s]: training loss=0.8810878992080688  validation ndcg@10=0.01596583487809868 [0.56s]\n",
      "epoch 26 [13.61s]:  training loss=0.8792513608932495                                      \n",
      "epoch 27 [13.16s]:  training loss=0.8523733019828796                                      \n",
      "epoch 28 [12.19s]:  training loss=0.8663598895072937                                      \n",
      "epoch 29 [12.63s]:  training loss=0.9181418418884277                                      \n",
      "epoch 30 [14.06s]: training loss=0.896857500076294  validation ndcg@10=0.017475197192622045 [0.64s]\n",
      "epoch 31 [13.74s]:  training loss=0.8408747911453247                                      \n",
      "epoch 32 [12.25s]:  training loss=0.8931378722190857                                      \n",
      "epoch 33 [12.05s]:  training loss=0.8993497490882874                                      \n",
      "epoch 34 [12.89s]:  training loss=0.8844227194786072                                      \n",
      "epoch 35 [13.94s]: training loss=0.9134306907653809  validation ndcg@10=0.017443300751249963 [0.52s]\n",
      "epoch 36 [12.76s]:  training loss=0.9077278971672058                                      \n",
      "epoch 37 [12.3s]:  training loss=0.9196756482124329                                       \n",
      "epoch 38 [12.51s]:  training loss=0.9158543348312378                                      \n",
      "epoch 39 [13.45s]:  training loss=0.9442360401153564                                      \n",
      "epoch 40 [13.5s]: training loss=0.9447239637374878  validation ndcg@10=0.018940141781240767 [0.45s]\n",
      "epoch 41 [12.12s]:  training loss=0.9773941040039062                                      \n",
      "epoch 42 [11.99s]:  training loss=0.8975799083709717                                      \n",
      "epoch 43 [12.81s]:  training loss=0.9649398326873779                                      \n",
      "epoch 44 [14.94s]:  training loss=0.9672784209251404                                      \n",
      "epoch 45 [12.79s]: training loss=0.9531636238098145  validation ndcg@10=0.017271181774132078 [0.45s]\n",
      "epoch 46 [12.12s]:  training loss=0.96524977684021                                        \n",
      "epoch 47 [12.21s]:  training loss=0.9238443374633789                                      \n",
      "epoch 48 [12.57s]:  training loss=0.9624965190887451                                      \n",
      "epoch 49 [14.93s]:  training loss=0.9713655114173889                                      \n",
      "epoch 50 [12.92s]: training loss=1.0022118091583252  validation ndcg@10=0.01772635459061112 [0.56s]\n",
      "epoch 51 [12.19s]:  training loss=1.0421265363693237                                      \n",
      "epoch 52 [12.55s]:  training loss=0.9905853867530823                                      \n",
      "epoch 53 [14.71s]:  training loss=1.005776047706604                                       \n",
      "epoch 54 [12.99s]:  training loss=1.003373384475708                                       \n",
      "epoch 55 [12.33s]: training loss=1.0336467027664185  validation ndcg@10=0.018390053351171953 [0.49s]\n",
      "epoch 56 [12.16s]:  training loss=1.0339605808258057                                      \n",
      "epoch 57 [13.25s]:  training loss=1.048917531967163                                       \n",
      "epoch 58 [15.08s]:  training loss=0.9873840808868408                                      \n",
      "epoch 59 [12.96s]:  training loss=1.0599937438964844                                      \n",
      "epoch 60 [11.99s]: training loss=0.9920119047164917  validation ndcg@10=0.018418640221129306 [0.45s]\n",
      "epoch 61 [12.46s]:  training loss=1.0388180017471313                                      \n",
      "epoch 62 [15.27s]:  training loss=1.091686725616455                                       \n",
      "epoch 63 [12.64s]:  training loss=1.1008456945419312                                      \n",
      "epoch 64 [12.04s]:  training loss=1.0301741361618042                                      \n",
      "epoch 65 [11.91s]: training loss=1.0479419231414795  validation ndcg@10=0.01796756489624026 [0.47s]\n",
      "epoch 1 [10.13s]:  training loss=0.33463019132614136                                      \n",
      "epoch 2 [13.38s]:  training loss=0.20911625027656555                                      \n",
      "epoch 3 [9.55s]:  training loss=0.16544674336910248                                       \n",
      "epoch 4 [10.17s]:  training loss=0.14511482417583466                                      \n",
      "epoch 5 [10.01s]: training loss=0.13027653098106384  validation ndcg@10=0.02051376664386955 [0.41s]\n",
      "epoch 6 [9.63s]:  training loss=0.11963944137096405                                       \n",
      "epoch 7 [10.91s]:  training loss=0.11179009824991226                                      \n",
      "epoch 8 [14.83s]:  training loss=0.10663723945617676                                      \n",
      "epoch 9 [10.79s]:  training loss=0.10325343161821365                                      \n",
      "epoch 10 [10.21s]: training loss=0.09958291053771973  validation ndcg@10=0.019071843560123224 [0.44s]\n",
      "epoch 11 [9.61s]:  training loss=0.09840358793735504                                      \n",
      "epoch 12 [9.45s]:  training loss=0.09344705194234848                                      \n",
      "epoch 13 [10.62s]:  training loss=0.09176444262266159                                     \n",
      "epoch 14 [14.14s]:  training loss=0.08709686994552612                                     \n",
      "epoch 15 [10.54s]: training loss=0.09095311164855957  validation ndcg@10=0.020482670434617226 [0.5s]\n",
      "epoch 16 [9.79s]:  training loss=0.08530797064304352                                      \n",
      "epoch 17 [9.79s]:  training loss=0.08565017580986023                                      \n",
      "epoch 18 [9.69s]:  training loss=0.08604694902896881                                      \n",
      "epoch 19 [12.64s]:  training loss=0.08150070905685425                                     \n",
      "epoch 20 [11.93s]: training loss=0.0789291262626648  validation ndcg@10=0.019972371828342272 [0.74s]\n",
      "epoch 21 [11.36s]:  training loss=0.07923198491334915                                     \n",
      "epoch 22 [9.47s]:  training loss=0.07889235764741898                                      \n",
      "epoch 23 [9.3s]:  training loss=0.0803910344839096                                        \n",
      "epoch 24 [10.67s]:  training loss=0.07915041595697403                                     \n",
      "epoch 25 [14.77s]: training loss=0.07892411202192307  validation ndcg@10=0.02064343532167166 [0.38s]\n",
      "epoch 26 [10.29s]:  training loss=0.07734141498804092                                     \n",
      "epoch 27 [9.64s]:  training loss=0.07502688467502594                                      \n",
      "epoch 28 [9.35s]:  training loss=0.07654023915529251                                      \n",
      "epoch 29 [10.09s]:  training loss=0.07733678072690964                                     \n",
      "epoch 30 [11.62s]: training loss=0.07656148076057434  validation ndcg@10=0.019871695525193092 [0.55s]\n",
      "epoch 31 [12.55s]:  training loss=0.07318452000617981                                     \n",
      "epoch 32 [10.22s]:  training loss=0.07264099270105362                                     \n",
      "epoch 33 [9.86s]:  training loss=0.07189630717039108                                      \n",
      "epoch 34 [9.41s]:  training loss=0.07337543368339539                                      \n",
      "epoch 35 [9.7s]: training loss=0.07141619175672531  validation ndcg@10=0.019848612335106795 [0.51s]\n",
      "epoch 36 [11.76s]:  training loss=0.07509283721446991                                     \n",
      "epoch 37 [12.51s]:  training loss=0.07347314059734344                                     \n",
      "epoch 38 [10.19s]:  training loss=0.07166104763746262                                     \n",
      "epoch 39 [9.59s]:  training loss=0.07472648471593857                                      \n",
      "epoch 40 [9.57s]: training loss=0.06854599714279175  validation ndcg@10=0.019268994968711456 [0.49s]\n",
      "epoch 41 [10.24s]:  training loss=0.0695876032114029                                      \n",
      "epoch 42 [15.04s]:  training loss=0.07421325147151947                                     \n",
      "epoch 43 [10.47s]:  training loss=0.07215358316898346                                     \n",
      "epoch 44 [9.82s]:  training loss=0.07022576779127121                                      \n",
      "epoch 45 [9.54s]: training loss=0.07001067698001862  validation ndcg@10=0.016884981362486114 [0.47s]\n",
      "epoch 46 [9.71s]:  training loss=0.07133367657661438                                      \n",
      "epoch 47 [12.07s]:  training loss=0.0745239108800888                                      \n",
      "epoch 48 [12.14s]:  training loss=0.07155665755271912                                     \n",
      "epoch 49 [10.18s]:  training loss=0.07251469790935516                                     \n",
      "epoch 50 [9.72s]: training loss=0.07119978219270706  validation ndcg@10=0.018475378191317556 [0.55s]\n",
      "epoch 1 [42.59s]:  training loss=0.3306048512458801                                       \n",
      "epoch 2 [39.11s]:  training loss=0.26427483558654785                                      \n",
      "epoch 3 [44.8s]:  training loss=0.24642856419086456                                       \n",
      "epoch 4 [44.29s]:  training loss=0.25567594170570374                                      \n",
      "epoch 5 [38.11s]: training loss=0.2520601451396942  validation ndcg@10=0.014714848827521599 [1.16s]\n",
      "epoch 6 [43.15s]:  training loss=0.24354127049446106                                      \n",
      "epoch 7 [43.69s]:  training loss=0.25359445810317993                                      \n",
      "epoch 8 [41.45s]:  training loss=0.2607131898403168                                       \n",
      "epoch 9 [41.0s]:  training loss=0.2526280879974365                                        \n",
      "epoch 10 [44.54s]: training loss=0.2589455544948578  validation ndcg@10=0.013925076265158917 [1.05s]\n",
      "epoch 11 [43.31s]:  training loss=0.26207444071769714                                     \n",
      "epoch 12 [43.04s]:  training loss=0.25386491417884827                                     \n",
      "epoch 13 [44.45s]:  training loss=0.27223098278045654                                     \n",
      "epoch 14 [43.6s]:  training loss=0.26242539286613464                                      \n",
      "epoch 15 [40.01s]: training loss=0.2620745599269867  validation ndcg@10=0.014446354442811171 [1.11s]\n",
      "epoch 16 [43.13s]:  training loss=0.2618882954120636                                      \n",
      "epoch 17 [43.72s]:  training loss=0.25902873277664185                                     \n",
      "epoch 18 [39.37s]:  training loss=0.2618342638015747                                      \n",
      "epoch 19 [43.17s]:  training loss=0.2633102238178253                                      \n",
      "epoch 20 [42.83s]: training loss=0.2749192416667938  validation ndcg@10=0.016091786206603308 [1.04s]\n",
      "epoch 21 [40.99s]:  training loss=0.2758704125881195                                      \n",
      "epoch 22 [42.04s]:  training loss=0.2765568196773529                                      \n",
      "epoch 23 [42.99s]:  training loss=0.2829495072364807                                      \n",
      "epoch 24 [43.41s]:  training loss=0.28004541993141174                                     \n",
      "epoch 25 [39.61s]: training loss=0.2908838391304016  validation ndcg@10=0.014337575016056424 [1.17s]\n",
      "epoch 26 [42.46s]:  training loss=0.2727218568325043                                      \n",
      "epoch 27 [42.89s]:  training loss=0.28783947229385376                                     \n",
      "epoch 28 [41.38s]:  training loss=0.29240506887435913                                     \n",
      "epoch 29 [43.6s]:  training loss=0.28703978657722473                                      \n",
      "epoch 30 [46.0s]: training loss=0.29051318764686584  validation ndcg@10=0.015751394339673784 [1.02s]\n",
      "epoch 31 [39.99s]:  training loss=0.28832656145095825                                     \n",
      "epoch 32 [41.95s]:  training loss=0.2882615029811859                                      \n",
      "epoch 33 [42.72s]:  training loss=0.29338058829307556                                     \n",
      "epoch 34 [38.79s]:  training loss=0.2818782925605774                                      \n",
      "epoch 35 [41.88s]: training loss=0.2811657190322876  validation ndcg@10=0.014252543474405802 [1.03s]\n",
      "epoch 36 [43.36s]:  training loss=0.2887854278087616                                      \n",
      "epoch 37 [40.37s]:  training loss=0.30901092290878296                                     \n",
      "epoch 38 [41.39s]:  training loss=0.2904260456562042                                      \n",
      "epoch 39 [42.68s]:  training loss=0.29430946707725525                                     \n",
      "epoch 40 [40.02s]: training loss=0.31387388706207275  validation ndcg@10=0.014876884502483236 [1.43s]\n",
      "epoch 41 [41.96s]:  training loss=0.30629244446754456                                     \n",
      "epoch 42 [43.39s]:  training loss=0.2940977215766907                                      \n",
      "epoch 43 [42.2s]:  training loss=0.29728540778160095                                      \n",
      "epoch 44 [39.76s]:  training loss=0.30060330033302307                                     \n",
      "epoch 45 [42.49s]: training loss=0.3077964782714844  validation ndcg@10=0.015425627682617701 [1.04s]\n",
      "epoch 1 [9.43s]:  training loss=0.6630109548568726                                        \n",
      "epoch 2 [10.09s]:  training loss=0.6444464325904846                                       \n",
      "epoch 3 [11.43s]:  training loss=0.6302924752235413                                       \n",
      "epoch 4 [12.08s]:  training loss=0.6159954071044922                                       \n",
      "epoch 5 [10.21s]: training loss=0.6041032671928406  validation ndcg@10=0.0035712138647925637 [0.4s]\n",
      "epoch 6 [9.7s]:  training loss=0.5943176746368408                                         \n",
      "epoch 7 [9.55s]:  training loss=0.5806646347045898                                        \n",
      "epoch 8 [9.75s]:  training loss=0.5768831372261047                                        \n",
      "epoch 9 [10.18s]:  training loss=0.562435507774353                                        \n",
      "epoch 10 [10.2s]: training loss=0.5541788935661316  validation ndcg@10=0.003432077743746344 [0.39s]\n",
      "epoch 11 [9.99s]:  training loss=0.5439191460609436                                       \n",
      "epoch 12 [9.82s]:  training loss=0.5392441749572754                                       \n",
      "epoch 13 [9.74s]:  training loss=0.5264962911605835                                       \n",
      "epoch 14 [9.89s]:  training loss=0.5150852203369141                                       \n",
      "epoch 15 [10.19s]: training loss=0.5137511491775513  validation ndcg@10=0.003514299234432823 [0.44s]\n",
      "epoch 16 [12.03s]:  training loss=0.5036865472793579                                      \n",
      "epoch 17 [12.16s]:  training loss=0.49837934970855713                                     \n",
      "epoch 18 [9.87s]:  training loss=0.48815295100212097                                      \n",
      "epoch 19 [10.4s]:  training loss=0.4820190966129303                                       \n",
      "epoch 20 [10.03s]: training loss=0.47216105461120605  validation ndcg@10=0.0036576426357230584 [0.4s]\n",
      "epoch 21 [9.93s]:  training loss=0.46486034989356995                                      \n",
      "epoch 22 [10.23s]:  training loss=0.4607512354850769                                      \n",
      "epoch 23 [10.0s]:  training loss=0.45241591334342957                                      \n",
      "epoch 24 [10.79s]:  training loss=0.4449360966682434                                      \n",
      "epoch 25 [10.96s]: training loss=0.43917372822761536  validation ndcg@10=0.004187189017062159 [0.46s]\n",
      "epoch 26 [10.81s]:  training loss=0.43341249227523804                                     \n",
      "epoch 27 [10.41s]:  training loss=0.4290688931941986                                      \n",
      "epoch 28 [9.66s]:  training loss=0.42205801606178284                                      \n",
      "epoch 29 [9.85s]:  training loss=0.4164210855960846                                       \n",
      "epoch 30 [10.06s]: training loss=0.41157618165016174  validation ndcg@10=0.004958534110131452 [0.44s]\n",
      "epoch 31 [9.71s]:  training loss=0.4054957330226898                                       \n",
      "epoch 32 [10.83s]:  training loss=0.40299928188323975                                     \n",
      "epoch 33 [11.24s]:  training loss=0.39874470233917236                                     \n",
      "epoch 34 [11.52s]:  training loss=0.391530841588974                                       \n",
      "epoch 35 [10.89s]: training loss=0.38838842511177063  validation ndcg@10=0.0057781421280597035 [0.61s]\n",
      "epoch 36 [10.5s]:  training loss=0.3871566355228424                                       \n",
      "epoch 37 [9.9s]:  training loss=0.37981465458869934                                       \n",
      "epoch 38 [9.65s]:  training loss=0.3713395297527313                                       \n",
      "epoch 39 [10.55s]:  training loss=0.3683827817440033                                      \n",
      "epoch 40 [11.15s]: training loss=0.3664153814315796  validation ndcg@10=0.007087207523281387 [0.45s]\n",
      "epoch 41 [11.88s]:  training loss=0.36294057965278625                                     \n",
      "epoch 42 [12.83s]:  training loss=0.35300126671791077                                     \n",
      "epoch 43 [11.52s]:  training loss=0.3510628640651703                                      \n",
      "epoch 44 [10.38s]:  training loss=0.3484784662723541                                      \n",
      "epoch 45 [9.93s]: training loss=0.3421768546104431  validation ndcg@10=0.00861707409451267 [0.39s]\n",
      "epoch 46 [9.94s]:  training loss=0.3409744203090668                                       \n",
      "epoch 47 [10.13s]:  training loss=0.34108203649520874                                     \n",
      "epoch 48 [11.27s]:  training loss=0.33314448595046997                                     \n",
      "epoch 49 [12.95s]:  training loss=0.3317536413669586                                      \n",
      "epoch 50 [12.63s]: training loss=0.32751816511154175  validation ndcg@10=0.010394006995028076 [0.47s]\n",
      "epoch 51 [12.65s]:  training loss=0.3277941644191742                                      \n",
      "epoch 52 [9.4s]:  training loss=0.32259616255760193                                       \n",
      "epoch 53 [9.74s]:  training loss=0.31676238775253296                                      \n",
      "epoch 54 [10.78s]:  training loss=0.3147766590118408                                      \n",
      "epoch 55 [9.86s]: training loss=0.31233036518096924  validation ndcg@10=0.011859402266870649 [0.41s]\n",
      "epoch 56 [9.59s]:  training loss=0.31283435225486755                                      \n",
      "epoch 57 [10.57s]:  training loss=0.3036397397518158                                      \n",
      "epoch 58 [9.47s]:  training loss=0.3020673096179962                                       \n",
      "epoch 59 [10.47s]:  training loss=0.3005048930644989                                      \n",
      "epoch 60 [9.88s]: training loss=0.2991056740283966  validation ndcg@10=0.013539282923894905 [0.48s]\n",
      "epoch 61 [9.98s]:  training loss=0.2957361042499542                                       \n",
      "epoch 62 [10.13s]:  training loss=0.29641613364219666                                     \n",
      "epoch 63 [10.33s]:  training loss=0.29224058985710144                                     \n",
      "epoch 64 [10.34s]:  training loss=0.28912389278411865                                     \n",
      "epoch 65 [10.57s]: training loss=0.2898043692111969  validation ndcg@10=0.01514553486082662 [0.44s]\n",
      "epoch 66 [9.95s]:  training loss=0.2820742130279541                                       \n",
      "epoch 67 [10.55s]:  training loss=0.28129076957702637                                     \n",
      "epoch 68 [9.96s]:  training loss=0.2795749604701996                                       \n",
      "epoch 69 [10.11s]:  training loss=0.2780192494392395                                      \n",
      "epoch 70 [10.23s]: training loss=0.27325043082237244  validation ndcg@10=0.016237587612029616 [0.43s]\n",
      "epoch 71 [9.99s]:  training loss=0.2751629054546356                                       \n",
      "epoch 72 [10.89s]:  training loss=0.2746051549911499                                      \n",
      "epoch 73 [10.67s]:  training loss=0.2730216085910797                                      \n",
      "epoch 74 [10.88s]:  training loss=0.26962733268737793                                     \n",
      "epoch 75 [10.87s]: training loss=0.2718750536441803  validation ndcg@10=0.017720230251027973 [0.41s]\n",
      "epoch 76 [10.01s]:  training loss=0.2691805958747864                                      \n",
      "epoch 77 [9.69s]:  training loss=0.26664796471595764                                      \n",
      "epoch 78 [9.65s]:  training loss=0.25904563069343567                                      \n",
      "epoch 79 [10.48s]:  training loss=0.2618352770805359                                      \n",
      "epoch 80 [11.35s]: training loss=0.26019710302352905  validation ndcg@10=0.018635594145871088 [0.47s]\n",
      "epoch 81 [10.95s]:  training loss=0.25745901465415955                                     \n",
      "epoch 82 [11.27s]:  training loss=0.2549439072608948                                      \n",
      "epoch 83 [10.44s]:  training loss=0.2556065618991852                                      \n",
      "epoch 84 [10.45s]:  training loss=0.2579208016395569                                      \n",
      "epoch 85 [9.99s]: training loss=0.25311434268951416  validation ndcg@10=0.019246110788056475 [0.4s]\n",
      "epoch 86 [9.86s]:  training loss=0.2542026937007904                                       \n",
      "epoch 87 [13.52s]:  training loss=0.24842622876167297                                     \n",
      "epoch 88 [12.07s]:  training loss=0.2509780526161194                                      \n",
      "epoch 89 [12.25s]:  training loss=0.2462753802537918                                      \n",
      "epoch 90 [12.31s]: training loss=0.24787135422229767  validation ndcg@10=0.01944481359581878 [0.47s]\n",
      "epoch 91 [10.4s]:  training loss=0.24249395728111267                                      \n",
      "epoch 92 [10.0s]:  training loss=0.2480962872505188                                       \n",
      "epoch 93 [9.8s]:  training loss=0.2463071197271347                                        \n",
      "epoch 94 [9.7s]:  training loss=0.24151496589183807                                       \n",
      "epoch 95 [10.29s]: training loss=0.24126826226711273  validation ndcg@10=0.01982966542491749 [0.42s]\n",
      "epoch 96 [11.87s]:  training loss=0.2434651255607605                                      \n",
      "epoch 97 [12.99s]:  training loss=0.24066835641860962                                     \n",
      "epoch 98 [12.81s]:  training loss=0.23734568059444427                                     \n",
      "epoch 99 [12.19s]:  training loss=0.23713752627372742                                     \n",
      "epoch 100 [9.33s]: training loss=0.23570074141025543  validation ndcg@10=0.020322032120876524 [0.42s]\n",
      "epoch 101 [10.22s]:  training loss=0.23863931000232697                                    \n",
      "epoch 102 [9.86s]:  training loss=0.23706717789173126                                     \n",
      "epoch 103 [9.7s]:  training loss=0.23092547059059143                                      \n",
      "epoch 104 [10.07s]:  training loss=0.23482105135917664                                    \n",
      "epoch 105 [9.83s]: training loss=0.2332039326429367  validation ndcg@10=0.02035858920988778 [0.41s]\n",
      "epoch 106 [10.1s]:  training loss=0.23360279202461243                                     \n",
      "epoch 107 [9.84s]:  training loss=0.2334858626127243                                      \n",
      "epoch 108 [9.85s]:  training loss=0.22662851214408875                                     \n",
      "epoch 109 [10.16s]:  training loss=0.2282230257987976                                     \n",
      "epoch 110 [9.88s]: training loss=0.23239751160144806  validation ndcg@10=0.020413767483698008 [0.4s]\n",
      "epoch 111 [10.27s]:  training loss=0.22551390528678894                                    \n",
      "epoch 112 [10.77s]:  training loss=0.2287902981042862                                     \n",
      "epoch 113 [10.61s]:  training loss=0.2274521440267563                                     \n",
      "epoch 114 [9.88s]:  training loss=0.22572043538093567                                     \n",
      "epoch 115 [10.91s]: training loss=0.22270125150680542  validation ndcg@10=0.020476997077709017 [0.46s]\n",
      "epoch 116 [9.61s]:  training loss=0.22269193828105927                                     \n",
      "epoch 117 [9.5s]:  training loss=0.22189289331436157                                      \n",
      "epoch 118 [9.92s]:  training loss=0.22101548314094543                                     \n",
      "epoch 119 [9.96s]:  training loss=0.2236328125                                            \n",
      "epoch 120 [10.45s]: training loss=0.21935229003429413  validation ndcg@10=0.020739088544653245 [0.44s]\n",
      "epoch 121 [10.78s]:  training loss=0.22215621173381805                                    \n",
      "epoch 122 [10.48s]:  training loss=0.2237793654203415                                     \n",
      "epoch 123 [10.47s]:  training loss=0.21895013749599457                                    \n",
      "epoch 124 [10.02s]:  training loss=0.21948206424713135                                    \n",
      "epoch 125 [9.91s]: training loss=0.21689991652965546  validation ndcg@10=0.020574051658741817 [0.4s]\n",
      "epoch 126 [9.47s]:  training loss=0.2158581018447876                                      \n",
      "epoch 127 [10.1s]:  training loss=0.21971194446086884                                     \n",
      "epoch 128 [10.97s]:  training loss=0.2174687534570694                                     \n",
      "epoch 129 [11.4s]:  training loss=0.21558089554309845                                     \n",
      "epoch 130 [11.57s]: training loss=0.21692389249801636  validation ndcg@10=0.020511241504818974 [0.48s]\n",
      "epoch 131 [10.28s]:  training loss=0.21329879760742188                                    \n",
      "epoch 132 [9.82s]:  training loss=0.21133579313755035                                     \n",
      "epoch 133 [9.74s]:  training loss=0.21511702239513397                                     \n",
      "epoch 134 [9.71s]:  training loss=0.21076324582099915                                     \n",
      "epoch 135 [10.04s]: training loss=0.2154238373041153  validation ndcg@10=0.02025087341654285 [0.42s]\n",
      "epoch 136 [11.58s]:  training loss=0.21443644165992737                                    \n",
      "epoch 137 [13.3s]:  training loss=0.21156801283359528                                     \n",
      "epoch 138 [13.63s]:  training loss=0.21028386056423187                                    \n",
      "epoch 139 [11.04s]:  training loss=0.2074415683746338                                     \n",
      "epoch 140 [10.2s]: training loss=0.21151389181613922  validation ndcg@10=0.02059461825447103 [0.41s]\n",
      "epoch 141 [9.84s]:  training loss=0.20789258182048798                                     \n",
      "epoch 142 [9.75s]:  training loss=0.20997901260852814                                     \n",
      "epoch 143 [10.05s]:  training loss=0.2096191644668579                                     \n",
      "epoch 144 [11.92s]:  training loss=0.2047688364982605                                     \n",
      "epoch 145 [13.37s]: training loss=0.20544466376304626  validation ndcg@10=0.02052138053387805 [0.44s]\n",
      "epoch 1 [31.68s]:  training loss=0.5387635231018066                                        \n",
      "epoch 2 [31.14s]:  training loss=0.4095836281776428                                        \n",
      "epoch 3 [29.39s]:  training loss=0.33150455355644226                                       \n",
      "epoch 4 [30.48s]:  training loss=0.277271032333374                                         \n",
      "epoch 5 [31.68s]: training loss=0.2520298957824707  validation ndcg@10=0.016915442989683775 [0.76s]\n",
      "epoch 6 [29.41s]:  training loss=0.23048816621303558                                       \n",
      "epoch 7 [30.53s]:  training loss=0.2170940488576889                                        \n",
      "epoch 8 [29.12s]:  training loss=0.20721572637557983                                       \n",
      "epoch 9 [29.13s]:  training loss=0.1975635439157486                                        \n",
      "epoch 10 [29.89s]: training loss=0.1872207075357437  validation ndcg@10=0.020687098578003743 [0.7s]\n",
      "epoch 11 [30.52s]:  training loss=0.18290004134178162                                      \n",
      "epoch 12 [30.16s]:  training loss=0.1775873899459839                                       \n",
      "epoch 13 [30.66s]:  training loss=0.17011867463588715                                      \n",
      "epoch 14 [29.9s]:  training loss=0.1654183715581894                                        \n",
      "epoch 15 [31.0s]: training loss=0.16132108867168427  validation ndcg@10=0.020823568186650612 [0.68s]\n",
      "epoch 16 [29.79s]:  training loss=0.153050035238266                                        \n",
      "epoch 17 [30.17s]:  training loss=0.14949898421764374                                      \n",
      "epoch 18 [30.25s]:  training loss=0.14705990254878998                                      \n",
      "epoch 19 [28.85s]:  training loss=0.1419014036655426                                       \n",
      "epoch 20 [30.32s]: training loss=0.13991503417491913  validation ndcg@10=0.021536669526480208 [0.68s]\n",
      "epoch 21 [29.34s]:  training loss=0.13495826721191406                                      \n",
      "epoch 22 [30.01s]:  training loss=0.13073594868183136                                      \n",
      "epoch 23 [29.86s]:  training loss=0.13083292543888092                                      \n",
      "epoch 24 [29.89s]:  training loss=0.12630896270275116                                      \n",
      "epoch 25 [29.93s]: training loss=0.1244850903749466  validation ndcg@10=0.021997774835445565 [0.77s]\n",
      "epoch 26 [30.84s]:  training loss=0.12080803513526917                                      \n",
      "epoch 27 [29.67s]:  training loss=0.11653535813093185                                      \n",
      "epoch 28 [29.4s]:  training loss=0.11528349667787552                                       \n",
      "epoch 29 [29.9s]:  training loss=0.11437270790338516                                       \n",
      "epoch 30 [29.39s]: training loss=0.1113048568367958  validation ndcg@10=0.02256152530346294 [0.66s]\n",
      "epoch 31 [30.41s]:  training loss=0.10941430181264877                                      \n",
      "epoch 32 [28.51s]:  training loss=0.1116926446557045                                       \n",
      "epoch 33 [32.99s]:  training loss=0.10874001681804657                                      \n",
      "epoch 34 [29.49s]:  training loss=0.10286114364862442                                      \n",
      "epoch 35 [30.66s]: training loss=0.10356085747480392  validation ndcg@10=0.02314990602680183 [0.7s]\n",
      "epoch 36 [29.51s]:  training loss=0.10166722536087036                                      \n",
      "epoch 37 [30.91s]:  training loss=0.09973825514316559                                      \n",
      "epoch 38 [30.24s]:  training loss=0.09908170998096466                                      \n",
      "epoch 39 [29.85s]:  training loss=0.09783767908811569                                      \n",
      "epoch 40 [30.98s]: training loss=0.09637864679098129  validation ndcg@10=0.024347624519801932 [0.66s]\n",
      "epoch 41 [29.63s]:  training loss=0.09390439838171005                                      \n",
      "epoch 42 [30.84s]:  training loss=0.09394213557243347                                      \n",
      "epoch 43 [28.55s]:  training loss=0.09106337279081345                                      \n",
      "epoch 44 [30.92s]:  training loss=0.0891427993774414                                       \n",
      "epoch 45 [29.09s]: training loss=0.08847864717245102  validation ndcg@10=0.025991879842559597 [0.76s]\n",
      "epoch 46 [30.61s]:  training loss=0.0882730484008789                                       \n",
      "epoch 47 [29.45s]:  training loss=0.08499699831008911                                      \n",
      "epoch 48 [30.9s]:  training loss=0.08425543457269669                                       \n",
      "epoch 49 [30.82s]:  training loss=0.08702462166547775                                      \n",
      "epoch 50 [29.93s]: training loss=0.0844079777598381  validation ndcg@10=0.025834691350883614 [0.67s]\n",
      "epoch 51 [31.51s]:  training loss=0.08110573887825012                                      \n",
      "epoch 52 [30.17s]:  training loss=0.08147595077753067                                      \n",
      "epoch 53 [31.91s]:  training loss=0.07656239718198776                                      \n",
      "epoch 54 [29.15s]:  training loss=0.08052195608615875                                      \n",
      "epoch 55 [31.43s]: training loss=0.07826850563287735  validation ndcg@10=0.02523750843567344 [0.7s]\n",
      "epoch 56 [29.11s]:  training loss=0.0759425014257431                                       \n",
      "epoch 57 [31.51s]:  training loss=0.07736066728830338                                      \n",
      "epoch 58 [29.64s]:  training loss=0.07388797402381897                                      \n",
      "epoch 59 [30.71s]:  training loss=0.0752536952495575                                       \n",
      "epoch 60 [29.69s]: training loss=0.07393013685941696  validation ndcg@10=0.02522829049887663 [1.1s]\n",
      "epoch 61 [31.79s]:  training loss=0.07251069694757462                                      \n",
      "epoch 62 [32.12s]:  training loss=0.07227001339197159                                      \n",
      "epoch 63 [29.58s]:  training loss=0.0733107477426529                                       \n",
      "epoch 64 [31.66s]:  training loss=0.07303052395582199                                      \n",
      "epoch 65 [28.49s]: training loss=0.06861153990030289  validation ndcg@10=0.02499992487796982 [0.73s]\n",
      "epoch 66 [31.65s]:  training loss=0.06924986094236374                                      \n",
      "epoch 67 [29.95s]:  training loss=0.07047107815742493                                      \n",
      "epoch 68 [32.47s]:  training loss=0.06917557120323181                                      \n",
      "epoch 69 [30.18s]:  training loss=0.06648927181959152                                      \n",
      "epoch 70 [32.57s]: training loss=0.06763999164104462  validation ndcg@10=0.02568193188807637 [0.68s]\n",
      "epoch 1 [5.01s]:  training loss=0.5649574398994446                                         \n",
      "epoch 2 [4.74s]:  training loss=0.4412994980812073                                         \n",
      "epoch 3 [4.97s]:  training loss=0.3687971234321594                                         \n",
      "epoch 4 [5.41s]:  training loss=0.3192305564880371                                         \n",
      "epoch 5 [5.71s]: training loss=0.2781210243701935  validation ndcg@10=0.013797212719423977 [0.35s]\n",
      "epoch 6 [6.2s]:  training loss=0.251203715801239                                           \n",
      "epoch 7 [5.37s]:  training loss=0.23726168274879456                                        \n",
      "epoch 8 [5.15s]:  training loss=0.22017613053321838                                        \n",
      "epoch 9 [4.91s]:  training loss=0.21226654946804047                                        \n",
      "epoch 10 [4.9s]: training loss=0.2040344923734665  validation ndcg@10=0.020407068716854607 [0.27s]\n",
      "epoch 11 [4.86s]:  training loss=0.19396436214447021                                       \n",
      "epoch 12 [4.96s]:  training loss=0.1898539811372757                                        \n",
      "epoch 13 [4.92s]:  training loss=0.18359819054603577                                       \n",
      "epoch 14 [5.47s]:  training loss=0.18086490035057068                                       \n",
      "epoch 15 [4.97s]: training loss=0.1749599277973175  validation ndcg@10=0.022081089366820388 [0.31s]\n",
      "epoch 16 [7.48s]:  training loss=0.16796915233135223                                       \n",
      "epoch 17 [5.06s]:  training loss=0.16730564832687378                                       \n",
      "epoch 18 [5.34s]:  training loss=0.1589285135269165                                        \n",
      "epoch 19 [4.77s]:  training loss=0.158359557390213                                         \n",
      "epoch 20 [5.12s]: training loss=0.15457601845264435  validation ndcg@10=0.02285290412776046 [0.36s]\n",
      "epoch 21 [4.9s]:  training loss=0.15333570539951324                                        \n",
      "epoch 22 [4.77s]:  training loss=0.14697536826133728                                       \n",
      "epoch 23 [4.79s]:  training loss=0.14478972554206848                                       \n",
      "epoch 24 [4.99s]:  training loss=0.14255200326442719                                       \n",
      "epoch 25 [5.14s]: training loss=0.1360052525997162  validation ndcg@10=0.02349038938295566 [0.35s]\n",
      "epoch 26 [7.24s]:  training loss=0.13736338913440704                                       \n",
      "epoch 27 [5.53s]:  training loss=0.13728117942810059                                       \n",
      "epoch 28 [5.45s]:  training loss=0.12738320231437683                                       \n",
      "epoch 29 [4.98s]:  training loss=0.12705014646053314                                       \n",
      "epoch 30 [4.91s]: training loss=0.13010001182556152  validation ndcg@10=0.02459327512509072 [0.28s]\n",
      "epoch 31 [5.2s]:  training loss=0.12424619495868683                                        \n",
      "epoch 32 [4.88s]:  training loss=0.12217651307582855                                       \n",
      "epoch 33 [4.71s]:  training loss=0.12205147743225098                                       \n",
      "epoch 34 [4.71s]:  training loss=0.11779244989156723                                       \n",
      "epoch 35 [4.93s]: training loss=0.11619015783071518  validation ndcg@10=0.023455063419383734 [0.27s]\n",
      "epoch 36 [5.78s]:  training loss=0.11385291069746017                                       \n",
      "epoch 37 [6.91s]:  training loss=0.11260198801755905                                       \n",
      "epoch 38 [4.54s]:  training loss=0.11247286200523376                                       \n",
      "epoch 39 [5.69s]:  training loss=0.10945109277963638                                       \n",
      "epoch 40 [5.08s]: training loss=0.10904514044523239  validation ndcg@10=0.02503790325953214 [0.28s]\n",
      "epoch 41 [5.02s]:  training loss=0.10563868284225464                                       \n",
      "epoch 42 [4.91s]:  training loss=0.10980802029371262                                       \n",
      "epoch 43 [4.6s]:  training loss=0.10285600274801254                                        \n",
      "epoch 44 [4.76s]:  training loss=0.10143455862998962                                       \n",
      "epoch 45 [4.99s]: training loss=0.10231004655361176  validation ndcg@10=0.025831435092367603 [0.3s]\n",
      "epoch 46 [4.96s]:  training loss=0.09764355421066284                                       \n",
      "epoch 47 [5.05s]:  training loss=0.0984884575009346                                        \n",
      "epoch 48 [7.79s]:  training loss=0.09638343751430511                                       \n",
      "epoch 49 [4.98s]:  training loss=0.09837997704744339                                       \n",
      "epoch 50 [5.2s]: training loss=0.09656026214361191  validation ndcg@10=0.024945512550422478 [0.34s]\n",
      "epoch 51 [4.84s]:  training loss=0.0965418890118599                                        \n",
      "epoch 52 [5.23s]:  training loss=0.09590093791484833                                       \n",
      "epoch 53 [4.99s]:  training loss=0.09489502757787704                                       \n",
      "epoch 54 [4.9s]:  training loss=0.0928669199347496                                         \n",
      "epoch 55 [4.9s]: training loss=0.0934380292892456  validation ndcg@10=0.025343028437267982 [0.29s]\n",
      "epoch 56 [5.16s]:  training loss=0.08957014232873917                                       \n",
      "epoch 57 [5.29s]:  training loss=0.08888475596904755                                       \n",
      "epoch 58 [6.86s]:  training loss=0.08877469599246979                                       \n",
      "epoch 59 [5.5s]:  training loss=0.08797357231378555                                        \n",
      "epoch 60 [5.59s]: training loss=0.08556611835956573  validation ndcg@10=0.026704898425225276 [0.28s]\n",
      "epoch 61 [5.17s]:  training loss=0.08393356949090958                                       \n",
      "epoch 62 [5.18s]:  training loss=0.08647027611732483                                       \n",
      "epoch 63 [5.04s]:  training loss=0.08400742709636688                                       \n",
      "epoch 64 [4.78s]:  training loss=0.08484025299549103                                       \n",
      "epoch 65 [5.05s]: training loss=0.08351688832044601  validation ndcg@10=0.026631951763917035 [0.29s]\n",
      "epoch 66 [4.96s]:  training loss=0.08128516376018524                                       \n",
      "epoch 67 [5.18s]:  training loss=0.08130285143852234                                       \n",
      "epoch 68 [4.97s]:  training loss=0.08213406056165695                                       \n",
      "epoch 69 [7.84s]:  training loss=0.08342652022838593                                       \n",
      "epoch 70 [5.03s]: training loss=0.07968787103891373  validation ndcg@10=0.025909024077815056 [0.3s]\n",
      "epoch 71 [5.37s]:  training loss=0.07678153365850449                                       \n",
      "epoch 72 [4.74s]:  training loss=0.07770264893770218                                       \n",
      "epoch 73 [5.14s]:  training loss=0.07754753530025482                                       \n",
      "epoch 74 [4.89s]:  training loss=0.07949285209178925                                       \n",
      "epoch 75 [4.76s]: training loss=0.07521525025367737  validation ndcg@10=0.026557354639733046 [0.37s]\n",
      "epoch 76 [4.77s]:  training loss=0.07463517785072327                                       \n",
      "epoch 77 [5.26s]:  training loss=0.07493539154529572                                       \n",
      "epoch 78 [5.24s]:  training loss=0.07459340989589691                                       \n",
      "epoch 79 [6.86s]:  training loss=0.07286833971738815                                       \n",
      "epoch 80 [5.54s]: training loss=0.0734795331954956  validation ndcg@10=0.026966641839573716 [0.28s]\n",
      "epoch 81 [5.52s]:  training loss=0.07409700751304626                                       \n",
      "epoch 82 [4.77s]:  training loss=0.07319070398807526                                       \n",
      "epoch 83 [4.92s]:  training loss=0.07005829364061356                                       \n",
      "epoch 84 [4.98s]:  training loss=0.07030012458562851                                       \n",
      "epoch 85 [4.84s]: training loss=0.0711909607052803  validation ndcg@10=0.026263547073160108 [0.34s]\n",
      "epoch 86 [5.13s]:  training loss=0.07021429389715195                                       \n",
      "epoch 87 [4.83s]:  training loss=0.06840170919895172                                       \n",
      "epoch 88 [5.0s]:  training loss=0.06896398216485977                                        \n",
      "epoch 89 [5.75s]:  training loss=0.07008565962314606                                       \n",
      "epoch 90 [7.22s]: training loss=0.06816093623638153  validation ndcg@10=0.026635948739087974 [0.3s]\n",
      "epoch 91 [4.74s]:  training loss=0.06650017946958542                                       \n",
      "epoch 92 [5.36s]:  training loss=0.0674409493803978                                        \n",
      "epoch 93 [5.11s]:  training loss=0.06595296412706375                                       \n",
      "epoch 94 [5.1s]:  training loss=0.06862770766019821                                        \n",
      "epoch 95 [4.89s]: training loss=0.0680534839630127  validation ndcg@10=0.026185133795323227 [0.28s]\n",
      "epoch 96 [4.92s]:  training loss=0.06660047173500061                                       \n",
      "epoch 97 [4.97s]:  training loss=0.06453026831150055                                       \n",
      "epoch 98 [4.9s]:  training loss=0.0642114207148552                                         \n",
      "epoch 99 [5.34s]:  training loss=0.06582719087600708                                       \n",
      "epoch 100 [4.96s]: training loss=0.06424863636493683  validation ndcg@10=0.026493550854435766 [0.3s]\n",
      "epoch 101 [8.2s]:  training loss=0.0640224739909172                                        \n",
      "epoch 102 [8.04s]:  training loss=0.06511230766773224                                      \n",
      "epoch 103 [4.92s]:  training loss=0.06371551007032394                                      \n",
      "epoch 104 [4.94s]:  training loss=0.061718836426734924                                     \n",
      "epoch 105 [5.05s]: training loss=0.06522192060947418  validation ndcg@10=0.02630178851961194 [0.38s]\n",
      "epoch 1 [9.37s]:  training loss=0.49701449275016785                                        \n",
      "epoch 2 [9.65s]:  training loss=0.33422285318374634                                        \n",
      "epoch 3 [13.04s]:  training loss=0.2623831629753113                                        \n",
      "epoch 4 [11.19s]:  training loss=0.23547102510929108                                       \n",
      "epoch 5 [10.04s]: training loss=0.2109241485595703  validation ndcg@10=0.020319871752906542 [0.58s]\n",
      "epoch 6 [9.33s]:  training loss=0.19656996428966522                                        \n",
      "epoch 7 [9.4s]:  training loss=0.18432815372943878                                         \n",
      "epoch 8 [10.62s]:  training loss=0.17545129358768463                                       \n",
      "epoch 9 [13.0s]:  training loss=0.16561272740364075                                        \n",
      "epoch 10 [10.91s]: training loss=0.15830481052398682  validation ndcg@10=0.022275416952251603 [0.4s]\n",
      "epoch 11 [9.71s]:  training loss=0.15141887962818146                                       \n",
      "epoch 12 [9.47s]:  training loss=0.14777937531471252                                       \n",
      "epoch 13 [9.95s]:  training loss=0.13761653006076813                                       \n",
      "epoch 14 [12.77s]:  training loss=0.13397327065467834                                      \n",
      "epoch 15 [10.13s]: training loss=0.13049140572547913  validation ndcg@10=0.022591608965174564 [0.43s]\n",
      "epoch 16 [9.68s]:  training loss=0.12358646839857101                                       \n",
      "epoch 17 [9.16s]:  training loss=0.12035848945379257                                       \n",
      "epoch 18 [10.13s]:  training loss=0.11900303512811661                                      \n",
      "epoch 19 [11.32s]:  training loss=0.1162208542227745                                       \n",
      "epoch 20 [10.92s]: training loss=0.11163273453712463  validation ndcg@10=0.023384558119218083 [0.42s]\n",
      "epoch 21 [10.18s]:  training loss=0.11063854396343231                                      \n",
      "epoch 22 [9.78s]:  training loss=0.10646016150712967                                       \n",
      "epoch 23 [9.73s]:  training loss=0.10378468036651611                                       \n",
      "epoch 24 [9.68s]:  training loss=0.09971021860837936                                       \n",
      "epoch 25 [12.76s]: training loss=0.09987261146306992  validation ndcg@10=0.02624080659740412 [0.4s]\n",
      "epoch 26 [10.21s]:  training loss=0.09987187385559082                                      \n",
      "epoch 27 [9.54s]:  training loss=0.0933215320110321                                        \n",
      "epoch 28 [9.24s]:  training loss=0.09354541450738907                                       \n",
      "epoch 29 [10.15s]:  training loss=0.09019432216882706                                      \n",
      "epoch 30 [11.64s]: training loss=0.09118690341711044  validation ndcg@10=0.025670805648903602 [0.66s]\n",
      "epoch 31 [11.26s]:  training loss=0.08652299642562866                                      \n",
      "epoch 32 [9.31s]:  training loss=0.08760425448417664                                       \n",
      "epoch 33 [9.64s]:  training loss=0.0873471051454544                                        \n",
      "epoch 34 [9.72s]:  training loss=0.0847005769610405                                        \n",
      "epoch 35 [9.82s]: training loss=0.0812731385231018  validation ndcg@10=0.025918276119389478 [0.59s]\n",
      "epoch 36 [12.31s]:  training loss=0.0809905081987381                                       \n",
      "epoch 37 [9.57s]:  training loss=0.08140141516923904                                       \n",
      "epoch 38 [9.6s]:  training loss=0.07684236764907837                                        \n",
      "epoch 39 [9.82s]:  training loss=0.07693570107221603                                       \n",
      "epoch 40 [10.23s]: training loss=0.07687997072935104  validation ndcg@10=0.02571422138892042 [0.45s]\n",
      "epoch 41 [12.33s]:  training loss=0.07437765598297119                                      \n",
      "epoch 42 [10.11s]:  training loss=0.0743359625339508                                       \n",
      "epoch 43 [9.43s]:  training loss=0.07228183001279831                                       \n",
      "epoch 44 [9.63s]:  training loss=0.07227569073438644                                       \n",
      "epoch 45 [9.83s]: training loss=0.07018844783306122  validation ndcg@10=0.026206142891126754 [0.42s]\n",
      "epoch 46 [12.52s]:  training loss=0.07055926322937012                                      \n",
      "epoch 47 [10.66s]:  training loss=0.06982053071260452                                      \n",
      "epoch 48 [10.47s]:  training loss=0.06718403100967407                                      \n",
      "epoch 49 [9.58s]:  training loss=0.07105907052755356                                       \n",
      "epoch 50 [9.31s]: training loss=0.06658536195755005  validation ndcg@10=0.02633707348089511 [0.39s]\n",
      "epoch 51 [10.11s]:  training loss=0.06506646424531937                                      \n",
      "epoch 52 [9.22s]:  training loss=0.06578752398490906                                       \n",
      "epoch 53 [9.94s]:  training loss=0.0671263188123703                                        \n",
      "epoch 54 [10.17s]:  training loss=0.06790070980787277                                      \n",
      "epoch 55 [9.37s]: training loss=0.06514547765254974  validation ndcg@10=0.024176174328400142 [0.62s]\n",
      "epoch 56 [9.36s]:  training loss=0.06448162347078323                                       \n",
      "epoch 57 [9.95s]:  training loss=0.06566590815782547                                       \n",
      "epoch 58 [9.71s]:  training loss=0.06228593364357948                                       \n",
      "epoch 59 [10.22s]:  training loss=0.06432157009840012                                      \n",
      "epoch 60 [9.72s]: training loss=0.061167679727077484  validation ndcg@10=0.025688140867700344 [0.43s]\n",
      "epoch 61 [10.23s]:  training loss=0.060480233281850815                                     \n",
      "epoch 62 [10.03s]:  training loss=0.06073189154267311                                      \n",
      "epoch 63 [9.27s]:  training loss=0.05887383967638016                                       \n",
      "epoch 64 [10.24s]:  training loss=0.05913673713803291                                      \n",
      "epoch 65 [9.66s]: training loss=0.06037478521466255  validation ndcg@10=0.02644366607581753 [0.4s]\n",
      "epoch 66 [10.23s]:  training loss=0.0587310753762722                                       \n",
      "epoch 67 [9.73s]:  training loss=0.0590178482234478                                        \n",
      "epoch 68 [10.23s]:  training loss=0.058386724442243576                                     \n",
      "epoch 69 [9.54s]:  training loss=0.05954000726342201                                       \n",
      "epoch 70 [9.72s]: training loss=0.05896992236375809  validation ndcg@10=0.025474887413633324 [0.51s]\n",
      "epoch 71 [10.28s]:  training loss=0.05548153445124626                                      \n",
      "epoch 72 [9.62s]:  training loss=0.05591406673192978                                       \n",
      "epoch 73 [10.37s]:  training loss=0.052741143852472305                                     \n",
      "epoch 74 [9.87s]:  training loss=0.05481133610010147                                       \n",
      "epoch 75 [10.6s]: training loss=0.0545920766890049  validation ndcg@10=0.02506359304880827 [0.42s]\n",
      "epoch 76 [9.69s]:  training loss=0.0554690845310688                                        \n",
      "epoch 77 [9.51s]:  training loss=0.053547486662864685                                      \n",
      "epoch 78 [10.15s]:  training loss=0.054087746888399124                                     \n",
      "epoch 79 [13.26s]:  training loss=0.0546843558549881                                       \n",
      "epoch 80 [10.84s]: training loss=0.05462871119379997  validation ndcg@10=0.025713238631676787 [0.47s]\n",
      "epoch 81 [10.35s]:  training loss=0.05163170024752617                                      \n",
      "epoch 82 [9.58s]:  training loss=0.05227459967136383                                       \n",
      "epoch 83 [10.02s]:  training loss=0.05111667141318321                                      \n",
      "epoch 84 [9.67s]:  training loss=0.05258418247103691                                       \n",
      "epoch 85 [9.79s]: training loss=0.04866835102438927  validation ndcg@10=0.02552522929288106 [0.7s]\n",
      "epoch 86 [10.06s]:  training loss=0.05128846690058708                                      \n",
      "epoch 87 [10.87s]:  training loss=0.051806557923555374                                     \n",
      "epoch 88 [9.85s]:  training loss=0.04828250780701637                                       \n",
      "epoch 89 [9.39s]:  training loss=0.04963544383645058                                       \n",
      "epoch 90 [10.71s]: training loss=0.0485195592045784  validation ndcg@10=0.024732073933916587 [0.38s]\n",
      "epoch 1 [43.03s]:  training loss=0.563981294631958                                         \n",
      "epoch 2 [43.39s]:  training loss=0.43393316864967346                                       \n",
      "epoch 3 [44.28s]:  training loss=0.3596894443035126                                        \n",
      "epoch 4 [42.39s]:  training loss=0.3141695559024811                                        \n",
      "epoch 5 [44.22s]: training loss=0.27718135714530945  validation ndcg@10=0.01657948421540883 [0.94s]\n",
      "epoch 6 [45.1s]:  training loss=0.25067949295043945                                        \n",
      "epoch 7 [43.79s]:  training loss=0.23441888391971588                                       \n",
      "epoch 8 [44.64s]:  training loss=0.2229471355676651                                        \n",
      "epoch 9 [42.45s]:  training loss=0.21545733511447906                                       \n",
      "epoch 10 [45.22s]: training loss=0.2076975554227829  validation ndcg@10=0.02002814962156878 [0.98s]\n",
      "epoch 11 [47.05s]:  training loss=0.19808605313301086                                      \n",
      "epoch 12 [43.1s]:  training loss=0.19253286719322205                                       \n",
      "epoch 13 [48.97s]:  training loss=0.18627704679965973                                      \n",
      "epoch 14 [47.89s]:  training loss=0.17936338484287262                                      \n",
      "epoch 15 [46.79s]: training loss=0.17443910241127014  validation ndcg@10=0.02145377935428075 [1.08s]\n",
      "epoch 16 [49.66s]:  training loss=0.17119769752025604                                      \n",
      "epoch 17 [45.59s]:  training loss=0.16660983860492706                                      \n",
      "epoch 18 [48.6s]:  training loss=0.1635420173406601                                        \n",
      "epoch 19 [52.3s]:  training loss=0.1593695431947708                                        \n",
      "epoch 20 [44.37s]: training loss=0.15468448400497437  validation ndcg@10=0.020434582946842753 [1.65s]\n",
      "epoch 21 [51.5s]:  training loss=0.15366125106811523                                       \n",
      "epoch 22 [53.5s]:  training loss=0.14815765619277954                                       \n",
      "epoch 23 [45.34s]:  training loss=0.14591068029403687                                      \n",
      "epoch 24 [52.88s]:  training loss=0.14485670626163483                                      \n",
      "epoch 25 [51.47s]: training loss=0.13898269832134247  validation ndcg@10=0.021087097956126892 [1.73s]\n",
      "epoch 26 [44.89s]:  training loss=0.13983286917209625                                      \n",
      "epoch 27 [52.02s]:  training loss=0.13308864831924438                                      \n",
      "epoch 28 [48.88s]:  training loss=0.1360531598329544                                       \n",
      "epoch 29 [45.96s]:  training loss=0.13033990561962128                                      \n",
      "epoch 30 [50.78s]: training loss=0.12968939542770386  validation ndcg@10=0.021336640172115433 [1.12s]\n",
      "epoch 31 [48.53s]:  training loss=0.12639954686164856                                      \n",
      "epoch 32 [47.34s]:  training loss=0.12478899955749512                                      \n",
      "epoch 33 [49.8s]:  training loss=0.12275964766740799                                       \n",
      "epoch 34 [44.24s]:  training loss=0.1206451877951622                                       \n",
      "epoch 35 [49.98s]: training loss=0.11756851524114609  validation ndcg@10=0.02328366643319365 [1.06s]\n",
      "epoch 36 [47.92s]:  training loss=0.11710110306739807                                      \n",
      "epoch 37 [46.92s]:  training loss=0.1176682636141777                                       \n",
      "epoch 38 [50.5s]:  training loss=0.11587557196617126                                       \n",
      "epoch 39 [46.11s]:  training loss=0.10952678322792053                                      \n",
      "epoch 40 [40.28s]: training loss=0.1107199490070343  validation ndcg@10=0.023190102827181678 [0.93s]\n",
      "epoch 41 [43.41s]:  training loss=0.10622607171535492                                      \n",
      "epoch 42 [43.13s]:  training loss=0.10849715024232864                                      \n",
      "epoch 43 [42.89s]:  training loss=0.10529322922229767                                      \n",
      "epoch 44 [43.43s]:  training loss=0.10295319557189941                                      \n",
      "epoch 45 [45.25s]: training loss=0.1053832396864891  validation ndcg@10=0.023586389940472883 [0.97s]\n",
      "epoch 46 [43.54s]:  training loss=0.10393334180116653                                      \n",
      "epoch 47 [43.64s]:  training loss=0.1017737090587616                                       \n",
      "epoch 48 [43.44s]:  training loss=0.09893463551998138                                      \n",
      "epoch 49 [43.86s]:  training loss=0.09819900989532471                                      \n",
      "epoch 50 [44.14s]: training loss=0.09982664883136749  validation ndcg@10=0.024042998594596923 [0.96s]\n",
      "epoch 51 [43.96s]:  training loss=0.09711959958076477                                      \n",
      "epoch 52 [43.38s]:  training loss=0.09508488327264786                                      \n",
      "epoch 53 [43.45s]:  training loss=0.09133203327655792                                      \n",
      "epoch 54 [43.14s]:  training loss=0.09129723906517029                                      \n",
      "epoch 55 [43.88s]: training loss=0.0923231691122055  validation ndcg@10=0.024863939810393226 [0.93s]\n",
      "epoch 56 [43.61s]:  training loss=0.09347518533468246                                      \n",
      "epoch 57 [45.33s]:  training loss=0.09102539718151093                                      \n",
      "epoch 58 [42.95s]:  training loss=0.09102019667625427                                      \n",
      "epoch 59 [43.39s]:  training loss=0.08865347504615784                                      \n",
      "epoch 60 [43.44s]: training loss=0.08743130415678024  validation ndcg@10=0.025284450244380356 [0.97s]\n",
      "epoch 61 [43.29s]:  training loss=0.08969946950674057                                      \n",
      "epoch 62 [43.78s]:  training loss=0.08761891722679138                                      \n",
      "epoch 63 [43.29s]:  training loss=0.08498939126729965                                      \n",
      "epoch 64 [42.88s]:  training loss=0.08578759431838989                                      \n",
      "epoch 65 [43.21s]: training loss=0.08643807470798492  validation ndcg@10=0.025055579231681877 [0.94s]\n",
      "epoch 66 [43.17s]:  training loss=0.08515524864196777                                      \n",
      "epoch 67 [42.48s]:  training loss=0.08173198252916336                                      \n",
      "epoch 68 [42.98s]:  training loss=0.08128108084201813                                      \n",
      "epoch 69 [45.52s]:  training loss=0.07921640574932098                                      \n",
      "epoch 70 [42.21s]: training loss=0.08058768510818481  validation ndcg@10=0.024950407397333965 [1.01s]\n",
      "epoch 71 [43.37s]:  training loss=0.08059456944465637                                      \n",
      "epoch 72 [43.73s]:  training loss=0.07880125939846039                                      \n",
      "epoch 73 [41.69s]:  training loss=0.07923407852649689                                      \n",
      "epoch 74 [43.03s]:  training loss=0.07700735330581665                                      \n",
      "epoch 75 [43.42s]: training loss=0.07529238611459732  validation ndcg@10=0.025336073242850237 [0.91s]\n",
      "epoch 76 [42.35s]:  training loss=0.07561846822500229                                      \n",
      "epoch 77 [42.96s]:  training loss=0.07613041996955872                                      \n",
      "epoch 78 [42.87s]:  training loss=0.07721647620201111                                      \n",
      "epoch 79 [43.52s]:  training loss=0.07425779849290848                                      \n",
      "epoch 80 [43.17s]: training loss=0.07249809056520462  validation ndcg@10=0.025594094454442655 [0.88s]\n",
      "epoch 81 [45.26s]:  training loss=0.07711588591337204                                      \n",
      "epoch 82 [42.43s]:  training loss=0.07119415700435638                                      \n",
      "epoch 83 [42.48s]:  training loss=0.07237052917480469                                      \n",
      "epoch 84 [42.68s]:  training loss=0.07394330203533173                                      \n",
      "epoch 85 [43.52s]: training loss=0.07108908891677856  validation ndcg@10=0.02528604154201795 [1.02s]\n",
      "epoch 86 [43.38s]:  training loss=0.0705137774348259                                       \n",
      "epoch 87 [44.88s]:  training loss=0.07340773940086365                                      \n",
      "epoch 88 [43.55s]:  training loss=0.075158029794693                                        \n",
      "epoch 89 [43.48s]:  training loss=0.07096846401691437                                      \n",
      "epoch 90 [43.91s]: training loss=0.0687369555234909  validation ndcg@10=0.025645247485731364 [0.98s]\n",
      "epoch 91 [43.3s]:  training loss=0.06984399259090424                                       \n",
      "epoch 92 [43.71s]:  training loss=0.06895329803228378                                      \n",
      "epoch 93 [43.08s]:  training loss=0.06890373677015305                                      \n",
      "epoch 94 [44.67s]:  training loss=0.06790471822023392                                      \n",
      "epoch 95 [43.06s]: training loss=0.06937619298696518  validation ndcg@10=0.02545838797244921 [0.94s]\n",
      "epoch 96 [43.15s]:  training loss=0.07002976536750793                                      \n",
      "epoch 97 [43.42s]:  training loss=0.066678486764431                                        \n",
      "epoch 98 [43.58s]:  training loss=0.06902837008237839                                      \n",
      "epoch 99 [44.07s]:  training loss=0.06671426445245743                                      \n",
      "epoch 100 [43.73s]: training loss=0.06747356057167053  validation ndcg@10=0.025281867750426238 [0.97s]\n",
      "epoch 101 [43.13s]:  training loss=0.06568413227796555                                     \n",
      "epoch 102 [43.76s]:  training loss=0.06940589845180511                                     \n",
      "epoch 103 [43.71s]:  training loss=0.06356315314769745                                     \n",
      "epoch 104 [43.98s]:  training loss=0.06393127888441086                                     \n",
      "epoch 105 [43.66s]: training loss=0.06701897829771042  validation ndcg@10=0.02530756896087082 [0.93s]\n",
      "epoch 106 [43.03s]:  training loss=0.06581632792949677                                     \n",
      "epoch 107 [40.44s]:  training loss=0.06535885483026505                                     \n",
      "epoch 108 [41.97s]:  training loss=0.06352248787879944                                     \n",
      "epoch 109 [42.72s]:  training loss=0.06449931114912033                                     \n",
      "epoch 110 [43.27s]: training loss=0.060610245913267136  validation ndcg@10=0.025930248299814836 [0.97s]\n",
      "epoch 111 [43.08s]:  training loss=0.06365185230970383                                     \n",
      "epoch 112 [44.18s]:  training loss=0.06354767084121704                                     \n",
      "epoch 113 [43.52s]:  training loss=0.06116583198308945                                     \n",
      "epoch 114 [43.1s]:  training loss=0.06084318086504936                                      \n",
      "epoch 115 [43.05s]: training loss=0.060010429471731186  validation ndcg@10=0.02536314273887798 [0.9s]\n",
      "epoch 116 [43.22s]:  training loss=0.05907474458217621                                     \n",
      "epoch 117 [43.01s]:  training loss=0.05977633595466614                                     \n",
      "epoch 118 [42.92s]:  training loss=0.0593232735991478                                      \n",
      "epoch 119 [41.94s]:  training loss=0.06095064803957939                                     \n",
      "epoch 120 [42.95s]: training loss=0.05930794030427933  validation ndcg@10=0.026776949406062376 [0.92s]\n",
      "epoch 121 [42.83s]:  training loss=0.05938268452882767                                     \n",
      "epoch 122 [43.04s]:  training loss=0.05940913408994675                                     \n",
      "epoch 123 [42.92s]:  training loss=0.060713209211826324                                    \n",
      "epoch 124 [43.25s]:  training loss=0.05686812847852707                                     \n",
      "epoch 125 [43.57s]: training loss=0.05770449712872505  validation ndcg@10=0.02641277388951883 [0.95s]\n",
      "epoch 126 [43.38s]:  training loss=0.05898324027657509                                     \n",
      "epoch 127 [42.22s]:  training loss=0.05806247517466545                                     \n",
      "epoch 128 [42.76s]:  training loss=0.05910680443048477                                     \n",
      "epoch 129 [42.51s]:  training loss=0.05780665948987007                                     \n",
      "epoch 130 [43.19s]: training loss=0.055110689252614975  validation ndcg@10=0.026381422140311722 [0.96s]\n",
      "epoch 131 [45.58s]:  training loss=0.05689461901783943                                     \n",
      "epoch 132 [42.61s]:  training loss=0.05663440749049187                                     \n",
      "epoch 133 [43.21s]:  training loss=0.05555403232574463                                     \n",
      "epoch 134 [43.48s]:  training loss=0.05524403229355812                                     \n",
      "epoch 135 [43.94s]: training loss=0.05629558861255646  validation ndcg@10=0.026958942364070714 [1.03s]\n",
      "epoch 136 [43.02s]:  training loss=0.05454722419381142                                     \n",
      "epoch 137 [43.27s]:  training loss=0.05677935108542442                                     \n",
      "epoch 138 [42.73s]:  training loss=0.05739050358533859                                     \n",
      "epoch 139 [42.91s]:  training loss=0.05774475261569023                                     \n",
      "epoch 140 [42.89s]: training loss=0.05301608145236969  validation ndcg@10=0.026511540196416522 [0.92s]\n",
      "epoch 141 [43.44s]:  training loss=0.056935250759124756                                    \n",
      "epoch 142 [42.8s]:  training loss=0.05315985158085823                                      \n",
      "epoch 143 [43.05s]:  training loss=0.053870413452386856                                    \n",
      "epoch 144 [45.35s]:  training loss=0.056275732815265656                                    \n",
      "epoch 145 [43.0s]: training loss=0.052999548614025116  validation ndcg@10=0.02630066056987861 [0.91s]\n",
      "epoch 146 [43.25s]:  training loss=0.05558725446462631                                     \n",
      "epoch 147 [43.21s]:  training loss=0.05313874036073685                                     \n",
      "epoch 148 [43.03s]:  training loss=0.05276850610971451                                     \n",
      "epoch 149 [43.09s]:  training loss=0.054587844759225845                                    \n",
      "epoch 150 [43.75s]: training loss=0.05096656084060669  validation ndcg@10=0.026675973249151965 [0.93s]\n",
      "epoch 151 [42.59s]:  training loss=0.05223965272307396                                     \n",
      "epoch 152 [42.58s]:  training loss=0.05300355702638626                                     \n",
      "epoch 153 [42.58s]:  training loss=0.05155489221215248                                     \n",
      "epoch 154 [42.95s]:  training loss=0.05258459597826004                                     \n",
      "epoch 155 [42.77s]: training loss=0.05187970772385597  validation ndcg@10=0.02547479402305293 [0.9s]\n",
      "epoch 156 [42.46s]:  training loss=0.05064263194799423                                     \n",
      "epoch 157 [44.32s]:  training loss=0.05313671752810478                                     \n",
      "epoch 158 [42.58s]:  training loss=0.05072116479277611                                     \n",
      "epoch 159 [42.74s]:  training loss=0.05099884420633316                                     \n",
      "epoch 160 [42.75s]: training loss=0.05175534263253212  validation ndcg@10=0.025551537679284055 [0.95s]\n",
      "epoch 1 [6.97s]:  training loss=0.33297452330589294                                        \n",
      "epoch 2 [6.54s]:  training loss=0.22961482405662537                                        \n",
      "epoch 3 [6.81s]:  training loss=0.21959875524044037                                        \n",
      "epoch 4 [6.79s]:  training loss=0.22351950407028198                                        \n",
      "epoch 5 [6.43s]: training loss=0.22308161854743958  validation ndcg@10=0.014685625468598232 [0.37s]\n",
      "epoch 6 [6.58s]:  training loss=0.22136789560317993                                        \n",
      "epoch 7 [6.87s]:  training loss=0.22100915014743805                                        \n",
      "epoch 8 [6.56s]:  training loss=0.22899222373962402                                        \n",
      "epoch 9 [6.49s]:  training loss=0.23892995715141296                                        \n",
      "epoch 10 [6.93s]: training loss=0.23785819113254547  validation ndcg@10=0.012824990359363554 [0.37s]\n",
      "epoch 11 [6.54s]:  training loss=0.23827023804187775                                       \n",
      "epoch 12 [7.11s]:  training loss=0.24247239530086517                                       \n",
      "epoch 13 [6.72s]:  training loss=0.24120782315731049                                       \n",
      "epoch 14 [6.48s]:  training loss=0.25114941596984863                                       \n",
      "epoch 15 [6.59s]: training loss=0.24635271728038788  validation ndcg@10=0.01433235891779347 [0.41s]\n",
      "epoch 16 [6.62s]:  training loss=0.25800320506095886                                       \n",
      "epoch 17 [6.5s]:  training loss=0.25276315212249756                                        \n",
      "epoch 18 [6.55s]:  training loss=0.26299527287483215                                       \n",
      "epoch 19 [6.54s]:  training loss=0.2650873064994812                                        \n",
      "epoch 20 [6.48s]: training loss=0.2577148973941803  validation ndcg@10=0.01606973222402889 [0.37s]\n",
      "epoch 21 [6.79s]:  training loss=0.26846593618392944                                       \n",
      "epoch 22 [6.56s]:  training loss=0.2570786774158478                                        \n",
      "epoch 23 [6.54s]:  training loss=0.2565169632434845                                        \n",
      "epoch 24 [6.81s]:  training loss=0.2624324858188629                                        \n",
      "epoch 25 [6.86s]: training loss=0.2640339136123657  validation ndcg@10=0.015653887248050783 [0.37s]\n",
      "epoch 26 [6.74s]:  training loss=0.2737126350402832                                        \n",
      "epoch 27 [6.79s]:  training loss=0.2792816758155823                                        \n",
      "epoch 28 [6.61s]:  training loss=0.27338919043540955                                       \n",
      "epoch 29 [6.86s]:  training loss=0.26116350293159485                                       \n",
      "epoch 30 [6.3s]: training loss=0.28888562321662903  validation ndcg@10=0.014187360300914708 [0.38s]\n",
      "epoch 31 [6.67s]:  training loss=0.28240880370140076                                       \n",
      "epoch 32 [6.31s]:  training loss=0.2763701379299164                                        \n",
      "epoch 33 [6.43s]:  training loss=0.26668301224708557                                       \n",
      "epoch 34 [6.82s]:  training loss=0.278179794549942                                         \n",
      "epoch 35 [6.84s]: training loss=0.270286500453949  validation ndcg@10=0.01534368300000554 [0.41s]\n",
      "epoch 36 [6.49s]:  training loss=0.27512842416763306                                       \n",
      "epoch 37 [6.47s]:  training loss=0.2701076865196228                                        \n",
      "epoch 38 [6.44s]:  training loss=0.28992727398872375                                       \n",
      "epoch 39 [6.88s]:  training loss=0.2856822609901428                                        \n",
      "epoch 40 [6.54s]: training loss=0.27599650621414185  validation ndcg@10=0.014574168800786847 [0.39s]\n",
      "epoch 41 [6.55s]:  training loss=0.27352091670036316                                       \n",
      "epoch 42 [6.58s]:  training loss=0.292087584733963                                         \n",
      "epoch 43 [6.7s]:  training loss=0.27866801619529724                                        \n",
      "epoch 44 [6.62s]:  training loss=0.27776679396629333                                       \n",
      "epoch 45 [6.71s]: training loss=0.283437579870224  validation ndcg@10=0.014996966359838042 [0.38s]\n",
      "epoch 1 [20.23s]:  training loss=1.188554048538208                                         \n",
      "epoch 2 [20.51s]:  training loss=1.8140262365341187                                        \n",
      "epoch 3 [20.47s]:  training loss=2.12687087059021                                          \n",
      "epoch 4 [20.43s]:  training loss=2.3129141330718994                                        \n",
      "epoch 5 [22.31s]: training loss=2.3291049003601074  validation ndcg@10=0.012551295728383307 [0.7s]\n",
      "epoch 6 [20.4s]:  training loss=2.454772472381592                                          \n",
      "epoch 7 [20.12s]:  training loss=2.482215642929077                                         \n",
      "epoch 8 [20.17s]:  training loss=2.411802291870117                                         \n",
      "epoch 9 [20.5s]:  training loss=2.692206621170044                                          \n",
      "epoch 10 [20.7s]: training loss=2.7568295001983643  validation ndcg@10=0.016354851173762935 [0.67s]\n",
      "epoch 11 [20.89s]:  training loss=2.6971187591552734                                       \n",
      "epoch 12 [20.66s]:  training loss=2.775285005569458                                        \n",
      "epoch 13 [20.55s]:  training loss=2.939988613128662                                        \n",
      "epoch 14 [20.54s]:  training loss=2.842705249786377                                        \n",
      "epoch 15 [20.62s]: training loss=2.927368402481079  validation ndcg@10=0.017424086943697143 [0.7s]\n",
      "epoch 16 [21.01s]:  training loss=3.024596691131592                                        \n",
      "epoch 17 [20.6s]:  training loss=3.0588276386260986                                        \n",
      "epoch 18 [20.75s]:  training loss=2.9915783405303955                                       \n",
      "epoch 19 [20.22s]:  training loss=3.072986125946045                                        \n",
      "epoch 20 [21.0s]: training loss=3.1843814849853516  validation ndcg@10=0.015147115375963305 [0.66s]\n",
      "epoch 21 [20.74s]:  training loss=3.1206088066101074                                       \n",
      "epoch 22 [20.77s]:  training loss=3.3319272994995117                                       \n",
      "epoch 23 [20.66s]:  training loss=3.266427993774414                                        \n",
      "epoch 24 [20.67s]:  training loss=3.2521235942840576                                       \n",
      "epoch 25 [21.08s]: training loss=3.263848066329956  validation ndcg@10=0.018122029422535702 [0.64s]\n",
      "epoch 26 [20.9s]:  training loss=3.285590887069702                                         \n",
      "epoch 27 [20.74s]:  training loss=3.4419376850128174                                       \n",
      "epoch 28 [20.81s]:  training loss=3.33065128326416                                         \n",
      "epoch 29 [20.9s]:  training loss=3.4265968799591064                                        \n",
      "epoch 30 [20.23s]: training loss=3.673309803009033  validation ndcg@10=0.017456493725047553 [0.71s]\n",
      "epoch 31 [22.7s]:  training loss=3.4558982849121094                                        \n",
      "epoch 32 [19.67s]:  training loss=3.485657215118408                                        \n",
      "epoch 33 [19.82s]:  training loss=3.512434959411621                                        \n",
      "epoch 34 [20.2s]:  training loss=3.391833543777466                                         \n",
      "epoch 35 [20.0s]: training loss=3.606403350830078  validation ndcg@10=0.017912411465332425 [0.67s]\n",
      "epoch 36 [20.8s]:  training loss=3.467763900756836                                         \n",
      "epoch 37 [20.31s]:  training loss=3.5398404598236084                                       \n",
      "epoch 38 [20.41s]:  training loss=3.7038779258728027                                       \n",
      "epoch 39 [20.21s]:  training loss=3.948237895965576                                        \n",
      "epoch 40 [20.55s]: training loss=3.7509453296661377  validation ndcg@10=0.01713496055795938 [0.62s]\n",
      "epoch 41 [21.2s]:  training loss=3.7904751300811768                                        \n",
      "epoch 42 [20.58s]:  training loss=3.690293550491333                                        \n",
      "epoch 43 [20.82s]:  training loss=3.678889751434326                                        \n",
      "epoch 44 [20.47s]:  training loss=3.7928147315979004                                       \n",
      "epoch 45 [20.62s]: training loss=3.6430716514587402  validation ndcg@10=0.015910697945941837 [0.71s]\n",
      "epoch 46 [20.44s]:  training loss=3.9570279121398926                                       \n",
      "epoch 47 [20.62s]:  training loss=3.9862899780273438                                       \n",
      "epoch 48 [20.28s]:  training loss=3.7771308422088623                                       \n",
      "epoch 49 [20.69s]:  training loss=3.9484071731567383                                       \n",
      "epoch 50 [20.79s]: training loss=3.8796818256378174  validation ndcg@10=0.01873427544369606 [0.65s]\n",
      "epoch 51 [21.07s]:  training loss=3.975109577178955                                        \n",
      "epoch 52 [20.34s]:  training loss=3.966365337371826                                        \n",
      "epoch 53 [20.09s]:  training loss=4.047005653381348                                        \n",
      "epoch 54 [20.46s]:  training loss=4.16901159286499                                         \n",
      "epoch 55 [20.6s]: training loss=3.722339153289795  validation ndcg@10=0.017404287267278124 [0.7s]\n",
      "epoch 56 [20.73s]:  training loss=4.030823230743408                                        \n",
      "epoch 57 [20.41s]:  training loss=4.2321085929870605                                       \n",
      "epoch 58 [22.38s]:  training loss=4.133656978607178                                        \n",
      "epoch 59 [19.7s]:  training loss=3.9880800247192383                                        \n",
      "epoch 60 [20.26s]: training loss=3.9925622940063477  validation ndcg@10=0.017763486356632933 [0.65s]\n",
      "epoch 61 [20.28s]:  training loss=4.2080583572387695                                       \n",
      "epoch 62 [20.06s]:  training loss=4.246629238128662                                        \n",
      "epoch 63 [20.78s]:  training loss=3.961026191711426                                        \n",
      "epoch 64 [20.38s]:  training loss=4.028430938720703                                        \n",
      "epoch 65 [20.6s]: training loss=4.245222091674805  validation ndcg@10=0.018320966283865453 [0.67s]\n",
      "epoch 66 [20.51s]:  training loss=4.1595001220703125                                       \n",
      "epoch 67 [21.05s]:  training loss=4.4405927658081055                                       \n",
      "epoch 68 [20.83s]:  training loss=4.196177005767822                                        \n",
      "epoch 69 [21.35s]:  training loss=4.24875545501709                                         \n",
      "epoch 70 [21.02s]: training loss=4.3702712059021  validation ndcg@10=0.018757169391190747 [0.67s]\n",
      "epoch 71 [20.45s]:  training loss=4.314754962921143                                        \n",
      "epoch 72 [20.7s]:  training loss=4.411478042602539                                         \n",
      "epoch 73 [20.94s]:  training loss=4.331810474395752                                        \n",
      "epoch 74 [20.64s]:  training loss=4.1497931480407715                                       \n",
      "epoch 75 [20.05s]: training loss=4.20790958404541  validation ndcg@10=0.018028647122405594 [0.67s]\n",
      "epoch 76 [21.0s]:  training loss=4.332362651824951                                         \n",
      "epoch 77 [20.92s]:  training loss=4.312136173248291                                        \n",
      "epoch 78 [20.3s]:  training loss=4.2158660888671875                                        \n",
      "epoch 79 [19.72s]:  training loss=4.398881435394287                                        \n",
      "epoch 80 [19.82s]: training loss=4.474050998687744  validation ndcg@10=0.01887404034777749 [0.71s]\n",
      "epoch 81 [19.98s]:  training loss=4.410991191864014                                        \n",
      "epoch 82 [20.1s]:  training loss=4.395333766937256                                         \n",
      "epoch 83 [20.03s]:  training loss=4.214347839355469                                        \n",
      "epoch 84 [19.9s]:  training loss=4.741312503814697                                         \n",
      "epoch 85 [21.85s]: training loss=4.220745086669922  validation ndcg@10=0.018221443585716195 [0.65s]\n",
      "epoch 86 [19.66s]:  training loss=4.482895374298096                                        \n",
      "epoch 87 [20.33s]:  training loss=4.660411834716797                                        \n",
      "epoch 88 [20.2s]:  training loss=4.5218000411987305                                        \n",
      "epoch 89 [20.62s]:  training loss=4.239706039428711                                        \n",
      "epoch 90 [20.24s]: training loss=4.520442485809326  validation ndcg@10=0.019424599032764195 [0.67s]\n",
      "epoch 91 [20.1s]:  training loss=4.649048805236816                                         \n",
      "epoch 92 [20.12s]:  training loss=4.873002529144287                                        \n",
      "epoch 93 [20.12s]:  training loss=4.395596027374268                                        \n",
      "epoch 94 [20.07s]:  training loss=4.44661283493042                                         \n",
      "epoch 95 [20.16s]: training loss=4.470882892608643  validation ndcg@10=0.018567978467502636 [0.75s]\n",
      "epoch 96 [20.68s]:  training loss=4.628437042236328                                        \n",
      "epoch 97 [20.5s]:  training loss=4.355478286743164                                         \n",
      "epoch 98 [20.21s]:  training loss=4.673476696014404                                        \n",
      "epoch 99 [20.16s]:  training loss=4.428189277648926                                        \n",
      "epoch 100 [20.32s]: training loss=4.776801109313965  validation ndcg@10=0.01962232138075716 [0.65s]\n",
      "epoch 101 [20.2s]:  training loss=4.797839641571045                                        \n",
      "epoch 102 [20.34s]:  training loss=4.866988658905029                                       \n",
      "epoch 103 [20.45s]:  training loss=4.613290786743164                                       \n",
      "epoch 104 [20.57s]:  training loss=4.796116352081299                                       \n",
      "epoch 105 [20.28s]: training loss=4.897130012512207  validation ndcg@10=0.019027013533560597 [0.65s]\n",
      "epoch 106 [20.79s]:  training loss=4.493862152099609                                       \n",
      "epoch 107 [20.46s]:  training loss=4.567421913146973                                       \n",
      "epoch 108 [20.28s]:  training loss=4.759474754333496                                       \n",
      "epoch 109 [20.17s]:  training loss=4.669619560241699                                       \n",
      "epoch 110 [20.49s]: training loss=4.964776992797852  validation ndcg@10=0.01865055993798216 [0.62s]\n",
      "epoch 111 [20.31s]:  training loss=4.8608551025390625                                      \n",
      "epoch 112 [22.43s]:  training loss=4.816125869750977                                       \n",
      "epoch 113 [20.2s]:  training loss=4.579136371612549                                        \n",
      "epoch 114 [20.02s]:  training loss=4.679379940032959                                       \n",
      "epoch 115 [20.64s]: training loss=4.703201770782471  validation ndcg@10=0.018410003111412283 [0.63s]\n",
      "epoch 116 [19.93s]:  training loss=4.63209342956543                                        \n",
      "epoch 117 [19.83s]:  training loss=5.101197242736816                                       \n",
      "epoch 118 [20.12s]:  training loss=4.796041488647461                                       \n",
      "epoch 119 [20.24s]:  training loss=4.654401779174805                                       \n",
      "epoch 120 [19.87s]: training loss=4.896978378295898  validation ndcg@10=0.01764252237540374 [0.59s]\n",
      "epoch 121 [20.04s]:  training loss=4.8715386390686035                                      \n",
      "epoch 122 [20.41s]:  training loss=4.7905731201171875                                      \n",
      "epoch 123 [20.67s]:  training loss=5.265068531036377                                       \n",
      "epoch 124 [20.29s]:  training loss=4.7750396728515625                                      \n",
      "epoch 125 [20.06s]: training loss=4.88016939163208  validation ndcg@10=0.018533961215053685 [0.66s]\n",
      "epoch 1 [11.03s]:  training loss=0.6604834794998169                                        \n",
      "epoch 2 [10.46s]:  training loss=0.6490577459335327                                        \n",
      "epoch 3 [10.44s]:  training loss=0.6354066133499146                                        \n",
      "epoch 4 [10.74s]:  training loss=0.626298189163208                                         \n",
      "epoch 5 [10.56s]: training loss=0.6047614812850952  validation ndcg@10=0.0049355806287428175 [0.44s]\n",
      "epoch 6 [9.87s]:  training loss=0.5954315066337585                                         \n",
      "epoch 7 [10.0s]:  training loss=0.5839574933052063                                         \n",
      "epoch 8 [10.51s]:  training loss=0.5740818381309509                                        \n",
      "epoch 9 [10.16s]:  training loss=0.5628770589828491                                        \n",
      "epoch 10 [10.32s]: training loss=0.5510897636413574  validation ndcg@10=0.004738745252982878 [0.49s]\n",
      "epoch 11 [10.29s]:  training loss=0.5432531833648682                                       \n",
      "epoch 12 [10.36s]:  training loss=0.5391867756843567                                       \n",
      "epoch 13 [10.23s]:  training loss=0.5280181169509888                                       \n",
      "epoch 14 [10.06s]:  training loss=0.5172820091247559                                       \n",
      "epoch 15 [10.37s]: training loss=0.506412148475647  validation ndcg@10=0.004780937244486961 [0.44s]\n",
      "epoch 16 [10.06s]:  training loss=0.5069186091423035                                       \n",
      "epoch 17 [9.97s]:  training loss=0.49198076128959656                                       \n",
      "epoch 18 [10.26s]:  training loss=0.4901270568370819                                       \n",
      "epoch 19 [9.93s]:  training loss=0.4799572825431824                                        \n",
      "epoch 20 [10.29s]: training loss=0.4701550602912903  validation ndcg@10=0.0050362515598346026 [0.42s]\n",
      "epoch 21 [9.89s]:  training loss=0.46484264731407166                                       \n",
      "epoch 22 [10.19s]:  training loss=0.4653252065181732                                       \n",
      "epoch 23 [10.16s]:  training loss=0.4586125314235687                                       \n",
      "epoch 24 [10.13s]:  training loss=0.4500599503517151                                       \n",
      "epoch 25 [10.02s]: training loss=0.4464476704597473  validation ndcg@10=0.005380365489809825 [0.46s]\n",
      "epoch 26 [11.17s]:  training loss=0.4388463497161865                                       \n",
      "epoch 27 [11.5s]:  training loss=0.4332999885082245                                        \n",
      "epoch 28 [9.78s]:  training loss=0.4313473403453827                                        \n",
      "epoch 29 [9.57s]:  training loss=0.41871753334999084                                       \n",
      "epoch 30 [9.8s]: training loss=0.4166812002658844  validation ndcg@10=0.005856439126306264 [0.4s]\n",
      "epoch 31 [9.59s]:  training loss=0.41425836086273193                                       \n",
      "epoch 32 [9.96s]:  training loss=0.4069123864173889                                        \n",
      "epoch 33 [9.76s]:  training loss=0.40461859107017517                                       \n",
      "epoch 34 [9.9s]:  training loss=0.3941701352596283                                         \n",
      "epoch 35 [9.7s]: training loss=0.39459356665611267  validation ndcg@10=0.006637820267506442 [0.48s]\n",
      "epoch 36 [9.76s]:  training loss=0.38750526309013367                                       \n",
      "epoch 37 [9.61s]:  training loss=0.38628941774368286                                       \n",
      "epoch 38 [10.01s]:  training loss=0.3875179588794708                                       \n",
      "epoch 39 [9.66s]:  training loss=0.378825843334198                                         \n",
      "epoch 40 [9.87s]: training loss=0.3728451132774353  validation ndcg@10=0.007492718482138775 [0.41s]\n",
      "epoch 41 [9.7s]:  training loss=0.36993950605392456                                        \n",
      "epoch 42 [9.68s]:  training loss=0.3677143454551697                                        \n",
      "epoch 43 [9.79s]:  training loss=0.3619886636734009                                        \n",
      "epoch 44 [9.94s]:  training loss=0.360419362783432                                         \n",
      "epoch 45 [9.73s]: training loss=0.350059449672699  validation ndcg@10=0.008294297872135547 [0.39s]\n",
      "epoch 46 [9.7s]:  training loss=0.35301509499549866                                        \n",
      "epoch 47 [9.71s]:  training loss=0.3476587235927582                                        \n",
      "epoch 48 [9.25s]:  training loss=0.34404006600379944                                       \n",
      "epoch 49 [10.74s]:  training loss=0.34174954891204834                                      \n",
      "epoch 50 [10.16s]: training loss=0.33530065417289734  validation ndcg@10=0.009407513882036925 [0.44s]\n",
      "epoch 51 [9.98s]:  training loss=0.3371394872665405                                        \n",
      "epoch 52 [10.42s]:  training loss=0.3288778066635132                                       \n",
      "epoch 53 [10.32s]:  training loss=0.3286169469356537                                       \n",
      "epoch 54 [10.09s]:  training loss=0.324996680021286                                        \n",
      "epoch 55 [10.29s]: training loss=0.32014527916908264  validation ndcg@10=0.010867716074803597 [0.5s]\n",
      "epoch 56 [10.06s]:  training loss=0.31909888982772827                                      \n",
      "epoch 57 [10.2s]:  training loss=0.3189990520477295                                        \n",
      "epoch 58 [10.33s]:  training loss=0.3117218613624573                                       \n",
      "epoch 59 [10.15s]:  training loss=0.3107733130455017                                       \n",
      "epoch 60 [9.85s]: training loss=0.310409277677536  validation ndcg@10=0.012413930010561434 [0.46s]\n",
      "epoch 61 [10.24s]:  training loss=0.3038596212863922                                       \n",
      "epoch 62 [10.34s]:  training loss=0.3031611144542694                                       \n",
      "epoch 63 [10.39s]:  training loss=0.2984370291233063                                       \n",
      "epoch 64 [10.19s]:  training loss=0.29505428671836853                                      \n",
      "epoch 65 [10.36s]: training loss=0.2975044548511505  validation ndcg@10=0.013937735759138339 [0.45s]\n",
      "epoch 66 [10.56s]:  training loss=0.29251620173454285                                      \n",
      "epoch 67 [9.69s]:  training loss=0.2934289872646332                                        \n",
      "epoch 68 [10.66s]:  training loss=0.28974267840385437                                      \n",
      "epoch 69 [10.24s]:  training loss=0.28784292936325073                                      \n",
      "epoch 70 [10.35s]: training loss=0.2875719666481018  validation ndcg@10=0.015323823834338588 [0.4s]\n",
      "epoch 71 [10.68s]:  training loss=0.28033170104026794                                      \n",
      "epoch 72 [10.27s]:  training loss=0.282454252243042                                        \n",
      "epoch 73 [10.17s]:  training loss=0.2777714133262634                                       \n",
      "epoch 74 [10.4s]:  training loss=0.27661725878715515                                       \n",
      "epoch 75 [10.22s]: training loss=0.27808260917663574  validation ndcg@10=0.01607012909061717 [0.49s]\n",
      "epoch 76 [10.08s]:  training loss=0.2743251919746399                                       \n",
      "epoch 77 [10.86s]:  training loss=0.2690783441066742                                       \n",
      "epoch 78 [10.44s]:  training loss=0.2695339620113373                                       \n",
      "epoch 79 [10.41s]:  training loss=0.2678219676017761                                       \n",
      "epoch 80 [10.32s]: training loss=0.26558634638786316  validation ndcg@10=0.017095479793766057 [0.4s]\n",
      "epoch 81 [12.64s]:  training loss=0.26692554354667664                                      \n",
      "epoch 82 [10.49s]:  training loss=0.2660505175590515                                       \n",
      "epoch 83 [10.47s]:  training loss=0.26366427540779114                                      \n",
      "epoch 84 [10.01s]:  training loss=0.26212337613105774                                      \n",
      "epoch 85 [10.12s]: training loss=0.25700926780700684  validation ndcg@10=0.017705301335594274 [0.4s]\n",
      "epoch 86 [10.32s]:  training loss=0.2581329941749573                                       \n",
      "epoch 87 [10.28s]:  training loss=0.25434815883636475                                      \n",
      "epoch 88 [10.7s]:  training loss=0.258463054895401                                         \n",
      "epoch 89 [10.06s]:  training loss=0.25680938363075256                                      \n",
      "epoch 90 [10.29s]: training loss=0.25425177812576294  validation ndcg@10=0.018470637527444676 [0.42s]\n",
      "epoch 91 [10.2s]:  training loss=0.2544885277748108                                        \n",
      "epoch 92 [9.88s]:  training loss=0.2507416903972626                                        \n",
      "epoch 93 [10.02s]:  training loss=0.24763773381710052                                      \n",
      "epoch 94 [9.54s]:  training loss=0.24804989993572235                                       \n",
      "epoch 95 [9.51s]: training loss=0.25011762976646423  validation ndcg@10=0.01903926937254442 [0.4s]\n",
      "epoch 96 [10.25s]:  training loss=0.2512664496898651                                       \n",
      "epoch 97 [9.59s]:  training loss=0.24375100433826447                                       \n",
      "epoch 98 [10.2s]:  training loss=0.24305637180805206                                       \n",
      "epoch 99 [9.56s]:  training loss=0.24312616884708405                                       \n",
      "epoch 100 [10.18s]: training loss=0.23877641558647156  validation ndcg@10=0.019693647607940554 [0.41s]\n",
      "epoch 101 [10.3s]:  training loss=0.24365201592445374                                      \n",
      "epoch 102 [10.39s]:  training loss=0.2422863394021988                                      \n",
      "epoch 103 [10.21s]:  training loss=0.23742687702178955                                     \n",
      "epoch 104 [10.05s]:  training loss=0.24334931373596191                                     \n",
      "epoch 105 [10.21s]: training loss=0.23639218509197235  validation ndcg@10=0.019599167362397328 [0.41s]\n",
      "epoch 106 [9.72s]:  training loss=0.23964613676071167                                      \n",
      "epoch 107 [10.1s]:  training loss=0.2345578968524933                                       \n",
      "epoch 108 [9.97s]:  training loss=0.23646801710128784                                      \n",
      "epoch 109 [10.03s]:  training loss=0.23086406290531158                                     \n",
      "epoch 110 [10.21s]: training loss=0.23193001747131348  validation ndcg@10=0.020098519034922276 [0.41s]\n",
      "epoch 111 [10.18s]:  training loss=0.2282605767250061                                      \n",
      "epoch 112 [9.72s]:  training loss=0.2321106642484665                                       \n",
      "epoch 113 [9.76s]:  training loss=0.22953511774539948                                      \n",
      "epoch 114 [10.26s]:  training loss=0.22995832562446594                                     \n",
      "epoch 115 [10.17s]: training loss=0.23179413378238678  validation ndcg@10=0.019973150052173677 [0.44s]\n",
      "epoch 116 [10.08s]:  training loss=0.23083709180355072                                     \n",
      "epoch 117 [10.04s]:  training loss=0.22612035274505615                                     \n",
      "epoch 118 [9.67s]:  training loss=0.2286785989999771                                       \n",
      "epoch 119 [10.11s]:  training loss=0.22720542550086975                                     \n",
      "epoch 120 [10.06s]: training loss=0.22357651591300964  validation ndcg@10=0.02025997485362954 [0.4s]\n",
      "epoch 121 [10.66s]:  training loss=0.22799557447433472                                     \n",
      "epoch 122 [10.02s]:  training loss=0.22610464692115784                                     \n",
      "epoch 123 [10.2s]:  training loss=0.22928239405155182                                      \n",
      "epoch 124 [9.8s]:  training loss=0.22456128895282745                                       \n",
      "epoch 125 [10.27s]: training loss=0.2239256501197815  validation ndcg@10=0.020001510628057547 [0.45s]\n",
      "epoch 126 [9.93s]:  training loss=0.22130604088306427                                      \n",
      "epoch 127 [10.47s]:  training loss=0.22139006853103638                                     \n",
      "epoch 128 [9.89s]:  training loss=0.2216753512620926                                       \n",
      "epoch 129 [10.59s]:  training loss=0.22130516171455383                                     \n",
      "epoch 130 [10.84s]: training loss=0.22041766345500946  validation ndcg@10=0.020129006212708543 [0.39s]\n",
      "epoch 131 [10.3s]:  training loss=0.22326652705669403                                      \n",
      "epoch 132 [9.97s]:  training loss=0.22095942497253418                                      \n",
      "epoch 133 [9.76s]:  training loss=0.21846623718738556                                      \n",
      "epoch 134 [9.84s]:  training loss=0.22020064294338226                                      \n",
      "epoch 135 [10.01s]: training loss=0.22031140327453613  validation ndcg@10=0.020348533582098684 [0.43s]\n",
      "epoch 136 [10.11s]:  training loss=0.21535013616085052                                     \n",
      "epoch 137 [11.81s]:  training loss=0.2185208946466446                                      \n",
      "epoch 138 [10.0s]:  training loss=0.2196156233549118                                       \n",
      "epoch 139 [10.14s]:  training loss=0.2112145870923996                                      \n",
      "epoch 140 [9.8s]: training loss=0.21260184049606323  validation ndcg@10=0.020368564888196106 [0.44s]\n",
      "epoch 141 [10.25s]:  training loss=0.21427513659000397                                     \n",
      "epoch 142 [10.01s]:  training loss=0.2115860879421234                                      \n",
      "epoch 143 [10.05s]:  training loss=0.21666088700294495                                     \n",
      "epoch 144 [9.81s]:  training loss=0.21268966794013977                                      \n",
      "epoch 145 [10.23s]: training loss=0.21218447387218475  validation ndcg@10=0.02027013609561007 [0.38s]\n",
      "epoch 146 [9.99s]:  training loss=0.21098265051841736                                      \n",
      "epoch 147 [9.87s]:  training loss=0.21047790348529816                                      \n",
      "epoch 148 [10.07s]:  training loss=0.20658007264137268                                     \n",
      "epoch 149 [10.09s]:  training loss=0.20807847380638123                                     \n",
      "epoch 150 [10.27s]: training loss=0.20654693245887756  validation ndcg@10=0.020240773447011494 [0.45s]\n",
      "epoch 151 [10.04s]:  training loss=0.2095012217760086                                      \n",
      "epoch 152 [9.91s]:  training loss=0.2093314379453659                                       \n",
      "epoch 153 [10.14s]:  training loss=0.20728720724582672                                     \n",
      "epoch 154 [9.94s]:  training loss=0.20272982120513916                                      \n",
      "epoch 155 [10.19s]: training loss=0.20681725442409515  validation ndcg@10=0.020355543072536446 [0.4s]\n",
      "epoch 156 [10.63s]:  training loss=0.20344163477420807                                     \n",
      "epoch 157 [10.22s]:  training loss=0.2062714844942093                                      \n",
      "epoch 158 [10.53s]:  training loss=0.2077365517616272                                      \n",
      "epoch 159 [9.85s]:  training loss=0.2042476385831833                                       \n",
      "epoch 160 [10.24s]: training loss=0.20462040603160858  validation ndcg@10=0.02015727062026362 [0.46s]\n",
      "epoch 161 [10.61s]:  training loss=0.20483365654945374                                     \n",
      "epoch 162 [10.04s]:  training loss=0.20168034732341766                                     \n",
      "epoch 163 [10.06s]:  training loss=0.1986442357301712                                      \n",
      "epoch 164 [10.04s]:  training loss=0.2008994221687317                                      \n",
      "epoch 165 [10.01s]: training loss=0.19831499457359314  validation ndcg@10=0.020440272970797816 [0.44s]\n",
      "epoch 166 [9.84s]:  training loss=0.20063915848731995                                      \n",
      "epoch 167 [10.27s]:  training loss=0.20294299721717834                                     \n",
      "epoch 168 [10.49s]:  training loss=0.19967414438724518                                     \n",
      "epoch 169 [10.14s]:  training loss=0.19657057523727417                                     \n",
      "epoch 170 [9.96s]: training loss=0.19803643226623535  validation ndcg@10=0.020065822315950315 [0.47s]\n",
      "epoch 171 [10.13s]:  training loss=0.20063799619674683                                     \n",
      "epoch 172 [10.43s]:  training loss=0.1986529678106308                                      \n",
      "epoch 173 [10.45s]:  training loss=0.19857050478458405                                     \n",
      "epoch 174 [10.23s]:  training loss=0.19853167235851288                                     \n",
      "epoch 175 [10.71s]: training loss=0.19792084395885468  validation ndcg@10=0.02049095894611913 [0.4s]\n",
      "epoch 176 [10.21s]:  training loss=0.1982656568288803                                      \n",
      "epoch 177 [10.1s]:  training loss=0.19934464991092682                                      \n",
      "epoch 178 [10.02s]:  training loss=0.19604091346263885                                     \n",
      "epoch 179 [10.54s]:  training loss=0.1928301304578781                                      \n",
      "epoch 180 [10.04s]: training loss=0.19589506089687347  validation ndcg@10=0.020388335846377514 [0.51s]\n",
      "epoch 181 [10.09s]:  training loss=0.19405794143676758                                     \n",
      "epoch 182 [10.19s]:  training loss=0.1945783495903015                                      \n",
      "epoch 183 [10.64s]:  training loss=0.19495998322963715                                     \n",
      "epoch 184 [10.35s]:  training loss=0.1952790468931198                                      \n",
      "epoch 185 [10.21s]: training loss=0.1957896500825882  validation ndcg@10=0.020608739099494094 [0.39s]\n",
      "epoch 186 [9.73s]:  training loss=0.19118115305900574                                      \n",
      "epoch 187 [10.23s]:  training loss=0.1921287178993225                                      \n",
      "epoch 188 [10.15s]:  training loss=0.19442018866539001                                     \n",
      "epoch 189 [9.72s]:  training loss=0.19259150326251984                                      \n",
      "epoch 190 [9.98s]: training loss=0.19304609298706055  validation ndcg@10=0.020721077494248386 [0.48s]\n",
      "epoch 191 [9.79s]:  training loss=0.18927522003650665                                      \n",
      "epoch 192 [12.16s]:  training loss=0.18986786901950836                                     \n",
      "epoch 193 [9.99s]:  training loss=0.18806253373622894                                      \n",
      "epoch 194 [10.04s]:  training loss=0.19021111726760864                                     \n",
      "epoch 195 [10.13s]: training loss=0.18862488865852356  validation ndcg@10=0.02067024075693698 [0.39s]\n",
      "epoch 196 [10.21s]:  training loss=0.18986743688583374                                     \n",
      "epoch 197 [9.9s]:  training loss=0.18746230006217957                                       \n",
      "epoch 198 [9.91s]:  training loss=0.19036506116390228                                      \n",
      "epoch 199 [9.9s]:  training loss=0.18988147377967834                                       \n",
      "epoch 200 [10.3s]: training loss=0.19167084991931915  validation ndcg@10=0.020913112149428138 [0.47s]\n",
      "epoch 1 [8.8s]:  training loss=0.631359338760376                                           \n",
      "epoch 2 [8.69s]:  training loss=0.5599478483200073                                         \n",
      "epoch 3 [8.61s]:  training loss=0.5137247443199158                                         \n",
      "epoch 4 [8.27s]:  training loss=0.47965922951698303                                        \n",
      "epoch 5 [8.38s]: training loss=0.44482895731925964  validation ndcg@10=0.0050498575728227905 [0.38s]\n",
      "epoch 6 [8.3s]:  training loss=0.4247456192970276                                          \n",
      "epoch 7 [8.26s]:  training loss=0.39442014694213867                                        \n",
      "epoch 8 [8.14s]:  training loss=0.37592434883117676                                        \n",
      "epoch 9 [8.3s]:  training loss=0.35652804374694824                                         \n",
      "epoch 10 [8.32s]: training loss=0.338329941034317  validation ndcg@10=0.007481069136916715 [0.36s]\n",
      "epoch 11 [8.27s]:  training loss=0.3215891718864441                                        \n",
      "epoch 12 [8.31s]:  training loss=0.31322014331817627                                       \n",
      "epoch 13 [8.27s]:  training loss=0.2930169403553009                                        \n",
      "epoch 14 [8.28s]:  training loss=0.2866278290748596                                        \n",
      "epoch 15 [8.14s]: training loss=0.27509164810180664  validation ndcg@10=0.011294919091668595 [0.31s]\n",
      "epoch 16 [8.22s]:  training loss=0.26286399364471436                                       \n",
      "epoch 17 [8.22s]:  training loss=0.25570204854011536                                       \n",
      "epoch 18 [8.24s]:  training loss=0.243373841047287                                         \n",
      "epoch 19 [8.25s]:  training loss=0.23963063955307007                                       \n",
      "epoch 20 [8.22s]: training loss=0.2321498692035675  validation ndcg@10=0.015503364985007487 [0.37s]\n",
      "epoch 21 [8.31s]:  training loss=0.22566628456115723                                       \n",
      "epoch 22 [8.21s]:  training loss=0.2250303030014038                                        \n",
      "epoch 23 [8.13s]:  training loss=0.21990449726581573                                       \n",
      "epoch 24 [8.21s]:  training loss=0.21427495777606964                                       \n",
      "epoch 25 [8.15s]: training loss=0.21024735271930695  validation ndcg@10=0.018484913783733663 [0.32s]\n",
      "epoch 26 [8.18s]:  training loss=0.2064262181520462                                        \n",
      "epoch 27 [8.03s]:  training loss=0.2065780758857727                                        \n",
      "epoch 28 [8.19s]:  training loss=0.2034866213798523                                        \n",
      "epoch 29 [7.97s]:  training loss=0.2008841335773468                                        \n",
      "epoch 30 [8.09s]: training loss=0.1950623244047165  validation ndcg@10=0.019722063322622423 [0.36s]\n",
      "epoch 31 [8.15s]:  training loss=0.19428057968616486                                       \n",
      "epoch 32 [8.05s]:  training loss=0.1881207376718521                                        \n",
      "epoch 33 [8.09s]:  training loss=0.18505699932575226                                       \n",
      "epoch 34 [8.16s]:  training loss=0.1884326934814453                                        \n",
      "epoch 35 [8.13s]: training loss=0.18484607338905334  validation ndcg@10=0.02083592020288909 [0.33s]\n",
      "epoch 36 [8.21s]:  training loss=0.18331089615821838                                       \n",
      "epoch 37 [7.95s]:  training loss=0.18067914247512817                                       \n",
      "epoch 38 [7.93s]:  training loss=0.18105828762054443                                       \n",
      "epoch 39 [7.91s]:  training loss=0.17556747794151306                                       \n",
      "epoch 40 [8.05s]: training loss=0.17312945425510406  validation ndcg@10=0.02124004142365832 [0.33s]\n",
      "epoch 41 [7.96s]:  training loss=0.172862708568573                                         \n",
      "epoch 42 [7.94s]:  training loss=0.172409325838089                                         \n",
      "epoch 43 [7.7s]:  training loss=0.17105872929096222                                        \n",
      "epoch 44 [8.01s]:  training loss=0.1699502170085907                                        \n",
      "epoch 45 [8.02s]: training loss=0.1674150824546814  validation ndcg@10=0.021739624850694802 [0.34s]\n",
      "epoch 46 [7.94s]:  training loss=0.16235904395580292                                       \n",
      "epoch 47 [7.98s]:  training loss=0.16305871307849884                                       \n",
      "epoch 48 [8.02s]:  training loss=0.16337552666664124                                       \n",
      "epoch 49 [7.85s]:  training loss=0.16137291491031647                                       \n",
      "epoch 50 [7.9s]: training loss=0.15870881080627441  validation ndcg@10=0.02183189970454688 [0.33s]\n",
      "epoch 51 [7.92s]:  training loss=0.1523844450712204                                        \n",
      "epoch 52 [8.06s]:  training loss=0.15782977640628815                                       \n",
      "epoch 53 [7.85s]:  training loss=0.15514010190963745                                       \n",
      "epoch 54 [7.92s]:  training loss=0.15287333726882935                                       \n",
      "epoch 55 [8.05s]: training loss=0.1526654213666916  validation ndcg@10=0.022203793613270233 [0.33s]\n",
      "epoch 56 [7.94s]:  training loss=0.15200908482074738                                       \n",
      "epoch 57 [7.83s]:  training loss=0.1478615254163742                                        \n",
      "epoch 58 [7.88s]:  training loss=0.1509678065776825                                        \n",
      "epoch 59 [7.73s]:  training loss=0.1467711478471756                                        \n",
      "epoch 60 [7.79s]: training loss=0.15218199789524078  validation ndcg@10=0.021790454962860247 [0.35s]\n",
      "epoch 61 [9.2s]:  training loss=0.14814196527004242                                        \n",
      "epoch 62 [7.66s]:  training loss=0.14502601325511932                                       \n",
      "epoch 63 [7.48s]:  training loss=0.14507028460502625                                       \n",
      "epoch 64 [7.65s]:  training loss=0.14137591421604156                                       \n",
      "epoch 65 [7.68s]: training loss=0.14280322194099426  validation ndcg@10=0.022269036415905795 [0.33s]\n",
      "epoch 66 [7.5s]:  training loss=0.14041677117347717                                        \n",
      "epoch 67 [7.55s]:  training loss=0.1400759220123291                                        \n",
      "epoch 68 [7.67s]:  training loss=0.14013944566249847                                       \n",
      "epoch 69 [7.71s]:  training loss=0.14075788855552673                                       \n",
      "epoch 70 [7.7s]: training loss=0.13617157936096191  validation ndcg@10=0.022704775596881582 [0.31s]\n",
      "epoch 71 [7.56s]:  training loss=0.13580846786499023                                       \n",
      "epoch 72 [7.54s]:  training loss=0.13094961643218994                                       \n",
      "epoch 73 [7.55s]:  training loss=0.13730579614639282                                       \n",
      "epoch 74 [7.58s]:  training loss=0.13122226297855377                                       \n",
      "epoch 75 [7.71s]: training loss=0.1302240937948227  validation ndcg@10=0.023248386031770183 [0.31s]\n",
      "epoch 76 [7.57s]:  training loss=0.13338540494441986                                       \n",
      "epoch 77 [7.7s]:  training loss=0.13021977245807648                                        \n",
      "epoch 78 [7.81s]:  training loss=0.13017776608467102                                       \n",
      "epoch 79 [7.86s]:  training loss=0.13086935877799988                                       \n",
      "epoch 80 [7.56s]: training loss=0.1287512332201004  validation ndcg@10=0.02320291143078641 [0.3s]\n",
      "epoch 81 [7.64s]:  training loss=0.12826021015644073                                       \n",
      "epoch 82 [7.54s]:  training loss=0.1289699375629425                                        \n",
      "epoch 83 [7.56s]:  training loss=0.12505656480789185                                       \n",
      "epoch 84 [7.55s]:  training loss=0.12520380318164825                                       \n",
      "epoch 85 [7.63s]: training loss=0.12426652014255524  validation ndcg@10=0.022955800076257943 [0.31s]\n",
      "epoch 86 [7.62s]:  training loss=0.12307768315076828                                       \n",
      "epoch 87 [7.6s]:  training loss=0.1211009994149208                                         \n",
      "epoch 88 [7.49s]:  training loss=0.11845941096544266                                       \n",
      "epoch 89 [7.53s]:  training loss=0.118647500872612                                         \n",
      "epoch 90 [7.53s]: training loss=0.12052110582590103  validation ndcg@10=0.023220659093879326 [0.34s]\n",
      "epoch 91 [7.63s]:  training loss=0.11667411029338837                                       \n",
      "epoch 92 [7.46s]:  training loss=0.12040568888187408                                       \n",
      "epoch 93 [7.45s]:  training loss=0.12118467688560486                                       \n",
      "epoch 94 [7.14s]:  training loss=0.11494763195514679                                       \n",
      "epoch 95 [7.19s]: training loss=0.11731155216693878  validation ndcg@10=0.023056761081869284 [0.31s]\n",
      "epoch 96 [7.18s]:  training loss=0.1161215528845787                                        \n",
      "epoch 97 [7.19s]:  training loss=0.11828720569610596                                       \n",
      "epoch 98 [7.16s]:  training loss=0.11523138731718063                                       \n",
      "epoch 99 [7.32s]:  training loss=0.11385598033666611                                       \n",
      "epoch 100 [7.12s]: training loss=0.113752581179142  validation ndcg@10=0.023796723633912675 [0.29s]\n",
      "epoch 101 [7.17s]:  training loss=0.11154624819755554                                      \n",
      "epoch 102 [7.13s]:  training loss=0.11188247054815292                                      \n",
      "epoch 103 [7.02s]:  training loss=0.11107412725687027                                      \n",
      "epoch 104 [7.01s]:  training loss=0.11073381453752518                                      \n",
      "epoch 105 [7.14s]: training loss=0.10998168587684631  validation ndcg@10=0.02425345348601188 [0.29s]\n",
      "epoch 106 [7.05s]:  training loss=0.11246298253536224                                      \n",
      "epoch 107 [7.05s]:  training loss=0.10777679830789566                                      \n",
      "epoch 108 [7.08s]:  training loss=0.11021076142787933                                      \n",
      "epoch 109 [6.97s]:  training loss=0.10956866294145584                                      \n",
      "epoch 110 [7.02s]: training loss=0.10923835635185242  validation ndcg@10=0.02417306125877101 [0.32s]\n",
      "epoch 111 [7.16s]:  training loss=0.10766781866550446                                      \n",
      "epoch 112 [7.08s]:  training loss=0.10871436446905136                                      \n",
      "epoch 113 [7.12s]:  training loss=0.1071721613407135                                       \n",
      "epoch 114 [7.18s]:  training loss=0.10655957460403442                                      \n",
      "epoch 115 [7.09s]: training loss=0.10551882535219193  validation ndcg@10=0.023917702213706375 [0.32s]\n",
      "epoch 116 [7.13s]:  training loss=0.10605160146951675                                      \n",
      "epoch 117 [7.05s]:  training loss=0.10463040322065353                                      \n",
      "epoch 118 [7.12s]:  training loss=0.10286256670951843                                      \n",
      "epoch 119 [7.26s]:  training loss=0.100457102060318                                        \n",
      "epoch 120 [7.29s]: training loss=0.10232672095298767  validation ndcg@10=0.02455146184768102 [0.31s]\n",
      "epoch 121 [7.17s]:  training loss=0.10164967179298401                                      \n",
      "epoch 122 [7.22s]:  training loss=0.10204017907381058                                      \n",
      "epoch 123 [7.32s]:  training loss=0.10257111489772797                                      \n",
      "epoch 124 [7.25s]:  training loss=0.10166413336992264                                      \n",
      "epoch 125 [7.41s]: training loss=0.1012122631072998  validation ndcg@10=0.024870255846210117 [0.3s]\n",
      "epoch 126 [7.32s]:  training loss=0.10145686566829681                                      \n",
      "epoch 127 [7.38s]:  training loss=0.09889648854732513                                      \n",
      "epoch 128 [7.39s]:  training loss=0.09877832233905792                                      \n",
      "epoch 129 [7.4s]:  training loss=0.100459523499012                                         \n",
      "epoch 130 [7.29s]: training loss=0.09943380951881409  validation ndcg@10=0.023725443160002323 [0.31s]\n",
      "epoch 131 [7.29s]:  training loss=0.1002129465341568                                       \n",
      "epoch 132 [7.35s]:  training loss=0.09932992607355118                                      \n",
      "epoch 133 [7.34s]:  training loss=0.09493505954742432                                      \n",
      "epoch 134 [7.39s]:  training loss=0.0976819396018982                                       \n",
      "epoch 135 [7.38s]: training loss=0.09802203625440598  validation ndcg@10=0.02509015579079581 [0.33s]\n",
      "epoch 136 [7.47s]:  training loss=0.09711377322673798                                      \n",
      "epoch 137 [7.35s]:  training loss=0.09868082404136658                                      \n",
      "epoch 138 [9.04s]:  training loss=0.09571853280067444                                      \n",
      "epoch 139 [7.34s]:  training loss=0.09433360397815704                                      \n",
      "epoch 140 [7.13s]: training loss=0.09126702696084976  validation ndcg@10=0.02450082112559249 [0.32s]\n",
      "epoch 141 [7.2s]:  training loss=0.09653080999851227                                       \n",
      "epoch 142 [7.27s]:  training loss=0.09230081737041473                                      \n",
      "epoch 143 [7.15s]:  training loss=0.09462294727563858                                      \n",
      "epoch 144 [7.21s]:  training loss=0.09761451929807663                                      \n",
      "epoch 145 [7.28s]: training loss=0.09556242823600769  validation ndcg@10=0.024644340517394444 [0.31s]\n",
      "epoch 146 [7.23s]:  training loss=0.09361860901117325                                      \n",
      "epoch 147 [7.21s]:  training loss=0.092353954911232                                        \n",
      "epoch 148 [7.29s]:  training loss=0.09326442331075668                                      \n",
      "epoch 149 [7.27s]:  training loss=0.09460677951574326                                      \n",
      "epoch 150 [7.31s]: training loss=0.09132074564695358  validation ndcg@10=0.025646676213791325 [0.35s]\n",
      "epoch 151 [7.28s]:  training loss=0.09215570241212845                                      \n",
      "epoch 152 [7.31s]:  training loss=0.08936294168233871                                      \n",
      "epoch 153 [7.59s]:  training loss=0.0915549099445343                                       \n",
      "epoch 154 [7.56s]:  training loss=0.09226028621196747                                      \n",
      "epoch 155 [7.37s]: training loss=0.08818378299474716  validation ndcg@10=0.025324184168872665 [0.3s]\n",
      "epoch 156 [7.33s]:  training loss=0.09218648076057434                                      \n",
      "epoch 157 [7.38s]:  training loss=0.0892396941781044                                       \n",
      "epoch 158 [7.32s]:  training loss=0.0873134508728981                                       \n",
      "epoch 159 [7.3s]:  training loss=0.08748826384544373                                       \n",
      "epoch 160 [7.3s]: training loss=0.08770641684532166  validation ndcg@10=0.025655526818725082 [0.31s]\n",
      "epoch 161 [7.24s]:  training loss=0.0880911573767662                                       \n",
      "epoch 162 [7.37s]:  training loss=0.08973322063684464                                      \n",
      "epoch 163 [7.33s]:  training loss=0.08819202333688736                                      \n",
      "epoch 164 [7.28s]:  training loss=0.08917596191167831                                      \n",
      "epoch 165 [7.38s]: training loss=0.0880466029047966  validation ndcg@10=0.025312204550177526 [0.31s]\n",
      "epoch 166 [7.35s]:  training loss=0.08711415529251099                                      \n",
      "epoch 167 [7.24s]:  training loss=0.08666640520095825                                      \n",
      "epoch 168 [7.52s]:  training loss=0.08622725307941437                                      \n",
      "epoch 169 [7.29s]:  training loss=0.08532438427209854                                      \n",
      "epoch 170 [7.4s]: training loss=0.08532949537038803  validation ndcg@10=0.025738721725172442 [0.34s]\n",
      "epoch 171 [7.28s]:  training loss=0.0854666531085968                                       \n",
      "epoch 172 [7.36s]:  training loss=0.08457367867231369                                      \n",
      "epoch 173 [7.33s]:  training loss=0.08648688346147537                                      \n",
      "epoch 174 [7.24s]:  training loss=0.08444724977016449                                      \n",
      "epoch 175 [7.36s]: training loss=0.08441237360239029  validation ndcg@10=0.02544739460547196 [0.31s]\n",
      "epoch 176 [7.36s]:  training loss=0.08440019935369492                                      \n",
      "epoch 177 [7.33s]:  training loss=0.08146528154611588                                      \n",
      "epoch 178 [7.32s]:  training loss=0.07984516769647598                                      \n",
      "epoch 179 [7.38s]:  training loss=0.084906667470932                                        \n",
      "epoch 180 [7.31s]: training loss=0.08422750234603882  validation ndcg@10=0.02581438098265469 [0.29s]\n",
      "epoch 181 [7.43s]:  training loss=0.08112283051013947                                      \n",
      "epoch 182 [7.32s]:  training loss=0.08202142268419266                                      \n",
      "epoch 183 [7.32s]:  training loss=0.08090049028396606                                      \n",
      "epoch 184 [7.21s]:  training loss=0.08295957744121552                                      \n",
      "epoch 185 [7.25s]: training loss=0.07968487590551376  validation ndcg@10=0.026186838868016895 [0.34s]\n",
      "epoch 186 [7.24s]:  training loss=0.07964193820953369                                      \n",
      "epoch 187 [7.29s]:  training loss=0.08441738784313202                                      \n",
      "epoch 188 [7.34s]:  training loss=0.07851310819387436                                      \n",
      "epoch 189 [7.33s]:  training loss=0.07885418832302094                                      \n",
      "epoch 190 [7.3s]: training loss=0.08067519962787628  validation ndcg@10=0.0257418799960144 [0.32s]\n",
      "epoch 191 [7.29s]:  training loss=0.08057247847318649                                      \n",
      "epoch 192 [7.26s]:  training loss=0.07934076339006424                                      \n",
      "epoch 193 [7.32s]:  training loss=0.07995132356882095                                      \n",
      "epoch 194 [7.28s]:  training loss=0.07966966927051544                                      \n",
      "epoch 195 [7.31s]: training loss=0.07724140584468842  validation ndcg@10=0.02568033614683579 [0.31s]\n",
      "epoch 196 [7.43s]:  training loss=0.07921362668275833                                      \n",
      "epoch 197 [7.3s]:  training loss=0.08040232211351395                                       \n",
      "epoch 198 [7.29s]:  training loss=0.07912110537290573                                      \n",
      "epoch 199 [7.25s]:  training loss=0.0775170624256134                                       \n",
      "epoch 200 [7.35s]: training loss=0.07589627802371979  validation ndcg@10=0.02568811775732221 [0.33s]\n",
      "epoch 1 [18.16s]:  training loss=0.6356679797172546                                        \n",
      "epoch 2 [18.69s]:  training loss=0.5951870083808899                                        \n",
      "epoch 3 [18.88s]:  training loss=0.5521864295005798                                        \n",
      "epoch 4 [18.62s]:  training loss=0.5217485427856445                                        \n",
      "epoch 5 [19.37s]: training loss=0.4977530539035797  validation ndcg@10=0.004283415780307776 [0.59s]\n",
      "epoch 6 [20.5s]:  training loss=0.46807196736335754                                        \n",
      "epoch 7 [18.08s]:  training loss=0.45207878947257996                                       \n",
      "epoch 8 [19.22s]:  training loss=0.42783787846565247                                       \n",
      "epoch 9 [19.92s]:  training loss=0.4080464243888855                                        \n",
      "epoch 10 [20.01s]: training loss=0.39436474442481995  validation ndcg@10=0.006126710224320135 [0.61s]\n",
      "epoch 11 [19.94s]:  training loss=0.38065019249916077                                      \n",
      "epoch 12 [20.01s]:  training loss=0.36037319898605347                                      \n",
      "epoch 13 [20.05s]:  training loss=0.351747989654541                                        \n",
      "epoch 14 [19.86s]:  training loss=0.33836647868156433                                      \n",
      "epoch 15 [19.77s]: training loss=0.3284497857093811  validation ndcg@10=0.008932926305001603 [0.59s]\n",
      "epoch 16 [19.8s]:  training loss=0.3178560137748718                                        \n",
      "epoch 17 [20.13s]:  training loss=0.30584558844566345                                      \n",
      "epoch 18 [20.05s]:  training loss=0.29685550928115845                                      \n",
      "epoch 19 [19.84s]:  training loss=0.2885030210018158                                       \n",
      "epoch 20 [19.46s]: training loss=0.28183430433273315  validation ndcg@10=0.013659858062519174 [0.59s]\n",
      "epoch 21 [19.37s]:  training loss=0.2774026691913605                                       \n",
      "epoch 22 [19.99s]:  training loss=0.26981815695762634                                      \n",
      "epoch 23 [21.16s]:  training loss=0.26157134771347046                                      \n",
      "epoch 24 [20.44s]:  training loss=0.25325262546539307                                      \n",
      "epoch 25 [19.58s]: training loss=0.2549706995487213  validation ndcg@10=0.016483260680131052 [0.58s]\n",
      "epoch 26 [19.54s]:  training loss=0.2452772855758667                                       \n",
      "epoch 27 [19.62s]:  training loss=0.24128621816635132                                      \n",
      "epoch 28 [19.46s]:  training loss=0.23639820516109467                                      \n",
      "epoch 29 [20.08s]:  training loss=0.23353536427021027                                      \n",
      "epoch 30 [20.02s]: training loss=0.23080895841121674  validation ndcg@10=0.018254568608046605 [0.63s]\n",
      "epoch 31 [20.15s]:  training loss=0.22751204669475555                                      \n",
      "epoch 32 [20.57s]:  training loss=0.22702307999134064                                      \n",
      "epoch 33 [20.28s]:  training loss=0.22164826095104218                                      \n",
      "epoch 34 [19.86s]:  training loss=0.21815082430839539                                      \n",
      "epoch 35 [20.8s]: training loss=0.22193698585033417  validation ndcg@10=0.019164584145563813 [0.61s]\n",
      "epoch 36 [20.77s]:  training loss=0.2160782814025879                                       \n",
      "epoch 37 [20.82s]:  training loss=0.2141740620136261                                       \n",
      "epoch 38 [21.26s]:  training loss=0.20976069569587708                                      \n",
      "epoch 39 [20.61s]:  training loss=0.20943424105644226                                      \n",
      "epoch 40 [20.06s]: training loss=0.20737811923027039  validation ndcg@10=0.01980480506027936 [0.57s]\n",
      "epoch 41 [20.23s]:  training loss=0.20614252984523773                                      \n",
      "epoch 42 [19.95s]:  training loss=0.20375236868858337                                      \n",
      "epoch 43 [20.61s]:  training loss=0.20079471170902252                                      \n",
      "epoch 44 [20.91s]:  training loss=0.1973341703414917                                       \n",
      "epoch 45 [22.62s]: training loss=0.2011476755142212  validation ndcg@10=0.019667354246026583 [0.6s]\n",
      "epoch 46 [20.41s]:  training loss=0.1979687660932541                                       \n",
      "epoch 47 [20.6s]:  training loss=0.1944090574979782                                        \n",
      "epoch 48 [20.12s]:  training loss=0.1904582679271698                                       \n",
      "epoch 49 [20.23s]:  training loss=0.19238795340061188                                      \n",
      "epoch 50 [19.65s]: training loss=0.19303768873214722  validation ndcg@10=0.02014412503556026 [0.6s]\n",
      "epoch 51 [20.28s]:  training loss=0.19140098989009857                                      \n",
      "epoch 52 [19.96s]:  training loss=0.18589887022972107                                      \n",
      "epoch 53 [20.9s]:  training loss=0.1890856921672821                                        \n",
      "epoch 54 [19.46s]:  training loss=0.18652252852916718                                      \n",
      "epoch 55 [20.14s]: training loss=0.18484900891780853  validation ndcg@10=0.019636249923882656 [0.6s]\n",
      "epoch 56 [20.82s]:  training loss=0.18235020339488983                                      \n",
      "epoch 57 [20.99s]:  training loss=0.18435615301132202                                      \n",
      "epoch 58 [20.37s]:  training loss=0.1796034723520279                                       \n",
      "epoch 59 [20.79s]:  training loss=0.1789442002773285                                       \n",
      "epoch 60 [20.84s]: training loss=0.17608563601970673  validation ndcg@10=0.020144734764436283 [0.6s]\n",
      "epoch 61 [21.44s]:  training loss=0.17659242451190948                                      \n",
      "epoch 62 [21.06s]:  training loss=0.17963579297065735                                      \n",
      "epoch 63 [20.76s]:  training loss=0.1742122918367386                                       \n",
      "epoch 64 [21.36s]:  training loss=0.17410802841186523                                      \n",
      "epoch 65 [20.68s]: training loss=0.17227183282375336  validation ndcg@10=0.020203241622810203 [0.6s]\n",
      "epoch 66 [21.41s]:  training loss=0.17063795030117035                                      \n",
      "epoch 67 [20.41s]:  training loss=0.1712634414434433                                       \n",
      "epoch 68 [21.12s]:  training loss=0.17403580248355865                                      \n",
      "epoch 69 [20.02s]:  training loss=0.1651286780834198                                       \n",
      "epoch 70 [20.38s]: training loss=0.16811475157737732  validation ndcg@10=0.0204224963971262 [0.62s]\n",
      "epoch 71 [20.73s]:  training loss=0.1655726432800293                                       \n",
      "epoch 72 [20.26s]:  training loss=0.16633178293704987                                      \n",
      "epoch 73 [20.64s]:  training loss=0.1597602814435959                                       \n",
      "epoch 74 [21.1s]:  training loss=0.16549378633499146                                       \n",
      "epoch 75 [21.17s]: training loss=0.16163814067840576  validation ndcg@10=0.02040193794908722 [0.63s]\n",
      "epoch 76 [20.83s]:  training loss=0.15836243331432343                                      \n",
      "epoch 77 [21.29s]:  training loss=0.16315805912017822                                      \n",
      "epoch 78 [20.22s]:  training loss=0.16081394255161285                                      \n",
      "epoch 79 [21.16s]:  training loss=0.16197612881660461                                      \n",
      "epoch 80 [20.97s]: training loss=0.15912844240665436  validation ndcg@10=0.020485967586456415 [0.63s]\n",
      "epoch 81 [20.33s]:  training loss=0.15422259271144867                                      \n",
      "epoch 82 [22.89s]:  training loss=0.15612874925136566                                      \n",
      "epoch 83 [20.18s]:  training loss=0.1592264473438263                                       \n",
      "epoch 84 [20.63s]:  training loss=0.1548500508069992                                       \n",
      "epoch 85 [20.49s]: training loss=0.15346886217594147  validation ndcg@10=0.020394155586394944 [0.6s]\n",
      "epoch 86 [20.32s]:  training loss=0.1531226933002472                                       \n",
      "epoch 87 [20.1s]:  training loss=0.15449033677577972                                       \n",
      "epoch 88 [19.93s]:  training loss=0.1526126116514206                                       \n",
      "epoch 89 [20.29s]:  training loss=0.14940685033798218                                      \n",
      "epoch 90 [21.97s]: training loss=0.15208593010902405  validation ndcg@10=0.020116217128432496 [0.64s]\n",
      "epoch 91 [20.42s]:  training loss=0.1489502489566803                                       \n",
      "epoch 92 [20.47s]:  training loss=0.151373028755188                                        \n",
      "epoch 93 [21.45s]:  training loss=0.14683732390403748                                      \n",
      "epoch 94 [21.45s]:  training loss=0.14739365875720978                                      \n",
      "epoch 95 [19.78s]: training loss=0.14531217515468597  validation ndcg@10=0.021395524737650995 [0.61s]\n",
      "epoch 96 [21.18s]:  training loss=0.1440598964691162                                       \n",
      "epoch 97 [20.73s]:  training loss=0.14507035911083221                                      \n",
      "epoch 98 [21.09s]:  training loss=0.14077822864055634                                      \n",
      "epoch 99 [20.41s]:  training loss=0.14479994773864746                                      \n",
      "epoch 100 [20.73s]: training loss=0.1448386013507843  validation ndcg@10=0.021087711264317726 [0.65s]\n",
      "epoch 101 [20.7s]:  training loss=0.13969722390174866                                      \n",
      "epoch 102 [20.34s]:  training loss=0.13940700888633728                                     \n",
      "epoch 103 [20.9s]:  training loss=0.13845373690128326                                      \n",
      "epoch 104 [21.27s]:  training loss=0.14027446508407593                                     \n",
      "epoch 105 [20.32s]: training loss=0.13835905492305756  validation ndcg@10=0.0218940407081309 [0.64s]\n",
      "epoch 106 [20.29s]:  training loss=0.13719186186790466                                     \n",
      "epoch 107 [19.91s]:  training loss=0.1393136829137802                                      \n",
      "epoch 108 [20.35s]:  training loss=0.13683944940567017                                     \n",
      "epoch 109 [20.39s]:  training loss=0.1386350393295288                                      \n",
      "epoch 110 [20.34s]: training loss=0.13621048629283905  validation ndcg@10=0.02171662612037841 [0.58s]\n",
      "epoch 111 [19.48s]:  training loss=0.1379806101322174                                      \n",
      "epoch 112 [20.07s]:  training loss=0.1360549032688141                                      \n",
      "epoch 113 [20.17s]:  training loss=0.13425885140895844                                     \n",
      "epoch 114 [19.6s]:  training loss=0.1345265507698059                                       \n",
      "epoch 115 [19.67s]: training loss=0.13551148772239685  validation ndcg@10=0.021824365125091425 [0.6s]\n",
      "epoch 116 [20.39s]:  training loss=0.13055579364299774                                     \n",
      "epoch 117 [20.91s]:  training loss=0.12954972684383392                                     \n",
      "epoch 118 [20.06s]:  training loss=0.1298341155052185                                      \n",
      "epoch 119 [21.59s]:  training loss=0.1342913657426834                                      \n",
      "epoch 120 [20.44s]: training loss=0.12715306878089905  validation ndcg@10=0.02183301110037316 [0.63s]\n",
      "epoch 121 [20.53s]:  training loss=0.13023899495601654                                     \n",
      "epoch 122 [21.09s]:  training loss=0.12808085978031158                                     \n",
      "epoch 123 [20.67s]:  training loss=0.12862862646579742                                     \n",
      "epoch 124 [20.47s]:  training loss=0.13076458871364594                                     \n",
      "epoch 125 [20.65s]: training loss=0.12897934019565582  validation ndcg@10=0.02268648187617168 [0.6s]\n",
      "epoch 126 [20.09s]:  training loss=0.1260431706905365                                      \n",
      "epoch 127 [20.45s]:  training loss=0.12560400366783142                                     \n",
      "epoch 128 [20.62s]:  training loss=0.12419059872627258                                     \n",
      "epoch 129 [20.55s]:  training loss=0.12533161044120789                                     \n",
      "epoch 130 [21.34s]: training loss=0.12213050574064255  validation ndcg@10=0.02214788370636694 [0.61s]\n",
      "epoch 131 [20.69s]:  training loss=0.12433815002441406                                     \n",
      "epoch 132 [20.17s]:  training loss=0.13041678071022034                                     \n",
      "epoch 133 [20.22s]:  training loss=0.12443830817937851                                     \n",
      "epoch 134 [20.64s]:  training loss=0.12203384935855865                                     \n",
      "epoch 135 [19.78s]: training loss=0.12331969290971756  validation ndcg@10=0.022921461018516505 [0.58s]\n",
      "epoch 136 [20.43s]:  training loss=0.1216854602098465                                      \n",
      "epoch 137 [20.14s]:  training loss=0.12389138340950012                                     \n",
      "epoch 138 [20.33s]:  training loss=0.12063778191804886                                     \n",
      "epoch 139 [19.87s]:  training loss=0.12093882262706757                                     \n",
      "epoch 140 [19.9s]: training loss=0.12131000310182571  validation ndcg@10=0.0228596306860651 [0.6s]\n",
      "epoch 141 [20.69s]:  training loss=0.1188826858997345                                      \n",
      "epoch 142 [19.9s]:  training loss=0.11966922134160995                                      \n",
      "epoch 143 [20.41s]:  training loss=0.1189069077372551                                      \n",
      "epoch 144 [20.71s]:  training loss=0.11911168694496155                                     \n",
      "epoch 145 [20.52s]: training loss=0.11928711086511612  validation ndcg@10=0.022914700100742758 [0.61s]\n",
      "epoch 146 [19.73s]:  training loss=0.11738546192646027                                     \n",
      "epoch 147 [19.98s]:  training loss=0.11698397248983383                                     \n",
      "epoch 148 [20.29s]:  training loss=0.11665821820497513                                     \n",
      "epoch 149 [19.82s]:  training loss=0.11542927473783493                                     \n",
      "epoch 150 [20.25s]: training loss=0.11619410663843155  validation ndcg@10=0.02266081240874488 [0.61s]\n",
      "epoch 151 [19.87s]:  training loss=0.11621919274330139                                     \n",
      "epoch 152 [20.47s]:  training loss=0.11390192061662674                                     \n",
      "epoch 153 [20.8s]:  training loss=0.11416855454444885                                      \n",
      "epoch 154 [21.05s]:  training loss=0.11502867937088013                                     \n",
      "epoch 155 [20.57s]: training loss=0.11301831156015396  validation ndcg@10=0.02365572984800375 [0.59s]\n",
      "epoch 156 [21.62s]:  training loss=0.11819691210985184                                     \n",
      "epoch 157 [19.81s]:  training loss=0.11520285904407501                                     \n",
      "epoch 158 [20.01s]:  training loss=0.11491414159536362                                     \n",
      "epoch 159 [19.38s]:  training loss=0.11503376811742783                                     \n",
      "epoch 160 [20.6s]: training loss=0.11448680609464645  validation ndcg@10=0.023174668312104286 [0.6s]\n",
      "epoch 161 [19.63s]:  training loss=0.11408987641334534                                     \n",
      "epoch 162 [19.92s]:  training loss=0.11084925383329391                                     \n",
      "epoch 163 [19.54s]:  training loss=0.1116146519780159                                      \n",
      "epoch 164 [20.09s]:  training loss=0.11509137600660324                                     \n",
      "epoch 165 [19.89s]: training loss=0.11030949652194977  validation ndcg@10=0.023075024681736177 [0.61s]\n",
      "epoch 166 [19.39s]:  training loss=0.1091826781630516                                      \n",
      "epoch 167 [20.26s]:  training loss=0.10887321084737778                                     \n",
      "epoch 168 [19.93s]:  training loss=0.10637065023183823                                     \n",
      "epoch 169 [19.82s]:  training loss=0.10845104604959488                                     \n",
      "epoch 170 [19.17s]: training loss=0.10885850340127945  validation ndcg@10=0.022776007356933928 [0.6s]\n",
      "epoch 171 [20.82s]:  training loss=0.10630490630865097                                     \n",
      "epoch 172 [19.17s]:  training loss=0.10832904279232025                                     \n",
      "epoch 173 [20.83s]:  training loss=0.10812561959028244                                     \n",
      "epoch 174 [19.68s]:  training loss=0.10380324721336365                                     \n",
      "epoch 175 [19.34s]: training loss=0.10900107026100159  validation ndcg@10=0.023322111900425147 [0.6s]\n",
      "epoch 176 [20.24s]:  training loss=0.10520845651626587                                     \n",
      "epoch 177 [19.74s]:  training loss=0.1055527850985527                                      \n",
      "epoch 178 [20.17s]:  training loss=0.10691066831350327                                     \n",
      "epoch 179 [19.96s]:  training loss=0.10502473264932632                                     \n",
      "epoch 180 [20.18s]: training loss=0.10528503358364105  validation ndcg@10=0.02327146871232634 [0.62s]\n",
      "epoch 1 [11.95s]:  training loss=1.9975558519363403                                        \n",
      "epoch 2 [11.83s]:  training loss=3.166074752807617                                         \n",
      "epoch 3 [11.31s]:  training loss=3.754539728164673                                         \n",
      "epoch 4 [11.68s]:  training loss=4.205800533294678                                         \n",
      "epoch 5 [11.69s]: training loss=4.330533504486084  validation ndcg@10=0.014456329910693736 [0.45s]\n",
      "epoch 6 [11.87s]:  training loss=4.568179607391357                                         \n",
      "epoch 7 [12.06s]:  training loss=4.6572675704956055                                        \n",
      "epoch 8 [11.71s]:  training loss=4.812230110168457                                         \n",
      "epoch 9 [11.65s]:  training loss=5.304434776306152                                         \n",
      "epoch 10 [11.75s]: training loss=5.268129825592041  validation ndcg@10=0.015184312182332462 [0.42s]\n",
      "epoch 11 [11.71s]:  training loss=5.202353477478027                                        \n",
      "epoch 12 [11.37s]:  training loss=5.45901346206665                                         \n",
      "epoch 13 [11.58s]:  training loss=5.424722671508789                                        \n",
      "epoch 14 [11.57s]:  training loss=5.495321273803711                                        \n",
      "epoch 15 [11.69s]: training loss=5.612147808074951  validation ndcg@10=0.015308606261638677 [0.46s]\n",
      "epoch 16 [11.77s]:  training loss=5.623367786407471                                        \n",
      "epoch 17 [11.52s]:  training loss=6.098364353179932                                        \n",
      "epoch 18 [11.74s]:  training loss=5.8340559005737305                                       \n",
      "epoch 19 [11.38s]:  training loss=6.052711486816406                                        \n",
      "epoch 20 [11.58s]: training loss=5.749886512756348  validation ndcg@10=0.014697817525204073 [0.46s]\n",
      "epoch 21 [11.56s]:  training loss=6.751177787780762                                        \n",
      "epoch 22 [11.87s]:  training loss=6.028982162475586                                        \n",
      "epoch 23 [11.46s]:  training loss=6.6586833000183105                                       \n",
      "epoch 24 [13.02s]:  training loss=6.159484386444092                                        \n",
      "epoch 25 [11.23s]: training loss=6.1650614738464355  validation ndcg@10=0.015644081002473786 [0.38s]\n",
      "epoch 26 [11.56s]:  training loss=6.364820957183838                                        \n",
      "epoch 27 [11.46s]:  training loss=6.200184345245361                                        \n",
      "epoch 28 [11.67s]:  training loss=6.939337253570557                                        \n",
      "epoch 29 [11.56s]:  training loss=6.748660087585449                                        \n",
      "epoch 30 [11.52s]: training loss=6.626974582672119  validation ndcg@10=0.018890454447250984 [0.43s]\n",
      "epoch 31 [11.85s]:  training loss=6.47708797454834                                         \n",
      "epoch 32 [11.39s]:  training loss=6.870977401733398                                        \n",
      "epoch 33 [11.31s]:  training loss=6.797008991241455                                        \n",
      "epoch 34 [11.41s]:  training loss=6.901490688323975                                        \n",
      "epoch 35 [11.39s]: training loss=6.829174041748047  validation ndcg@10=0.015652000845541917 [0.39s]\n",
      "epoch 36 [11.34s]:  training loss=7.459794998168945                                        \n",
      "epoch 37 [11.48s]:  training loss=7.488057613372803                                        \n",
      "epoch 38 [11.32s]:  training loss=7.870641708374023                                        \n",
      "epoch 39 [11.18s]:  training loss=6.7789177894592285                                       \n",
      "epoch 40 [11.31s]: training loss=7.699656963348389  validation ndcg@10=0.018478691049211033 [0.42s]\n",
      "epoch 41 [11.13s]:  training loss=7.508946895599365                                        \n",
      "epoch 42 [11.35s]:  training loss=7.193234920501709                                        \n",
      "epoch 43 [11.6s]:  training loss=7.508059978485107                                         \n",
      "epoch 44 [11.75s]:  training loss=7.7607293128967285                                       \n",
      "epoch 45 [11.3s]: training loss=7.84445333480835  validation ndcg@10=0.016974048792487702 [0.4s]\n",
      "epoch 46 [11.16s]:  training loss=7.7472968101501465                                       \n",
      "epoch 47 [11.32s]:  training loss=7.819349765777588                                        \n",
      "epoch 48 [11.62s]:  training loss=7.779385566711426                                        \n",
      "epoch 49 [11.14s]:  training loss=7.669319152832031                                        \n",
      "epoch 50 [11.38s]: training loss=8.207320213317871  validation ndcg@10=0.016195151301321497 [0.44s]\n",
      "epoch 51 [11.25s]:  training loss=7.5474324226379395                                       \n",
      "epoch 52 [11.14s]:  training loss=7.583571910858154                                        \n",
      "epoch 53 [11.21s]:  training loss=7.987457752227783                                        \n",
      "epoch 54 [11.27s]:  training loss=7.68540096282959                                         \n",
      "epoch 55 [11.76s]: training loss=8.150978088378906  validation ndcg@10=0.016784209959911055 [0.44s]\n",
      "epoch 1 [24.64s]:  training loss=0.46005284786224365                                       \n",
      "epoch 2 [22.6s]:  training loss=0.2864358127117157                                         \n",
      "epoch 3 [22.22s]:  training loss=0.23018686473369598                                       \n",
      "epoch 4 [22.52s]:  training loss=0.2057524472475052                                        \n",
      "epoch 5 [22.83s]: training loss=0.1879880577325821  validation ndcg@10=0.02144134022177913 [0.55s]\n",
      "epoch 6 [22.68s]:  training loss=0.1757216602563858                                        \n",
      "epoch 7 [22.21s]:  training loss=0.15957671403884888                                       \n",
      "epoch 8 [22.03s]:  training loss=0.1499967724084854                                        \n",
      "epoch 9 [21.82s]:  training loss=0.14483888447284698                                       \n",
      "epoch 10 [21.82s]: training loss=0.13637641072273254  validation ndcg@10=0.02132505109582859 [0.54s]\n",
      "epoch 11 [21.71s]:  training loss=0.1277327686548233                                       \n",
      "epoch 12 [22.04s]:  training loss=0.1262541115283966                                       \n",
      "epoch 13 [22.01s]:  training loss=0.12005072832107544                                      \n",
      "epoch 14 [21.05s]:  training loss=0.11705571413040161                                      \n",
      "epoch 15 [21.35s]: training loss=0.11093146353960037  validation ndcg@10=0.024143690846139366 [0.59s]\n",
      "epoch 16 [21.28s]:  training loss=0.10970941185951233                                      \n",
      "epoch 17 [21.53s]:  training loss=0.10398611426353455                                       \n",
      "epoch 18 [21.47s]:  training loss=0.10179086029529572                                       \n",
      "epoch 19 [22.81s]:  training loss=0.0995299369096756                                        \n",
      "epoch 20 [21.24s]: training loss=0.09554597735404968  validation ndcg@10=0.02486135045575821 [0.59s]\n",
      "epoch 21 [21.37s]:  training loss=0.09675460308790207                                       \n",
      "epoch 22 [21.4s]:  training loss=0.09069941192865372                                        \n",
      "epoch 23 [21.37s]:  training loss=0.0902586430311203                                        \n",
      "epoch 24 [21.36s]:  training loss=0.08540301769971848                                       \n",
      "epoch 25 [21.27s]: training loss=0.08665193617343903  validation ndcg@10=0.025418396786905547 [0.57s]\n",
      "epoch 26 [21.18s]:  training loss=0.08272191882133484                                       \n",
      "epoch 27 [21.21s]:  training loss=0.07974734902381897                                       \n",
      "epoch 28 [21.2s]:  training loss=0.08204234391450882                                        \n",
      "epoch 29 [21.62s]:  training loss=0.07919683307409286                                       \n",
      "epoch 30 [21.23s]: training loss=0.07846168428659439  validation ndcg@10=0.025618184960223517 [0.52s]\n",
      "epoch 31 [21.51s]:  training loss=0.075973741710186                                         \n",
      "epoch 32 [21.61s]:  training loss=0.07518088072538376                                       \n",
      "epoch 33 [21.42s]:  training loss=0.07359688729047775                                       \n",
      "epoch 34 [21.34s]:  training loss=0.07054787129163742                                       \n",
      "epoch 35 [21.08s]: training loss=0.07045463472604752  validation ndcg@10=0.025470099814291262 [0.55s]\n",
      "epoch 36 [21.34s]:  training loss=0.06935089081525803                                       \n",
      "epoch 37 [21.13s]:  training loss=0.07041770219802856                                       \n",
      "epoch 38 [21.8s]:  training loss=0.06757014989852905                                        \n",
      "epoch 39 [21.28s]:  training loss=0.06709977984428406                                       \n",
      "epoch 40 [20.89s]: training loss=0.06462641060352325  validation ndcg@10=0.02539287894345076 [0.58s]\n",
      "epoch 41 [21.17s]:  training loss=0.06316330283880234                                       \n",
      "epoch 42 [21.11s]:  training loss=0.06274790316820145                                       \n",
      "epoch 43 [21.37s]:  training loss=0.06441028416156769                                       \n",
      "epoch 44 [21.38s]:  training loss=0.0629417821764946                                        \n",
      "epoch 45 [21.4s]: training loss=0.06185947731137276  validation ndcg@10=0.025735074780437735 [0.57s]\n",
      "epoch 46 [21.58s]:  training loss=0.06014836207032204                                       \n",
      "epoch 47 [21.25s]:  training loss=0.06031514331698418                                       \n",
      "epoch 48 [21.34s]:  training loss=0.05947698652744293                                       \n",
      "epoch 49 [21.45s]:  training loss=0.060665689408779144                                      \n",
      "epoch 50 [21.07s]: training loss=0.05854270979762077  validation ndcg@10=0.025406349002054428 [0.54s]\n",
      "epoch 51 [21.36s]:  training loss=0.056139010936021805                                      \n",
      "epoch 52 [21.66s]:  training loss=0.05607691779732704                                       \n",
      "epoch 53 [20.97s]:  training loss=0.054309651255607605                                      \n",
      "epoch 54 [21.7s]:  training loss=0.05729810893535614                                        \n",
      "epoch 55 [23.72s]: training loss=0.05588548257946968  validation ndcg@10=0.025290303696284534 [0.55s]\n",
      "epoch 56 [21.37s]:  training loss=0.05296891927719116                                       \n",
      "epoch 57 [21.3s]:  training loss=0.055251672863960266                                       \n",
      "epoch 58 [21.25s]:  training loss=0.05383327975869179                                       \n",
      "epoch 59 [21.56s]:  training loss=0.052841827273368835                                      \n",
      "epoch 60 [21.84s]: training loss=0.05535618215799332  validation ndcg@10=0.025290679793581534 [0.52s]\n",
      "epoch 61 [21.27s]:  training loss=0.05372115224599838                                       \n",
      "epoch 62 [21.33s]:  training loss=0.052016280591487885                                      \n",
      "epoch 63 [21.68s]:  training loss=0.050772495567798615                                      \n",
      "epoch 64 [21.55s]:  training loss=0.05008261650800705                                       \n",
      "epoch 65 [21.04s]: training loss=0.050877708941698074  validation ndcg@10=0.025678037338395725 [0.55s]\n",
      "epoch 66 [21.78s]:  training loss=0.05021495744585991                                       \n",
      "epoch 67 [21.27s]:  training loss=0.05100131407380104                                       \n",
      "epoch 68 [21.63s]:  training loss=0.049051109701395035                                      \n",
      "epoch 69 [21.34s]:  training loss=0.04693511500954628                                       \n",
      "epoch 70 [21.45s]: training loss=0.049425382167100906  validation ndcg@10=0.025608113016145292 [0.55s]\n",
      "epoch 1 [23.14s]:  training loss=0.3171049654483795                                         \n",
      "epoch 2 [23.15s]:  training loss=0.20030200481414795                                        \n",
      "epoch 3 [23.17s]:  training loss=0.1609586775302887                                         \n",
      "epoch 4 [23.16s]:  training loss=0.14264583587646484                                        \n",
      "epoch 5 [23.22s]: training loss=0.13063661754131317  validation ndcg@10=0.02204127257433829 [0.58s]\n",
      "epoch 6 [23.15s]:  training loss=0.11833403259515762                                        \n",
      "epoch 7 [23.3s]:  training loss=0.11123697459697723                                         \n",
      "epoch 8 [23.23s]:  training loss=0.1068383976817131                                         \n",
      "epoch 9 [23.11s]:  training loss=0.10041394829750061                                        \n",
      "epoch 10 [23.28s]: training loss=0.09762445837259293  validation ndcg@10=0.021781119476551362 [0.57s]\n",
      "epoch 11 [23.2s]:  training loss=0.09246846288442612                                        \n",
      "epoch 12 [23.12s]:  training loss=0.0902814194560051                                        \n",
      "epoch 13 [23.12s]:  training loss=0.08386548608541489                                       \n",
      "epoch 14 [23.03s]:  training loss=0.08302810043096542                                       \n",
      "epoch 15 [23.25s]: training loss=0.08486371487379074  validation ndcg@10=0.019741931304694028 [0.6s]\n",
      "epoch 16 [23.24s]:  training loss=0.07933321595191956                                       \n",
      "epoch 17 [23.7s]:  training loss=0.0818595439195633                                         \n",
      "epoch 18 [23.17s]:  training loss=0.07877486199140549                                       \n",
      "epoch 19 [23.0s]:  training loss=0.07203162461519241                                        \n",
      "epoch 20 [24.86s]: training loss=0.07515230029821396  validation ndcg@10=0.019548254585051876 [0.56s]\n",
      "epoch 21 [23.05s]:  training loss=0.07420704513788223                                       \n",
      "epoch 22 [23.04s]:  training loss=0.07439681142568588                                       \n",
      "epoch 23 [23.12s]:  training loss=0.06968481838703156                                       \n",
      "epoch 24 [22.98s]:  training loss=0.06930886209011078                                       \n",
      "epoch 25 [22.94s]: training loss=0.07282049208879471  validation ndcg@10=0.019840603419588366 [0.6s]\n",
      "epoch 26 [23.05s]:  training loss=0.06872882694005966                                       \n",
      "epoch 27 [22.94s]:  training loss=0.06791568547487259                                       \n",
      "epoch 28 [23.09s]:  training loss=0.07217513024806976                                       \n",
      "epoch 29 [22.94s]:  training loss=0.06914019584655762                                       \n",
      "epoch 30 [22.97s]: training loss=0.0636952742934227  validation ndcg@10=0.0216741235668069 [0.6s]\n",
      "epoch 1 [13.45s]:  training loss=0.6770113706588745                                         \n",
      "epoch 2 [14.69s]:  training loss=0.5679852366447449                                         \n",
      "epoch 3 [14.97s]:  training loss=0.49012291431427                                           \n",
      "epoch 4 [14.58s]:  training loss=0.435821533203125                                          \n",
      "epoch 5 [14.99s]: training loss=0.39301571249961853  validation ndcg@10=0.012018965121489419 [0.49s]\n",
      "epoch 6 [15.11s]:  training loss=0.35623446106910706                                        \n",
      "epoch 7 [14.84s]:  training loss=0.33045393228530884                                        \n",
      "epoch 8 [15.11s]:  training loss=0.3099535405635834                                         \n",
      "epoch 9 [14.96s]:  training loss=0.29304230213165283                                        \n",
      "epoch 10 [15.11s]: training loss=0.27860715985298157  validation ndcg@10=0.0206804800103159 [0.53s]\n",
      "epoch 11 [15.29s]:  training loss=0.27232781052589417                                       \n",
      "epoch 12 [15.04s]:  training loss=0.26148831844329834                                       \n",
      "epoch 13 [15.15s]:  training loss=0.25283998250961304                                       \n",
      "epoch 14 [15.22s]:  training loss=0.24821822345256805                                       \n",
      "epoch 15 [15.19s]: training loss=0.24105395376682281  validation ndcg@10=0.021234807096244354 [0.57s]\n",
      "epoch 16 [16.04s]:  training loss=0.24111725389957428                                       \n",
      "epoch 17 [15.3s]:  training loss=0.22760483622550964                                        \n",
      "epoch 18 [15.24s]:  training loss=0.22651995718479156                                       \n",
      "epoch 19 [15.23s]:  training loss=0.22194287180900574                                       \n",
      "epoch 20 [15.19s]: training loss=0.2216821163892746  validation ndcg@10=0.021696567921999102 [0.55s]\n",
      "epoch 21 [15.17s]:  training loss=0.21022112667560577                                       \n",
      "epoch 22 [15.11s]:  training loss=0.21347467601299286                                       \n",
      "epoch 23 [15.15s]:  training loss=0.20794789493083954                                       \n",
      "epoch 24 [15.39s]:  training loss=0.20508666336536407                                       \n",
      "epoch 25 [14.95s]: training loss=0.20229601860046387  validation ndcg@10=0.022571240638630717 [0.54s]\n",
      "epoch 26 [15.08s]:  training loss=0.195425882935524                                         \n",
      "epoch 27 [14.85s]:  training loss=0.19394515454769135                                       \n",
      "epoch 28 [15.39s]:  training loss=0.192421555519104                                         \n",
      "epoch 29 [15.06s]:  training loss=0.19267337024211884                                       \n",
      "epoch 30 [15.2s]: training loss=0.18806903064250946  validation ndcg@10=0.023209512057660947 [0.53s]\n",
      "epoch 31 [15.23s]:  training loss=0.18259820342063904                                       \n",
      "epoch 32 [14.92s]:  training loss=0.1830812692642212                                        \n",
      "epoch 33 [15.18s]:  training loss=0.17919427156448364                                       \n",
      "epoch 34 [17.14s]:  training loss=0.17704328894615173                                       \n",
      "epoch 35 [14.88s]: training loss=0.17532826960086823  validation ndcg@10=0.024019167660280952 [0.56s]\n",
      "epoch 36 [15.03s]:  training loss=0.17287427186965942                                       \n",
      "epoch 37 [14.99s]:  training loss=0.17255078256130219                                       \n",
      "epoch 38 [15.02s]:  training loss=0.1695622354745865                                        \n",
      "epoch 39 [14.97s]:  training loss=0.16783323884010315                                       \n",
      "epoch 40 [14.62s]: training loss=0.16675381362438202  validation ndcg@10=0.02450578608379812 [0.54s]\n",
      "epoch 41 [15.03s]:  training loss=0.16271474957466125                                       \n",
      "epoch 42 [15.09s]:  training loss=0.16530174016952515                                       \n",
      "epoch 43 [14.83s]:  training loss=0.16035783290863037                                       \n",
      "epoch 44 [14.87s]:  training loss=0.15936891734600067                                       \n",
      "epoch 45 [14.89s]: training loss=0.15995192527770996  validation ndcg@10=0.02427851861880615 [0.52s]\n",
      "epoch 46 [14.96s]:  training loss=0.15790443122386932                                       \n",
      "epoch 47 [15.04s]:  training loss=0.15768340229988098                                       \n",
      "epoch 48 [14.83s]:  training loss=0.1563979834318161                                        \n",
      "epoch 49 [15.19s]:  training loss=0.15239980816841125                                       \n",
      "epoch 50 [15.19s]: training loss=0.15242648124694824  validation ndcg@10=0.025339466287227354 [0.51s]\n",
      "epoch 51 [15.46s]:  training loss=0.14820940792560577                                       \n",
      "epoch 52 [16.09s]:  training loss=0.14873628318309784                                       \n",
      "epoch 53 [15.47s]:  training loss=0.14663682878017426                                       \n",
      "epoch 54 [15.48s]:  training loss=0.1456928551197052                                        \n",
      "epoch 55 [15.71s]: training loss=0.14281438291072845  validation ndcg@10=0.024744255860686723 [0.54s]\n",
      "epoch 56 [15.47s]:  training loss=0.14573247730731964                                       \n",
      "epoch 57 [15.22s]:  training loss=0.14303255081176758                                       \n",
      "epoch 58 [15.26s]:  training loss=0.1405104547739029                                        \n",
      "epoch 59 [15.68s]:  training loss=0.13734516501426697                                       \n",
      "epoch 60 [15.36s]: training loss=0.13579481840133667  validation ndcg@10=0.025396195807275437 [0.53s]\n",
      "epoch 61 [15.62s]:  training loss=0.13682234287261963                                       \n",
      "epoch 62 [15.32s]:  training loss=0.13690753281116486                                       \n",
      "epoch 63 [15.5s]:  training loss=0.1359708309173584                                         \n",
      "epoch 64 [15.13s]:  training loss=0.13261134922504425                                       \n",
      "epoch 65 [15.47s]: training loss=0.13189417123794556  validation ndcg@10=0.025944199273629565 [0.52s]\n",
      "epoch 66 [15.19s]:  training loss=0.13598594069480896                                       \n",
      "epoch 67 [15.36s]:  training loss=0.13532039523124695                                       \n",
      "epoch 68 [14.95s]:  training loss=0.13212616741657257                                       \n",
      "epoch 69 [15.0s]:  training loss=0.13253898918628693                                        \n",
      "epoch 70 [14.88s]: training loss=0.12787814438343048  validation ndcg@10=0.02497703071689565 [0.5s]\n",
      "epoch 71 [15.3s]:  training loss=0.13159894943237305                                        \n",
      "epoch 72 [15.36s]:  training loss=0.1310756504535675                                        \n",
      "epoch 73 [14.89s]:  training loss=0.1278083771467209                                        \n",
      "epoch 74 [15.12s]:  training loss=0.12881521880626678                                       \n",
      "epoch 75 [15.29s]: training loss=0.1262063831090927  validation ndcg@10=0.02493924071212049 [0.53s]\n",
      "epoch 76 [14.86s]:  training loss=0.126747727394104                                         \n",
      "epoch 77 [14.94s]:  training loss=0.1212434396147728                                        \n",
      "epoch 78 [14.78s]:  training loss=0.12259700149297714                                       \n",
      "epoch 79 [14.98s]:  training loss=0.12179986387491226                                       \n",
      "epoch 80 [15.14s]: training loss=0.1235739216208458  validation ndcg@10=0.02545928571727667 [0.5s]\n",
      "epoch 81 [14.8s]:  training loss=0.12357819825410843                                        \n",
      "epoch 82 [15.11s]:  training loss=0.12179087102413177                                       \n",
      "epoch 83 [15.09s]:  training loss=0.12032350897789001                                       \n",
      "epoch 84 [16.87s]:  training loss=0.12214962393045425                                       \n",
      "epoch 85 [15.08s]: training loss=0.11961528658866882  validation ndcg@10=0.025726418961245633 [0.51s]\n",
      "epoch 86 [14.94s]:  training loss=0.11905394494533539                                       \n",
      "epoch 87 [15.85s]:  training loss=0.11608966439962387                                       \n",
      "epoch 88 [15.12s]:  training loss=0.1187465712428093                                        \n",
      "epoch 89 [15.19s]:  training loss=0.1170286238193512                                        \n",
      "epoch 90 [15.11s]: training loss=0.11620134860277176  validation ndcg@10=0.02509185953317759 [0.54s]\n",
      "epoch 1 [9.58s]:  training loss=0.650010347366333                                           \n",
      "epoch 2 [9.29s]:  training loss=0.6464301347732544                                          \n",
      "epoch 3 [8.78s]:  training loss=0.6391268372535706                                          \n",
      "epoch 4 [9.1s]:  training loss=0.6302058696746826                                           \n",
      "epoch 5 [9.37s]: training loss=0.6271291971206665  validation ndcg@10=0.003922337840363773 [0.38s]\n",
      "epoch 6 [9.48s]:  training loss=0.6260156631469727                                          \n",
      "epoch 7 [9.09s]:  training loss=0.6138198971748352                                          \n",
      "epoch 8 [9.4s]:  training loss=0.6120986342430115                                           \n",
      "epoch 9 [9.1s]:  training loss=0.6049835085868835                                           \n",
      "epoch 10 [9.22s]: training loss=0.6043752431869507  validation ndcg@10=0.004105711638296698 [0.35s]\n",
      "epoch 11 [9.17s]:  training loss=0.5979012846946716                                         \n",
      "epoch 12 [9.11s]:  training loss=0.5881156325340271                                         \n",
      "epoch 13 [9.69s]:  training loss=0.5899237394332886                                         \n",
      "epoch 14 [9.28s]:  training loss=0.580273449420929                                          \n",
      "epoch 15 [9.28s]: training loss=0.5784720182418823  validation ndcg@10=0.0040031573606288855 [0.38s]\n",
      "epoch 16 [9.31s]:  training loss=0.576095461845398                                          \n",
      "epoch 17 [9.7s]:  training loss=0.5698069334030151                                          \n",
      "epoch 18 [9.37s]:  training loss=0.5643522143363953                                         \n",
      "epoch 19 [9.39s]:  training loss=0.5560402870178223                                         \n",
      "epoch 20 [9.58s]: training loss=0.5503359436988831  validation ndcg@10=0.004075425737047071 [0.39s]\n",
      "epoch 21 [9.2s]:  training loss=0.5479587316513062                                          \n",
      "epoch 22 [9.46s]:  training loss=0.5481505990028381                                         \n",
      "epoch 23 [9.22s]:  training loss=0.5430665016174316                                         \n",
      "epoch 24 [9.51s]:  training loss=0.5450731515884399                                         \n",
      "epoch 25 [9.34s]: training loss=0.5395147204399109  validation ndcg@10=0.004072943338035725 [0.36s]\n",
      "epoch 26 [9.39s]:  training loss=0.5297945737838745                                         \n",
      "epoch 27 [9.06s]:  training loss=0.5253954529762268                                         \n",
      "epoch 28 [9.46s]:  training loss=0.5275880098342896                                         \n",
      "epoch 29 [9.34s]:  training loss=0.5205270648002625                                         \n",
      "epoch 30 [9.59s]: training loss=0.5176962018013  validation ndcg@10=0.004116122715069273 [0.37s]\n",
      "epoch 31 [9.33s]:  training loss=0.5133732557296753                                         \n",
      "epoch 32 [9.25s]:  training loss=0.5132193565368652                                         \n",
      "epoch 33 [9.44s]:  training loss=0.508391797542572                                          \n",
      "epoch 34 [9.53s]:  training loss=0.5036614537239075                                         \n",
      "epoch 35 [9.66s]: training loss=0.49901068210601807  validation ndcg@10=0.004231378542133784 [0.35s]\n",
      "epoch 36 [9.17s]:  training loss=0.4946228861808777                                         \n",
      "epoch 37 [9.2s]:  training loss=0.49300986528396606                                         \n",
      "epoch 38 [9.31s]:  training loss=0.49114227294921875                                        \n",
      "epoch 39 [9.31s]:  training loss=0.48914721608161926                                        \n",
      "epoch 40 [9.38s]: training loss=0.48458218574523926  validation ndcg@10=0.004359778557146081 [0.39s]\n",
      "epoch 41 [9.23s]:  training loss=0.48375359177589417                                        \n",
      "epoch 42 [9.25s]:  training loss=0.48084861040115356                                        \n",
      "epoch 43 [9.47s]:  training loss=0.48089170455932617                                        \n",
      "epoch 44 [9.12s]:  training loss=0.4729021489620209                                         \n",
      "epoch 45 [9.4s]: training loss=0.4746955633163452  validation ndcg@10=0.004516769796678641 [0.38s]\n",
      "epoch 46 [9.36s]:  training loss=0.4712905287742615                                         \n",
      "epoch 47 [9.11s]:  training loss=0.4651796221733093                                         \n",
      "epoch 48 [9.38s]:  training loss=0.4619567096233368                                         \n",
      "epoch 49 [9.18s]:  training loss=0.46365872025489807                                        \n",
      "epoch 50 [9.58s]: training loss=0.4526611864566803  validation ndcg@10=0.004617479969973882 [0.34s]\n",
      "epoch 51 [9.29s]:  training loss=0.4543089270591736                                         \n",
      "epoch 52 [9.23s]:  training loss=0.4577805697917938                                         \n",
      "epoch 53 [9.11s]:  training loss=0.4486263692378998                                         \n",
      "epoch 54 [9.59s]:  training loss=0.45024746656417847                                        \n",
      "epoch 55 [9.85s]: training loss=0.44474998116493225  validation ndcg@10=0.004822123690634363 [0.37s]\n",
      "epoch 56 [9.3s]:  training loss=0.43952125310897827                                         \n",
      "epoch 57 [8.93s]:  training loss=0.4475739300251007                                         \n",
      "epoch 58 [9.14s]:  training loss=0.4399408996105194                                         \n",
      "epoch 59 [9.14s]:  training loss=0.43262478709220886                                        \n",
      "epoch 60 [8.98s]: training loss=0.43727508187294006  validation ndcg@10=0.005062641776215145 [0.34s]\n",
      "epoch 61 [8.97s]:  training loss=0.43610048294067383                                        \n",
      "epoch 62 [9.3s]:  training loss=0.4354378879070282                                          \n",
      "epoch 63 [9.23s]:  training loss=0.4316711127758026                                         \n",
      "epoch 64 [8.98s]:  training loss=0.42550039291381836                                        \n",
      "epoch 65 [9.27s]: training loss=0.42131462693214417  validation ndcg@10=0.005454557846814539 [0.4s]\n",
      "epoch 66 [8.79s]:  training loss=0.42120617628097534                                        \n",
      "epoch 67 [9.13s]:  training loss=0.41828060150146484                                        \n",
      "epoch 68 [9.16s]:  training loss=0.4143291711807251                                         \n",
      "epoch 69 [8.94s]:  training loss=0.41275379061698914                                        \n",
      "epoch 70 [9.24s]: training loss=0.41063541173934937  validation ndcg@10=0.005813022474286527 [0.35s]\n",
      "epoch 71 [9.26s]:  training loss=0.40978729724884033                                        \n",
      "epoch 72 [10.65s]:  training loss=0.4078470468521118                                        \n",
      "epoch 73 [8.9s]:  training loss=0.4072972238063812                                          \n",
      "epoch 74 [9.06s]:  training loss=0.4073270559310913                                         \n",
      "epoch 75 [8.87s]: training loss=0.40243053436279297  validation ndcg@10=0.006159570387610244 [0.38s]\n",
      "epoch 76 [9.24s]:  training loss=0.4003950357437134                                         \n",
      "epoch 77 [8.9s]:  training loss=0.39600759744644165                                         \n",
      "epoch 78 [9.17s]:  training loss=0.40031248331069946                                        \n",
      "epoch 79 [8.95s]:  training loss=0.3905007243156433                                         \n",
      "epoch 80 [9.05s]: training loss=0.3951115012168884  validation ndcg@10=0.006440737349341548 [0.35s]\n",
      "epoch 81 [8.94s]:  training loss=0.39231598377227783                                        \n",
      "epoch 82 [8.99s]:  training loss=0.3901617228984833                                         \n",
      "epoch 83 [9.16s]:  training loss=0.38630205392837524                                        \n",
      "epoch 84 [9.21s]:  training loss=0.38260743021965027                                        \n",
      "epoch 85 [9.04s]: training loss=0.37833231687545776  validation ndcg@10=0.006646436470776117 [0.38s]\n",
      "epoch 86 [9.05s]:  training loss=0.38268715143203735                                        \n",
      "epoch 87 [8.8s]:  training loss=0.38030147552490234                                         \n",
      "epoch 88 [9.3s]:  training loss=0.37939274311065674                                         \n",
      "epoch 89 [8.71s]:  training loss=0.3782092332839966                                         \n",
      "epoch 90 [9.23s]: training loss=0.37738725543022156  validation ndcg@10=0.006856505379461656 [0.35s]\n",
      "epoch 91 [9.17s]:  training loss=0.37489137053489685                                        \n",
      "epoch 92 [9.06s]:  training loss=0.3715309500694275                                         \n",
      "epoch 93 [8.94s]:  training loss=0.3714917302131653                                         \n",
      "epoch 94 [8.69s]:  training loss=0.3674810230731964                                         \n",
      "epoch 95 [9.06s]: training loss=0.36444291472435  validation ndcg@10=0.0071867905481803945 [0.39s]\n",
      "epoch 96 [9.41s]:  training loss=0.3636992573738098                                         \n",
      "epoch 97 [8.96s]:  training loss=0.36028558015823364                                        \n",
      "epoch 98 [9.29s]:  training loss=0.358298122882843                                          \n",
      "epoch 99 [9.17s]:  training loss=0.35830333828926086                                        \n",
      "epoch 100 [9.13s]: training loss=0.35889196395874023  validation ndcg@10=0.0077263314767958184 [0.36s]\n",
      "epoch 101 [9.06s]:  training loss=0.35982245206832886                                       \n",
      "epoch 102 [8.97s]:  training loss=0.35156190395355225                                       \n",
      "epoch 103 [9.11s]:  training loss=0.3529069125652313                                        \n",
      "epoch 104 [8.99s]:  training loss=0.3460817039012909                                        \n",
      "epoch 105 [9.1s]: training loss=0.34820449352264404  validation ndcg@10=0.007999314290926814 [0.35s]\n",
      "epoch 106 [8.76s]:  training loss=0.34927061200141907                                       \n",
      "epoch 107 [9.43s]:  training loss=0.3470417857170105                                        \n",
      "epoch 108 [9.33s]:  training loss=0.3461325764656067                                        \n",
      "epoch 109 [9.23s]:  training loss=0.3454749584197998                                        \n",
      "epoch 110 [9.01s]: training loss=0.3434227705001831  validation ndcg@10=0.00843370399358195 [0.37s]\n",
      "epoch 111 [9.53s]:  training loss=0.3429446220397949                                        \n",
      "epoch 112 [8.97s]:  training loss=0.34023576974868774                                       \n",
      "epoch 113 [9.54s]:  training loss=0.33782756328582764                                       \n",
      "epoch 114 [9.55s]:  training loss=0.33433839678764343                                       \n",
      "epoch 115 [8.74s]: training loss=0.336350679397583  validation ndcg@10=0.009148577953814302 [0.37s]\n",
      "epoch 116 [9.35s]:  training loss=0.3351759612560272                                        \n",
      "epoch 117 [8.97s]:  training loss=0.3300352096557617                                        \n",
      "epoch 118 [9.15s]:  training loss=0.32906436920166016                                       \n",
      "epoch 119 [9.08s]:  training loss=0.3282392919063568                                        \n",
      "epoch 120 [9.09s]: training loss=0.32434675097465515  validation ndcg@10=0.009692094119783654 [0.38s]\n",
      "epoch 121 [9.17s]:  training loss=0.3238733112812042                                        \n",
      "epoch 122 [9.13s]:  training loss=0.32192569971084595                                       \n",
      "epoch 123 [8.88s]:  training loss=0.32287144660949707                                       \n",
      "epoch 124 [8.98s]:  training loss=0.3230034410953522                                        \n",
      "epoch 125 [9.1s]: training loss=0.3205412030220032  validation ndcg@10=0.010443933981293606 [0.36s]\n",
      "epoch 126 [9.34s]:  training loss=0.31472286581993103                                       \n",
      "epoch 127 [9.25s]:  training loss=0.31884127855300903                                       \n",
      "epoch 128 [9.8s]:  training loss=0.31662845611572266                                        \n",
      "epoch 129 [9.15s]:  training loss=0.3138444721698761                                        \n",
      "epoch 130 [9.07s]: training loss=0.3142681419849396  validation ndcg@10=0.010953970483803163 [0.38s]\n",
      "epoch 131 [9.15s]:  training loss=0.3087436258792877                                        \n",
      "epoch 132 [9.01s]:  training loss=0.3142531216144562                                        \n",
      "epoch 133 [9.41s]:  training loss=0.31255802512168884                                       \n",
      "epoch 134 [9.48s]:  training loss=0.30879661440849304                                       \n",
      "epoch 135 [9.12s]: training loss=0.31351345777511597  validation ndcg@10=0.01155364412207825 [0.38s]\n",
      "epoch 136 [8.89s]:  training loss=0.3071398138999939                                        \n",
      "epoch 137 [9.15s]:  training loss=0.3079598844051361                                        \n",
      "epoch 138 [8.93s]:  training loss=0.3013018071651459                                        \n",
      "epoch 139 [9.38s]:  training loss=0.30742254853248596                                       \n",
      "epoch 140 [9.37s]: training loss=0.30354630947113037  validation ndcg@10=0.011977342416410056 [0.38s]\n",
      "epoch 141 [9.24s]:  training loss=0.30295291543006897                                       \n",
      "epoch 142 [9.12s]:  training loss=0.30267104506492615                                       \n",
      "epoch 143 [9.76s]:  training loss=0.2998618483543396                                        \n",
      "epoch 144 [9.01s]:  training loss=0.29846006631851196                                       \n",
      "epoch 145 [9.51s]: training loss=0.29630017280578613  validation ndcg@10=0.012624453183234905 [0.37s]\n",
      "epoch 146 [9.28s]:  training loss=0.2992321252822876                                        \n",
      "epoch 147 [9.59s]:  training loss=0.2951807379722595                                        \n",
      "epoch 148 [9.16s]:  training loss=0.29215672612190247                                       \n",
      "epoch 149 [9.41s]:  training loss=0.2943187654018402                                        \n",
      "epoch 150 [9.38s]: training loss=0.29478704929351807  validation ndcg@10=0.013084575318217294 [0.37s]\n",
      "epoch 151 [9.48s]:  training loss=0.2920404076576233                                        \n",
      "epoch 152 [9.17s]:  training loss=0.2912329137325287                                        \n",
      "epoch 153 [9.08s]:  training loss=0.29232311248779297                                       \n",
      "epoch 154 [9.61s]:  training loss=0.2898467481136322                                        \n",
      "epoch 155 [10.83s]: training loss=0.28730690479278564  validation ndcg@10=0.013666013164036572 [0.33s]\n",
      "epoch 156 [9.16s]:  training loss=0.2811487019062042                                        \n",
      "epoch 157 [9.17s]:  training loss=0.283598393201828                                         \n",
      "epoch 158 [9.51s]:  training loss=0.28337642550468445                                       \n",
      "epoch 159 [9.36s]:  training loss=0.284688800573349                                         \n",
      "epoch 160 [9.18s]: training loss=0.28188496828079224  validation ndcg@10=0.014081795813169764 [0.36s]\n",
      "epoch 161 [8.84s]:  training loss=0.28194957971572876                                       \n",
      "epoch 162 [9.23s]:  training loss=0.2808399498462677                                        \n",
      "epoch 163 [9.04s]:  training loss=0.2794121503829956                                        \n",
      "epoch 164 [8.95s]:  training loss=0.27554231882095337                                       \n",
      "epoch 165 [8.9s]: training loss=0.2806391716003418  validation ndcg@10=0.014707701714161029 [0.37s]\n",
      "epoch 166 [9.12s]:  training loss=0.2763468027114868                                        \n",
      "epoch 167 [8.8s]:  training loss=0.2779451012611389                                         \n",
      "epoch 168 [9.43s]:  training loss=0.2742348909378052                                        \n",
      "epoch 169 [9.19s]:  training loss=0.27285853028297424                                       \n",
      "epoch 170 [9.23s]: training loss=0.27294784784317017  validation ndcg@10=0.01512703715332415 [0.38s]\n",
      "epoch 171 [8.86s]:  training loss=0.27120721340179443                                       \n",
      "epoch 172 [9.11s]:  training loss=0.2713843882083893                                        \n",
      "epoch 173 [9.57s]:  training loss=0.27216553688049316                                       \n",
      "epoch 174 [9.19s]:  training loss=0.2739526927471161                                        \n",
      "epoch 175 [9.16s]: training loss=0.26895871758461  validation ndcg@10=0.01556331302403952 [0.34s]\n",
      "epoch 176 [9.34s]:  training loss=0.26736581325531006                                       \n",
      "epoch 177 [9.13s]:  training loss=0.27106720209121704                                       \n",
      "epoch 178 [8.95s]:  training loss=0.26702138781547546                                       \n",
      "epoch 179 [9.07s]:  training loss=0.26754313707351685                                       \n",
      "epoch 180 [8.89s]: training loss=0.26873278617858887  validation ndcg@10=0.01594940695026888 [0.37s]\n",
      "epoch 181 [9.72s]:  training loss=0.2630278170108795                                        \n",
      "epoch 182 [9.58s]:  training loss=0.26265251636505127                                       \n",
      "epoch 183 [9.41s]:  training loss=0.2603309452533722                                        \n",
      "epoch 184 [9.44s]:  training loss=0.2638952434062958                                        \n",
      "epoch 185 [9.34s]: training loss=0.2593739628791809  validation ndcg@10=0.016306371174291685 [0.37s]\n",
      "epoch 186 [9.67s]:  training loss=0.26321646571159363                                       \n",
      "epoch 187 [9.3s]:  training loss=0.26338204741477966                                        \n",
      "epoch 188 [9.41s]:  training loss=0.265032023191452                                         \n",
      "epoch 189 [8.93s]:  training loss=0.2574247121810913                                        \n",
      "epoch 190 [9.16s]: training loss=0.25873589515686035  validation ndcg@10=0.016551598841234055 [0.37s]\n",
      "epoch 191 [9.03s]:  training loss=0.2572005093097687                                        \n",
      "epoch 192 [9.37s]:  training loss=0.25930455327033997                                       \n",
      "epoch 193 [9.42s]:  training loss=0.25505322217941284                                       \n",
      "epoch 194 [9.46s]:  training loss=0.2555551826953888                                        \n",
      "epoch 195 [9.32s]: training loss=0.25575265288352966  validation ndcg@10=0.016707652419102676 [0.39s]\n",
      "epoch 196 [9.31s]:  training loss=0.2506273090839386                                        \n",
      "epoch 197 [8.78s]:  training loss=0.2549125850200653                                        \n",
      "epoch 198 [9.17s]:  training loss=0.2582654058933258                                        \n",
      "epoch 199 [9.29s]:  training loss=0.25327008962631226                                       \n",
      "epoch 200 [9.29s]: training loss=0.25693508982658386  validation ndcg@10=0.017203143053185013 [0.38s]\n",
      "epoch 1 [22.12s]:  training loss=0.4336482584476471                                         \n",
      "epoch 2 [21.67s]:  training loss=0.264724463224411                                          \n",
      "epoch 3 [22.77s]:  training loss=0.22067731618881226                                        \n",
      "epoch 4 [22.85s]:  training loss=0.19439566135406494                                        \n",
      "epoch 5 [22.12s]: training loss=0.17656391859054565  validation ndcg@10=0.019936614436880395 [0.59s]\n",
      "epoch 6 [21.69s]:  training loss=0.16022320091724396                                        \n",
      "epoch 7 [22.43s]:  training loss=0.14901040494441986                                        \n",
      "epoch 8 [22.15s]:  training loss=0.1397499144077301                                         \n",
      "epoch 9 [21.75s]:  training loss=0.12973622977733612                                        \n",
      "epoch 10 [21.03s]: training loss=0.12767910957336426  validation ndcg@10=0.021590844678648274 [0.54s]\n",
      "epoch 11 [21.65s]:  training loss=0.11856505274772644                                       \n",
      "epoch 12 [21.97s]:  training loss=0.11273931711912155                                       \n",
      "epoch 13 [22.61s]:  training loss=0.1087261438369751                                        \n",
      "epoch 14 [22.06s]:  training loss=0.10405726730823517                                       \n",
      "epoch 15 [21.42s]: training loss=0.10003793239593506  validation ndcg@10=0.025158244003821963 [0.56s]\n",
      "epoch 16 [23.75s]:  training loss=0.09961380809545517                                       \n",
      "epoch 17 [21.25s]:  training loss=0.09543308615684509                                       \n",
      "epoch 18 [21.59s]:  training loss=0.09223993867635727                                       \n",
      "epoch 19 [21.29s]:  training loss=0.08875172585248947                                       \n",
      "epoch 20 [21.33s]: training loss=0.08730074763298035  validation ndcg@10=0.02434705873722852 [0.5s]\n",
      "epoch 21 [21.55s]:  training loss=0.0819517970085144                                        \n",
      "epoch 22 [21.11s]:  training loss=0.08009056746959686                                       \n",
      "epoch 23 [20.97s]:  training loss=0.08039078116416931                                       \n",
      "epoch 24 [21.53s]:  training loss=0.07866974920034409                                       \n",
      "epoch 25 [21.4s]: training loss=0.07545205950737  validation ndcg@10=0.02648599779673207 [0.56s]\n",
      "epoch 26 [21.68s]:  training loss=0.07432085275650024                                       \n",
      "epoch 27 [20.79s]:  training loss=0.07291777431964874                                       \n",
      "epoch 28 [21.49s]:  training loss=0.07488508522510529                                       \n",
      "epoch 29 [21.7s]:  training loss=0.07123726606369019                                        \n",
      "epoch 30 [21.59s]: training loss=0.06958277523517609  validation ndcg@10=0.02515185443352369 [0.53s]\n",
      "epoch 31 [21.88s]:  training loss=0.06939422339200974                                       \n",
      "epoch 32 [21.72s]:  training loss=0.06734494119882584                                       \n",
      "epoch 33 [21.93s]:  training loss=0.06694754958152771                                       \n",
      "epoch 34 [22.0s]:  training loss=0.06341753900051117                                        \n",
      "epoch 35 [21.71s]: training loss=0.06466300785541534  validation ndcg@10=0.024910951937518064 [0.56s]\n",
      "epoch 36 [21.88s]:  training loss=0.0642319992184639                                        \n",
      "epoch 37 [21.7s]:  training loss=0.06150339916348457                                        \n",
      "epoch 38 [22.52s]:  training loss=0.06330912560224533                                       \n",
      "epoch 39 [21.73s]:  training loss=0.0596410296857357                                        \n",
      "epoch 40 [21.18s]: training loss=0.05920124426484108  validation ndcg@10=0.02477721210995845 [0.54s]\n",
      "epoch 41 [21.68s]:  training loss=0.058207228779792786                                      \n",
      "epoch 42 [20.54s]:  training loss=0.06134069710969925                                       \n",
      "epoch 43 [21.04s]:  training loss=0.058749839663505554                                      \n",
      "epoch 44 [21.56s]:  training loss=0.06053312495350838                                       \n",
      "epoch 45 [21.02s]: training loss=0.0566265694797039  validation ndcg@10=0.0247756734085683 [0.52s]\n",
      "epoch 46 [21.21s]:  training loss=0.0544450469315052                                        \n",
      "epoch 47 [21.26s]:  training loss=0.05345991626381874                                       \n",
      "epoch 48 [21.57s]:  training loss=0.05676586553454399                                       \n",
      "epoch 49 [21.59s]:  training loss=0.056364092975854874                                      \n",
      "epoch 50 [21.48s]: training loss=0.0532100573182106  validation ndcg@10=0.025481262920002155 [0.58s]\n",
      "epoch 1 [43.72s]:  training loss=0.301496297121048                                          \n",
      "epoch 2 [45.75s]:  training loss=0.20355868339538574                                       \n",
      "epoch 3 [43.63s]:  training loss=0.1705261617898941                                        \n",
      "epoch 4 [47.05s]:  training loss=0.1579371988773346                                        \n",
      "epoch 5 [46.46s]: training loss=0.15105068683624268  validation ndcg@10=0.019866183574314 [0.88s]\n",
      "epoch 6 [45.22s]:  training loss=0.14703106880187988                                       \n",
      "epoch 7 [46.54s]:  training loss=0.14068439602851868                                       \n",
      "epoch 8 [45.66s]:  training loss=0.13717225193977356                                       \n",
      "epoch 9 [47.15s]:  training loss=0.13327252864837646                                       \n",
      "epoch 10 [46.61s]: training loss=0.126820370554924  validation ndcg@10=0.0184193037073234 [0.9s]\n",
      "epoch 11 [46.35s]:  training loss=0.12742352485656738                                      \n",
      "epoch 12 [45.78s]:  training loss=0.1275288313627243                                       \n",
      "epoch 13 [45.79s]:  training loss=0.12659867107868195                                      \n",
      "epoch 14 [45.69s]:  training loss=0.12285201251506805                                      \n",
      "epoch 15 [45.86s]: training loss=0.1258530467748642  validation ndcg@10=0.019234722500954402 [0.88s]\n",
      "epoch 16 [45.47s]:  training loss=0.1263243556022644                                       \n",
      "epoch 17 [46.16s]:  training loss=0.1260540634393692                                       \n",
      "epoch 18 [46.4s]:  training loss=0.12288069725036621                                       \n",
      "epoch 19 [47.75s]:  training loss=0.11845355480909348                                      \n",
      "epoch 20 [44.58s]: training loss=0.12425002455711365  validation ndcg@10=0.016424492735623207 [0.93s]\n",
      "epoch 21 [45.85s]:  training loss=0.11886490881443024                                      \n",
      "epoch 22 [45.2s]:  training loss=0.11960188299417496                                       \n",
      "epoch 23 [45.52s]:  training loss=0.12197690457105637                                      \n",
      "epoch 24 [45.98s]:  training loss=0.12191885709762573                                      \n",
      "epoch 25 [45.41s]: training loss=0.12152460962533951  validation ndcg@10=0.0179000829140544 [0.91s]\n",
      "epoch 26 [46.11s]:  training loss=0.12257994711399078                                      \n",
      "epoch 27 [46.35s]:  training loss=0.11595489084720612                                      \n",
      "epoch 28 [46.99s]:  training loss=0.12220317125320435                                      \n",
      "epoch 29 [46.44s]:  training loss=0.1254938244819641                                       \n",
      "epoch 30 [46.17s]: training loss=0.12342597544193268  validation ndcg@10=0.018017776673347297 [0.9s]\n",
      "epoch 1 [6.73s]:  training loss=0.6816545128822327                                         \n",
      "epoch 2 [6.46s]:  training loss=0.6486985683441162                                         \n",
      "epoch 3 [6.48s]:  training loss=0.619698166847229                                          \n",
      "epoch 4 [6.38s]:  training loss=0.5913376808166504                                         \n",
      "epoch 5 [6.44s]: training loss=0.5618529319763184  validation ndcg@10=0.0037778504153193116 [0.37s]\n",
      "epoch 6 [6.33s]:  training loss=0.53892982006073                                           \n",
      "epoch 7 [6.31s]:  training loss=0.5195163488388062                                         \n",
      "epoch 8 [6.31s]:  training loss=0.49435093998908997                                        \n",
      "epoch 9 [6.28s]:  training loss=0.4827331304550171                                         \n",
      "epoch 10 [6.28s]: training loss=0.4619528651237488  validation ndcg@10=0.005343167039919601 [0.42s]\n",
      "epoch 11 [6.55s]:  training loss=0.44578349590301514                                       \n",
      "epoch 12 [6.33s]:  training loss=0.43005266785621643                                       \n",
      "epoch 13 [6.41s]:  training loss=0.4196333587169647                                        \n",
      "epoch 14 [6.41s]:  training loss=0.4017219543457031                                        \n",
      "epoch 15 [6.35s]: training loss=0.3943396210670471  validation ndcg@10=0.008894933318488106 [0.41s]\n",
      "epoch 16 [6.4s]:  training loss=0.3840288817882538                                         \n",
      "epoch 17 [6.41s]:  training loss=0.36765241622924805                                       \n",
      "epoch 18 [6.57s]:  training loss=0.363792359828949                                         \n",
      "epoch 19 [6.5s]:  training loss=0.3512375056743622                                         \n",
      "epoch 20 [6.27s]: training loss=0.3482125997543335  validation ndcg@10=0.012776703114878787 [0.43s]\n",
      "epoch 21 [6.5s]:  training loss=0.3303070068359375                                         \n",
      "epoch 22 [6.61s]:  training loss=0.3293546140193939                                        \n",
      "epoch 23 [6.42s]:  training loss=0.3191952407360077                                        \n",
      "epoch 24 [6.18s]:  training loss=0.3124719560146332                                        \n",
      "epoch 25 [6.54s]: training loss=0.30621522665023804  validation ndcg@10=0.016369645123197198 [0.37s]\n",
      "epoch 26 [6.45s]:  training loss=0.299591600894928                                         \n",
      "epoch 27 [6.64s]:  training loss=0.30034589767456055                                       \n",
      "epoch 28 [6.6s]:  training loss=0.2913510501384735                                         \n",
      "epoch 29 [6.4s]:  training loss=0.2878723442554474                                         \n",
      "epoch 30 [6.31s]: training loss=0.2834322452545166  validation ndcg@10=0.018279381086457876 [0.37s]\n",
      "epoch 31 [6.6s]:  training loss=0.2773062586784363                                         \n",
      "epoch 32 [6.58s]:  training loss=0.27402082085609436                                       \n",
      "epoch 33 [6.32s]:  training loss=0.26917245984077454                                       \n",
      "epoch 34 [6.52s]:  training loss=0.2715100049972534                                        \n",
      "epoch 35 [6.33s]: training loss=0.2643491327762604  validation ndcg@10=0.019684271282808842 [0.4s]\n",
      "epoch 36 [8.3s]:  training loss=0.2670128345489502                                         \n",
      "epoch 37 [6.1s]:  training loss=0.25943902134895325                                        \n",
      "epoch 38 [6.48s]:  training loss=0.2562898099422455                                        \n",
      "epoch 39 [6.25s]:  training loss=0.2578218877315521                                        \n",
      "epoch 40 [6.45s]: training loss=0.24827775359153748  validation ndcg@10=0.020517023648883567 [0.42s]\n",
      "epoch 41 [6.26s]:  training loss=0.2490464746952057                                        \n",
      "epoch 42 [6.46s]:  training loss=0.24713681638240814                                       \n",
      "epoch 43 [6.27s]:  training loss=0.24491220712661743                                       \n",
      "epoch 44 [6.32s]:  training loss=0.23889848589897156                                       \n",
      "epoch 45 [6.43s]: training loss=0.2404307872056961  validation ndcg@10=0.020851958738800993 [0.36s]\n",
      "epoch 46 [6.27s]:  training loss=0.236874520778656                                         \n",
      "epoch 47 [6.38s]:  training loss=0.24123486876487732                                       \n",
      "epoch 48 [6.55s]:  training loss=0.23895545303821564                                       \n",
      "epoch 49 [6.46s]:  training loss=0.23715269565582275                                       \n",
      "epoch 50 [6.32s]: training loss=0.2299359291791916  validation ndcg@10=0.021306977438607672 [0.39s]\n",
      "epoch 51 [5.9s]:  training loss=0.2327118217945099                                         \n",
      "epoch 52 [6.62s]:  training loss=0.23027823865413666                                       \n",
      "epoch 53 [6.12s]:  training loss=0.22538308799266815                                       \n",
      "epoch 54 [6.13s]:  training loss=0.22468797862529755                                       \n",
      "epoch 55 [6.41s]: training loss=0.23168733716011047  validation ndcg@10=0.021545263455405486 [0.36s]\n",
      "epoch 56 [6.36s]:  training loss=0.22259612381458282                                       \n",
      "epoch 57 [6.42s]:  training loss=0.226259246468544                                         \n",
      "epoch 58 [6.44s]:  training loss=0.21938465535640717                                       \n",
      "epoch 59 [6.47s]:  training loss=0.22087784111499786                                       \n",
      "epoch 60 [6.4s]: training loss=0.21648478507995605  validation ndcg@10=0.021651969085181116 [0.41s]\n",
      "epoch 61 [6.17s]:  training loss=0.21521474421024323                                       \n",
      "epoch 62 [6.24s]:  training loss=0.21261560916900635                                       \n",
      "epoch 63 [6.31s]:  training loss=0.2163192629814148                                        \n",
      "epoch 64 [6.39s]:  training loss=0.21240130066871643                                       \n",
      "epoch 65 [6.34s]: training loss=0.21026171743869781  validation ndcg@10=0.021662732747480276 [0.37s]\n",
      "epoch 66 [5.98s]:  training loss=0.20578476786613464                                       \n",
      "epoch 67 [6.35s]:  training loss=0.20799612998962402                                       \n",
      "epoch 68 [6.0s]:  training loss=0.21101577579975128                                        \n",
      "epoch 69 [6.6s]:  training loss=0.20571573078632355                                        \n",
      "epoch 70 [6.6s]: training loss=0.20370985567569733  validation ndcg@10=0.02165168141715495 [0.38s]\n",
      "epoch 71 [6.49s]:  training loss=0.20809626579284668                                       \n",
      "epoch 72 [6.3s]:  training loss=0.2054007649421692                                         \n",
      "epoch 73 [6.34s]:  training loss=0.20196375250816345                                       \n",
      "epoch 74 [6.24s]:  training loss=0.2006886601448059                                        \n",
      "epoch 75 [6.27s]: training loss=0.20288658142089844  validation ndcg@10=0.021331157262590685 [0.43s]\n",
      "epoch 76 [6.26s]:  training loss=0.1996520608663559                                        \n",
      "epoch 77 [6.25s]:  training loss=0.19908109307289124                                       \n",
      "epoch 78 [6.24s]:  training loss=0.19945500791072845                                       \n",
      "epoch 79 [5.99s]:  training loss=0.1972876638174057                                        \n",
      "epoch 80 [6.39s]: training loss=0.19672782719135284  validation ndcg@10=0.02157863481707445 [0.38s]\n",
      "epoch 81 [6.46s]:  training loss=0.19228217005729675                                       \n",
      "epoch 82 [6.41s]:  training loss=0.1979048103094101                                        \n",
      "epoch 83 [6.41s]:  training loss=0.19516797363758087                                       \n",
      "epoch 84 [6.25s]:  training loss=0.19546100497245789                                       \n",
      "epoch 85 [6.21s]: training loss=0.19097141921520233  validation ndcg@10=0.021933937541761393 [0.38s]\n",
      "epoch 86 [6.01s]:  training loss=0.1890605241060257                                        \n",
      "epoch 87 [6.36s]:  training loss=0.18803130090236664                                       \n",
      "epoch 88 [5.96s]:  training loss=0.18965445458889008                                       \n",
      "epoch 89 [6.43s]:  training loss=0.19098371267318726                                       \n",
      "epoch 90 [6.02s]: training loss=0.18697555363178253  validation ndcg@10=0.02140685238932411 [0.37s]\n",
      "epoch 91 [6.16s]:  training loss=0.1865926831960678                                        \n",
      "epoch 92 [6.05s]:  training loss=0.18717454373836517                                       \n",
      "epoch 93 [6.5s]:  training loss=0.18417960405349731                                        \n",
      "epoch 94 [6.16s]:  training loss=0.1840713918209076                                        \n",
      "epoch 95 [6.12s]: training loss=0.18421177566051483  validation ndcg@10=0.02166228833766786 [0.39s]\n",
      "epoch 96 [6.5s]:  training loss=0.18266187608242035                                        \n",
      "epoch 97 [6.06s]:  training loss=0.18064524233341217                                       \n",
      "epoch 98 [6.28s]:  training loss=0.18116654455661774                                       \n",
      "epoch 99 [5.99s]:  training loss=0.17986531555652618                                       \n",
      "epoch 100 [6.21s]: training loss=0.17867590487003326  validation ndcg@10=0.021916009967946445 [0.42s]\n",
      "epoch 101 [6.64s]:  training loss=0.1743868589401245                                       \n",
      "epoch 102 [6.11s]:  training loss=0.17817647755146027                                      \n",
      "epoch 103 [6.65s]:  training loss=0.17759837210178375                                      \n",
      "epoch 104 [6.07s]:  training loss=0.17860107123851776                                      \n",
      "epoch 105 [6.38s]: training loss=0.17467641830444336  validation ndcg@10=0.02187277701102169 [0.38s]\n",
      "epoch 106 [6.4s]:  training loss=0.17635703086853027                                       \n",
      "epoch 107 [6.36s]:  training loss=0.1719598025083542                                       \n",
      "epoch 108 [6.28s]:  training loss=0.17262235283851624                                      \n",
      "epoch 109 [6.51s]:  training loss=0.1760621964931488                                       \n",
      "epoch 110 [6.29s]: training loss=0.170090913772583  validation ndcg@10=0.022482877942342955 [0.4s]\n",
      "epoch 111 [6.19s]:  training loss=0.17125457525253296                                      \n",
      "epoch 112 [6.38s]:  training loss=0.17009581625461578                                      \n",
      "epoch 113 [6.39s]:  training loss=0.16842202842235565                                      \n",
      "epoch 114 [6.06s]:  training loss=0.1721123605966568                                       \n",
      "epoch 115 [6.35s]: training loss=0.16665315628051758  validation ndcg@10=0.022518901945014252 [0.42s]\n",
      "epoch 116 [6.3s]:  training loss=0.1685287207365036                                        \n",
      "epoch 117 [6.39s]:  training loss=0.1718187779188156                                       \n",
      "epoch 118 [6.08s]:  training loss=0.16987863183021545                                      \n",
      "epoch 119 [6.31s]:  training loss=0.16760951280593872                                      \n",
      "epoch 120 [6.39s]: training loss=0.16823267936706543  validation ndcg@10=0.022832800245654726 [0.35s]\n",
      "epoch 121 [6.26s]:  training loss=0.16893164813518524                                      \n",
      "epoch 122 [6.37s]:  training loss=0.16754378378391266                                      \n",
      "epoch 123 [6.14s]:  training loss=0.16044074296951294                                      \n",
      "epoch 124 [6.17s]:  training loss=0.16943860054016113                                      \n",
      "epoch 125 [6.32s]: training loss=0.16532652080059052  validation ndcg@10=0.023021695263300432 [0.38s]\n",
      "epoch 126 [6.26s]:  training loss=0.15944693982601166                                      \n",
      "epoch 127 [6.35s]:  training loss=0.16357801854610443                                      \n",
      "epoch 128 [6.31s]:  training loss=0.1601153016090393                                       \n",
      "epoch 129 [6.22s]:  training loss=0.16223491728305817                                      \n",
      "epoch 130 [6.8s]: training loss=0.15913259983062744  validation ndcg@10=0.023300552621991818 [0.39s]\n",
      "epoch 131 [6.11s]:  training loss=0.15929071605205536                                      \n",
      "epoch 132 [6.35s]:  training loss=0.15660659968852997                                      \n",
      "epoch 133 [6.09s]:  training loss=0.15893056988716125                                      \n",
      "epoch 134 [6.33s]:  training loss=0.16050532460212708                                      \n",
      "epoch 135 [6.23s]: training loss=0.15891815721988678  validation ndcg@10=0.023232649923642903 [0.4s]\n",
      "epoch 136 [6.06s]:  training loss=0.1556430608034134                                       \n",
      "epoch 137 [6.27s]:  training loss=0.15599046647548676                                      \n",
      "epoch 138 [6.26s]:  training loss=0.15671178698539734                                      \n",
      "epoch 139 [6.48s]:  training loss=0.1531652957201004                                       \n",
      "epoch 140 [6.54s]: training loss=0.15229205787181854  validation ndcg@10=0.023794232419559868 [0.37s]\n",
      "epoch 141 [6.3s]:  training loss=0.15534567832946777                                       \n",
      "epoch 142 [6.33s]:  training loss=0.15377214550971985                                      \n",
      "epoch 143 [6.13s]:  training loss=0.15007294714450836                                      \n",
      "epoch 144 [6.69s]:  training loss=0.1572168469429016                                       \n",
      "epoch 145 [6.04s]: training loss=0.15144090354442596  validation ndcg@10=0.02396874537550343 [0.4s]\n",
      "epoch 146 [6.17s]:  training loss=0.15442463755607605                                      \n",
      "epoch 147 [6.12s]:  training loss=0.14925113320350647                                      \n",
      "epoch 148 [6.23s]:  training loss=0.152884840965271                                        \n",
      "epoch 149 [6.41s]:  training loss=0.15175098180770874                                      \n",
      "epoch 150 [6.39s]: training loss=0.15289191901683807  validation ndcg@10=0.02358705385879972 [0.36s]\n",
      "epoch 151 [6.54s]:  training loss=0.1514796018600464                                       \n",
      "epoch 152 [6.26s]:  training loss=0.1498778909444809                                       \n",
      "epoch 153 [6.25s]:  training loss=0.15025945007801056                                      \n",
      "epoch 154 [6.2s]:  training loss=0.1470738649368286                                        \n",
      "epoch 155 [6.19s]: training loss=0.14425891637802124  validation ndcg@10=0.02348774744383133 [0.41s]\n",
      "epoch 156 [7.75s]:  training loss=0.14631393551826477                                      \n",
      "epoch 157 [6.05s]:  training loss=0.14812636375427246                                      \n",
      "epoch 158 [6.2s]:  training loss=0.14835470914840698                                       \n",
      "epoch 159 [6.2s]:  training loss=0.14517690241336823                                       \n",
      "epoch 160 [6.17s]: training loss=0.14494694769382477  validation ndcg@10=0.02374545695170633 [0.4s]\n",
      "epoch 161 [6.3s]:  training loss=0.14270418882369995                                       \n",
      "epoch 162 [5.93s]:  training loss=0.14550085365772247                                      \n",
      "epoch 163 [6.32s]:  training loss=0.14812254905700684                                      \n",
      "epoch 164 [6.24s]:  training loss=0.1449405699968338                                       \n",
      "epoch 165 [6.34s]: training loss=0.1434723436832428  validation ndcg@10=0.023883425523379034 [0.38s]\n",
      "epoch 166 [6.21s]:  training loss=0.14382052421569824                                      \n",
      "epoch 167 [6.6s]:  training loss=0.144365131855011                                         \n",
      "epoch 168 [6.8s]:  training loss=0.1429143249988556                                        \n",
      "epoch 169 [6.12s]:  training loss=0.14135856926441193                                      \n",
      "epoch 170 [6.52s]: training loss=0.14112599194049835  validation ndcg@10=0.023897612575673927 [0.37s]\n",
      "epoch 1 [5.54s]:  training loss=0.6371880769729614                                         \n",
      "epoch 2 [5.52s]:  training loss=0.6192289590835571                                         \n",
      "epoch 3 [5.47s]:  training loss=0.60235196352005                                           \n",
      "epoch 4 [5.49s]:  training loss=0.5900586843490601                                         \n",
      "epoch 5 [5.38s]: training loss=0.5686700940132141  validation ndcg@10=0.004246718824944405 [0.29s]\n",
      "epoch 6 [5.38s]:  training loss=0.5556305050849915                                         \n",
      "epoch 7 [5.39s]:  training loss=0.5451532602310181                                         \n",
      "epoch 8 [5.23s]:  training loss=0.5274911522865295                                         \n",
      "epoch 9 [5.04s]:  training loss=0.522903323173523                                          \n",
      "epoch 10 [5.51s]: training loss=0.5071537494659424  validation ndcg@10=0.004582058473172006 [0.3s]\n",
      "epoch 11 [5.46s]:  training loss=0.4946119487285614                                        \n",
      "epoch 12 [5.38s]:  training loss=0.4874294698238373                                        \n",
      "epoch 13 [5.42s]:  training loss=0.47333189845085144                                       \n",
      "epoch 14 [5.31s]:  training loss=0.46841493248939514                                       \n",
      "epoch 15 [5.3s]: training loss=0.4560166001319885  validation ndcg@10=0.004717399649757846 [0.28s]\n",
      "epoch 16 [5.37s]:  training loss=0.45306316018104553                                       \n",
      "epoch 17 [5.12s]:  training loss=0.44367238879203796                                       \n",
      "epoch 18 [5.34s]:  training loss=0.4368555247783661                                        \n",
      "epoch 19 [5.36s]:  training loss=0.42617636919021606                                       \n",
      "epoch 20 [5.39s]: training loss=0.4210706055164337  validation ndcg@10=0.005381555648449966 [0.31s]\n",
      "epoch 21 [5.33s]:  training loss=0.41478094458580017                                       \n",
      "epoch 22 [5.35s]:  training loss=0.40621259808540344                                       \n",
      "epoch 23 [5.32s]:  training loss=0.4035709500312805                                        \n",
      "epoch 24 [5.37s]:  training loss=0.38859182596206665                                       \n",
      "epoch 25 [5.35s]: training loss=0.3861357271671295  validation ndcg@10=0.0061708387402874786 [0.3s]\n",
      "epoch 26 [5.39s]:  training loss=0.37992408871650696                                       \n",
      "epoch 27 [5.5s]:  training loss=0.36717382073402405                                        \n",
      "epoch 28 [5.49s]:  training loss=0.3668856620788574                                        \n",
      "epoch 29 [5.15s]:  training loss=0.3612925112247467                                        \n",
      "epoch 30 [5.42s]: training loss=0.3552483022212982  validation ndcg@10=0.0070267987727377435 [0.27s]\n",
      "epoch 31 [5.03s]:  training loss=0.3491118252277374                                        \n",
      "epoch 32 [5.25s]:  training loss=0.34070703387260437                                       \n",
      "epoch 33 [5.21s]:  training loss=0.3370148241519928                                        \n",
      "epoch 34 [5.23s]:  training loss=0.33235660195350647                                       \n",
      "epoch 35 [5.18s]: training loss=0.32729214429855347  validation ndcg@10=0.008572781354035396 [0.3s]\n",
      "epoch 36 [5.25s]:  training loss=0.3298127353191376                                        \n",
      "epoch 37 [5.01s]:  training loss=0.3202764391899109                                        \n",
      "epoch 38 [5.3s]:  training loss=0.3104080557823181                                         \n",
      "epoch 39 [5.17s]:  training loss=0.31176358461380005                                       \n",
      "epoch 40 [5.21s]: training loss=0.3082170784473419  validation ndcg@10=0.010062249420313422 [0.29s]\n",
      "epoch 41 [5.19s]:  training loss=0.3056355118751526                                        \n",
      "epoch 42 [5.35s]:  training loss=0.2985474467277527                                        \n",
      "epoch 43 [5.11s]:  training loss=0.29623767733573914                                       \n",
      "epoch 44 [5.21s]:  training loss=0.2934885323047638                                        \n",
      "epoch 45 [5.22s]: training loss=0.2916559875011444  validation ndcg@10=0.011741581244311804 [0.3s]\n",
      "epoch 46 [5.3s]:  training loss=0.28434017300605774                                        \n",
      "epoch 47 [5.1s]:  training loss=0.2840178608894348                                         \n",
      "epoch 48 [5.17s]:  training loss=0.2791757881641388                                        \n",
      "epoch 49 [5.19s]:  training loss=0.27283650636672974                                       \n",
      "epoch 50 [5.23s]: training loss=0.2771289646625519  validation ndcg@10=0.013621387284569608 [0.34s]\n",
      "epoch 51 [5.12s]:  training loss=0.26819300651550293                                       \n",
      "epoch 52 [5.16s]:  training loss=0.26604071259498596                                       \n",
      "epoch 53 [5.04s]:  training loss=0.26579806208610535                                       \n",
      "epoch 54 [5.08s]:  training loss=0.2616115212440491                                        \n",
      "epoch 55 [5.28s]: training loss=0.2589831054210663  validation ndcg@10=0.015395452745339575 [0.31s]\n",
      "epoch 56 [5.25s]:  training loss=0.2547811269760132                                        \n",
      "epoch 57 [5.4s]:  training loss=0.25526487827301025                                        \n",
      "epoch 58 [5.29s]:  training loss=0.254233717918396                                         \n",
      "epoch 59 [5.12s]:  training loss=0.24924691021442413                                       \n",
      "epoch 60 [5.18s]: training loss=0.2473645657300949  validation ndcg@10=0.0167731635339654 [0.3s]\n",
      "epoch 61 [5.02s]:  training loss=0.2420554757118225                                        \n",
      "epoch 62 [5.1s]:  training loss=0.2443384975194931                                         \n",
      "epoch 63 [5.06s]:  training loss=0.24386507272720337                                       \n",
      "epoch 64 [5.08s]:  training loss=0.241228848695755                                         \n",
      "epoch 65 [5.23s]: training loss=0.236603781580925  validation ndcg@10=0.017680054781194523 [0.32s]\n",
      "epoch 66 [5.11s]:  training loss=0.2419130653142929                                        \n",
      "epoch 67 [5.12s]:  training loss=0.23581410944461823                                       \n",
      "epoch 68 [5.2s]:  training loss=0.23100033402442932                                        \n",
      "epoch 69 [5.3s]:  training loss=0.23500655591487885                                        \n",
      "epoch 70 [5.23s]: training loss=0.23235715925693512  validation ndcg@10=0.018456104441530004 [0.3s]\n",
      "epoch 71 [5.22s]:  training loss=0.23517192900180817                                       \n",
      "epoch 72 [4.91s]:  training loss=0.22903598845005035                                       \n",
      "epoch 73 [5.07s]:  training loss=0.2301432341337204                                        \n",
      "epoch 74 [5.0s]:  training loss=0.2295333296060562                                         \n",
      "epoch 75 [5.11s]: training loss=0.2253158986568451  validation ndcg@10=0.019579744070430862 [0.27s]\n",
      "epoch 76 [5.13s]:  training loss=0.22774283587932587                                       \n",
      "epoch 77 [5.05s]:  training loss=0.22423550486564636                                       \n",
      "epoch 78 [5.11s]:  training loss=0.22447173297405243                                       \n",
      "epoch 79 [4.88s]:  training loss=0.22392690181732178                                       \n",
      "epoch 80 [4.93s]: training loss=0.2216610610485077  validation ndcg@10=0.01989995727960079 [0.28s]\n",
      "epoch 81 [5.01s]:  training loss=0.22336311638355255                                       \n",
      "epoch 82 [5.05s]:  training loss=0.21739019453525543                                       \n",
      "epoch 83 [4.85s]:  training loss=0.21445663273334503                                       \n",
      "epoch 84 [5.04s]:  training loss=0.2115505188703537                                        \n",
      "epoch 85 [5.11s]: training loss=0.21492092311382294  validation ndcg@10=0.01988348732212894 [0.3s]\n",
      "epoch 86 [5.15s]:  training loss=0.21201200783252716                                       \n",
      "epoch 87 [4.89s]:  training loss=0.21093399822711945                                       \n",
      "epoch 88 [5.05s]:  training loss=0.2105455845594406                                        \n",
      "epoch 89 [4.97s]:  training loss=0.2075314074754715                                        \n",
      "epoch 90 [4.93s]: training loss=0.20743954181671143  validation ndcg@10=0.020087752464038506 [0.31s]\n",
      "epoch 91 [5.13s]:  training loss=0.2095188945531845                                        \n",
      "epoch 92 [4.96s]:  training loss=0.20315435528755188                                       \n",
      "epoch 93 [4.94s]:  training loss=0.20453158020973206                                       \n",
      "epoch 94 [4.97s]:  training loss=0.2057645320892334                                        \n",
      "epoch 95 [4.56s]: training loss=0.20855113863945007  validation ndcg@10=0.020633770587532914 [0.31s]\n",
      "epoch 96 [4.94s]:  training loss=0.20443691313266754                                       \n",
      "epoch 97 [4.96s]:  training loss=0.20362044870853424                                       \n",
      "epoch 98 [4.9s]:  training loss=0.20026016235351562                                        \n",
      "epoch 99 [4.92s]:  training loss=0.20091722905635834                                       \n",
      "epoch 100 [4.9s]: training loss=0.199843168258667  validation ndcg@10=0.0204482749969755 [0.29s]\n",
      "epoch 101 [4.83s]:  training loss=0.19931823015213013                                      \n",
      "epoch 102 [4.84s]:  training loss=0.19902321696281433                                      \n",
      "epoch 103 [4.79s]:  training loss=0.20186392962932587                                      \n",
      "epoch 104 [4.61s]:  training loss=0.19950994849205017                                      \n",
      "epoch 105 [4.69s]: training loss=0.1976379156112671  validation ndcg@10=0.021144169260749968 [0.28s]\n",
      "epoch 106 [4.41s]:  training loss=0.19635701179504395                                      \n",
      "epoch 107 [4.6s]:  training loss=0.1938675493001938                                        \n",
      "epoch 108 [4.59s]:  training loss=0.19716528058052063                                      \n",
      "epoch 109 [4.4s]:  training loss=0.19329191744327545                                       \n",
      "epoch 110 [4.49s]: training loss=0.19416625797748566  validation ndcg@10=0.020893712346611727 [0.28s]\n",
      "epoch 111 [4.6s]:  training loss=0.19599655270576477                                       \n",
      "epoch 112 [4.61s]:  training loss=0.19734841585159302                                      \n",
      "epoch 113 [4.5s]:  training loss=0.19473469257354736                                       \n",
      "epoch 114 [4.59s]:  training loss=0.18939851224422455                                      \n",
      "epoch 115 [4.57s]: training loss=0.18965275585651398  validation ndcg@10=0.020947248441047225 [0.29s]\n",
      "epoch 116 [4.74s]:  training loss=0.19197605550289154                                      \n",
      "epoch 117 [4.67s]:  training loss=0.18964000046253204                                      \n",
      "epoch 118 [4.59s]:  training loss=0.18635353446006775                                      \n",
      "epoch 119 [4.69s]:  training loss=0.19252978265285492                                      \n",
      "epoch 120 [4.74s]: training loss=0.18655294179916382  validation ndcg@10=0.021160602461667538 [0.28s]\n",
      "epoch 121 [4.6s]:  training loss=0.1878935545682907                                        \n",
      "epoch 122 [4.72s]:  training loss=0.1873127520084381                                       \n",
      "epoch 123 [4.67s]:  training loss=0.18963441252708435                                      \n",
      "epoch 124 [4.51s]:  training loss=0.1822776347398758                                       \n",
      "epoch 125 [4.59s]: training loss=0.18490096926689148  validation ndcg@10=0.020784369398372547 [0.3s]\n",
      "epoch 126 [4.55s]:  training loss=0.18700499832630157                                      \n",
      "epoch 127 [4.57s]:  training loss=0.181414395570755                                        \n",
      "epoch 128 [4.55s]:  training loss=0.1851106882095337                                       \n",
      "epoch 129 [4.63s]:  training loss=0.18218740820884705                                      \n",
      "epoch 130 [4.62s]: training loss=0.18217583000659943  validation ndcg@10=0.02166289296343242 [0.29s]\n",
      "epoch 131 [4.52s]:  training loss=0.1809811145067215                                       \n",
      "epoch 132 [4.57s]:  training loss=0.18145766854286194                                      \n",
      "epoch 133 [4.56s]:  training loss=0.18017764389514923                                      \n",
      "epoch 134 [4.54s]:  training loss=0.18580518662929535                                      \n",
      "epoch 135 [6.68s]: training loss=0.17864342033863068  validation ndcg@10=0.021356939633662416 [0.27s]\n",
      "epoch 136 [4.22s]:  training loss=0.17574253678321838                                      \n",
      "epoch 137 [4.47s]:  training loss=0.1790584921836853                                       \n",
      "epoch 138 [4.45s]:  training loss=0.18116626143455505                                      \n",
      "epoch 139 [4.53s]:  training loss=0.17808064818382263                                      \n",
      "epoch 140 [4.56s]: training loss=0.1753198504447937  validation ndcg@10=0.021789161031215278 [0.31s]\n",
      "epoch 141 [4.54s]:  training loss=0.17734099924564362                                      \n",
      "epoch 142 [4.52s]:  training loss=0.17791663110256195                                      \n",
      "epoch 143 [4.55s]:  training loss=0.17415009438991547                                      \n",
      "epoch 144 [4.48s]:  training loss=0.17425571382045746                                      \n",
      "epoch 145 [4.5s]: training loss=0.1770159751176834  validation ndcg@10=0.021814114639391913 [0.29s]\n",
      "epoch 146 [4.51s]:  training loss=0.17434395849704742                                      \n",
      "epoch 147 [4.56s]:  training loss=0.17206236720085144                                      \n",
      "epoch 148 [4.54s]:  training loss=0.1749858558177948                                       \n",
      "epoch 149 [4.57s]:  training loss=0.1709066927433014                                       \n",
      "epoch 150 [4.53s]: training loss=0.1731615513563156  validation ndcg@10=0.02146909477976267 [0.29s]\n",
      "epoch 151 [4.54s]:  training loss=0.169645756483078                                        \n",
      "epoch 152 [4.49s]:  training loss=0.1685798466205597                                       \n",
      "epoch 153 [4.63s]:  training loss=0.17091135680675507                                      \n",
      "epoch 154 [4.56s]:  training loss=0.16606463491916656                                      \n",
      "epoch 155 [4.47s]: training loss=0.1693776547908783  validation ndcg@10=0.02153214143062407 [0.28s]\n",
      "epoch 156 [4.52s]:  training loss=0.16973914206027985                                      \n",
      "epoch 157 [4.58s]:  training loss=0.16766640543937683                                      \n",
      "epoch 158 [4.55s]:  training loss=0.16686533391475677                                      \n",
      "epoch 159 [4.64s]:  training loss=0.16651295125484467                                      \n",
      "epoch 160 [4.55s]: training loss=0.16951292753219604  validation ndcg@10=0.02138410848483688 [0.28s]\n",
      "epoch 161 [4.5s]:  training loss=0.16801024973392487                                       \n",
      "epoch 162 [4.6s]:  training loss=0.16627062857151031                                       \n",
      "epoch 163 [4.55s]:  training loss=0.1673547774553299                                       \n",
      "epoch 164 [4.56s]:  training loss=0.16619695723056793                                      \n",
      "epoch 165 [4.46s]: training loss=0.16238676011562347  validation ndcg@10=0.02126951596756011 [0.27s]\n",
      "epoch 166 [4.54s]:  training loss=0.16575603187084198                                      \n",
      "epoch 167 [4.69s]:  training loss=0.16642774641513824                                      \n",
      "epoch 168 [4.54s]:  training loss=0.16585609316825867                                      \n",
      "epoch 169 [4.53s]:  training loss=0.16227199137210846                                      \n",
      "epoch 170 [4.39s]: training loss=0.16252878308296204  validation ndcg@10=0.021435681789326307 [0.27s]\n",
      "epoch 1 [35.59s]:  training loss=0.6430135369300842                                        \n",
      "epoch 2 [41.1s]:  training loss=0.6243640780448914                                         \n",
      "epoch 3 [40.63s]:  training loss=0.6123335957527161                                        \n",
      "epoch 4 [40.78s]:  training loss=0.5946900844573975                                        \n",
      "epoch 5 [41.06s]: training loss=0.5816517472267151  validation ndcg@10=0.0035983768110541244 [0.84s]\n",
      "epoch 6 [39.8s]:  training loss=0.5686715841293335                                         \n",
      "epoch 7 [40.92s]:  training loss=0.550523042678833                                         \n",
      "epoch 8 [41.07s]:  training loss=0.5434020161628723                                        \n",
      "epoch 9 [40.93s]:  training loss=0.5307338833808899                                        \n",
      "epoch 10 [40.85s]: training loss=0.5215738415718079  validation ndcg@10=0.0038778237883768506 [0.84s]\n",
      "epoch 11 [39.37s]:  training loss=0.5085594654083252                                       \n",
      "epoch 12 [39.75s]:  training loss=0.4956270456314087                                       \n",
      "epoch 13 [40.3s]:  training loss=0.4923580586910248                                        \n",
      "epoch 14 [39.55s]:  training loss=0.4809650182723999                                       \n",
      "epoch 15 [40.44s]: training loss=0.4668324887752533  validation ndcg@10=0.004463896909845012 [0.84s]\n",
      "epoch 16 [42.74s]:  training loss=0.4640446901321411                                       \n",
      "epoch 17 [39.75s]:  training loss=0.45558542013168335                                      \n",
      "epoch 18 [40.77s]:  training loss=0.45458370447158813                                      \n",
      "epoch 19 [40.76s]:  training loss=0.4424733817577362                                       \n",
      "epoch 20 [41.34s]: training loss=0.43925967812538147  validation ndcg@10=0.00509406895658855 [0.82s]\n",
      "epoch 21 [40.65s]:  training loss=0.4289560616016388                                       \n",
      "epoch 22 [40.5s]:  training loss=0.4248230457305908                                        \n",
      "epoch 23 [40.94s]:  training loss=0.41538602113723755                                      \n",
      "epoch 24 [40.22s]:  training loss=0.4100494980812073                                       \n",
      "epoch 25 [39.77s]: training loss=0.4066394865512848  validation ndcg@10=0.005693693610422365 [0.84s]\n",
      "epoch 26 [40.3s]:  training loss=0.399164080619812                                         \n",
      "epoch 27 [41.07s]:  training loss=0.3946753740310669                                       \n",
      "epoch 28 [40.97s]:  training loss=0.38934892416000366                                      \n",
      "epoch 29 [40.48s]:  training loss=0.38216152787208557                                      \n",
      "epoch 30 [40.35s]: training loss=0.37807559967041016  validation ndcg@10=0.00621401773657014 [0.86s]\n",
      "epoch 31 [41.11s]:  training loss=0.3762940466403961                                       \n",
      "epoch 32 [39.76s]:  training loss=0.3680974543094635                                       \n",
      "epoch 33 [40.92s]:  training loss=0.361770361661911                                        \n",
      "epoch 34 [40.48s]:  training loss=0.35814931988716125                                      \n",
      "epoch 35 [40.24s]: training loss=0.354203999042511  validation ndcg@10=0.0078808883992466 [0.83s]\n",
      "epoch 36 [41.89s]:  training loss=0.3501081168651581                                       \n",
      "epoch 37 [39.8s]:  training loss=0.35017111897468567                                       \n",
      "epoch 38 [40.4s]:  training loss=0.3446924388408661                                        \n",
      "epoch 39 [40.38s]:  training loss=0.33813604712486267                                      \n",
      "epoch 40 [40.65s]: training loss=0.33804523944854736  validation ndcg@10=0.009590724408064891 [0.84s]\n",
      "epoch 41 [40.33s]:  training loss=0.32305270433425903                                      \n",
      "epoch 42 [40.2s]:  training loss=0.3303035795688629                                        \n",
      "epoch 43 [41.11s]:  training loss=0.3251492977142334                                       \n",
      "epoch 44 [40.78s]:  training loss=0.3193739950656891                                       \n",
      "epoch 45 [41.08s]: training loss=0.3179602324962616  validation ndcg@10=0.011373863351914216 [0.81s]\n",
      "epoch 46 [40.45s]:  training loss=0.3138482868671417                                       \n",
      "epoch 47 [40.98s]:  training loss=0.31037408113479614                                      \n",
      "epoch 48 [40.57s]:  training loss=0.30477261543273926                                      \n",
      "epoch 49 [40.63s]:  training loss=0.29966992139816284                                      \n",
      "epoch 50 [40.11s]: training loss=0.30373290181159973  validation ndcg@10=0.012977012063748503 [0.83s]\n",
      "epoch 51 [40.23s]:  training loss=0.2964766323566437                                       \n",
      "epoch 52 [40.84s]:  training loss=0.2935641407966614                                       \n",
      "epoch 53 [39.92s]:  training loss=0.29196980595588684                                      \n",
      "epoch 54 [40.51s]:  training loss=0.287086546421051                                        \n",
      "epoch 55 [43.0s]: training loss=0.2844008207321167  validation ndcg@10=0.014662869146712726 [0.84s]\n",
      "epoch 56 [40.17s]:  training loss=0.28152990341186523                                      \n",
      "epoch 57 [40.42s]:  training loss=0.279224693775177                                        \n",
      "epoch 58 [40.54s]:  training loss=0.2741307318210602                                       \n",
      "epoch 59 [39.64s]:  training loss=0.27781251072883606                                      \n",
      "epoch 60 [41.19s]: training loss=0.2757489085197449  validation ndcg@10=0.015888269275816557 [0.84s]\n",
      "epoch 61 [40.43s]:  training loss=0.2694299519062042                                       \n",
      "epoch 62 [40.56s]:  training loss=0.26411083340644836                                      \n",
      "epoch 63 [40.89s]:  training loss=0.26869821548461914                                      \n",
      "epoch 64 [40.76s]:  training loss=0.2614749073982239                                       \n",
      "epoch 65 [40.15s]: training loss=0.26403188705444336  validation ndcg@10=0.01671506924395106 [0.85s]\n",
      "epoch 66 [40.76s]:  training loss=0.26182201504707336                                      \n",
      "epoch 67 [40.47s]:  training loss=0.2605442404747009                                       \n",
      "epoch 68 [40.42s]:  training loss=0.255999356508255                                        \n",
      "epoch 69 [40.91s]:  training loss=0.2528080344200134                                       \n",
      "epoch 70 [39.53s]: training loss=0.2505643367767334  validation ndcg@10=0.017420382922910482 [0.83s]\n",
      "epoch 71 [40.57s]:  training loss=0.2507743239402771                                       \n",
      "epoch 72 [39.98s]:  training loss=0.25321292877197266                                      \n",
      "epoch 73 [40.66s]:  training loss=0.25013378262519836                                      \n",
      "epoch 74 [39.69s]:  training loss=0.24695181846618652                                      \n",
      "epoch 75 [41.78s]: training loss=0.2504848539829254  validation ndcg@10=0.018238305493338682 [0.82s]\n",
      "epoch 76 [40.1s]:  training loss=0.24648736417293549                                       \n",
      "epoch 77 [40.2s]:  training loss=0.2466973215341568                                        \n",
      "epoch 78 [40.96s]:  training loss=0.23913560807704926                                      \n",
      "epoch 79 [40.62s]:  training loss=0.23804931342601776                                      \n",
      "epoch 80 [40.27s]: training loss=0.23954693973064423  validation ndcg@10=0.018736063176596974 [0.93s]\n",
      "epoch 81 [40.86s]:  training loss=0.23426011204719543                                      \n",
      "epoch 82 [40.11s]:  training loss=0.23834258317947388                                      \n",
      "epoch 83 [39.3s]:  training loss=0.2365531474351883                                        \n",
      "epoch 84 [40.75s]:  training loss=0.23560599982738495                                      \n",
      "epoch 85 [39.66s]: training loss=0.2284790575504303  validation ndcg@10=0.019322927874690372 [0.83s]\n",
      "epoch 86 [39.55s]:  training loss=0.2304815649986267                                       \n",
      "epoch 87 [41.6s]:  training loss=0.23104289174079895                                       \n",
      "epoch 88 [41.13s]:  training loss=0.23215454816818237                                      \n",
      "epoch 89 [39.83s]:  training loss=0.22886347770690918                                      \n",
      "epoch 90 [40.68s]: training loss=0.22628724575042725  validation ndcg@10=0.019573572644366665 [0.87s]\n",
      "epoch 91 [41.29s]:  training loss=0.22418715059757233                                      \n",
      "epoch 92 [41.21s]:  training loss=0.22347822785377502                                      \n",
      "epoch 93 [38.25s]:  training loss=0.2292313426733017                                       \n",
      "epoch 94 [41.13s]:  training loss=0.22485463321208954                                      \n",
      "epoch 95 [40.31s]: training loss=0.22661973536014557  validation ndcg@10=0.019322800724204166 [0.81s]\n",
      "epoch 96 [39.69s]:  training loss=0.22181503474712372                                      \n",
      "epoch 97 [40.82s]:  training loss=0.22263041138648987                                      \n",
      "epoch 98 [39.91s]:  training loss=0.2221156656742096                                       \n",
      "epoch 99 [40.63s]:  training loss=0.22171010076999664                                      \n",
      "epoch 100 [40.79s]: training loss=0.21813653409481049  validation ndcg@10=0.019818292540158655 [0.86s]\n",
      "epoch 101 [41.91s]:  training loss=0.21834224462509155                                     \n",
      "epoch 102 [39.81s]:  training loss=0.21703387796878815                                     \n",
      "epoch 103 [40.37s]:  training loss=0.21847444772720337                                     \n",
      "epoch 104 [41.04s]:  training loss=0.21298034489154816                                     \n",
      "epoch 105 [40.43s]: training loss=0.21725881099700928  validation ndcg@10=0.01950343564210848 [0.82s]\n",
      "epoch 106 [39.74s]:  training loss=0.21228687465190887                                     \n",
      "epoch 107 [40.95s]:  training loss=0.21000456809997559                                     \n",
      "epoch 108 [40.09s]:  training loss=0.213395893573761                                       \n",
      "epoch 109 [40.75s]:  training loss=0.21368931233882904                                     \n",
      "epoch 110 [41.4s]: training loss=0.21027253568172455  validation ndcg@10=0.02001563523413776 [0.89s]\n",
      "epoch 111 [40.4s]:  training loss=0.21036387979984283                                      \n",
      "epoch 112 [40.88s]:  training loss=0.2100253403186798                                      \n",
      "epoch 113 [40.61s]:  training loss=0.21012622117996216                                     \n",
      "epoch 114 [43.15s]:  training loss=0.20903867483139038                                     \n",
      "epoch 115 [39.87s]: training loss=0.207082599401474  validation ndcg@10=0.019451350991563945 [0.84s]\n",
      "epoch 116 [39.8s]:  training loss=0.20905868709087372                                      \n",
      "epoch 117 [40.81s]:  training loss=0.20459936559200287                                     \n",
      "epoch 118 [40.34s]:  training loss=0.2093084156513214                                      \n",
      "epoch 119 [40.75s]:  training loss=0.2044772207736969                                      \n",
      "epoch 120 [40.76s]: training loss=0.20685596764087677  validation ndcg@10=0.020002569554756867 [0.85s]\n",
      "epoch 121 [39.8s]:  training loss=0.20537211000919342                                      \n",
      "epoch 122 [40.25s]:  training loss=0.20379289984703064                                     \n",
      "epoch 123 [40.74s]:  training loss=0.20334860682487488                                     \n",
      "epoch 124 [40.56s]:  training loss=0.20304040610790253                                     \n",
      "epoch 125 [40.65s]: training loss=0.2077590972185135  validation ndcg@10=0.020063336396747684 [0.82s]\n",
      "epoch 126 [41.21s]:  training loss=0.2001265585422516                                      \n",
      "epoch 127 [41.65s]:  training loss=0.2017495036125183                                      \n",
      "epoch 128 [39.86s]:  training loss=0.19936378300189972                                     \n",
      "epoch 129 [40.14s]:  training loss=0.1983535885810852                                      \n",
      "epoch 130 [40.11s]: training loss=0.20145587623119354  validation ndcg@10=0.0194729367621975 [0.85s]\n",
      "epoch 131 [40.26s]:  training loss=0.20086020231246948                                     \n",
      "epoch 132 [40.04s]:  training loss=0.1963985413312912                                      \n",
      "epoch 133 [39.99s]:  training loss=0.19986902177333832                                     \n",
      "epoch 134 [41.14s]:  training loss=0.19907031953334808                                     \n",
      "epoch 135 [40.58s]: training loss=0.1951696276664734  validation ndcg@10=0.020292156684015475 [0.85s]\n",
      "epoch 136 [40.62s]:  training loss=0.20056858658790588                                     \n",
      "epoch 137 [40.91s]:  training loss=0.198906809091568                                       \n",
      "epoch 138 [40.71s]:  training loss=0.1946234107017517                                      \n",
      "epoch 139 [40.98s]:  training loss=0.19557002186775208                                     \n",
      "epoch 140 [40.38s]: training loss=0.19412721693515778  validation ndcg@10=0.020019483697923467 [0.85s]\n",
      "epoch 141 [42.17s]:  training loss=0.1936561018228531                                      \n",
      "epoch 142 [41.17s]:  training loss=0.19386126101016998                                     \n",
      "epoch 143 [40.94s]:  training loss=0.19498422741889954                                     \n",
      "epoch 144 [41.04s]:  training loss=0.19288191199302673                                     \n",
      "epoch 145 [40.19s]: training loss=0.19179126620292664  validation ndcg@10=0.02029429481808366 [0.84s]\n",
      "epoch 146 [40.92s]:  training loss=0.19254782795906067                                     \n",
      "epoch 147 [40.78s]:  training loss=0.19401097297668457                                     \n",
      "epoch 148 [40.08s]:  training loss=0.19208623468875885                                     \n",
      "epoch 149 [40.61s]:  training loss=0.19275151193141937                                     \n",
      "epoch 150 [40.74s]: training loss=0.18945057690143585  validation ndcg@10=0.019824244995752256 [0.87s]\n",
      "epoch 151 [41.01s]:  training loss=0.19139602780342102                                     \n",
      "epoch 152 [41.68s]:  training loss=0.1879311352968216                                      \n",
      "epoch 153 [41.64s]:  training loss=0.18836617469787598                                     \n",
      "epoch 154 [40.87s]:  training loss=0.18578146398067474                                     \n",
      "epoch 155 [40.13s]: training loss=0.18437790870666504  validation ndcg@10=0.020183464177279124 [0.85s]\n",
      "epoch 156 [40.97s]:  training loss=0.18500438332557678                                     \n",
      "epoch 157 [40.21s]:  training loss=0.1872432827949524                                      \n",
      "epoch 158 [40.79s]:  training loss=0.18711380660533905                                     \n",
      "epoch 159 [40.76s]:  training loss=0.18827731907367706                                     \n",
      "epoch 160 [40.61s]: training loss=0.18322451412677765  validation ndcg@10=0.01992895057140902 [0.84s]\n",
      "epoch 161 [40.77s]:  training loss=0.1859290599822998                                      \n",
      "epoch 162 [40.97s]:  training loss=0.18417492508888245                                     \n",
      "epoch 163 [40.29s]:  training loss=0.18622353672981262                                     \n",
      "epoch 164 [40.73s]:  training loss=0.18464413285255432                                     \n",
      "epoch 165 [41.06s]: training loss=0.18775445222854614  validation ndcg@10=0.020020214953783793 [0.85s]\n",
      "epoch 166 [40.54s]:  training loss=0.18543510138988495                                     \n",
      "epoch 167 [41.21s]:  training loss=0.18440720438957214                                     \n",
      "epoch 168 [40.53s]:  training loss=0.18237043917179108                                     \n",
      "epoch 169 [40.39s]:  training loss=0.1833878457546234                                      \n",
      "epoch 170 [40.51s]: training loss=0.18090662360191345  validation ndcg@10=0.0199110502242421 [0.84s]\n",
      "epoch 1 [11.47s]:  training loss=0.6045681834220886                                         \n",
      "epoch 2 [12.35s]:  training loss=0.5240497589111328                                         \n",
      "epoch 3 [10.97s]:  training loss=0.4779145419597626                                         \n",
      "epoch 4 [11.09s]:  training loss=0.43391987681388855                                        \n",
      "epoch 5 [10.81s]: training loss=0.40188682079315186  validation ndcg@10=0.005939737494285405 [0.41s]\n",
      "epoch 6 [11.21s]:  training loss=0.3728099465370178                                         \n",
      "epoch 7 [11.27s]:  training loss=0.34820327162742615                                        \n",
      "epoch 8 [10.31s]:  training loss=0.3276023864746094                                         \n",
      "epoch 9 [10.41s]:  training loss=0.3023449778556824                                         \n",
      "epoch 10 [10.78s]: training loss=0.2921482026576996  validation ndcg@10=0.01197721743156857 [0.39s]\n",
      "epoch 11 [10.34s]:  training loss=0.27439606189727783                                       \n",
      "epoch 12 [10.77s]:  training loss=0.2618102729320526                                        \n",
      "epoch 13 [10.7s]:  training loss=0.2510615587234497                                         \n",
      "epoch 14 [10.87s]:  training loss=0.24163733422756195                                       \n",
      "epoch 15 [10.54s]: training loss=0.23545344173908234  validation ndcg@10=0.017341502547692792 [0.45s]\n",
      "epoch 16 [10.58s]:  training loss=0.23118393123149872                                       \n",
      "epoch 17 [10.57s]:  training loss=0.22284188866615295                                       \n",
      "epoch 18 [10.92s]:  training loss=0.21458140015602112                                       \n",
      "epoch 19 [10.51s]:  training loss=0.21273469924926758                                       \n",
      "epoch 20 [10.36s]: training loss=0.21021632850170135  validation ndcg@10=0.019669331738103257 [0.41s]\n",
      "epoch 21 [9.99s]:  training loss=0.20237374305725098                                        \n",
      "epoch 22 [10.25s]:  training loss=0.200679212808609                                         \n",
      "epoch 23 [10.26s]:  training loss=0.19694216549396515                                       \n",
      "epoch 24 [10.28s]:  training loss=0.1934620589017868                                        \n",
      "epoch 25 [10.08s]: training loss=0.19224081933498383  validation ndcg@10=0.020048129166214936 [0.4s]\n",
      "epoch 26 [9.74s]:  training loss=0.18614722788333893                                        \n",
      "epoch 27 [9.43s]:  training loss=0.18297603726387024                                        \n",
      "epoch 28 [9.8s]:  training loss=0.18256482481956482                                         \n",
      "epoch 29 [9.63s]:  training loss=0.18112409114837646                                        \n",
      "epoch 30 [9.83s]: training loss=0.17758560180664062  validation ndcg@10=0.021154648458941723 [0.37s]\n",
      "epoch 31 [9.63s]:  training loss=0.17457151412963867                                        \n",
      "epoch 32 [9.91s]:  training loss=0.1710081547498703                                         \n",
      "epoch 33 [9.72s]:  training loss=0.17044468224048615                                        \n",
      "epoch 34 [9.92s]:  training loss=0.17191247642040253                                        \n",
      "epoch 35 [9.62s]: training loss=0.16690991818904877  validation ndcg@10=0.021302742787913383 [0.37s]\n",
      "epoch 36 [10.01s]:  training loss=0.16312429308891296                                       \n",
      "epoch 37 [9.89s]:  training loss=0.16132335364818573                                        \n",
      "epoch 38 [9.69s]:  training loss=0.16636812686920166                                        \n",
      "epoch 39 [9.73s]:  training loss=0.16031868755817413                                        \n",
      "epoch 40 [9.78s]: training loss=0.1590758115053177  validation ndcg@10=0.021982047050340237 [0.39s]\n",
      "epoch 41 [10.2s]:  training loss=0.1568196564912796                                         \n",
      "epoch 42 [10.18s]:  training loss=0.15699069201946259                                       \n",
      "epoch 43 [9.87s]:  training loss=0.15513679385185242                                        \n",
      "epoch 44 [9.91s]:  training loss=0.1517348289489746                                         \n",
      "epoch 45 [10.53s]: training loss=0.15065273642539978  validation ndcg@10=0.021706635380949038 [0.38s]\n",
      "epoch 46 [10.58s]:  training loss=0.14766256511211395                                       \n",
      "epoch 47 [9.59s]:  training loss=0.14408057928085327                                        \n",
      "epoch 48 [9.72s]:  training loss=0.14621612429618835                                        \n",
      "epoch 49 [10.34s]:  training loss=0.14200586080551147                                       \n",
      "epoch 50 [10.02s]: training loss=0.14486372470855713  validation ndcg@10=0.022320468401738798 [0.36s]\n",
      "epoch 51 [10.02s]:  training loss=0.14181259274482727                                       \n",
      "epoch 52 [9.58s]:  training loss=0.14150279760360718                                        \n",
      "epoch 53 [9.85s]:  training loss=0.13987503945827484                                        \n",
      "epoch 54 [9.73s]:  training loss=0.13729487359523773                                        \n",
      "epoch 55 [9.96s]: training loss=0.13656270503997803  validation ndcg@10=0.022657976686403843 [0.4s]\n",
      "epoch 56 [10.01s]:  training loss=0.1365978717803955                                        \n",
      "epoch 57 [9.73s]:  training loss=0.1345062255859375                                         \n",
      "epoch 58 [9.45s]:  training loss=0.13389144837856293                                        \n",
      "epoch 59 [9.36s]:  training loss=0.1312263011932373                                         \n",
      "epoch 60 [9.81s]: training loss=0.1310715526342392  validation ndcg@10=0.022318013182883463 [0.4s]\n",
      "epoch 61 [9.71s]:  training loss=0.13180460035800934                                        \n",
      "epoch 62 [9.91s]:  training loss=0.1304198056459427                                         \n",
      "epoch 63 [9.55s]:  training loss=0.12731695175170898                                        \n",
      "epoch 64 [9.5s]:  training loss=0.12703028321266174                                         \n",
      "epoch 65 [9.48s]: training loss=0.12477060407400131  validation ndcg@10=0.023514658468386296 [0.4s]\n",
      "epoch 66 [9.8s]:  training loss=0.1228632852435112                                          \n",
      "epoch 67 [9.67s]:  training loss=0.12501856684684753                                        \n",
      "epoch 68 [9.52s]:  training loss=0.12404503673315048                                        \n",
      "epoch 69 [9.34s]:  training loss=0.12257171422243118                                        \n",
      "epoch 70 [9.67s]: training loss=0.12306927889585495  validation ndcg@10=0.0237028288766653 [0.37s]\n",
      "epoch 71 [9.61s]:  training loss=0.12127859145402908                                        \n",
      "epoch 72 [9.55s]:  training loss=0.11888645589351654                                        \n",
      "epoch 73 [9.87s]:  training loss=0.11780869215726852                                        \n",
      "epoch 74 [10.1s]:  training loss=0.11902516335248947                                        \n",
      "epoch 75 [9.85s]: training loss=0.11676202714443207  validation ndcg@10=0.023587288608673027 [0.41s]\n",
      "epoch 76 [11.54s]:  training loss=0.11587706208229065                                       \n",
      "epoch 77 [9.17s]:  training loss=0.11398927867412567                                        \n",
      "epoch 78 [9.38s]:  training loss=0.11372289806604385                                        \n",
      "epoch 79 [9.66s]:  training loss=0.11361861228942871                                        \n",
      "epoch 80 [9.29s]: training loss=0.11447877436876297  validation ndcg@10=0.02361825856369886 [0.37s]\n",
      "epoch 81 [9.56s]:  training loss=0.10957921296358109                                        \n",
      "epoch 82 [9.41s]:  training loss=0.112460657954216                                          \n",
      "epoch 83 [9.43s]:  training loss=0.11064053326845169                                        \n",
      "epoch 84 [9.33s]:  training loss=0.11070821434259415                                        \n",
      "epoch 85 [9.33s]: training loss=0.10898672789335251  validation ndcg@10=0.023764178686348794 [0.37s]\n",
      "epoch 86 [9.39s]:  training loss=0.10489866882562637                                        \n",
      "epoch 87 [9.52s]:  training loss=0.10677258670330048                                        \n",
      "epoch 88 [9.5s]:  training loss=0.10894661396741867                                         \n",
      "epoch 89 [9.45s]:  training loss=0.10665629059076309                                        \n",
      "epoch 90 [9.47s]: training loss=0.1055203378200531  validation ndcg@10=0.024124136479708576 [0.35s]\n",
      "epoch 91 [9.43s]:  training loss=0.10508953034877777                                        \n",
      "epoch 92 [9.61s]:  training loss=0.10387295484542847                                        \n",
      "epoch 93 [9.61s]:  training loss=0.1042107418179512                                         \n",
      "epoch 94 [9.52s]:  training loss=0.10391133278608322                                        \n",
      "epoch 95 [9.65s]: training loss=0.10256552696228027  validation ndcg@10=0.024169621582072073 [0.4s]\n",
      "epoch 96 [9.84s]:  training loss=0.10214215517044067                                        \n",
      "epoch 97 [9.85s]:  training loss=0.10129040479660034                                        \n",
      "epoch 98 [9.63s]:  training loss=0.09962987154722214                                        \n",
      "epoch 99 [9.5s]:  training loss=0.09914346039295197                                         \n",
      "epoch 100 [9.9s]: training loss=0.10038220137357712  validation ndcg@10=0.02479464614855699 [0.4s]\n",
      "epoch 101 [9.9s]:  training loss=0.09983479231595993                                        \n",
      "epoch 102 [9.96s]:  training loss=0.09819328784942627                                       \n",
      "epoch 103 [9.65s]:  training loss=0.09730634093284607                                       \n",
      "epoch 104 [9.54s]:  training loss=0.09527668356895447                                       \n",
      "epoch 105 [10.05s]: training loss=0.09663881361484528  validation ndcg@10=0.024506313008010126 [0.39s]\n",
      "epoch 106 [10.19s]:  training loss=0.0949842557311058                                       \n",
      "epoch 107 [10.0s]:  training loss=0.09595293551683426                                       \n",
      "epoch 108 [9.94s]:  training loss=0.09840109199285507                                       \n",
      "epoch 109 [9.7s]:  training loss=0.09938300400972366                                        \n",
      "epoch 110 [9.74s]: training loss=0.0920020043849945  validation ndcg@10=0.024270285079424483 [0.39s]\n",
      "epoch 111 [9.86s]:  training loss=0.09429161995649338                                       \n",
      "epoch 112 [9.81s]:  training loss=0.09531205147504807                                       \n",
      "epoch 113 [9.67s]:  training loss=0.09227534383535385                                       \n",
      "epoch 114 [9.62s]:  training loss=0.09252230823040009                                       \n",
      "epoch 115 [9.77s]: training loss=0.09213533252477646  validation ndcg@10=0.02488724878494126 [0.39s]\n",
      "epoch 116 [10.12s]:  training loss=0.0911652073264122                                       \n",
      "epoch 117 [9.81s]:  training loss=0.09072854369878769                                       \n",
      "epoch 118 [9.59s]:  training loss=0.08775895833969116                                       \n",
      "epoch 119 [9.86s]:  training loss=0.08733496069908142                                       \n",
      "epoch 120 [9.52s]: training loss=0.09121234714984894  validation ndcg@10=0.025106815594273448 [0.41s]\n",
      "epoch 121 [9.96s]:  training loss=0.08984430134296417                                       \n",
      "epoch 122 [9.77s]:  training loss=0.08683718740940094                                       \n",
      "epoch 123 [9.74s]:  training loss=0.08795762062072754                                       \n",
      "epoch 124 [9.91s]:  training loss=0.08844058215618134                                       \n",
      "epoch 125 [9.92s]: training loss=0.08490736782550812  validation ndcg@10=0.02474625427905647 [0.36s]\n",
      "epoch 126 [9.59s]:  training loss=0.08869905024766922                                       \n",
      "epoch 127 [9.65s]:  training loss=0.08663526177406311                                       \n",
      "epoch 128 [9.73s]:  training loss=0.08591175824403763                                       \n",
      "epoch 129 [10.06s]:  training loss=0.08624761551618576                                      \n",
      "epoch 130 [9.77s]: training loss=0.08823707699775696  validation ndcg@10=0.02486661967780501 [0.4s]\n",
      "epoch 131 [9.42s]:  training loss=0.0872405543923378                                        \n",
      "epoch 132 [9.76s]:  training loss=0.0855882316827774                                        \n",
      "epoch 133 [9.51s]:  training loss=0.08525657653808594                                       \n",
      "epoch 134 [10.03s]:  training loss=0.0834202840924263                                       \n",
      "epoch 135 [9.65s]: training loss=0.08333998918533325  validation ndcg@10=0.02575059100100659 [0.37s]\n",
      "epoch 136 [9.76s]:  training loss=0.08401982486248016                                       \n",
      "epoch 137 [9.75s]:  training loss=0.0816037654876709                                        \n",
      "epoch 138 [9.96s]:  training loss=0.08280142396688461                                       \n",
      "epoch 139 [9.97s]:  training loss=0.08395402878522873                                       \n",
      "epoch 140 [9.64s]: training loss=0.08200834691524506  validation ndcg@10=0.02544276272642454 [0.38s]\n",
      "epoch 141 [9.62s]:  training loss=0.0822090357542038                                        \n",
      "epoch 142 [9.97s]:  training loss=0.07883713394403458                                       \n",
      "epoch 143 [9.82s]:  training loss=0.08179876953363419                                       \n",
      "epoch 144 [9.64s]:  training loss=0.07983506470918655                                       \n",
      "epoch 145 [9.79s]: training loss=0.07855010032653809  validation ndcg@10=0.02540230937832876 [0.38s]\n",
      "epoch 146 [9.61s]:  training loss=0.08119992166757584                                       \n",
      "epoch 147 [9.76s]:  training loss=0.07937470078468323                                       \n",
      "epoch 148 [9.36s]:  training loss=0.07955855876207352                                       \n",
      "epoch 149 [9.46s]:  training loss=0.08016958832740784                                       \n",
      "epoch 150 [9.92s]: training loss=0.07995673269033432  validation ndcg@10=0.025016737372867224 [0.39s]\n",
      "epoch 151 [9.85s]:  training loss=0.07697874307632446                                       \n",
      "epoch 152 [9.66s]:  training loss=0.08156120777130127                                       \n",
      "epoch 153 [11.44s]:  training loss=0.07753029465675354                                      \n",
      "epoch 154 [9.69s]:  training loss=0.08084528893232346                                       \n",
      "epoch 155 [9.79s]: training loss=0.07677628844976425  validation ndcg@10=0.025504793823000364 [0.41s]\n",
      "epoch 156 [9.79s]:  training loss=0.07402385026216507                                       \n",
      "epoch 157 [9.79s]:  training loss=0.07815120369195938                                       \n",
      "epoch 158 [9.46s]:  training loss=0.07502904534339905                                       \n",
      "epoch 159 [9.77s]:  training loss=0.07679219543933868                                       \n",
      "epoch 160 [9.92s]: training loss=0.07686038315296173  validation ndcg@10=0.025665924459327704 [0.39s]\n",
      "epoch 1 [22.1s]:  training loss=0.45420998334884644                                         \n",
      "epoch 2 [21.58s]:  training loss=0.284552663564682                                          \n",
      "epoch 3 [21.59s]:  training loss=0.23195809125900269                                        \n",
      "epoch 4 [21.56s]:  training loss=0.2100542038679123                                         \n",
      "epoch 5 [21.17s]: training loss=0.1885910928249359  validation ndcg@10=0.020665847392609245 [0.59s]\n",
      "epoch 6 [22.28s]:  training loss=0.17461268603801727                                        \n",
      "epoch 7 [21.96s]:  training loss=0.1654742956161499                                         \n",
      "epoch 8 [22.16s]:  training loss=0.15762409567832947                                        \n",
      "epoch 9 [21.99s]:  training loss=0.14267005026340485                                        \n",
      "epoch 10 [22.17s]: training loss=0.1369294375181198  validation ndcg@10=0.023528557514722926 [0.58s]\n",
      "epoch 11 [22.1s]:  training loss=0.12569063901901245                                        \n",
      "epoch 12 [22.04s]:  training loss=0.12604868412017822                                       \n",
      "epoch 13 [21.21s]:  training loss=0.12005624920129776                                       \n",
      "epoch 14 [22.4s]:  training loss=0.11415029317140579                                        \n",
      "epoch 15 [22.03s]: training loss=0.1109251007437706  validation ndcg@10=0.02364028122222758 [0.57s]\n",
      "epoch 16 [22.0s]:  training loss=0.10683761537075043                                        \n",
      "epoch 17 [22.0s]:  training loss=0.10536453127861023                                        \n",
      "epoch 18 [21.8s]:  training loss=0.09935023635625839                                        \n",
      "epoch 19 [21.88s]:  training loss=0.10036322474479675                                       \n",
      "epoch 20 [21.81s]: training loss=0.09516260027885437  validation ndcg@10=0.023259868080258567 [0.59s]\n",
      "epoch 21 [21.83s]:  training loss=0.09467259049415588                                       \n",
      "epoch 22 [21.8s]:  training loss=0.09037631005048752                                        \n",
      "epoch 23 [21.52s]:  training loss=0.09072738140821457                                       \n",
      "epoch 24 [21.13s]:  training loss=0.08575459569692612                                       \n",
      "epoch 25 [22.42s]: training loss=0.08605899661779404  validation ndcg@10=0.025099511995408965 [0.57s]\n",
      "epoch 26 [21.22s]:  training loss=0.08146829903125763                                       \n",
      "epoch 27 [21.96s]:  training loss=0.07933878898620605                                       \n",
      "epoch 28 [21.7s]:  training loss=0.08060500770807266                                        \n",
      "epoch 29 [21.84s]:  training loss=0.0764620304107666                                        \n",
      "epoch 30 [21.58s]: training loss=0.07882250845432281  validation ndcg@10=0.02572399352097993 [0.56s]\n",
      "epoch 31 [23.33s]:  training loss=0.07532894611358643                                       \n",
      "epoch 32 [21.9s]:  training loss=0.075237937271595                                          \n",
      "epoch 33 [21.93s]:  training loss=0.07160896807909012                                       \n",
      "epoch 34 [21.83s]:  training loss=0.0738641619682312                                        \n",
      "epoch 35 [21.8s]: training loss=0.07309924811124802  validation ndcg@10=0.025834360068021617 [0.54s]\n",
      "epoch 36 [22.57s]:  training loss=0.06918060779571533                                       \n",
      "epoch 37 [21.54s]:  training loss=0.07119154930114746                                       \n",
      "epoch 38 [22.17s]:  training loss=0.069582000374794                                         \n",
      "epoch 39 [21.85s]:  training loss=0.06751089543104172                                       \n",
      "epoch 40 [21.36s]: training loss=0.06872328370809555  validation ndcg@10=0.02604279534549387 [0.59s]\n",
      "epoch 41 [21.45s]:  training loss=0.06513409316539764                                       \n",
      "epoch 42 [22.33s]:  training loss=0.06471947580575943                                       \n",
      "epoch 43 [21.89s]:  training loss=0.06632184237241745                                       \n",
      "epoch 44 [21.98s]:  training loss=0.06361707299947739                                       \n",
      "epoch 45 [22.25s]: training loss=0.06249770149588585  validation ndcg@10=0.02661201156352735 [0.58s]\n",
      "epoch 46 [22.59s]:  training loss=0.06096319109201431                                       \n",
      "epoch 47 [22.77s]:  training loss=0.0627758726477623                                        \n",
      "epoch 48 [22.34s]:  training loss=0.05861352011561394                                       \n",
      "epoch 49 [22.53s]:  training loss=0.05908175930380821                                       \n",
      "epoch 50 [22.44s]: training loss=0.05673931911587715  validation ndcg@10=0.024661740680883986 [0.58s]\n",
      "epoch 51 [22.06s]:  training loss=0.05893246456980705                                       \n",
      "epoch 52 [22.69s]:  training loss=0.05860146880149841                                       \n",
      "epoch 53 [22.8s]:  training loss=0.05822835862636566                                        \n",
      "epoch 54 [22.39s]:  training loss=0.057685572654008865                                      \n",
      "epoch 55 [21.71s]: training loss=0.05561089888215065  validation ndcg@10=0.024778994917324604 [0.57s]\n",
      "epoch 56 [21.93s]:  training loss=0.055518463253974915                                      \n",
      "epoch 57 [22.12s]:  training loss=0.054386869072914124                                      \n",
      "epoch 58 [22.2s]:  training loss=0.05422021821141243                                        \n",
      "epoch 59 [22.35s]:  training loss=0.05317958444356918                                       \n",
      "epoch 60 [21.97s]: training loss=0.05370960012078285  validation ndcg@10=0.025834136501935414 [0.59s]\n",
      "epoch 61 [22.08s]:  training loss=0.05536360293626785                                       \n",
      "epoch 62 [21.74s]:  training loss=0.052101753652095795                                      \n",
      "epoch 63 [21.97s]:  training loss=0.053772103041410446                                      \n",
      "epoch 64 [24.03s]:  training loss=0.04988075792789459                                       \n",
      "epoch 65 [20.53s]: training loss=0.05461777374148369  validation ndcg@10=0.025195341161773414 [0.58s]\n",
      "epoch 66 [21.43s]:  training loss=0.053326528519392014                                      \n",
      "epoch 67 [21.12s]:  training loss=0.05014977604150772                                       \n",
      "epoch 68 [22.15s]:  training loss=0.05001214146614075                                       \n",
      "epoch 69 [21.87s]:  training loss=0.05187869444489479                                       \n",
      "epoch 70 [21.71s]: training loss=0.0512692891061306  validation ndcg@10=0.026092816417300053 [0.55s]\n",
      "epoch 1 [34.1s]:  training loss=0.3297714293003082                                          \n",
      "epoch 2 [29.46s]:  training loss=0.21164041757583618                                        \n",
      "epoch 3 [30.06s]:  training loss=0.1638983190059662                                         \n",
      "epoch 4 [28.66s]:  training loss=0.14925487339496613                                        \n",
      "epoch 5 [29.54s]: training loss=0.13272450864315033  validation ndcg@10=0.022489881983208643 [0.71s]\n",
      "epoch 6 [29.9s]:  training loss=0.11868315935134888                                         \n",
      "epoch 7 [29.7s]:  training loss=0.1126827523112297                                          \n",
      "epoch 8 [29.94s]:  training loss=0.10657583177089691                                        \n",
      "epoch 9 [28.98s]:  training loss=0.09692524373531342                                        \n",
      "epoch 10 [30.06s]: training loss=0.09243016690015793  validation ndcg@10=0.022490123010928977 [0.71s]\n",
      "epoch 11 [30.14s]:  training loss=0.08852189779281616                                       \n",
      "epoch 12 [30.09s]:  training loss=0.0836806520819664                                        \n",
      "epoch 13 [29.6s]:  training loss=0.08260276168584824                                        \n",
      "epoch 14 [29.14s]:  training loss=0.08076223731040955                                       \n",
      "epoch 15 [28.84s]: training loss=0.0799856185913086  validation ndcg@10=0.022504588594672012 [0.66s]\n",
      "epoch 16 [28.53s]:  training loss=0.07555141299962997                                       \n",
      "epoch 17 [27.36s]:  training loss=0.07426950335502625                                       \n",
      "epoch 18 [26.79s]:  training loss=0.07321752607822418                                       \n",
      "epoch 19 [26.44s]:  training loss=0.07006796449422836                                       \n",
      "epoch 20 [26.07s]: training loss=0.07019523531198502  validation ndcg@10=0.021428226791986308 [0.62s]\n",
      "epoch 21 [24.9s]:  training loss=0.0681290253996849                                         \n",
      "epoch 22 [26.76s]:  training loss=0.06840179115533829                                       \n",
      "epoch 23 [24.22s]:  training loss=0.06543951481580734                                       \n",
      "epoch 24 [24.33s]:  training loss=0.06493020057678223                                       \n",
      "epoch 25 [23.85s]: training loss=0.0654086023569107  validation ndcg@10=0.019521601511974553 [0.6s]\n",
      "epoch 26 [24.53s]:  training loss=0.06259552389383316                                       \n",
      "epoch 27 [24.49s]:  training loss=0.06156149506568909                                       \n",
      "epoch 28 [24.0s]:  training loss=0.061882443726062775                                       \n",
      "epoch 29 [24.59s]:  training loss=0.062276557087898254                                      \n",
      "epoch 30 [24.26s]: training loss=0.06136586517095566  validation ndcg@10=0.020612660251485622 [0.57s]\n",
      "epoch 31 [24.34s]:  training loss=0.0577242486178875                                        \n",
      "epoch 32 [24.32s]:  training loss=0.05644366517663002                                       \n",
      "epoch 33 [23.87s]:  training loss=0.05893704295158386                                       \n",
      "epoch 34 [24.31s]:  training loss=0.057625021785497665                                      \n",
      "epoch 35 [24.17s]: training loss=0.05616375431418419  validation ndcg@10=0.021765958159587404 [0.56s]\n",
      "epoch 36 [24.41s]:  training loss=0.05350926145911217                                       \n",
      "epoch 37 [24.23s]:  training loss=0.05817524716258049                                       \n",
      "epoch 38 [24.37s]:  training loss=0.05436351150274277                                       \n",
      "epoch 39 [24.44s]:  training loss=0.05620648339390755                                       \n",
      "epoch 40 [24.08s]: training loss=0.053402405232191086  validation ndcg@10=0.019345564754297745 [0.6s]\n",
      "epoch 1 [19.9s]:  training loss=0.31517675518989563                                         \n",
      "epoch 2 [21.47s]:  training loss=0.23871882259845734                                       \n",
      "epoch 3 [22.34s]:  training loss=0.22132016718387604                                       \n",
      "epoch 4 [22.69s]:  training loss=0.21243977546691895                                       \n",
      "epoch 5 [22.63s]: training loss=0.2119779735803604  validation ndcg@10=0.016565462442612998 [0.57s]\n",
      "epoch 6 [22.58s]:  training loss=0.19947823882102966                                       \n",
      "epoch 7 [23.03s]:  training loss=0.20506532490253448                                       \n",
      "epoch 8 [22.08s]:  training loss=0.20968683063983917                                       \n",
      "epoch 9 [23.41s]:  training loss=0.20670495927333832                                       \n",
      "epoch 10 [22.52s]: training loss=0.21304281055927277  validation ndcg@10=0.014824474297101296 [0.59s]\n",
      "epoch 11 [23.39s]:  training loss=0.21088609099388123                                      \n",
      "epoch 12 [23.48s]:  training loss=0.21284270286560059                                      \n",
      "epoch 13 [23.02s]:  training loss=0.2093431055545807                                       \n",
      "epoch 14 [24.79s]:  training loss=0.21159584820270538                                      \n",
      "epoch 15 [23.08s]: training loss=0.2131168097257614  validation ndcg@10=0.01640351910898049 [0.54s]\n",
      "epoch 16 [22.54s]:  training loss=0.2016444355249405                                       \n",
      "epoch 17 [22.16s]:  training loss=0.21269840002059937                                      \n",
      "epoch 18 [22.28s]:  training loss=0.20632269978523254                                      \n",
      "epoch 19 [22.3s]:  training loss=0.23028163611888885                                       \n",
      "epoch 20 [22.0s]: training loss=0.2073417454957962  validation ndcg@10=0.01698967038543299 [0.54s]\n",
      "epoch 21 [22.84s]:  training loss=0.2157987356185913                                       \n",
      "epoch 22 [22.54s]:  training loss=0.22194725275039673                                      \n",
      "epoch 23 [22.21s]:  training loss=0.21998777985572815                                      \n",
      "epoch 24 [22.72s]:  training loss=0.23432227969169617                                      \n",
      "epoch 25 [22.73s]: training loss=0.2192603498697281  validation ndcg@10=0.016301228335725917 [0.57s]\n",
      "epoch 26 [21.75s]:  training loss=0.21618634462356567                                      \n",
      "epoch 27 [23.05s]:  training loss=0.22543774545192719                                      \n",
      "epoch 28 [22.94s]:  training loss=0.22055058181285858                                      \n",
      "epoch 29 [23.46s]:  training loss=0.22565245628356934                                      \n",
      "epoch 30 [22.54s]: training loss=0.21791771054267883  validation ndcg@10=0.016807730601791138 [0.62s]\n",
      "epoch 31 [23.93s]:  training loss=0.23489199578762054                                      \n",
      "epoch 32 [23.42s]:  training loss=0.2281835377216339                                       \n",
      "epoch 33 [23.0s]:  training loss=0.22468657791614532                                       \n",
      "epoch 34 [22.98s]:  training loss=0.24971221387386322                                      \n",
      "epoch 35 [22.44s]: training loss=0.22853434085845947  validation ndcg@10=0.015779248678517984 [0.57s]\n",
      "epoch 36 [22.78s]:  training loss=0.24731145799160004                                      \n",
      "epoch 37 [22.19s]:  training loss=0.23944814503192902                                      \n",
      "epoch 38 [22.61s]:  training loss=0.2231641411781311                                       \n",
      "epoch 39 [22.71s]:  training loss=0.23286625742912292                                      \n",
      "epoch 40 [22.41s]: training loss=0.2301403135061264  validation ndcg@10=0.015304662704746905 [0.56s]\n",
      "epoch 41 [22.54s]:  training loss=0.2517194449901581                                       \n",
      "epoch 42 [22.76s]:  training loss=0.2531664967536926                                       \n",
      "epoch 43 [22.89s]:  training loss=0.2451724112033844                                       \n",
      "epoch 44 [23.28s]:  training loss=0.2569842040538788                                       \n",
      "epoch 45 [22.71s]: training loss=0.24255749583244324  validation ndcg@10=0.014899584008854931 [0.6s]\n",
      "epoch 1 [29.2s]:  training loss=0.32437756657600403                                        \n",
      "epoch 2 [26.78s]:  training loss=0.2057545930147171                                        \n",
      "epoch 3 [28.35s]:  training loss=0.16672851145267487                                       \n",
      "epoch 4 [28.25s]:  training loss=0.14483733475208282                                       \n",
      "epoch 5 [28.17s]: training loss=0.1303725391626358  validation ndcg@10=0.021581072928227283 [0.67s]\n",
      "epoch 6 [28.38s]:  training loss=0.11891435831785202                                       \n",
      "epoch 7 [28.46s]:  training loss=0.11490990221500397                                       \n",
      "epoch 8 [28.45s]:  training loss=0.10586649179458618                                       \n",
      "epoch 9 [28.38s]:  training loss=0.10106527805328369                                       \n",
      "epoch 10 [28.09s]: training loss=0.09954177588224411  validation ndcg@10=0.019822150578829544 [0.67s]\n",
      "epoch 11 [27.9s]:  training loss=0.09403891116380692                                       \n",
      "epoch 12 [28.03s]:  training loss=0.0944732204079628                                       \n",
      "epoch 13 [27.96s]:  training loss=0.0913744866847992                                       \n",
      "epoch 14 [28.44s]:  training loss=0.0893724113702774                                       \n",
      "epoch 15 [28.04s]: training loss=0.08497525006532669  validation ndcg@10=0.018520187121461577 [0.65s]\n",
      "epoch 16 [27.99s]:  training loss=0.08181881904602051                                      \n",
      "epoch 17 [27.74s]:  training loss=0.087919682264328                                        \n",
      "epoch 18 [28.92s]:  training loss=0.08087842911481857                                      \n",
      "epoch 19 [27.83s]:  training loss=0.08299743384122849                                      \n",
      "epoch 20 [28.6s]: training loss=0.08397283405065536  validation ndcg@10=0.019973837102709134 [0.71s]\n",
      "epoch 21 [28.34s]:  training loss=0.07947544008493423                                      \n",
      "epoch 22 [28.27s]:  training loss=0.07968698441982269                                      \n",
      "epoch 23 [28.03s]:  training loss=0.07813039422035217                                      \n",
      "epoch 24 [28.04s]:  training loss=0.07325203716754913                                      \n",
      "epoch 25 [27.66s]: training loss=0.07694979757070541  validation ndcg@10=0.01618902374141086 [0.67s]\n",
      "epoch 26 [28.39s]:  training loss=0.07176757603883743                                      \n",
      "epoch 27 [28.68s]:  training loss=0.07456483691930771                                      \n",
      "epoch 28 [29.86s]:  training loss=0.07313887774944305                                      \n",
      "epoch 29 [28.16s]:  training loss=0.07417713850736618                                      \n",
      "epoch 30 [26.88s]: training loss=0.07012204080820084  validation ndcg@10=0.018390895869255644 [0.66s]\n",
      "epoch 1 [11.11s]:  training loss=0.6080179810523987                                        \n",
      "epoch 2 [10.67s]:  training loss=0.5235597491264343                                        \n",
      "epoch 3 [10.99s]:  training loss=0.473827987909317                                         \n",
      "epoch 4 [11.01s]:  training loss=0.4306390881538391                                        \n",
      "epoch 5 [11.01s]: training loss=0.3997374176979065  validation ndcg@10=0.006517008853499377 [0.37s]\n",
      "epoch 6 [10.58s]:  training loss=0.37362056970596313                                       \n",
      "epoch 7 [10.91s]:  training loss=0.3421015739440918                                        \n",
      "epoch 8 [10.65s]:  training loss=0.31686919927597046                                       \n",
      "epoch 9 [11.0s]:  training loss=0.29963406920433044                                        \n",
      "epoch 10 [10.94s]: training loss=0.2843747138977051  validation ndcg@10=0.012309935654481373 [0.4s]\n",
      "epoch 11 [10.78s]:  training loss=0.2697046101093292                                       \n",
      "epoch 12 [11.23s]:  training loss=0.2620203495025635                                       \n",
      "epoch 13 [10.72s]:  training loss=0.2480534166097641                                       \n",
      "epoch 14 [10.57s]:  training loss=0.24326321482658386                                      \n",
      "epoch 15 [10.67s]: training loss=0.23133960366249084  validation ndcg@10=0.017366099885733198 [0.37s]\n",
      "epoch 16 [10.69s]:  training loss=0.22632429003715515                                      \n",
      "epoch 17 [10.99s]:  training loss=0.21811887621879578                                      \n",
      "epoch 18 [10.51s]:  training loss=0.21644040942192078                                      \n",
      "epoch 19 [10.59s]:  training loss=0.20923027396202087                                      \n",
      "epoch 20 [10.92s]: training loss=0.20664750039577484  validation ndcg@10=0.01931030942784037 [0.4s]\n",
      "epoch 21 [10.47s]:  training loss=0.19869464635849                                         \n",
      "epoch 22 [10.93s]:  training loss=0.19651706516742706                                      \n",
      "epoch 23 [10.78s]:  training loss=0.19442637264728546                                      \n",
      "epoch 24 [10.84s]:  training loss=0.1959277093410492                                       \n",
      "epoch 25 [10.46s]: training loss=0.18622393906116486  validation ndcg@10=0.02095340029584307 [0.36s]\n",
      "epoch 26 [10.41s]:  training loss=0.18624813854694366                                      \n",
      "epoch 27 [10.02s]:  training loss=0.18185769021511078                                      \n",
      "epoch 28 [10.35s]:  training loss=0.18036624789237976                                      \n",
      "epoch 29 [10.32s]:  training loss=0.1810387223958969                                       \n",
      "epoch 30 [10.28s]: training loss=0.17813348770141602  validation ndcg@10=0.020968067172968195 [0.4s]\n",
      "epoch 31 [10.07s]:  training loss=0.1725178211927414                                       \n",
      "epoch 32 [10.49s]:  training loss=0.17197835445404053                                      \n",
      "epoch 33 [10.0s]:  training loss=0.16786843538284302                                       \n",
      "epoch 34 [9.96s]:  training loss=0.17154990136623383                                       \n",
      "epoch 35 [9.87s]: training loss=0.16463463008403778  validation ndcg@10=0.021309508344098664 [0.37s]\n",
      "epoch 36 [10.16s]:  training loss=0.16475410759449005                                      \n",
      "epoch 37 [10.33s]:  training loss=0.1603030115365982                                       \n",
      "epoch 38 [9.74s]:  training loss=0.1612313687801361                                        \n",
      "epoch 39 [10.19s]:  training loss=0.1581219583749771                                       \n",
      "epoch 40 [10.33s]: training loss=0.15858672559261322  validation ndcg@10=0.02103106197038153 [0.36s]\n",
      "epoch 41 [10.19s]:  training loss=0.15613023936748505                                      \n",
      "epoch 42 [9.95s]:  training loss=0.1554182767868042                                        \n",
      "epoch 43 [10.17s]:  training loss=0.15365242958068848                                      \n",
      "epoch 44 [9.57s]:  training loss=0.1504175364971161                                        \n",
      "epoch 45 [10.24s]: training loss=0.14431163668632507  validation ndcg@10=0.021815992568223988 [0.42s]\n",
      "epoch 46 [10.12s]:  training loss=0.14313775300979614                                      \n",
      "epoch 47 [10.41s]:  training loss=0.14809861779212952                                      \n",
      "epoch 48 [10.19s]:  training loss=0.1468033790588379                                       \n",
      "epoch 49 [10.21s]:  training loss=0.14534595608711243                                      \n",
      "epoch 50 [10.43s]: training loss=0.14225085079669952  validation ndcg@10=0.022637949519430357 [0.39s]\n",
      "epoch 51 [10.17s]:  training loss=0.14056134223937988                                      \n",
      "epoch 52 [10.13s]:  training loss=0.1381322592496872                                       \n",
      "epoch 53 [10.25s]:  training loss=0.1363389939069748                                       \n",
      "epoch 54 [10.13s]:  training loss=0.13808247447013855                                      \n",
      "epoch 55 [10.16s]: training loss=0.13450267910957336  validation ndcg@10=0.022747307785032707 [0.36s]\n",
      "epoch 56 [10.0s]:  training loss=0.1318502277135849                                        \n",
      "epoch 57 [9.83s]:  training loss=0.13164649903774261                                       \n",
      "epoch 58 [9.84s]:  training loss=0.13304977118968964                                       \n",
      "epoch 59 [9.49s]:  training loss=0.13461937010288239                                       \n",
      "epoch 60 [9.67s]: training loss=0.12558463215827942  validation ndcg@10=0.022518842676600564 [0.36s]\n",
      "epoch 61 [9.73s]:  training loss=0.1280389428138733                                        \n",
      "epoch 62 [10.18s]:  training loss=0.12453942745923996                                      \n",
      "epoch 63 [10.25s]:  training loss=0.12353027611970901                                      \n",
      "epoch 64 [10.51s]:  training loss=0.125658318400383                                        \n",
      "epoch 65 [10.35s]: training loss=0.12227556109428406  validation ndcg@10=0.023680001579629913 [0.67s]\n",
      "epoch 66 [11.22s]:  training loss=0.12310786545276642                                      \n",
      "epoch 67 [10.16s]:  training loss=0.12399350106716156                                      \n",
      "epoch 68 [10.1s]:  training loss=0.12430643290281296                                       \n",
      "epoch 69 [10.4s]:  training loss=0.11907779425382614                                       \n",
      "epoch 70 [10.17s]: training loss=0.12075911462306976  validation ndcg@10=0.023412907662638007 [0.38s]\n",
      "epoch 71 [10.09s]:  training loss=0.11938772350549698                                      \n",
      "epoch 72 [10.23s]:  training loss=0.11793777346611023                                      \n",
      "epoch 73 [9.99s]:  training loss=0.11914156377315521                                       \n",
      "epoch 74 [10.33s]:  training loss=0.11542456597089767                                      \n",
      "epoch 75 [10.3s]: training loss=0.11538903415203094  validation ndcg@10=0.0233732075570426 [0.39s]\n",
      "epoch 76 [10.14s]:  training loss=0.11229102313518524                                      \n",
      "epoch 77 [10.47s]:  training loss=0.11678273975849152                                      \n",
      "epoch 78 [10.62s]:  training loss=0.11358117312192917                                      \n",
      "epoch 79 [10.27s]:  training loss=0.10898967832326889                                      \n",
      "epoch 80 [10.04s]: training loss=0.11110323667526245  validation ndcg@10=0.024275072179580026 [0.36s]\n",
      "epoch 81 [10.71s]:  training loss=0.11190855503082275                                      \n",
      "epoch 82 [10.83s]:  training loss=0.11182671785354614                                      \n",
      "epoch 83 [10.62s]:  training loss=0.11220292747020721                                      \n",
      "epoch 84 [10.94s]:  training loss=0.10704303532838821                                      \n",
      "epoch 85 [10.84s]: training loss=0.10631559044122696  validation ndcg@10=0.02425770775456884 [0.39s]\n",
      "epoch 86 [10.79s]:  training loss=0.10761384665966034                                      \n",
      "epoch 87 [10.8s]:  training loss=0.10572078078985214                                       \n",
      "epoch 88 [10.97s]:  training loss=0.10654662549495697                                      \n",
      "epoch 89 [10.76s]:  training loss=0.10607292503118515                                      \n",
      "epoch 90 [11.09s]: training loss=0.10421708971261978  validation ndcg@10=0.02451493651827568 [0.39s]\n",
      "epoch 91 [10.91s]:  training loss=0.10344887524843216                                      \n",
      "epoch 92 [10.72s]:  training loss=0.10345339775085449                                      \n",
      "epoch 93 [10.68s]:  training loss=0.10424737632274628                                      \n",
      "epoch 94 [10.9s]:  training loss=0.10316179692745209                                       \n",
      "epoch 95 [10.77s]: training loss=0.10106990486383438  validation ndcg@10=0.024321596145895817 [0.35s]\n",
      "epoch 96 [10.56s]:  training loss=0.10044029355049133                                      \n",
      "epoch 97 [10.47s]:  training loss=0.09831097722053528                                      \n",
      "epoch 98 [10.82s]:  training loss=0.09902477264404297                                      \n",
      "epoch 99 [10.75s]:  training loss=0.09991354495286942                                      \n",
      "epoch 100 [10.63s]: training loss=0.09910643845796585  validation ndcg@10=0.024097569970966373 [0.39s]\n",
      "epoch 101 [10.86s]:  training loss=0.10001207143068314                                     \n",
      "epoch 102 [10.47s]:  training loss=0.10053970664739609                                     \n",
      "epoch 103 [10.53s]:  training loss=0.09660603851079941                                     \n",
      "epoch 104 [10.99s]:  training loss=0.09619604796171188                                     \n",
      "epoch 105 [10.82s]: training loss=0.09444386512041092  validation ndcg@10=0.024888338470561204 [0.4s]\n",
      "epoch 106 [10.88s]:  training loss=0.09658239036798477                                     \n",
      "epoch 107 [10.81s]:  training loss=0.09366374462842941                                     \n",
      "epoch 108 [11.03s]:  training loss=0.09417320042848587                                     \n",
      "epoch 109 [10.73s]:  training loss=0.09143973886966705                                     \n",
      "epoch 110 [10.5s]: training loss=0.09197460114955902  validation ndcg@10=0.025108092108405686 [0.4s]\n",
      "epoch 111 [10.81s]:  training loss=0.09653838723897934                                     \n",
      "epoch 112 [10.62s]:  training loss=0.09415087848901749                                     \n",
      "epoch 113 [10.76s]:  training loss=0.09070947021245956                                     \n",
      "epoch 114 [10.71s]:  training loss=0.09400498867034912                                     \n",
      "epoch 115 [10.71s]: training loss=0.09158267080783844  validation ndcg@10=0.025180260061958334 [0.42s]\n",
      "epoch 116 [10.77s]:  training loss=0.09368544816970825                                     \n",
      "epoch 117 [10.67s]:  training loss=0.08845578134059906                                     \n",
      "epoch 118 [10.65s]:  training loss=0.09079508483409882                                     \n",
      "epoch 119 [10.47s]:  training loss=0.08984227478504181                                     \n",
      "epoch 120 [10.78s]: training loss=0.0895456001162529  validation ndcg@10=0.025695294729565846 [0.41s]\n",
      "epoch 121 [10.62s]:  training loss=0.08818703144788742                                     \n",
      "epoch 122 [10.92s]:  training loss=0.08980778604745865                                     \n",
      "epoch 123 [10.6s]:  training loss=0.08685746043920517                                      \n",
      "epoch 124 [10.75s]:  training loss=0.08763755857944489                                     \n",
      "epoch 125 [10.83s]: training loss=0.08867867290973663  validation ndcg@10=0.025441672484978507 [0.38s]\n",
      "epoch 126 [10.95s]:  training loss=0.08703972399234772                                     \n",
      "epoch 127 [10.19s]:  training loss=0.0860099047422409                                      \n",
      "epoch 128 [10.62s]:  training loss=0.08689556270837784                                     \n",
      "epoch 129 [10.07s]:  training loss=0.0871867686510086                                      \n",
      "epoch 130 [10.33s]: training loss=0.08443821966648102  validation ndcg@10=0.025955289749957765 [0.36s]\n",
      "epoch 131 [10.4s]:  training loss=0.08368358016014099                                      \n",
      "epoch 132 [10.03s]:  training loss=0.08318352699279785                                     \n",
      "epoch 133 [10.21s]:  training loss=0.08242398500442505                                     \n",
      "epoch 134 [10.08s]:  training loss=0.08367426693439484                                     \n",
      "epoch 135 [10.2s]: training loss=0.08543005585670471  validation ndcg@10=0.02572952863165934 [0.36s]\n",
      "epoch 136 [10.16s]:  training loss=0.08299293369054794                                     \n",
      "epoch 137 [11.75s]:  training loss=0.08305276185274124                                     \n",
      "epoch 138 [9.67s]:  training loss=0.08361484855413437                                      \n",
      "epoch 139 [9.78s]:  training loss=0.08030833303928375                                      \n",
      "epoch 140 [9.78s]: training loss=0.0820852518081665  validation ndcg@10=0.025892347612365176 [0.37s]\n",
      "epoch 141 [9.86s]:  training loss=0.08085072040557861                                      \n",
      "epoch 142 [10.33s]:  training loss=0.07882048934698105                                     \n",
      "epoch 143 [10.41s]:  training loss=0.08050036430358887                                     \n",
      "epoch 144 [10.47s]:  training loss=0.0806896910071373                                      \n",
      "epoch 145 [10.03s]: training loss=0.0816640704870224  validation ndcg@10=0.025851076723024988 [0.35s]\n",
      "epoch 146 [10.3s]:  training loss=0.08003028482198715                                      \n",
      "epoch 147 [10.24s]:  training loss=0.0806746855378151                                      \n",
      "epoch 148 [10.42s]:  training loss=0.07818208634853363                                     \n",
      "epoch 149 [10.1s]:  training loss=0.07890966534614563                                      \n",
      "epoch 150 [10.61s]: training loss=0.07877487689256668  validation ndcg@10=0.025378862278873596 [0.37s]\n",
      "epoch 151 [10.5s]:  training loss=0.07855582237243652                                      \n",
      "epoch 152 [10.53s]:  training loss=0.07767834514379501                                     \n",
      "epoch 153 [10.05s]:  training loss=0.07814091444015503                                     \n",
      "epoch 154 [10.33s]:  training loss=0.07738552987575531                                     \n",
      "epoch 155 [10.43s]: training loss=0.0759362205862999  validation ndcg@10=0.02599366808126181 [0.39s]\n",
      "epoch 156 [10.62s]:  training loss=0.07834837585687637                                     \n",
      "epoch 157 [10.32s]:  training loss=0.07630511373281479                                     \n",
      "epoch 158 [10.63s]:  training loss=0.07593554258346558                                     \n",
      "epoch 159 [10.54s]:  training loss=0.0747789591550827                                      \n",
      "epoch 160 [10.82s]: training loss=0.07618606090545654  validation ndcg@10=0.02617199091849607 [0.36s]\n",
      "epoch 161 [10.24s]:  training loss=0.07496363669633865                                     \n",
      "epoch 162 [10.31s]:  training loss=0.0739826112985611                                      \n",
      "epoch 163 [10.38s]:  training loss=0.07545927166938782                                     \n",
      "epoch 164 [10.68s]:  training loss=0.07521243393421173                                     \n",
      "epoch 165 [10.48s]: training loss=0.07479751110076904  validation ndcg@10=0.025515891631352126 [0.4s]\n",
      "epoch 166 [10.48s]:  training loss=0.07452057301998138                                     \n",
      "epoch 167 [10.15s]:  training loss=0.07160816341638565                                     \n",
      "epoch 168 [9.75s]:  training loss=0.07346878200769424                                      \n",
      "epoch 169 [9.84s]:  training loss=0.0711381733417511                                       \n",
      "epoch 170 [9.97s]: training loss=0.07280288636684418  validation ndcg@10=0.025642975689021894 [0.37s]\n",
      "epoch 171 [10.19s]:  training loss=0.07147867977619171                                     \n",
      "epoch 172 [10.41s]:  training loss=0.06998337805271149                                     \n",
      "epoch 173 [10.35s]:  training loss=0.07368474453687668                                     \n",
      "epoch 174 [10.27s]:  training loss=0.07320612668991089                                     \n",
      "epoch 175 [9.75s]: training loss=0.0721345767378807  validation ndcg@10=0.026709903644074516 [0.35s]\n",
      "epoch 176 [10.12s]:  training loss=0.07028398662805557                                     \n",
      "epoch 177 [10.84s]:  training loss=0.07139213383197784                                     \n",
      "epoch 178 [10.56s]:  training loss=0.06985259056091309                                     \n",
      "epoch 179 [10.99s]:  training loss=0.06922613084316254                                     \n",
      "epoch 180 [10.87s]: training loss=0.06983530521392822  validation ndcg@10=0.026188679245380726 [0.39s]\n",
      "epoch 181 [10.74s]:  training loss=0.06944949924945831                                     \n",
      "epoch 182 [10.64s]:  training loss=0.0702611580491066                                      \n",
      "epoch 183 [11.0s]:  training loss=0.06773681193590164                                      \n",
      "epoch 184 [10.85s]:  training loss=0.06818737834692001                                     \n",
      "epoch 185 [10.06s]: training loss=0.06732310354709625  validation ndcg@10=0.02635438335725082 [0.39s]\n",
      "epoch 186 [10.53s]:  training loss=0.07126765698194504                                     \n",
      "epoch 187 [10.47s]:  training loss=0.06830643117427826                                     \n",
      "epoch 188 [10.61s]:  training loss=0.06776874512434006                                     \n",
      "epoch 189 [10.65s]:  training loss=0.0690726563334465                                      \n",
      "epoch 190 [10.3s]: training loss=0.06622321158647537  validation ndcg@10=0.02601607974397742 [0.35s]\n",
      "epoch 191 [10.26s]:  training loss=0.07043053209781647                                     \n",
      "epoch 192 [10.69s]:  training loss=0.06894273310899734                                     \n",
      "epoch 193 [10.47s]:  training loss=0.06676232814788818                                     \n",
      "epoch 194 [10.15s]:  training loss=0.06784499436616898                                     \n",
      "epoch 195 [11.03s]: training loss=0.06707066297531128  validation ndcg@10=0.02595507198787324 [0.37s]\n",
      "epoch 196 [10.41s]:  training loss=0.06659473478794098                                     \n",
      "epoch 197 [10.33s]:  training loss=0.06582774221897125                                     \n",
      "epoch 198 [10.2s]:  training loss=0.06512825936079025                                      \n",
      "epoch 199 [10.33s]:  training loss=0.06521493196487427                                     \n",
      "epoch 200 [10.29s]: training loss=0.06826742738485336  validation ndcg@10=0.02643739156603785 [0.37s]\n",
      "epoch 1 [3.5s]:  training loss=0.6528987884521484                                          \n",
      "epoch 2 [3.41s]:  training loss=0.5731150507926941                                         \n",
      "epoch 3 [3.48s]:  training loss=0.5107253193855286                                         \n",
      "epoch 4 [3.39s]:  training loss=0.4662148654460907                                         \n",
      "epoch 5 [3.6s]: training loss=0.41742396354675293  validation ndcg@10=0.0075471009045063615 [0.3s]\n",
      "epoch 6 [3.38s]:  training loss=0.3877808749675751                                         \n",
      "epoch 7 [3.34s]:  training loss=0.35989469289779663                                        \n",
      "epoch 8 [3.45s]:  training loss=0.3345116972923279                                         \n",
      "epoch 9 [3.32s]:  training loss=0.312798410654068                                          \n",
      "epoch 10 [3.42s]: training loss=0.29557523131370544  validation ndcg@10=0.015914681980851404 [0.29s]\n",
      "epoch 11 [3.33s]:  training loss=0.2844427525997162                                        \n",
      "epoch 12 [3.41s]:  training loss=0.2704162001609802                                        \n",
      "epoch 13 [3.49s]:  training loss=0.2636871337890625                                        \n",
      "epoch 14 [3.43s]:  training loss=0.2552563548088074                                        \n",
      "epoch 15 [3.47s]: training loss=0.2513883709907532  validation ndcg@10=0.020842493479833708 [0.32s]\n",
      "epoch 16 [3.51s]:  training loss=0.243496373295784                                         \n",
      "epoch 17 [3.51s]:  training loss=0.23720401525497437                                       \n",
      "epoch 18 [3.38s]:  training loss=0.23629173636436462                                       \n",
      "epoch 19 [3.66s]:  training loss=0.22811713814735413                                       \n",
      "epoch 20 [3.54s]: training loss=0.22735002636909485  validation ndcg@10=0.021545484192673235 [0.29s]\n",
      "epoch 21 [3.5s]:  training loss=0.22211162745952606                                        \n",
      "epoch 22 [3.55s]:  training loss=0.21738244593143463                                       \n",
      "epoch 23 [3.45s]:  training loss=0.2101190984249115                                        \n",
      "epoch 24 [3.45s]:  training loss=0.2111889272928238                                        \n",
      "epoch 25 [3.48s]: training loss=0.20619122684001923  validation ndcg@10=0.02144142648587768 [0.3s]\n",
      "epoch 26 [3.54s]:  training loss=0.2048216611146927                                        \n",
      "epoch 27 [3.58s]:  training loss=0.20094020664691925                                       \n",
      "epoch 28 [3.31s]:  training loss=0.2000572234392166                                        \n",
      "epoch 29 [5.19s]:  training loss=0.1930931955575943                                        \n",
      "epoch 30 [3.38s]: training loss=0.19323031604290009  validation ndcg@10=0.022000262781779203 [0.28s]\n",
      "epoch 31 [3.45s]:  training loss=0.18859559297561646                                       \n",
      "epoch 32 [3.32s]:  training loss=0.1858278065919876                                        \n",
      "epoch 33 [3.28s]:  training loss=0.18766093254089355                                       \n",
      "epoch 34 [3.33s]:  training loss=0.18340016901493073                                       \n",
      "epoch 35 [3.29s]: training loss=0.1818050742149353  validation ndcg@10=0.02162908113732158 [0.3s]\n",
      "epoch 36 [3.36s]:  training loss=0.17684528231620789                                       \n",
      "epoch 37 [3.35s]:  training loss=0.17700085043907166                                       \n",
      "epoch 38 [3.38s]:  training loss=0.1769072562456131                                        \n",
      "epoch 39 [3.33s]:  training loss=0.17249974608421326                                       \n",
      "epoch 40 [3.53s]: training loss=0.17095749080181122  validation ndcg@10=0.022817866966843237 [0.29s]\n",
      "epoch 41 [3.36s]:  training loss=0.16640357673168182                                       \n",
      "epoch 42 [3.29s]:  training loss=0.16917356848716736                                       \n",
      "epoch 43 [3.42s]:  training loss=0.16606581211090088                                       \n",
      "epoch 44 [3.39s]:  training loss=0.16714341938495636                                       \n",
      "epoch 45 [3.5s]: training loss=0.1602889895439148  validation ndcg@10=0.022517855309338627 [0.27s]\n",
      "epoch 46 [3.3s]:  training loss=0.15785181522369385                                        \n",
      "epoch 47 [3.52s]:  training loss=0.15699635446071625                                       \n",
      "epoch 48 [3.5s]:  training loss=0.1591891050338745                                         \n",
      "epoch 49 [3.37s]:  training loss=0.15693563222885132                                       \n",
      "epoch 50 [3.26s]: training loss=0.15201053023338318  validation ndcg@10=0.02430124996291985 [0.26s]\n",
      "epoch 51 [3.42s]:  training loss=0.15463651716709137                                       \n",
      "epoch 52 [3.27s]:  training loss=0.1531704217195511                                        \n",
      "epoch 53 [3.43s]:  training loss=0.1507444977760315                                        \n",
      "epoch 54 [3.45s]:  training loss=0.1475003957748413                                        \n",
      "epoch 55 [3.55s]: training loss=0.14756251871585846  validation ndcg@10=0.023651848090795843 [0.28s]\n",
      "epoch 56 [3.55s]:  training loss=0.1463213711977005                                        \n",
      "epoch 57 [3.4s]:  training loss=0.14196762442588806                                        \n",
      "epoch 58 [3.48s]:  training loss=0.14427128434181213                                       \n",
      "epoch 59 [3.58s]:  training loss=0.14104469120502472                                       \n",
      "epoch 60 [3.5s]: training loss=0.14232154190540314  validation ndcg@10=0.0242311629865575 [0.3s]\n",
      "epoch 61 [3.4s]:  training loss=0.13945986330509186                                        \n",
      "epoch 62 [3.48s]:  training loss=0.13973236083984375                                       \n",
      "epoch 63 [3.44s]:  training loss=0.14160877466201782                                       \n",
      "epoch 64 [3.32s]:  training loss=0.13965457677841187                                       \n",
      "epoch 65 [3.52s]: training loss=0.13612200319766998  validation ndcg@10=0.024300084753676475 [0.3s]\n",
      "epoch 66 [3.67s]:  training loss=0.132676362991333                                         \n",
      "epoch 67 [3.39s]:  training loss=0.13153290748596191                                       \n",
      "epoch 68 [3.4s]:  training loss=0.13276799023151398                                        \n",
      "epoch 69 [3.54s]:  training loss=0.13252703845500946                                       \n",
      "epoch 70 [3.34s]: training loss=0.12796689569950104  validation ndcg@10=0.02426371537018302 [0.29s]\n",
      "epoch 71 [3.39s]:  training loss=0.12899981439113617                                       \n",
      "epoch 72 [3.58s]:  training loss=0.1273353397846222                                        \n",
      "epoch 73 [3.48s]:  training loss=0.12857796251773834                                       \n",
      "epoch 74 [3.44s]:  training loss=0.12786762416362762                                       \n",
      "epoch 75 [3.23s]: training loss=0.12546895444393158  validation ndcg@10=0.02494138986255953 [0.28s]\n",
      "epoch 76 [3.36s]:  training loss=0.1280568242073059                                        \n",
      "epoch 77 [3.4s]:  training loss=0.12394595146179199                                        \n",
      "epoch 78 [3.57s]:  training loss=0.12250687181949615                                       \n",
      "epoch 79 [3.42s]:  training loss=0.12356144934892654                                       \n",
      "epoch 80 [3.48s]: training loss=0.1211598664522171  validation ndcg@10=0.024372824678323014 [0.29s]\n",
      "epoch 81 [3.61s]:  training loss=0.11910706013441086                                       \n",
      "epoch 82 [3.27s]:  training loss=0.1222875714302063                                        \n",
      "epoch 83 [3.48s]:  training loss=0.12072443962097168                                       \n",
      "epoch 84 [3.36s]:  training loss=0.11541932821273804                                       \n",
      "epoch 85 [3.44s]: training loss=0.1199864074587822  validation ndcg@10=0.024697391131237728 [0.28s]\n",
      "epoch 86 [3.41s]:  training loss=0.11856865137815475                                       \n",
      "epoch 87 [3.53s]:  training loss=0.11796646565198898                                       \n",
      "epoch 88 [3.42s]:  training loss=0.1162092313170433                                        \n",
      "epoch 89 [3.32s]:  training loss=0.11397209018468857                                       \n",
      "epoch 90 [3.62s]: training loss=0.1144031286239624  validation ndcg@10=0.025031513087947342 [0.29s]\n",
      "epoch 91 [3.48s]:  training loss=0.11544995754957199                                       \n",
      "epoch 92 [3.47s]:  training loss=0.11251319199800491                                       \n",
      "epoch 93 [3.36s]:  training loss=0.11228562891483307                                       \n",
      "epoch 94 [3.51s]:  training loss=0.11289460957050323                                       \n",
      "epoch 95 [3.41s]: training loss=0.10881710052490234  validation ndcg@10=0.026260809491126264 [0.29s]\n",
      "epoch 96 [3.41s]:  training loss=0.10958912968635559                                       \n",
      "epoch 97 [3.56s]:  training loss=0.11016330868005753                                       \n",
      "epoch 98 [3.49s]:  training loss=0.10907676815986633                                       \n",
      "epoch 99 [3.49s]:  training loss=0.10757649689912796                                       \n",
      "epoch 100 [3.51s]: training loss=0.10734819620847702  validation ndcg@10=0.026179803831264562 [0.27s]\n",
      "epoch 101 [3.6s]:  training loss=0.11012892425060272                                       \n",
      "epoch 102 [3.65s]:  training loss=0.10595463961362839                                      \n",
      "epoch 103 [3.68s]:  training loss=0.10925799608230591                                      \n",
      "epoch 104 [3.38s]:  training loss=0.10657017678022385                                      \n",
      "epoch 105 [3.36s]: training loss=0.10725608468055725  validation ndcg@10=0.02677173690472124 [0.27s]\n",
      "epoch 106 [3.42s]:  training loss=0.10524804145097733                                      \n",
      "epoch 107 [3.51s]:  training loss=0.10230948776006699                                      \n",
      "epoch 108 [3.53s]:  training loss=0.10562295466661453                                      \n",
      "epoch 109 [3.54s]:  training loss=0.1048903614282608                                       \n",
      "epoch 110 [3.46s]: training loss=0.1013195812702179  validation ndcg@10=0.02641279458815101 [0.28s]\n",
      "epoch 111 [3.56s]:  training loss=0.1029239222407341                                       \n",
      "epoch 112 [3.53s]:  training loss=0.10203859210014343                                      \n",
      "epoch 113 [3.55s]:  training loss=0.10076528042554855                                      \n",
      "epoch 114 [3.29s]:  training loss=0.09997644275426865                                      \n",
      "epoch 115 [3.49s]: training loss=0.10108374059200287  validation ndcg@10=0.026146681851924362 [0.27s]\n",
      "epoch 116 [3.42s]:  training loss=0.09695110470056534                                      \n",
      "epoch 117 [3.56s]:  training loss=0.09922129660844803                                      \n",
      "epoch 118 [3.46s]:  training loss=0.09724203497171402                                      \n",
      "epoch 119 [3.38s]:  training loss=0.09596209973096848                                      \n",
      "epoch 120 [3.56s]: training loss=0.09747280180454254  validation ndcg@10=0.025080191010601886 [0.29s]\n",
      "epoch 121 [3.45s]:  training loss=0.09748779982328415                                      \n",
      "epoch 122 [3.4s]:  training loss=0.09814547747373581                                       \n",
      "epoch 123 [3.36s]:  training loss=0.09835570305585861                                      \n",
      "epoch 124 [3.33s]:  training loss=0.09481389075517654                                      \n",
      "epoch 125 [3.47s]: training loss=0.0983717292547226  validation ndcg@10=0.02631596883014346 [0.29s]\n",
      "epoch 126 [3.48s]:  training loss=0.09580879658460617                                      \n",
      "epoch 127 [3.36s]:  training loss=0.0954047366976738                                       \n",
      "epoch 128 [3.33s]:  training loss=0.0964897945523262                                       \n",
      "epoch 129 [3.48s]:  training loss=0.09517872333526611                                      \n",
      "epoch 130 [3.31s]: training loss=0.09539380669593811  validation ndcg@10=0.026757432602628355 [0.26s]\n",
      "epoch 1 [36.59s]:  training loss=0.4007628858089447                                        \n",
      "epoch 2 [39.58s]:  training loss=0.2492251992225647                                        \n",
      "epoch 3 [40.07s]:  training loss=0.20612940192222595                                       \n",
      "epoch 4 [39.67s]:  training loss=0.18125325441360474                                       \n",
      "epoch 5 [39.87s]: training loss=0.16697239875793457  validation ndcg@10=0.021524966356333382 [0.98s]\n",
      "epoch 6 [40.13s]:  training loss=0.15736448764801025                                       \n",
      "epoch 7 [40.16s]:  training loss=0.14088593423366547                                       \n",
      "epoch 8 [40.01s]:  training loss=0.13366098701953888                                       \n",
      "epoch 9 [39.74s]:  training loss=0.12099716067314148                                       \n",
      "epoch 10 [39.64s]: training loss=0.11571480333805084  validation ndcg@10=0.023836021438157048 [2.1s]\n",
      "epoch 11 [38.8s]:  training loss=0.11056786775588989                                       \n",
      "epoch 12 [40.03s]:  training loss=0.1070609912276268                                       \n",
      "epoch 13 [39.85s]:  training loss=0.09837403893470764                                      \n",
      "epoch 14 [39.46s]:  training loss=0.0960877314209938                                       \n",
      "epoch 15 [40.62s]: training loss=0.09300082176923752  validation ndcg@10=0.02465527113616213 [0.97s]\n",
      "epoch 16 [39.56s]:  training loss=0.08897960931062698                                      \n",
      "epoch 17 [40.13s]:  training loss=0.0865599736571312                                       \n",
      "epoch 18 [39.71s]:  training loss=0.08365075290203094                                      \n",
      "epoch 19 [39.64s]:  training loss=0.08118059486150742                                      \n",
      "epoch 20 [39.62s]: training loss=0.07849576324224472  validation ndcg@10=0.024884779530706787 [0.96s]\n",
      "epoch 21 [39.67s]:  training loss=0.07784594595432281                                      \n",
      "epoch 22 [39.57s]:  training loss=0.07516977936029434                                      \n",
      "epoch 23 [39.45s]:  training loss=0.07351629436016083                                      \n",
      "epoch 24 [39.72s]:  training loss=0.0703779011964798                                       \n",
      "epoch 25 [40.44s]: training loss=0.07192496210336685  validation ndcg@10=0.026020114299172066 [0.98s]\n",
      "epoch 26 [39.85s]:  training loss=0.07035285979509354                                      \n",
      "epoch 27 [39.88s]:  training loss=0.06675700843334198                                      \n",
      "epoch 28 [40.16s]:  training loss=0.06527300924062729                                      \n",
      "epoch 29 [40.66s]:  training loss=0.06346738338470459                                      \n",
      "epoch 30 [40.09s]: training loss=0.06557392328977585  validation ndcg@10=0.02509514149483322 [0.94s]\n",
      "epoch 31 [39.65s]:  training loss=0.0638844221830368                                       \n",
      "epoch 32 [39.89s]:  training loss=0.06412167102098465                                      \n",
      "epoch 33 [39.72s]:  training loss=0.061419542878866196                                     \n",
      "epoch 34 [39.43s]:  training loss=0.057126522064208984                                     \n",
      "epoch 35 [39.81s]: training loss=0.05649927631020546  validation ndcg@10=0.023719145756878984 [0.92s]\n",
      "epoch 36 [39.97s]:  training loss=0.05800015106797218                                      \n",
      "epoch 37 [40.16s]:  training loss=0.05794350057840347                                      \n",
      "epoch 38 [39.94s]:  training loss=0.05595916882157326                                      \n",
      "epoch 39 [40.02s]:  training loss=0.05367234721779823                                      \n",
      "epoch 40 [39.76s]: training loss=0.053565848618745804  validation ndcg@10=0.024623292632913754 [0.96s]\n",
      "epoch 41 [39.82s]:  training loss=0.05687178298830986                                      \n",
      "epoch 42 [40.04s]:  training loss=0.05490756407380104                                      \n",
      "epoch 43 [39.99s]:  training loss=0.053173113614320755                                     \n",
      "epoch 44 [39.8s]:  training loss=0.0525440014898777                                        \n",
      "epoch 45 [39.47s]: training loss=0.05044122040271759  validation ndcg@10=0.026116929838503156 [0.97s]\n",
      "epoch 46 [39.91s]:  training loss=0.05161743238568306                                      \n",
      "epoch 47 [40.01s]:  training loss=0.0496879443526268                                       \n",
      "epoch 48 [41.68s]:  training loss=0.049459509551525116                                     \n",
      "epoch 49 [39.35s]:  training loss=0.049602482467889786                                     \n",
      "epoch 50 [39.94s]: training loss=0.05244635418057442  validation ndcg@10=0.02481609785794954 [1.0s]\n",
      "epoch 51 [39.61s]:  training loss=0.04858766868710518                                      \n",
      "epoch 52 [39.76s]:  training loss=0.04854420945048332                                      \n",
      "epoch 53 [39.94s]:  training loss=0.04782877489924431                                      \n",
      "epoch 54 [39.18s]:  training loss=0.049427393823862076                                     \n",
      "epoch 55 [39.48s]: training loss=0.049758072942495346  validation ndcg@10=0.023625573081448505 [0.96s]\n",
      "epoch 56 [39.5s]:  training loss=0.04495726898312569                                       \n",
      "epoch 57 [39.68s]:  training loss=0.04727102816104889                                      \n",
      "epoch 58 [39.89s]:  training loss=0.04810063913464546                                      \n",
      "epoch 59 [39.41s]:  training loss=0.04605986922979355                                      \n",
      "epoch 60 [39.69s]: training loss=0.04520491510629654  validation ndcg@10=0.024959920404409936 [0.95s]\n",
      "epoch 61 [39.54s]:  training loss=0.04484844580292702                                      \n",
      "epoch 62 [39.78s]:  training loss=0.04287092015147209                                      \n",
      "epoch 63 [39.95s]:  training loss=0.0440949946641922                                       \n",
      "epoch 64 [37.86s]:  training loss=0.04397350177168846                                      \n",
      "epoch 65 [38.38s]: training loss=0.04482109472155571  validation ndcg@10=0.023305874596311272 [0.95s]\n",
      "epoch 66 [38.48s]:  training loss=0.04340407997369766                                      \n",
      "epoch 67 [39.52s]:  training loss=0.0434287004172802                                       \n",
      "epoch 68 [40.51s]:  training loss=0.041920725256204605                                     \n",
      "epoch 69 [39.02s]:  training loss=0.0427398718893528                                       \n",
      "epoch 70 [39.84s]: training loss=0.0407925546169281  validation ndcg@10=0.024931178060602473 [0.96s]\n",
      "epoch 1 [47.3s]:  training loss=0.6168021559715271                                         \n",
      "epoch 2 [45.18s]:  training loss=0.5487349629402161                                        \n",
      "epoch 3 [43.39s]:  training loss=0.502883791923523                                         \n",
      "epoch 4 [42.27s]:  training loss=0.46372827887535095                                       \n",
      "epoch 5 [41.45s]: training loss=0.4341035783290863  validation ndcg@10=0.005485632101960907 [0.92s]\n",
      "epoch 6 [40.1s]:  training loss=0.4035464823246002                                         \n",
      "epoch 7 [38.84s]:  training loss=0.37683388590812683                                       \n",
      "epoch 8 [38.98s]:  training loss=0.35169661045074463                                       \n",
      "epoch 9 [38.88s]:  training loss=0.3385356664657593                                        \n",
      "epoch 10 [38.95s]: training loss=0.3203422725200653  validation ndcg@10=0.009933238698168102 [0.87s]\n",
      "epoch 11 [38.71s]:  training loss=0.30878064036369324                                      \n",
      "epoch 12 [38.92s]:  training loss=0.2883160710334778                                       \n",
      "epoch 13 [39.21s]:  training loss=0.2780854105949402                                       \n",
      "epoch 14 [38.86s]:  training loss=0.27210474014282227                                      \n",
      "epoch 15 [38.69s]: training loss=0.26104676723480225  validation ndcg@10=0.01547999034512532 [0.85s]\n",
      "epoch 16 [38.73s]:  training loss=0.24441799521446228                                      \n",
      "epoch 17 [40.73s]:  training loss=0.23991751670837402                                      \n",
      "epoch 18 [38.9s]:  training loss=0.23699617385864258                                       \n",
      "epoch 19 [38.75s]:  training loss=0.22969092428684235                                      \n",
      "epoch 20 [38.79s]: training loss=0.22751638293266296  validation ndcg@10=0.01799362152619372 [0.86s]\n",
      "epoch 21 [38.79s]:  training loss=0.219587504863739                                        \n",
      "epoch 22 [39.06s]:  training loss=0.21660774946212769                                      \n",
      "epoch 23 [39.0s]:  training loss=0.21482209861278534                                       \n",
      "epoch 24 [39.1s]:  training loss=0.21349100768566132                                       \n",
      "epoch 25 [39.11s]: training loss=0.210838183760643  validation ndcg@10=0.019216048835917127 [0.86s]\n",
      "epoch 26 [39.07s]:  training loss=0.20732884109020233                                      \n",
      "epoch 27 [39.7s]:  training loss=0.2020089328289032                                        \n",
      "epoch 28 [39.04s]:  training loss=0.1989029496908188                                       \n",
      "epoch 29 [39.08s]:  training loss=0.19534806907176971                                      \n",
      "epoch 30 [39.15s]: training loss=0.1956312358379364  validation ndcg@10=0.01950318930175093 [0.87s]\n",
      "epoch 31 [39.11s]:  training loss=0.19065696001052856                                      \n",
      "epoch 32 [39.0s]:  training loss=0.19012290239334106                                       \n",
      "epoch 33 [39.02s]:  training loss=0.19111573696136475                                      \n",
      "epoch 34 [38.93s]:  training loss=0.18668195605278015                                      \n",
      "epoch 35 [38.96s]: training loss=0.1853489875793457  validation ndcg@10=0.020079280848599775 [0.84s]\n",
      "epoch 36 [39.03s]:  training loss=0.1840331256389618                                       \n",
      "epoch 37 [40.7s]:  training loss=0.18198446929454803                                       \n",
      "epoch 38 [38.94s]:  training loss=0.17855197191238403                                      \n",
      "epoch 39 [39.04s]:  training loss=0.18061859905719757                                      \n",
      "epoch 40 [38.92s]: training loss=0.1768973171710968  validation ndcg@10=0.020537066129322566 [0.86s]\n",
      "epoch 41 [39.51s]:  training loss=0.17470614612102509                                      \n",
      "epoch 42 [38.9s]:  training loss=0.1704467535018921                                        \n",
      "epoch 43 [38.9s]:  training loss=0.16993847489356995                                       \n",
      "epoch 44 [39.55s]:  training loss=0.17092280089855194                                      \n",
      "epoch 45 [38.88s]: training loss=0.17035835981369019  validation ndcg@10=0.020324211185130045 [0.85s]\n",
      "epoch 46 [38.95s]:  training loss=0.16746948659420013                                      \n",
      "epoch 47 [38.53s]:  training loss=0.16765789687633514                                      \n",
      "epoch 48 [39.01s]:  training loss=0.16066236793994904                                      \n",
      "epoch 49 [38.85s]:  training loss=0.16559742391109467                                      \n",
      "epoch 50 [38.88s]: training loss=0.16057084500789642  validation ndcg@10=0.020949591147543384 [0.88s]\n",
      "epoch 51 [39.03s]:  training loss=0.16274110972881317                                      \n",
      "epoch 52 [38.99s]:  training loss=0.16087937355041504                                      \n",
      "epoch 53 [39.12s]:  training loss=0.1571192592382431                                       \n",
      "epoch 54 [39.06s]:  training loss=0.15681084990501404                                      \n",
      "epoch 55 [39.61s]: training loss=0.15783703327178955  validation ndcg@10=0.02108070211978473 [0.88s]\n",
      "epoch 56 [39.1s]:  training loss=0.15634943544864655                                       \n",
      "epoch 57 [40.66s]:  training loss=0.1548822820186615                                       \n",
      "epoch 58 [39.0s]:  training loss=0.14854440093040466                                       \n",
      "epoch 59 [38.45s]:  training loss=0.15211322903633118                                      \n",
      "epoch 60 [39.02s]: training loss=0.1491585522890091  validation ndcg@10=0.021805337868421076 [0.85s]\n",
      "epoch 61 [39.19s]:  training loss=0.1497740000486374                                       \n",
      "epoch 62 [39.1s]:  training loss=0.14712752401828766                                       \n",
      "epoch 63 [38.97s]:  training loss=0.14750054478645325                                      \n",
      "epoch 64 [39.12s]:  training loss=0.14594772458076477                                      \n",
      "epoch 65 [38.93s]: training loss=0.14305660128593445  validation ndcg@10=0.021746731519306874 [0.86s]\n",
      "epoch 66 [38.81s]:  training loss=0.1446475237607956                                       \n",
      "epoch 67 [39.08s]:  training loss=0.14425329864025116                                      \n",
      "epoch 68 [38.93s]:  training loss=0.14372195303440094                                      \n",
      "epoch 69 [39.42s]:  training loss=0.14561697840690613                                      \n",
      "epoch 70 [38.88s]: training loss=0.1394699215888977  validation ndcg@10=0.02172054193351601 [0.86s]\n",
      "epoch 71 [38.95s]:  training loss=0.13910029828548431                                      \n",
      "epoch 72 [39.08s]:  training loss=0.13904184103012085                                      \n",
      "epoch 73 [38.95s]:  training loss=0.1389840990304947                                       \n",
      "epoch 74 [39.12s]:  training loss=0.1392780840396881                                       \n",
      "epoch 75 [39.12s]: training loss=0.1335294097661972  validation ndcg@10=0.021971177187684113 [0.85s]\n",
      "epoch 76 [39.1s]:  training loss=0.135351300239563                                         \n",
      "epoch 77 [40.62s]:  training loss=0.13612332940101624                                      \n",
      "epoch 78 [39.05s]:  training loss=0.1332850605249405                                       \n",
      "epoch 79 [38.79s]:  training loss=0.12960375845432281                                      \n",
      "epoch 80 [39.14s]: training loss=0.13406245410442352  validation ndcg@10=0.022417711575399683 [0.89s]\n",
      "epoch 81 [38.9s]:  training loss=0.13242220878601074                                       \n",
      "epoch 82 [39.0s]:  training loss=0.13046425580978394                                       \n",
      "epoch 83 [39.73s]:  training loss=0.13077743351459503                                      \n",
      "epoch 84 [38.98s]:  training loss=0.1302298754453659                                       \n",
      "epoch 85 [38.96s]: training loss=0.13103923201560974  validation ndcg@10=0.02209342719044214 [0.86s]\n",
      "epoch 86 [38.86s]:  training loss=0.12791766226291656                                      \n",
      "epoch 87 [38.92s]:  training loss=0.12772314250469208                                      \n",
      "epoch 88 [38.98s]:  training loss=0.12356538325548172                                      \n",
      "epoch 89 [38.81s]:  training loss=0.12573184072971344                                      \n",
      "epoch 90 [38.7s]: training loss=0.12567652761936188  validation ndcg@10=0.02277183723187878 [0.87s]\n",
      "epoch 91 [38.99s]:  training loss=0.12361247092485428                                      \n",
      "epoch 92 [38.93s]:  training loss=0.12281531095504761                                      \n",
      "epoch 93 [39.06s]:  training loss=0.1216186061501503                                       \n",
      "epoch 94 [39.21s]:  training loss=0.11960858106613159                                      \n",
      "epoch 95 [39.04s]: training loss=0.12207457423210144  validation ndcg@10=0.022836673244499315 [0.85s]\n",
      "epoch 96 [41.08s]:  training loss=0.11976107954978943                                      \n",
      "epoch 97 [39.14s]:  training loss=0.11763405054807663                                      \n",
      "epoch 98 [39.05s]:  training loss=0.12020333856344223                                      \n",
      "epoch 99 [39.25s]:  training loss=0.12089549005031586                                      \n",
      "epoch 100 [38.97s]: training loss=0.11945437639951706  validation ndcg@10=0.022465039548324166 [0.87s]\n",
      "epoch 101 [39.1s]:  training loss=0.11765722185373306                                      \n",
      "epoch 102 [38.89s]:  training loss=0.11669938266277313                                     \n",
      "epoch 103 [38.97s]:  training loss=0.11978093534708023                                     \n",
      "epoch 104 [38.98s]:  training loss=0.11679338663816452                                     \n",
      "epoch 105 [38.97s]: training loss=0.11656112223863602  validation ndcg@10=0.023411794601084196 [0.87s]\n",
      "epoch 106 [38.84s]:  training loss=0.11597571521997452                                     \n",
      "epoch 107 [38.76s]:  training loss=0.1127845048904419                                      \n",
      "epoch 108 [38.87s]:  training loss=0.1162143126130104                                      \n",
      "epoch 109 [38.98s]:  training loss=0.11262842267751694                                     \n",
      "epoch 110 [38.83s]: training loss=0.11332288384437561  validation ndcg@10=0.022468285425411832 [0.86s]\n",
      "epoch 111 [39.36s]:  training loss=0.11298651993274689                                     \n",
      "epoch 112 [38.79s]:  training loss=0.11467679589986801                                     \n",
      "epoch 113 [38.99s]:  training loss=0.11246165633201599                                     \n",
      "epoch 114 [38.96s]:  training loss=0.11060573905706406                                     \n",
      "epoch 115 [38.82s]: training loss=0.11220850050449371  validation ndcg@10=0.023724394377365933 [0.85s]\n",
      "epoch 116 [40.79s]:  training loss=0.11147185415029526                                     \n",
      "epoch 117 [38.83s]:  training loss=0.10950782150030136                                     \n",
      "epoch 118 [39.08s]:  training loss=0.10698121786117554                                     \n",
      "epoch 119 [38.81s]:  training loss=0.10904167592525482                                     \n",
      "epoch 120 [38.94s]: training loss=0.109367236495018  validation ndcg@10=0.024510384715505696 [0.87s]\n",
      "epoch 121 [38.91s]:  training loss=0.10884687304496765                                     \n",
      "epoch 122 [39.13s]:  training loss=0.10641247779130936                                     \n",
      "epoch 123 [38.83s]:  training loss=0.10806921124458313                                     \n",
      "epoch 124 [38.59s]:  training loss=0.10613798350095749                                     \n",
      "epoch 125 [39.49s]: training loss=0.10466304421424866  validation ndcg@10=0.02381526454979335 [0.86s]\n",
      "epoch 126 [38.95s]:  training loss=0.10455374419689178                                     \n",
      "epoch 127 [38.87s]:  training loss=0.10473864525556564                                     \n",
      "epoch 128 [38.81s]:  training loss=0.10357390344142914                                     \n",
      "epoch 129 [38.81s]:  training loss=0.10504130274057388                                     \n",
      "epoch 130 [38.84s]: training loss=0.10145244747400284  validation ndcg@10=0.024219957382285447 [0.87s]\n",
      "epoch 131 [38.85s]:  training loss=0.10528138279914856                                     \n",
      "epoch 132 [38.63s]:  training loss=0.10324316471815109                                     \n",
      "epoch 133 [38.92s]:  training loss=0.10111131519079208                                     \n",
      "epoch 134 [39.03s]:  training loss=0.10320086032152176                                     \n",
      "epoch 135 [38.92s]: training loss=0.10353340208530426  validation ndcg@10=0.024248027106612828 [0.84s]\n",
      "epoch 136 [41.09s]:  training loss=0.1020428016781807                                      \n",
      "epoch 137 [38.8s]:  training loss=0.10070917755365372                                      \n",
      "epoch 138 [38.94s]:  training loss=0.10045605152845383                                     \n",
      "epoch 139 [39.49s]:  training loss=0.09996116906404495                                     \n",
      "epoch 140 [39.08s]: training loss=0.10356133431196213  validation ndcg@10=0.024540373232542496 [0.86s]\n",
      "epoch 141 [38.88s]:  training loss=0.09744132310152054                                     \n",
      "epoch 142 [38.9s]:  training loss=0.09747039526700974                                      \n",
      "epoch 143 [38.67s]:  training loss=0.09986615180969238                                     \n",
      "epoch 144 [38.92s]:  training loss=0.098188117146492                                       \n",
      "epoch 145 [38.99s]: training loss=0.09915847331285477  validation ndcg@10=0.024173268159172215 [0.92s]\n",
      "epoch 146 [38.74s]:  training loss=0.09868687391281128                                     \n",
      "epoch 147 [38.92s]:  training loss=0.09659942984580994                                     \n",
      "epoch 148 [39.01s]:  training loss=0.09326818585395813                                     \n",
      "epoch 149 [38.84s]:  training loss=0.09602676331996918                                     \n",
      "epoch 150 [39.12s]: training loss=0.0967133417725563  validation ndcg@10=0.024403554600929352 [0.88s]\n",
      "epoch 151 [39.19s]:  training loss=0.09518595784902573                                     \n",
      "epoch 152 [39.01s]:  training loss=0.09578044712543488                                     \n",
      "epoch 153 [39.53s]:  training loss=0.09300653636455536                                     \n",
      "epoch 154 [38.98s]:  training loss=0.09333678334951401                                     \n",
      "epoch 155 [38.94s]: training loss=0.09517015516757965  validation ndcg@10=0.024772842954043337 [0.86s]\n",
      "epoch 156 [40.37s]:  training loss=0.0905996710062027                                      \n",
      "epoch 157 [38.81s]:  training loss=0.09304725378751755                                     \n",
      "epoch 158 [38.87s]:  training loss=0.09195637702941895                                     \n",
      "epoch 159 [38.95s]:  training loss=0.09250675141811371                                     \n",
      "epoch 160 [38.93s]: training loss=0.09279730170965195  validation ndcg@10=0.0248963967689765 [0.87s]\n",
      "epoch 161 [38.7s]:  training loss=0.09271202236413956                                      \n",
      "epoch 162 [38.9s]:  training loss=0.09400662779808044                                      \n",
      "epoch 163 [39.0s]:  training loss=0.09384845942258835                                      \n",
      "epoch 164 [38.83s]:  training loss=0.0915210172533989                                      \n",
      "epoch 165 [39.04s]: training loss=0.09173621982336044  validation ndcg@10=0.024638788730793306 [0.86s]\n",
      "epoch 166 [39.04s]:  training loss=0.09101694077253342                                     \n",
      "epoch 167 [39.11s]:  training loss=0.08932119607925415                                     \n",
      "epoch 168 [39.0s]:  training loss=0.09253320097923279                                      \n",
      "epoch 169 [38.89s]:  training loss=0.08956527709960938                                     \n",
      "epoch 170 [38.9s]: training loss=0.09024938195943832  validation ndcg@10=0.024666168550572944 [0.85s]\n",
      "epoch 171 [38.9s]:  training loss=0.08872400969266891                                      \n",
      "epoch 172 [38.87s]:  training loss=0.0887758731842041                                      \n",
      "epoch 173 [39.0s]:  training loss=0.08884358406066895                                      \n",
      "epoch 174 [38.91s]:  training loss=0.09069014340639114                                     \n",
      "epoch 175 [38.87s]: training loss=0.08787256479263306  validation ndcg@10=0.02516681000724732 [0.86s]\n",
      "epoch 176 [40.68s]:  training loss=0.08632314205169678                                     \n",
      "epoch 177 [38.94s]:  training loss=0.08523142337799072                                     \n",
      "epoch 178 [38.7s]:  training loss=0.08944767713546753                                      \n",
      "epoch 179 [38.97s]:  training loss=0.08851735293865204                                     \n",
      "epoch 180 [39.02s]: training loss=0.08650768548250198  validation ndcg@10=0.02508431731878332 [0.91s]\n",
      "epoch 181 [38.99s]:  training loss=0.08651801198720932                                     \n",
      "epoch 182 [38.98s]:  training loss=0.08751029521226883                                     \n",
      "epoch 183 [38.87s]:  training loss=0.08697744458913803                                     \n",
      "epoch 184 [38.98s]:  training loss=0.08448032289743423                                     \n",
      "epoch 185 [39.05s]: training loss=0.08793719857931137  validation ndcg@10=0.025378264507709945 [0.86s]\n",
      "epoch 186 [39.24s]:  training loss=0.08083092421293259                                     \n",
      "epoch 187 [39.04s]:  training loss=0.08463110029697418                                     \n",
      "epoch 188 [38.95s]:  training loss=0.08358217030763626                                     \n",
      "epoch 189 [38.84s]:  training loss=0.08489980548620224                                     \n",
      "epoch 190 [39.15s]: training loss=0.08386624604463577  validation ndcg@10=0.024935209467661302 [0.82s]\n",
      "epoch 191 [39.03s]:  training loss=0.08526694029569626                                     \n",
      "epoch 192 [38.86s]:  training loss=0.08558137714862823                                     \n",
      "epoch 193 [39.02s]:  training loss=0.0811941921710968                                      \n",
      "epoch 194 [39.16s]:  training loss=0.08361131697893143                                     \n",
      "epoch 195 [39.35s]: training loss=0.08419158309698105  validation ndcg@10=0.025221286690586143 [1.13s]\n",
      "epoch 196 [40.14s]:  training loss=0.08302558958530426                                     \n",
      "epoch 197 [38.86s]:  training loss=0.08216928690671921                                     \n",
      "epoch 198 [38.79s]:  training loss=0.08285289257764816                                     \n",
      "epoch 199 [39.02s]:  training loss=0.0837339535355568                                      \n",
      "epoch 200 [38.84s]: training loss=0.08456961065530777  validation ndcg@10=0.025420024230992325 [0.84s]\n",
      "epoch 1 [7.11s]:  training loss=0.6650083661079407                                          \n",
      "epoch 2 [7.29s]:  training loss=0.6482433676719666                                          \n",
      "epoch 3 [7.62s]:  training loss=0.6455155611038208                                          \n",
      "epoch 4 [7.39s]:  training loss=0.6307254433631897                                          \n",
      "epoch 5 [7.52s]: training loss=0.6249524354934692  validation ndcg@10=0.003909507985301859 [0.34s]\n",
      "epoch 6 [7.58s]:  training loss=0.6193146705627441                                          \n",
      "epoch 7 [7.92s]:  training loss=0.6153356432914734                                          \n",
      "epoch 8 [7.88s]:  training loss=0.6026616096496582                                          \n",
      "epoch 9 [7.78s]:  training loss=0.5923838019371033                                          \n",
      "epoch 10 [7.98s]: training loss=0.5873063802719116  validation ndcg@10=0.0045857670512303 [0.41s]\n",
      "epoch 11 [8.19s]:  training loss=0.5798325538635254                                         \n",
      "epoch 12 [7.66s]:  training loss=0.5696970820426941                                         \n",
      "epoch 13 [7.91s]:  training loss=0.5673398971557617                                         \n",
      "epoch 14 [7.99s]:  training loss=0.5559263229370117                                         \n",
      "epoch 15 [8.11s]: training loss=0.5540257096290588  validation ndcg@10=0.004520863839549676 [0.35s]\n",
      "epoch 16 [7.83s]:  training loss=0.5499081611633301                                         \n",
      "epoch 17 [7.92s]:  training loss=0.5495441555976868                                         \n",
      "epoch 18 [8.16s]:  training loss=0.5385065078735352                                         \n",
      "epoch 19 [7.92s]:  training loss=0.5322217345237732                                         \n",
      "epoch 20 [7.72s]: training loss=0.5272698998451233  validation ndcg@10=0.004716734208654217 [0.36s]\n",
      "epoch 21 [8.36s]:  training loss=0.5225443243980408                                         \n",
      "epoch 22 [8.09s]:  training loss=0.5195834636688232                                         \n",
      "epoch 23 [8.38s]:  training loss=0.5157291293144226                                         \n",
      "epoch 24 [8.48s]:  training loss=0.5127816200256348                                         \n",
      "epoch 25 [8.28s]: training loss=0.5058161616325378  validation ndcg@10=0.004973308780789065 [0.36s]\n",
      "epoch 26 [7.75s]:  training loss=0.4992823898792267                                         \n",
      "epoch 27 [8.08s]:  training loss=0.49378398060798645                                        \n",
      "epoch 28 [8.15s]:  training loss=0.49161118268966675                                        \n",
      "epoch 29 [8.02s]:  training loss=0.4875769317150116                                         \n",
      "epoch 30 [7.84s]: training loss=0.48712044954299927  validation ndcg@10=0.00516105127305475 [0.37s]\n",
      "epoch 31 [7.86s]:  training loss=0.48563316464424133                                        \n",
      "epoch 32 [7.8s]:  training loss=0.4777739346027374                                          \n",
      "epoch 33 [8.08s]:  training loss=0.4734382927417755                                         \n",
      "epoch 34 [8.09s]:  training loss=0.4692569971084595                                         \n",
      "epoch 35 [8.02s]: training loss=0.4677415192127228  validation ndcg@10=0.005179860686090866 [0.37s]\n",
      "epoch 36 [8.23s]:  training loss=0.46427422761917114                                        \n",
      "epoch 37 [8.2s]:  training loss=0.4569493234157562                                          \n",
      "epoch 38 [8.4s]:  training loss=0.4560042917728424                                          \n",
      "epoch 39 [8.38s]:  training loss=0.4539705514907837                                         \n",
      "epoch 40 [7.87s]: training loss=0.44710028171539307  validation ndcg@10=0.005358305478973815 [0.36s]\n",
      "epoch 41 [8.32s]:  training loss=0.4471859633922577                                         \n",
      "epoch 42 [8.18s]:  training loss=0.44432270526885986                                        \n",
      "epoch 43 [7.87s]:  training loss=0.4351290166378021                                         \n",
      "epoch 44 [8.09s]:  training loss=0.4352787435054779                                         \n",
      "epoch 45 [7.92s]: training loss=0.4333089292049408  validation ndcg@10=0.005439963708703544 [0.34s]\n",
      "epoch 46 [8.25s]:  training loss=0.4298752248287201                                         \n",
      "epoch 47 [7.9s]:  training loss=0.42659518122673035                                         \n",
      "epoch 48 [7.82s]:  training loss=0.42702531814575195                                        \n",
      "epoch 49 [8.23s]:  training loss=0.4198821783065796                                         \n",
      "epoch 50 [8.11s]: training loss=0.41385143995285034  validation ndcg@10=0.005749095683728121 [0.37s]\n",
      "epoch 51 [8.08s]:  training loss=0.41608673334121704                                        \n",
      "epoch 52 [8.23s]:  training loss=0.4140856862068176                                         \n",
      "epoch 53 [8.11s]:  training loss=0.4156310558319092                                         \n",
      "epoch 54 [8.01s]:  training loss=0.40838557481765747                                        \n",
      "epoch 55 [8.19s]: training loss=0.40414363145828247  validation ndcg@10=0.00599127318334597 [0.39s]\n",
      "epoch 56 [8.22s]:  training loss=0.4075626730918884                                         \n",
      "epoch 57 [8.05s]:  training loss=0.3971188962459564                                         \n",
      "epoch 58 [8.24s]:  training loss=0.3979521095752716                                         \n",
      "epoch 59 [8.34s]:  training loss=0.4011719822883606                                         \n",
      "epoch 60 [8.04s]: training loss=0.3947584927082062  validation ndcg@10=0.006402034894361079 [0.38s]\n",
      "epoch 61 [7.97s]:  training loss=0.39011695981025696                                        \n",
      "epoch 62 [8.3s]:  training loss=0.38429147005081177                                         \n",
      "epoch 63 [7.78s]:  training loss=0.38586390018463135                                        \n",
      "epoch 64 [8.2s]:  training loss=0.378498375415802                                           \n",
      "epoch 65 [8.18s]: training loss=0.3753682076931  validation ndcg@10=0.006737545912421132 [0.38s]\n",
      "epoch 66 [7.86s]:  training loss=0.3744773268699646                                         \n",
      "epoch 67 [7.95s]:  training loss=0.3798089921474457                                         \n",
      "epoch 68 [8.37s]:  training loss=0.37086841464042664                                        \n",
      "epoch 69 [7.88s]:  training loss=0.3730861246585846                                         \n",
      "epoch 70 [7.9s]: training loss=0.36155208945274353  validation ndcg@10=0.007043274738101123 [0.39s]\n",
      "epoch 71 [9.47s]:  training loss=0.3674498200416565                                         \n",
      "epoch 72 [7.84s]:  training loss=0.36850935220718384                                        \n",
      "epoch 73 [7.68s]:  training loss=0.3648834824562073                                         \n",
      "epoch 74 [8.07s]:  training loss=0.35884565114974976                                        \n",
      "epoch 75 [8.09s]: training loss=0.3568572402000427  validation ndcg@10=0.007591633346076542 [0.35s]\n",
      "epoch 76 [7.84s]:  training loss=0.35840511322021484                                        \n",
      "epoch 77 [7.74s]:  training loss=0.3531299829483032                                         \n",
      "epoch 78 [7.93s]:  training loss=0.35078173875808716                                        \n",
      "epoch 79 [7.79s]:  training loss=0.34722718596458435                                        \n",
      "epoch 80 [7.92s]: training loss=0.3500361442565918  validation ndcg@10=0.008021774997560894 [0.41s]\n",
      "epoch 81 [7.62s]:  training loss=0.3451690971851349                                         \n",
      "epoch 82 [7.64s]:  training loss=0.3423892855644226                                         \n",
      "epoch 83 [8.02s]:  training loss=0.34222373366355896                                        \n",
      "epoch 84 [7.85s]:  training loss=0.33172816038131714                                        \n",
      "epoch 85 [7.87s]: training loss=0.3360063433647156  validation ndcg@10=0.008737114719534788 [0.34s]\n",
      "epoch 86 [7.78s]:  training loss=0.33104926347732544                                        \n",
      "epoch 87 [7.82s]:  training loss=0.33498093485832214                                        \n",
      "epoch 88 [8.17s]:  training loss=0.32847049832344055                                        \n",
      "epoch 89 [7.87s]:  training loss=0.3240056037902832                                         \n",
      "epoch 90 [7.8s]: training loss=0.32334205508232117  validation ndcg@10=0.009393833952243044 [0.38s]\n",
      "epoch 91 [7.97s]:  training loss=0.3247321546077728                                         \n",
      "epoch 92 [7.77s]:  training loss=0.3217940032482147                                         \n",
      "epoch 93 [8.09s]:  training loss=0.31857213377952576                                        \n",
      "epoch 94 [7.56s]:  training loss=0.31797513365745544                                        \n",
      "epoch 95 [8.0s]: training loss=0.32220903038978577  validation ndcg@10=0.010366369972898259 [0.36s]\n",
      "epoch 96 [8.0s]:  training loss=0.3134464919567108                                          \n",
      "epoch 97 [7.74s]:  training loss=0.3094039857387543                                         \n",
      "epoch 98 [7.77s]:  training loss=0.31447604298591614                                        \n",
      "epoch 99 [7.95s]:  training loss=0.31023773550987244                                        \n",
      "epoch 100 [8.0s]: training loss=0.3068441152572632  validation ndcg@10=0.011118471267291697 [0.33s]\n",
      "epoch 101 [7.9s]:  training loss=0.3048766255378723                                         \n",
      "epoch 102 [8.1s]:  training loss=0.3042546808719635                                         \n",
      "epoch 103 [8.3s]:  training loss=0.3037792444229126                                         \n",
      "epoch 104 [8.02s]:  training loss=0.30051594972610474                                       \n",
      "epoch 105 [8.11s]: training loss=0.2992304861545563  validation ndcg@10=0.012017466234870853 [0.39s]\n",
      "epoch 106 [8.26s]:  training loss=0.29937583208084106                                       \n",
      "epoch 107 [8.19s]:  training loss=0.29904255270957947                                       \n",
      "epoch 108 [8.16s]:  training loss=0.2967003881931305                                        \n",
      "epoch 109 [7.89s]:  training loss=0.2932755649089813                                        \n",
      "epoch 110 [7.94s]: training loss=0.293082594871521  validation ndcg@10=0.01295025003346828 [0.35s]\n",
      "epoch 111 [8.51s]:  training loss=0.2915255129337311                                        \n",
      "epoch 112 [7.96s]:  training loss=0.2914522588253021                                        \n",
      "epoch 113 [7.99s]:  training loss=0.2888868749141693                                        \n",
      "epoch 114 [8.14s]:  training loss=0.28427284955978394                                       \n",
      "epoch 115 [7.88s]: training loss=0.28553032875061035  validation ndcg@10=0.013967577616914258 [0.41s]\n",
      "epoch 116 [8.0s]:  training loss=0.2830618917942047                                         \n",
      "epoch 117 [8.06s]:  training loss=0.2865177094936371                                        \n",
      "epoch 118 [8.12s]:  training loss=0.2838420271873474                                        \n",
      "epoch 119 [7.79s]:  training loss=0.284334659576416                                         \n",
      "epoch 120 [8.33s]: training loss=0.27673599123954773  validation ndcg@10=0.01469134750413641 [0.35s]\n",
      "epoch 121 [8.21s]:  training loss=0.2818842828273773                                        \n",
      "epoch 122 [8.32s]:  training loss=0.28076913952827454                                       \n",
      "epoch 123 [8.15s]:  training loss=0.2725248336791992                                        \n",
      "epoch 124 [7.94s]:  training loss=0.2769891917705536                                        \n",
      "epoch 125 [8.05s]: training loss=0.27040186524391174  validation ndcg@10=0.015339395162101017 [0.38s]\n",
      "epoch 126 [8.18s]:  training loss=0.2707700729370117                                        \n",
      "epoch 127 [7.72s]:  training loss=0.2687729597091675                                        \n",
      "epoch 128 [8.2s]:  training loss=0.2716713845729828                                         \n",
      "epoch 129 [8.36s]:  training loss=0.27157869935035706                                       \n",
      "epoch 130 [8.0s]: training loss=0.2696229815483093  validation ndcg@10=0.015609533268133692 [0.36s]\n",
      "epoch 131 [8.36s]:  training loss=0.2698220908641815                                        \n",
      "epoch 132 [8.38s]:  training loss=0.26413777470588684                                       \n",
      "epoch 133 [8.07s]:  training loss=0.2653351426124573                                        \n",
      "epoch 134 [7.98s]:  training loss=0.26132798194885254                                       \n",
      "epoch 135 [7.89s]: training loss=0.2607609033584595  validation ndcg@10=0.01621481019969796 [0.35s]\n",
      "epoch 136 [7.71s]:  training loss=0.25573089718818665                                       \n",
      "epoch 137 [7.84s]:  training loss=0.25938430428504944                                       \n",
      "epoch 138 [7.87s]:  training loss=0.2637852430343628                                        \n",
      "epoch 139 [7.59s]:  training loss=0.25792932510375977                                       \n",
      "epoch 140 [8.11s]: training loss=0.26231658458709717  validation ndcg@10=0.016993642706058628 [0.4s]\n",
      "epoch 141 [7.89s]:  training loss=0.25761252641677856                                       \n",
      "epoch 142 [7.89s]:  training loss=0.25512760877609253                                       \n",
      "epoch 143 [7.94s]:  training loss=0.25200697779655457                                       \n",
      "epoch 144 [7.76s]:  training loss=0.25264832377433777                                       \n",
      "epoch 145 [7.65s]: training loss=0.25159570574760437  validation ndcg@10=0.01742496148627095 [0.37s]\n",
      "epoch 146 [7.95s]:  training loss=0.25354692339897156                                       \n",
      "epoch 147 [7.88s]:  training loss=0.253606379032135                                         \n",
      "epoch 148 [7.86s]:  training loss=0.25085532665252686                                       \n",
      "epoch 149 [7.83s]:  training loss=0.2514842748641968                                        \n",
      "epoch 150 [7.68s]: training loss=0.2524849772453308  validation ndcg@10=0.017893718505714952 [0.41s]\n",
      "epoch 151 [8.03s]:  training loss=0.24806438386440277                                       \n",
      "epoch 152 [7.87s]:  training loss=0.24860873818397522                                       \n",
      "epoch 153 [8.11s]:  training loss=0.24551351368427277                                       \n",
      "epoch 154 [7.77s]:  training loss=0.24470283091068268                                       \n",
      "epoch 155 [8.07s]: training loss=0.24504423141479492  validation ndcg@10=0.018310467668679636 [0.4s]\n",
      "epoch 156 [7.75s]:  training loss=0.24376076459884644                                       \n",
      "epoch 157 [8.0s]:  training loss=0.242584228515625                                          \n",
      "epoch 158 [8.13s]:  training loss=0.2453310638666153                                        \n",
      "epoch 159 [7.84s]:  training loss=0.24350106716156006                                       \n",
      "epoch 160 [7.87s]: training loss=0.244289830327034  validation ndcg@10=0.018843744669582378 [0.38s]\n",
      "epoch 161 [8.14s]:  training loss=0.23887380957603455                                       \n",
      "epoch 162 [7.72s]:  training loss=0.24058204889297485                                       \n",
      "epoch 163 [8.0s]:  training loss=0.23894959688186646                                        \n",
      "epoch 164 [7.97s]:  training loss=0.24322599172592163                                       \n",
      "epoch 165 [8.15s]: training loss=0.2401915341615677  validation ndcg@10=0.018814524378289583 [0.36s]\n",
      "epoch 166 [9.54s]:  training loss=0.23760682344436646                                       \n",
      "epoch 167 [8.01s]:  training loss=0.23869004845619202                                       \n",
      "epoch 168 [7.65s]:  training loss=0.23633265495300293                                       \n",
      "epoch 169 [7.5s]:  training loss=0.23744569718837738                                        \n",
      "epoch 170 [7.83s]: training loss=0.23966026306152344  validation ndcg@10=0.019273104933787692 [0.34s]\n",
      "epoch 171 [7.78s]:  training loss=0.2364499568939209                                        \n",
      "epoch 172 [8.04s]:  training loss=0.23648905754089355                                       \n",
      "epoch 173 [7.8s]:  training loss=0.23668448626995087                                        \n",
      "epoch 174 [8.22s]:  training loss=0.23082810640335083                                       \n",
      "epoch 175 [7.85s]: training loss=0.2312340885400772  validation ndcg@10=0.019429704257621326 [0.37s]\n",
      "epoch 176 [7.84s]:  training loss=0.23321053385734558                                       \n",
      "epoch 177 [7.97s]:  training loss=0.2325538843870163                                        \n",
      "epoch 178 [8.1s]:  training loss=0.22632014751434326                                        \n",
      "epoch 179 [8.01s]:  training loss=0.22834280133247375                                       \n",
      "epoch 180 [7.78s]: training loss=0.22982147336006165  validation ndcg@10=0.019525318132502693 [0.36s]\n",
      "epoch 181 [8.18s]:  training loss=0.22947728633880615                                       \n",
      "epoch 182 [7.98s]:  training loss=0.22906994819641113                                       \n",
      "epoch 183 [8.21s]:  training loss=0.2250734567642212                                        \n",
      "epoch 184 [8.38s]:  training loss=0.22512765228748322                                       \n",
      "epoch 185 [7.93s]: training loss=0.23127298057079315  validation ndcg@10=0.019544111412281975 [0.39s]\n",
      "epoch 186 [8.09s]:  training loss=0.22878895699977875                                       \n",
      "epoch 187 [8.04s]:  training loss=0.22542217373847961                                       \n",
      "epoch 188 [8.05s]:  training loss=0.22061285376548767                                       \n",
      "epoch 189 [8.07s]:  training loss=0.22447843849658966                                       \n",
      "epoch 190 [8.03s]: training loss=0.22378414869308472  validation ndcg@10=0.020034081769758877 [0.38s]\n",
      "epoch 191 [8.16s]:  training loss=0.22555285692214966                                       \n",
      "epoch 192 [8.3s]:  training loss=0.2259795367717743                                         \n",
      "epoch 193 [7.97s]:  training loss=0.22278741002082825                                       \n",
      "epoch 194 [7.83s]:  training loss=0.22404782474040985                                       \n",
      "epoch 195 [7.89s]: training loss=0.2215747833251953  validation ndcg@10=0.020234475578445918 [0.38s]\n",
      "epoch 196 [7.64s]:  training loss=0.2201651781797409                                        \n",
      "epoch 197 [7.74s]:  training loss=0.22340719401836395                                       \n",
      "epoch 198 [8.15s]:  training loss=0.22017958760261536                                       \n",
      "epoch 199 [7.89s]:  training loss=0.22162558138370514                                       \n",
      "epoch 200 [7.83s]: training loss=0.22090642154216766  validation ndcg@10=0.020315773969391535 [0.35s]\n",
      "epoch 1 [22.9s]:  training loss=0.5987422466278076                                          \n",
      "epoch 2 [24.17s]:  training loss=0.520143449306488                                         \n",
      "epoch 3 [23.89s]:  training loss=0.46681907773017883                                       \n",
      "epoch 4 [23.44s]:  training loss=0.4215962290763855                                        \n",
      "epoch 5 [23.51s]: training loss=0.3871632218360901  validation ndcg@10=0.006705419104249276 [0.7s]\n",
      "epoch 6 [23.39s]:  training loss=0.351064532995224                                         \n",
      "epoch 7 [23.37s]:  training loss=0.32801035046577454                                       \n",
      "epoch 8 [22.57s]:  training loss=0.3086206316947937                                        \n",
      "epoch 9 [22.46s]:  training loss=0.2859162390232086                                        \n",
      "epoch 10 [22.05s]: training loss=0.27143973112106323  validation ndcg@10=0.016794599561551493 [0.67s]\n",
      "epoch 11 [21.77s]:  training loss=0.26117339730262756                                      \n",
      "epoch 12 [21.91s]:  training loss=0.2494840770959854                                       \n",
      "epoch 13 [21.84s]:  training loss=0.24397782981395721                                      \n",
      "epoch 14 [21.77s]:  training loss=0.2314586639404297                                       \n",
      "epoch 15 [22.35s]: training loss=0.22518371045589447  validation ndcg@10=0.019401490797630736 [0.67s]\n",
      "epoch 16 [21.57s]:  training loss=0.2258559763431549                                       \n",
      "epoch 17 [21.51s]:  training loss=0.21519626677036285                                      \n",
      "epoch 18 [21.36s]:  training loss=0.2116697132587433                                       \n",
      "epoch 19 [21.49s]:  training loss=0.2055133581161499                                       \n",
      "epoch 20 [21.05s]: training loss=0.20126916468143463  validation ndcg@10=0.019714832761230157 [0.68s]\n",
      "epoch 21 [20.76s]:  training loss=0.19992129504680634                                      \n",
      "epoch 22 [21.9s]:  training loss=0.194423109292984                                         \n",
      "epoch 23 [19.95s]:  training loss=0.19252128899097443                                      \n",
      "epoch 24 [19.54s]:  training loss=0.19352148473262787                                      \n",
      "epoch 25 [18.26s]: training loss=0.19164122641086578  validation ndcg@10=0.020065205275303376 [0.59s]\n",
      "epoch 26 [17.4s]:  training loss=0.1825961023569107                                        \n",
      "epoch 27 [17.46s]:  training loss=0.18161126971244812                                      \n",
      "epoch 28 [17.49s]:  training loss=0.17952467501163483                                      \n",
      "epoch 29 [17.47s]:  training loss=0.17394039034843445                                      \n",
      "epoch 30 [17.31s]: training loss=0.17573292553424835  validation ndcg@10=0.020404606496840096 [0.61s]\n",
      "epoch 31 [17.4s]:  training loss=0.17345096170902252                                       \n",
      "epoch 32 [17.35s]:  training loss=0.16905562579631805                                      \n",
      "epoch 33 [17.44s]:  training loss=0.16557063162326813                                      \n",
      "epoch 34 [17.31s]:  training loss=0.1632198542356491                                       \n",
      "epoch 35 [17.21s]: training loss=0.16368544101715088  validation ndcg@10=0.020656027219563576 [0.53s]\n",
      "epoch 36 [17.22s]:  training loss=0.16275475919246674                                      \n",
      "epoch 37 [17.27s]:  training loss=0.15876658260822296                                      \n",
      "epoch 38 [17.2s]:  training loss=0.1582283228635788                                        \n",
      "epoch 39 [17.17s]:  training loss=0.15681226551532745                                      \n",
      "epoch 40 [17.23s]: training loss=0.1546989232301712  validation ndcg@10=0.021732485864872978 [0.58s]\n",
      "epoch 41 [17.16s]:  training loss=0.15519273281097412                                      \n",
      "epoch 42 [17.21s]:  training loss=0.14915364980697632                                      \n",
      "epoch 43 [17.21s]:  training loss=0.1513000875711441                                       \n",
      "epoch 44 [17.88s]:  training loss=0.14448662102222443                                      \n",
      "epoch 45 [17.53s]: training loss=0.1453297734260559  validation ndcg@10=0.022103070108856437 [0.56s]\n",
      "epoch 46 [17.22s]:  training loss=0.14840549230575562                                      \n",
      "epoch 47 [17.38s]:  training loss=0.14154469966888428                                      \n",
      "epoch 48 [17.3s]:  training loss=0.13973860442638397                                       \n",
      "epoch 49 [17.37s]:  training loss=0.14301323890686035                                      \n",
      "epoch 50 [17.21s]: training loss=0.1383344680070877  validation ndcg@10=0.0226084537367797 [0.57s]\n",
      "epoch 51 [17.41s]:  training loss=0.13875912129878998                                      \n",
      "epoch 52 [17.39s]:  training loss=0.14068959653377533                                      \n",
      "epoch 53 [17.36s]:  training loss=0.13509206473827362                                      \n",
      "epoch 54 [17.48s]:  training loss=0.13319428265094757                                      \n",
      "epoch 55 [17.24s]: training loss=0.1346540004014969  validation ndcg@10=0.022414684172298756 [0.59s]\n",
      "epoch 56 [17.21s]:  training loss=0.1314956098794937                                       \n",
      "epoch 57 [17.23s]:  training loss=0.1323285698890686                                       \n",
      "epoch 58 [17.32s]:  training loss=0.13114558160305023                                      \n",
      "epoch 59 [17.35s]:  training loss=0.1298578679561615                                       \n",
      "epoch 60 [17.19s]: training loss=0.1285325437784195  validation ndcg@10=0.022614581498132284 [0.57s]\n",
      "epoch 61 [17.23s]:  training loss=0.12610949575901031                                      \n",
      "epoch 62 [17.21s]:  training loss=0.12584277987480164                                      \n",
      "epoch 63 [17.13s]:  training loss=0.12252833694219589                                      \n",
      "epoch 64 [17.2s]:  training loss=0.12273592501878738                                       \n",
      "epoch 65 [17.2s]: training loss=0.12129570543766022  validation ndcg@10=0.023264836483391397 [0.58s]\n",
      "epoch 66 [18.92s]:  training loss=0.12145844101905823                                      \n",
      "epoch 67 [17.15s]:  training loss=0.12024855613708496                                      \n",
      "epoch 68 [17.14s]:  training loss=0.12111054360866547                                      \n",
      "epoch 69 [17.25s]:  training loss=0.11967328935861588                                      \n",
      "epoch 70 [17.13s]: training loss=0.1187438815832138  validation ndcg@10=0.023134413502265916 [0.54s]\n",
      "epoch 71 [17.08s]:  training loss=0.11528727412223816                                      \n",
      "epoch 72 [17.27s]:  training loss=0.11027245968580246                                      \n",
      "epoch 73 [17.44s]:  training loss=0.11700581014156342                                      \n",
      "epoch 74 [17.25s]:  training loss=0.11481740325689316                                      \n",
      "epoch 75 [17.53s]: training loss=0.11584754288196564  validation ndcg@10=0.023069524868994645 [0.59s]\n",
      "epoch 76 [18.14s]:  training loss=0.11386121809482574                                      \n",
      "epoch 77 [17.18s]:  training loss=0.11373364925384521                                      \n",
      "epoch 78 [17.18s]:  training loss=0.11147937923669815                                      \n",
      "epoch 79 [17.31s]:  training loss=0.11038175225257874                                      \n",
      "epoch 80 [17.09s]: training loss=0.10907207429409027  validation ndcg@10=0.022810355831087422 [0.52s]\n",
      "epoch 81 [17.21s]:  training loss=0.10825128853321075                                      \n",
      "epoch 82 [17.14s]:  training loss=0.10736512392759323                                      \n",
      "epoch 83 [17.27s]:  training loss=0.10875127464532852                                      \n",
      "epoch 84 [17.19s]:  training loss=0.10771427303552628                                      \n",
      "epoch 85 [17.2s]: training loss=0.10576499998569489  validation ndcg@10=0.023290649640390723 [0.56s]\n",
      "epoch 86 [17.48s]:  training loss=0.10777724534273148                                      \n",
      "epoch 87 [17.31s]:  training loss=0.10434333980083466                                      \n",
      "epoch 88 [17.16s]:  training loss=0.10382994264364243                                      \n",
      "epoch 89 [17.26s]:  training loss=0.10615331679582596                                      \n",
      "epoch 90 [17.24s]: training loss=0.10666169971227646  validation ndcg@10=0.02342365939211356 [0.54s]\n",
      "epoch 91 [17.24s]:  training loss=0.10328179597854614                                      \n",
      "epoch 92 [17.16s]:  training loss=0.10391303896903992                                      \n",
      "epoch 93 [17.14s]:  training loss=0.10225481539964676                                      \n",
      "epoch 94 [17.42s]:  training loss=0.10249312967061996                                      \n",
      "epoch 95 [17.39s]: training loss=0.1000535786151886  validation ndcg@10=0.0241777810627769 [0.55s]\n",
      "epoch 96 [17.14s]:  training loss=0.09934448450803757                                      \n",
      "epoch 97 [17.18s]:  training loss=0.10052477568387985                                      \n",
      "epoch 98 [17.14s]:  training loss=0.09841404110193253                                      \n",
      "epoch 99 [17.14s]:  training loss=0.09937682747840881                                      \n",
      "epoch 100 [17.18s]: training loss=0.09730616211891174  validation ndcg@10=0.023986432923668824 [0.52s]\n",
      "epoch 101 [17.26s]:  training loss=0.09993083029985428                                     \n",
      "epoch 102 [17.14s]:  training loss=0.09880805760622025                                     \n",
      "epoch 103 [17.21s]:  training loss=0.09854429960250854                                     \n",
      "epoch 104 [17.31s]:  training loss=0.09655985236167908                                     \n",
      "epoch 105 [17.36s]: training loss=0.09535772353410721  validation ndcg@10=0.024471440569459494 [0.57s]\n",
      "epoch 106 [17.17s]:  training loss=0.09682060033082962                                     \n",
      "epoch 107 [17.75s]:  training loss=0.09811326861381531                                     \n",
      "epoch 108 [17.55s]:  training loss=0.09226839244365692                                     \n",
      "epoch 109 [17.15s]:  training loss=0.09257517755031586                                     \n",
      "epoch 110 [18.96s]: training loss=0.09261393547058105  validation ndcg@10=0.024694917946278115 [0.52s]\n",
      "epoch 111 [17.16s]:  training loss=0.09157819300889969                                     \n",
      "epoch 112 [17.31s]:  training loss=0.0894598513841629                                      \n",
      "epoch 113 [17.19s]:  training loss=0.09333597123622894                                     \n",
      "epoch 114 [17.31s]:  training loss=0.09147291630506516                                     \n",
      "epoch 115 [17.18s]: training loss=0.0888572707772255  validation ndcg@10=0.024815954045727444 [0.52s]\n",
      "epoch 116 [17.27s]:  training loss=0.09040842950344086                                     \n",
      "epoch 117 [17.35s]:  training loss=0.09159717708826065                                     \n",
      "epoch 118 [17.19s]:  training loss=0.09055507183074951                                     \n",
      "epoch 119 [17.36s]:  training loss=0.08806847035884857                                     \n",
      "epoch 120 [17.31s]: training loss=0.0875496119260788  validation ndcg@10=0.02480878277914337 [0.53s]\n",
      "epoch 121 [17.38s]:  training loss=0.08829483389854431                                     \n",
      "epoch 122 [17.27s]:  training loss=0.08944393694400787                                     \n",
      "epoch 123 [17.21s]:  training loss=0.0892396792769432                                      \n",
      "epoch 124 [17.11s]:  training loss=0.08663307130336761                                     \n",
      "epoch 125 [17.11s]: training loss=0.0871475413441658  validation ndcg@10=0.0244804645378224 [0.56s]\n",
      "epoch 126 [17.29s]:  training loss=0.08618441224098206                                     \n",
      "epoch 127 [17.16s]:  training loss=0.08807480335235596                                     \n",
      "epoch 128 [17.18s]:  training loss=0.08855917304754257                                     \n",
      "epoch 129 [17.11s]:  training loss=0.08426252007484436                                     \n",
      "epoch 130 [17.19s]: training loss=0.08298906683921814  validation ndcg@10=0.02464028400486233 [0.57s]\n",
      "epoch 131 [17.26s]:  training loss=0.08684465289115906                                     \n",
      "epoch 132 [17.16s]:  training loss=0.08674005419015884                                     \n",
      "epoch 133 [17.25s]:  training loss=0.08267000317573547                                     \n",
      "epoch 134 [17.27s]:  training loss=0.08312012255191803                                     \n",
      "epoch 135 [17.13s]: training loss=0.08441390842199326  validation ndcg@10=0.024362576133940814 [0.54s]\n",
      "epoch 136 [17.17s]:  training loss=0.0835072472691536                                      \n",
      "epoch 137 [17.22s]:  training loss=0.08274903148412704                                     \n",
      "epoch 138 [17.45s]:  training loss=0.0806664302945137                                      \n",
      "epoch 139 [17.97s]:  training loss=0.08181964606046677                                     \n",
      "epoch 140 [17.34s]: training loss=0.08177151530981064  validation ndcg@10=0.02480556069273817 [0.55s]\n",
      "epoch 1 [26.34s]:  training loss=0.4982839822769165                                        \n",
      "epoch 2 [28.94s]:  training loss=0.6022301316261292                                        \n",
      "epoch 3 [28.63s]:  training loss=0.6287360191345215                                        \n",
      "epoch 4 [28.63s]:  training loss=0.7064732909202576                                        \n",
      "epoch 5 [28.2s]: training loss=0.7133824825286865  validation ndcg@10=0.014214377069981254 [0.62s]\n",
      "epoch 6 [28.89s]:  training loss=0.7146686911582947                                        \n",
      "epoch 7 [29.95s]:  training loss=0.7045018076896667                                        \n",
      "epoch 8 [29.8s]:  training loss=0.7553250193595886                                         \n",
      "epoch 9 [32.84s]:  training loss=0.7737706899642944                                        \n",
      "epoch 10 [30.82s]: training loss=0.7340540289878845  validation ndcg@10=0.014933933832873966 [0.62s]\n",
      "epoch 11 [29.69s]:  training loss=0.7308573722839355                                       \n",
      "epoch 12 [28.97s]:  training loss=0.807585597038269                                        \n",
      "epoch 13 [28.9s]:  training loss=0.8092763423919678                                        \n",
      "epoch 14 [29.32s]:  training loss=0.8013560771942139                                       \n",
      "epoch 15 [29.55s]: training loss=0.801533579826355  validation ndcg@10=0.015861365957519934 [0.65s]\n",
      "epoch 16 [29.86s]:  training loss=0.7923413515090942                                       \n",
      "epoch 17 [29.95s]:  training loss=0.8317359089851379                                       \n",
      "epoch 18 [28.7s]:  training loss=0.8800635933876038                                        \n",
      "epoch 19 [28.95s]:  training loss=0.8529255390167236                                       \n",
      "epoch 20 [28.99s]: training loss=0.894649088382721  validation ndcg@10=0.016303981044590285 [0.68s]\n",
      "epoch 21 [30.27s]:  training loss=0.8769114017486572                                       \n",
      "epoch 22 [30.56s]:  training loss=0.9106053113937378                                       \n",
      "epoch 23 [29.55s]:  training loss=0.9168387651443481                                       \n",
      "epoch 24 [31.65s]:  training loss=0.9235680103302002                                       \n",
      "epoch 25 [30.48s]: training loss=0.8962252736091614  validation ndcg@10=0.016662933627796795 [0.64s]\n",
      "epoch 26 [30.6s]:  training loss=0.9154058694839478                                        \n",
      "epoch 27 [30.35s]:  training loss=0.9579111933708191                                       \n",
      "epoch 28 [29.81s]:  training loss=0.9533244371414185                                       \n",
      "epoch 29 [28.53s]:  training loss=0.9405882358551025                                       \n",
      "epoch 30 [29.15s]: training loss=1.0028786659240723  validation ndcg@10=0.016115968944832207 [0.6s]\n",
      "epoch 31 [29.12s]:  training loss=0.9500882029533386                                       \n",
      "epoch 32 [29.75s]:  training loss=0.9413703680038452                                       \n",
      "epoch 33 [29.67s]:  training loss=0.9108219146728516                                       \n",
      "epoch 34 [29.8s]:  training loss=0.9960801005363464                                        \n",
      "epoch 35 [30.06s]: training loss=0.9538377523422241  validation ndcg@10=0.0153395719076819 [0.62s]\n",
      "epoch 36 [29.63s]:  training loss=0.9629008769989014                                       \n",
      "epoch 37 [30.22s]:  training loss=1.0194759368896484                                       \n",
      "epoch 38 [30.02s]:  training loss=1.0574244260787964                                       \n",
      "epoch 39 [30.49s]:  training loss=1.0962131023406982                                       \n",
      "epoch 40 [29.43s]: training loss=1.0473552942276  validation ndcg@10=0.014376246780685852 [0.61s]\n",
      "epoch 41 [29.28s]:  training loss=0.9898273944854736                                       \n",
      "epoch 42 [28.88s]:  training loss=1.0852936506271362                                       \n",
      "epoch 43 [29.76s]:  training loss=1.0772314071655273                                       \n",
      "epoch 44 [29.9s]:  training loss=1.0832809209823608                                        \n",
      "epoch 45 [29.79s]: training loss=1.0680285692214966  validation ndcg@10=0.015442407592790695 [0.65s]\n",
      "epoch 46 [29.45s]:  training loss=1.0601545572280884                                       \n",
      "epoch 47 [29.41s]:  training loss=1.0483485460281372                                       \n",
      "epoch 48 [30.36s]:  training loss=1.1479063034057617                                       \n",
      "epoch 49 [31.09s]:  training loss=1.068648099899292                                        \n",
      "epoch 50 [30.62s]: training loss=1.0645673274993896  validation ndcg@10=0.017626551475213203 [0.59s]\n",
      "epoch 51 [29.81s]:  training loss=1.0002943277359009                                       \n",
      "epoch 52 [30.46s]:  training loss=1.118152141571045                                        \n",
      "epoch 53 [32.06s]:  training loss=1.0952602624893188                                       \n",
      "epoch 54 [31.57s]:  training loss=1.0823566913604736                                       \n",
      "epoch 55 [31.29s]: training loss=1.1741180419921875  validation ndcg@10=0.014758366074824484 [0.67s]\n",
      "epoch 56 [30.9s]:  training loss=1.1470445394515991                                        \n",
      "epoch 57 [31.13s]:  training loss=1.2759877443313599                                       \n",
      "epoch 58 [30.23s]:  training loss=1.1093870401382446                                       \n",
      "epoch 59 [29.23s]:  training loss=1.156501054763794                                        \n",
      "epoch 60 [29.34s]: training loss=1.2009410858154297  validation ndcg@10=0.015511009645727889 [0.59s]\n",
      "epoch 61 [29.91s]:  training loss=1.2510449886322021                                       \n",
      "epoch 62 [29.53s]:  training loss=1.1590557098388672                                       \n",
      "epoch 63 [28.54s]:  training loss=1.2208174467086792                                       \n",
      "epoch 64 [29.54s]:  training loss=1.1969211101531982                                       \n",
      "epoch 65 [29.68s]: training loss=1.1945933103561401  validation ndcg@10=0.015288166886449395 [0.61s]\n",
      "epoch 66 [29.78s]:  training loss=1.1657739877700806                                       \n",
      "epoch 67 [29.65s]:  training loss=1.1493558883666992                                       \n",
      "epoch 68 [29.92s]:  training loss=1.2171971797943115                                       \n",
      "epoch 69 [29.18s]:  training loss=1.2831671237945557                                       \n",
      "epoch 70 [28.19s]: training loss=1.2186084985733032  validation ndcg@10=0.016899121020164585 [0.63s]\n",
      "epoch 71 [30.11s]:  training loss=1.2362395524978638                                       \n",
      "epoch 72 [31.73s]:  training loss=1.2092472314834595                                       \n",
      "epoch 73 [30.44s]:  training loss=1.193519949913025                                        \n",
      "epoch 74 [30.41s]:  training loss=1.257925033569336                                        \n",
      "epoch 75 [30.79s]: training loss=1.1965630054473877  validation ndcg@10=0.015478091669639971 [0.62s]\n",
      "epoch 1 [14.64s]:  training loss=0.41534116864204407                                       \n",
      "epoch 2 [14.54s]:  training loss=0.2527131736278534                                        \n",
      "epoch 3 [14.57s]:  training loss=0.20963066816329956                                       \n",
      "epoch 4 [14.92s]:  training loss=0.18612827360630035                                       \n",
      "epoch 5 [14.99s]: training loss=0.16763482987880707  validation ndcg@10=0.019784742874550093 [0.52s]\n",
      "epoch 6 [15.39s]:  training loss=0.15610864758491516                                       \n",
      "epoch 7 [14.78s]:  training loss=0.14407004415988922                                       \n",
      "epoch 8 [15.1s]:  training loss=0.1327858418226242                                         \n",
      "epoch 9 [15.13s]:  training loss=0.12649789452552795                                       \n",
      "epoch 10 [15.48s]: training loss=0.11612661182880402  validation ndcg@10=0.024283360299407756 [0.51s]\n",
      "epoch 11 [15.1s]:  training loss=0.10934251546859741                                       \n",
      "epoch 12 [15.0s]:  training loss=0.10689613968133926                                       \n",
      "epoch 13 [14.87s]:  training loss=0.10086647421121597                                      \n",
      "epoch 14 [15.55s]:  training loss=0.09681930392980576                                      \n",
      "epoch 15 [14.79s]: training loss=0.09213677793741226  validation ndcg@10=0.02444453563947708 [0.54s]\n",
      "epoch 16 [15.24s]:  training loss=0.09020820260047913                                      \n",
      "epoch 17 [14.5s]:  training loss=0.08844957500696182                                       \n",
      "epoch 18 [14.74s]:  training loss=0.08501150459051132                                      \n",
      "epoch 19 [14.51s]:  training loss=0.0801158919930458                                       \n",
      "epoch 20 [15.0s]: training loss=0.07648018002510071  validation ndcg@10=0.025570354238165855 [0.53s]\n",
      "epoch 21 [14.41s]:  training loss=0.07690513134002686                                      \n",
      "epoch 22 [15.06s]:  training loss=0.07743968814611435                                      \n",
      "epoch 23 [14.99s]:  training loss=0.07329602539539337                                      \n",
      "epoch 24 [16.14s]:  training loss=0.07240042835474014                                      \n",
      "epoch 25 [14.65s]: training loss=0.07072549313306808  validation ndcg@10=0.026499787928846694 [0.53s]\n",
      "epoch 26 [14.95s]:  training loss=0.06745103001594543                                      \n",
      "epoch 27 [15.15s]:  training loss=0.0658593401312828                                       \n",
      "epoch 28 [15.03s]:  training loss=0.06529812514781952                                      \n",
      "epoch 29 [15.08s]:  training loss=0.06544439494609833                                      \n",
      "epoch 30 [15.58s]: training loss=0.06528174132108688  validation ndcg@10=0.02639644784565087 [0.54s]\n",
      "epoch 31 [15.06s]:  training loss=0.06031689792871475                                      \n",
      "epoch 32 [14.68s]:  training loss=0.06228061765432358                                      \n",
      "epoch 33 [15.57s]:  training loss=0.059631142765283585                                     \n",
      "epoch 34 [15.13s]:  training loss=0.061504922807216644                                     \n",
      "epoch 35 [14.88s]: training loss=0.05989972874522209  validation ndcg@10=0.025142150822806714 [0.57s]\n",
      "epoch 36 [15.29s]:  training loss=0.05867981165647507                                      \n",
      "epoch 37 [15.24s]:  training loss=0.05696392431855202                                      \n",
      "epoch 38 [15.43s]:  training loss=0.05834265425801277                                      \n",
      "epoch 39 [15.09s]:  training loss=0.05447418615221977                                      \n",
      "epoch 40 [15.11s]: training loss=0.057139601558446884  validation ndcg@10=0.024300653108921773 [0.52s]\n",
      "epoch 41 [14.76s]:  training loss=0.055817268788814545                                     \n",
      "epoch 42 [15.07s]:  training loss=0.05436443164944649                                      \n",
      "epoch 43 [15.04s]:  training loss=0.05362695828080177                                      \n",
      "epoch 44 [14.56s]:  training loss=0.050452541559934616                                     \n",
      "epoch 45 [14.89s]: training loss=0.05414457619190216  validation ndcg@10=0.025006757981637276 [0.54s]\n",
      "epoch 46 [15.44s]:  training loss=0.052231285721063614                                     \n",
      "epoch 47 [15.24s]:  training loss=0.0525985024869442                                       \n",
      "epoch 48 [15.38s]:  training loss=0.05248544365167618                                      \n",
      "epoch 49 [15.4s]:  training loss=0.05120888352394104                                       \n",
      "epoch 50 [15.66s]: training loss=0.04987313598394394  validation ndcg@10=0.025651380871784268 [0.53s]\n",
      "epoch 1 [34.86s]:  training loss=0.5007005929946899                                        \n",
      "epoch 2 [34.43s]:  training loss=0.3511800765991211                                        \n",
      "epoch 3 [33.12s]:  training loss=0.277466744184494                                         \n",
      "epoch 4 [32.77s]:  training loss=0.23661617934703827                                       \n",
      "epoch 5 [31.5s]: training loss=0.2089425027370453  validation ndcg@10=0.019313359638582238 [0.67s]\n",
      "epoch 6 [31.8s]:  training loss=0.2004183530807495                                         \n",
      "epoch 7 [32.23s]:  training loss=0.18638232350349426                                       \n",
      "epoch 8 [31.65s]:  training loss=0.17716683447360992                                       \n",
      "epoch 9 [31.13s]:  training loss=0.16703088581562042                                       \n",
      "epoch 10 [31.16s]: training loss=0.16461695730686188  validation ndcg@10=0.021671926341077175 [0.68s]\n",
      "epoch 11 [31.3s]:  training loss=0.15384763479232788                                       \n",
      "epoch 12 [33.04s]:  training loss=0.14999568462371826                                      \n",
      "epoch 13 [30.51s]:  training loss=0.14450454711914062                                      \n",
      "epoch 14 [30.93s]:  training loss=0.13992860913276672                                      \n",
      "epoch 15 [30.33s]: training loss=0.1364903599023819  validation ndcg@10=0.022815555919797163 [0.64s]\n",
      "epoch 16 [31.6s]:  training loss=0.1302405446767807                                        \n",
      "epoch 17 [31.94s]:  training loss=0.12622158229351044                                      \n",
      "epoch 18 [31.4s]:  training loss=0.12020307779312134                                       \n",
      "epoch 19 [32.36s]:  training loss=0.1182532086968422                                       \n",
      "epoch 20 [32.31s]: training loss=0.11599449068307877  validation ndcg@10=0.023605341433139197 [0.67s]\n",
      "epoch 21 [31.96s]:  training loss=0.1131620928645134                                       \n",
      "epoch 22 [31.14s]:  training loss=0.1075885072350502                                       \n",
      "epoch 23 [30.88s]:  training loss=0.10583378374576569                                      \n",
      "epoch 24 [31.51s]:  training loss=0.10282319039106369                                      \n",
      "epoch 25 [31.34s]: training loss=0.10558462888002396  validation ndcg@10=0.023358419535053444 [0.68s]\n",
      "epoch 26 [30.66s]:  training loss=0.10127654671669006                                      \n",
      "epoch 27 [31.58s]:  training loss=0.09991113841533661                                      \n",
      "epoch 28 [32.35s]:  training loss=0.09610022604465485                                      \n",
      "epoch 29 [32.59s]:  training loss=0.09671111404895782                                      \n",
      "epoch 30 [32.34s]: training loss=0.09381530433893204  validation ndcg@10=0.024982851406240965 [0.68s]\n",
      "epoch 31 [31.76s]:  training loss=0.09234798699617386                                      \n",
      "epoch 32 [32.39s]:  training loss=0.09155171364545822                                      \n",
      "epoch 33 [32.66s]:  training loss=0.08700112253427505                                      \n",
      "epoch 34 [32.47s]:  training loss=0.08911409974098206                                      \n",
      "epoch 35 [32.77s]: training loss=0.08724448829889297  validation ndcg@10=0.025199095166708783 [0.7s]\n",
      "epoch 36 [33.89s]:  training loss=0.08486943691968918                                      \n",
      "epoch 37 [30.8s]:  training loss=0.08284475654363632                                       \n",
      "epoch 38 [31.37s]:  training loss=0.08079583197832108                                      \n",
      "epoch 39 [31.83s]:  training loss=0.08274014294147491                                      \n",
      "epoch 40 [32.56s]: training loss=0.08005628734827042  validation ndcg@10=0.02514598656510772 [0.65s]\n",
      "epoch 41 [32.38s]:  training loss=0.08141210675239563                                      \n",
      "epoch 42 [32.68s]:  training loss=0.07757988572120667                                      \n",
      "epoch 43 [31.98s]:  training loss=0.07884851098060608                                      \n",
      "epoch 44 [31.93s]:  training loss=0.07727578282356262                                      \n",
      "epoch 45 [32.29s]: training loss=0.07462690770626068  validation ndcg@10=0.02548928344036958 [0.66s]\n",
      "epoch 46 [32.45s]:  training loss=0.07353378087282181                                      \n",
      "epoch 47 [30.99s]:  training loss=0.07571633905172348                                      \n",
      "epoch 48 [31.8s]:  training loss=0.07341797649860382                                       \n",
      "epoch 49 [32.19s]:  training loss=0.07150442898273468                                      \n",
      "epoch 50 [32.61s]: training loss=0.07295822352170944  validation ndcg@10=0.026175008880953976 [0.71s]\n",
      "epoch 51 [31.62s]:  training loss=0.07093066722154617                                      \n",
      "epoch 52 [31.7s]:  training loss=0.06973173469305038                                       \n",
      "epoch 53 [31.89s]:  training loss=0.069564588367939                                        \n",
      "epoch 54 [31.63s]:  training loss=0.06822856515645981                                      \n",
      "epoch 55 [30.1s]: training loss=0.06760084629058838  validation ndcg@10=0.025313660389018073 [0.67s]\n",
      "epoch 56 [31.45s]:  training loss=0.06843511760234833                                      \n",
      "epoch 57 [30.58s]:  training loss=0.0654924213886261                                       \n",
      "epoch 58 [31.62s]:  training loss=0.06794656068086624                                      \n",
      "epoch 59 [31.44s]:  training loss=0.06449814885854721                                      \n",
      "epoch 60 [33.75s]: training loss=0.06663014739751816  validation ndcg@10=0.026911084273355458 [0.66s]\n",
      "epoch 61 [31.54s]:  training loss=0.06444410979747772                                      \n",
      "epoch 62 [31.72s]:  training loss=0.06134217232465744                                      \n",
      "epoch 63 [31.91s]:  training loss=0.0604168176651001                                       \n",
      "epoch 64 [31.93s]:  training loss=0.06331011652946472                                      \n",
      "epoch 65 [30.97s]: training loss=0.0607309564948082  validation ndcg@10=0.026392758709387194 [0.69s]\n",
      "epoch 66 [31.12s]:  training loss=0.059362348169088364                                     \n",
      "epoch 67 [32.19s]:  training loss=0.05988132953643799                                      \n",
      "epoch 68 [32.01s]:  training loss=0.061246294528245926                                     \n",
      "epoch 69 [31.65s]:  training loss=0.060058947652578354                                     \n",
      "epoch 70 [31.79s]: training loss=0.059043705463409424  validation ndcg@10=0.02677450730699982 [0.65s]\n",
      "epoch 71 [31.78s]:  training loss=0.059491150081157684                                     \n",
      "epoch 72 [31.76s]:  training loss=0.06047780066728592                                      \n",
      "epoch 73 [31.7s]:  training loss=0.059064555913209915                                      \n",
      "epoch 74 [31.27s]:  training loss=0.06128174811601639                                      \n",
      "epoch 75 [31.45s]: training loss=0.057363081723451614  validation ndcg@10=0.027085250187805634 [0.67s]\n",
      "epoch 76 [31.94s]:  training loss=0.05463004857301712                                      \n",
      "epoch 77 [31.17s]:  training loss=0.05659196153283119                                      \n",
      "epoch 78 [31.23s]:  training loss=0.05428062751889229                                      \n",
      "epoch 79 [31.15s]:  training loss=0.05415157228708267                                      \n",
      "epoch 80 [30.58s]: training loss=0.05525006726384163  validation ndcg@10=0.02691248103887215 [0.66s]\n",
      "epoch 81 [30.77s]:  training loss=0.054253824055194855                                     \n",
      "epoch 82 [31.16s]:  training loss=0.05202899128198624                                      \n",
      "epoch 83 [31.34s]:  training loss=0.05227237939834595                                      \n",
      "epoch 84 [31.53s]:  training loss=0.0520833320915699                                       \n",
      "epoch 85 [30.65s]: training loss=0.05360950529575348  validation ndcg@10=0.025623070053626174 [0.61s]\n",
      "epoch 86 [30.71s]:  training loss=0.05323105305433273                                      \n",
      "epoch 87 [31.4s]:  training loss=0.05217653140425682                                       \n",
      "epoch 88 [31.47s]:  training loss=0.05253202095627785                                      \n",
      "epoch 89 [31.72s]:  training loss=0.05243777483701706                                      \n",
      "epoch 90 [30.99s]: training loss=0.0514802411198616  validation ndcg@10=0.025632313865585538 [0.66s]\n",
      "epoch 91 [31.31s]:  training loss=0.05155543237924576                                      \n",
      "epoch 92 [30.91s]:  training loss=0.050181467086076736                                     \n",
      "epoch 93 [31.45s]:  training loss=0.05015876144170761                                      \n",
      "epoch 94 [31.76s]:  training loss=0.0507337711751461                                       \n",
      "epoch 95 [31.99s]: training loss=0.05115475878119469  validation ndcg@10=0.025339652450774727 [0.67s]\n",
      "epoch 96 [31.85s]:  training loss=0.04951922968029976                                      \n",
      "epoch 97 [30.93s]:  training loss=0.04893010854721069                                      \n",
      "epoch 98 [30.94s]:  training loss=0.047853730618953705                                     \n",
      "epoch 99 [31.73s]:  training loss=0.04962186887860298                                      \n",
      "epoch 100 [31.34s]: training loss=0.049254197627305984  validation ndcg@10=0.026087398801648058 [0.67s]\n",
      "epoch 1 [7.31s]:  training loss=0.5883300304412842                                         \n",
      "epoch 2 [7.13s]:  training loss=0.48662418127059937                                        \n",
      "epoch 3 [7.36s]:  training loss=0.4252437949180603                                         \n",
      "epoch 4 [7.13s]:  training loss=0.3788110911846161                                         \n",
      "epoch 5 [7.14s]: training loss=0.331849068403244  validation ndcg@10=0.008607242709545049 [0.34s]\n",
      "epoch 6 [7.16s]:  training loss=0.30527448654174805                                        \n",
      "epoch 7 [7.13s]:  training loss=0.27489209175109863                                        \n",
      "epoch 8 [7.08s]:  training loss=0.2587805986404419                                         \n",
      "epoch 9 [6.91s]:  training loss=0.2430466115474701                                         \n",
      "epoch 10 [7.17s]: training loss=0.2313578873872757  validation ndcg@10=0.017567310093600193 [0.32s]\n",
      "epoch 11 [7.21s]:  training loss=0.22057758271694183                                       \n",
      "epoch 12 [7.25s]:  training loss=0.21164050698280334                                       \n",
      "epoch 13 [7.1s]:  training loss=0.20267803966999054                                        \n",
      "epoch 14 [7.2s]:  training loss=0.19690380990505219                                        \n",
      "epoch 15 [7.09s]: training loss=0.18942421674728394  validation ndcg@10=0.020190187006038276 [0.34s]\n",
      "epoch 16 [6.99s]:  training loss=0.1887742131948471                                        \n",
      "epoch 17 [7.11s]:  training loss=0.1864648014307022                                        \n",
      "epoch 18 [7.02s]:  training loss=0.17876142263412476                                       \n",
      "epoch 19 [7.08s]:  training loss=0.1776648312807083                                        \n",
      "epoch 20 [7.11s]: training loss=0.17694230377674103  validation ndcg@10=0.021496752662585786 [0.33s]\n",
      "epoch 21 [7.19s]:  training loss=0.1676923781633377                                        \n",
      "epoch 22 [7.01s]:  training loss=0.16088612377643585                                       \n",
      "epoch 23 [7.22s]:  training loss=0.16565488278865814                                       \n",
      "epoch 24 [7.11s]:  training loss=0.1606687754392624                                        \n",
      "epoch 25 [6.92s]: training loss=0.15609753131866455  validation ndcg@10=0.022512014158615524 [0.38s]\n",
      "epoch 26 [7.02s]:  training loss=0.1539425551891327                                        \n",
      "epoch 27 [6.93s]:  training loss=0.1517561376094818                                        \n",
      "epoch 28 [7.07s]:  training loss=0.14837834239006042                                       \n",
      "epoch 29 [7.0s]:  training loss=0.15117406845092773                                        \n",
      "epoch 30 [7.29s]: training loss=0.14892220497131348  validation ndcg@10=0.02305372452835182 [0.34s]\n",
      "epoch 31 [7.2s]:  training loss=0.14277523756027222                                        \n",
      "epoch 32 [7.03s]:  training loss=0.14331765472888947                                       \n",
      "epoch 33 [7.07s]:  training loss=0.14035311341285706                                       \n",
      "epoch 34 [7.12s]:  training loss=0.13928930461406708                                       \n",
      "epoch 35 [7.11s]: training loss=0.13380557298660278  validation ndcg@10=0.022784889556447786 [0.31s]\n",
      "epoch 36 [7.18s]:  training loss=0.13551771640777588                                       \n",
      "epoch 37 [8.87s]:  training loss=0.1301402896642685                                        \n",
      "epoch 38 [6.89s]:  training loss=0.13234764337539673                                       \n",
      "epoch 39 [7.15s]:  training loss=0.1288042962551117                                        \n",
      "epoch 40 [7.01s]: training loss=0.12325282394886017  validation ndcg@10=0.023646363275105414 [0.34s]\n",
      "epoch 41 [6.91s]:  training loss=0.1259850561618805                                        \n",
      "epoch 42 [7.06s]:  training loss=0.1245475634932518                                        \n",
      "epoch 43 [7.09s]:  training loss=0.1208714172244072                                        \n",
      "epoch 44 [7.08s]:  training loss=0.12186994403600693                                       \n",
      "epoch 45 [7.26s]: training loss=0.11690966784954071  validation ndcg@10=0.023561435866795412 [0.31s]\n",
      "epoch 46 [7.3s]:  training loss=0.11909487098455429                                        \n",
      "epoch 47 [7.28s]:  training loss=0.11310543119907379                                       \n",
      "epoch 48 [7.17s]:  training loss=0.11246474832296371                                       \n",
      "epoch 49 [6.95s]:  training loss=0.11574380844831467                                       \n",
      "epoch 50 [6.93s]: training loss=0.11131085455417633  validation ndcg@10=0.024653832565563766 [0.32s]\n",
      "epoch 51 [6.82s]:  training loss=0.1114155501127243                                        \n",
      "epoch 52 [7.27s]:  training loss=0.11076246947050095                                       \n",
      "epoch 53 [7.31s]:  training loss=0.11011466383934021                                       \n",
      "epoch 54 [7.14s]:  training loss=0.11014343798160553                                       \n",
      "epoch 55 [7.39s]: training loss=0.1077294796705246  validation ndcg@10=0.024856590569361875 [0.31s]\n",
      "epoch 56 [7.3s]:  training loss=0.10488833487033844                                        \n",
      "epoch 57 [7.22s]:  training loss=0.10924816131591797                                       \n",
      "epoch 58 [6.97s]:  training loss=0.10701355338096619                                       \n",
      "epoch 59 [7.2s]:  training loss=0.10190773755311966                                        \n",
      "epoch 60 [7.12s]: training loss=0.10334412753582001  validation ndcg@10=0.025206613366924945 [0.34s]\n",
      "epoch 61 [6.98s]:  training loss=0.10221932083368301                                       \n",
      "epoch 62 [7.08s]:  training loss=0.10287182033061981                                       \n",
      "epoch 63 [7.05s]:  training loss=0.09864196926355362                                       \n",
      "epoch 64 [7.11s]:  training loss=0.09906477481126785                                       \n",
      "epoch 65 [7.22s]: training loss=0.09807687252759933  validation ndcg@10=0.025046689001197622 [0.3s]\n",
      "epoch 66 [7.09s]:  training loss=0.09666003286838531                                       \n",
      "epoch 67 [6.97s]:  training loss=0.09744378924369812                                       \n",
      "epoch 68 [7.25s]:  training loss=0.09599529206752777                                       \n",
      "epoch 69 [7.09s]:  training loss=0.09700261056423187                                       \n",
      "epoch 70 [7.2s]: training loss=0.09276272356510162  validation ndcg@10=0.02521729407143957 [0.32s]\n",
      "epoch 71 [7.07s]:  training loss=0.09015513956546783                                       \n",
      "epoch 72 [7.11s]:  training loss=0.09227538853883743                                       \n",
      "epoch 73 [7.04s]:  training loss=0.090814009308815                                         \n",
      "epoch 74 [6.91s]:  training loss=0.0931636169552803                                        \n",
      "epoch 75 [6.95s]: training loss=0.0890144482254982  validation ndcg@10=0.025605332298133658 [0.31s]\n",
      "epoch 76 [6.92s]:  training loss=0.09014103561639786                                       \n",
      "epoch 77 [6.9s]:  training loss=0.08806398510932922                                        \n",
      "epoch 78 [7.18s]:  training loss=0.0877838134765625                                        \n",
      "epoch 79 [7.24s]:  training loss=0.08847537636756897                                       \n",
      "epoch 80 [7.0s]: training loss=0.08780979365110397  validation ndcg@10=0.02600936166586175 [0.3s]\n",
      "epoch 81 [7.06s]:  training loss=0.08900339901447296                                       \n",
      "epoch 82 [6.8s]:  training loss=0.08582180738449097                                        \n",
      "epoch 83 [7.29s]:  training loss=0.08736725896596909                                       \n",
      "epoch 84 [7.08s]:  training loss=0.08583152294158936                                       \n",
      "epoch 85 [7.09s]: training loss=0.0844915434718132  validation ndcg@10=0.02562002881114131 [0.32s]\n",
      "epoch 86 [7.06s]:  training loss=0.08339140564203262                                       \n",
      "epoch 87 [7.11s]:  training loss=0.08336758613586426                                       \n",
      "epoch 88 [7.07s]:  training loss=0.08210968971252441                                       \n",
      "epoch 89 [7.04s]:  training loss=0.08374030888080597                                       \n",
      "epoch 90 [7.11s]: training loss=0.0787830576300621  validation ndcg@10=0.02576069107716139 [0.32s]\n",
      "epoch 91 [7.18s]:  training loss=0.08042693883180618                                       \n",
      "epoch 92 [7.03s]:  training loss=0.07916080206632614                                       \n",
      "epoch 93 [6.85s]:  training loss=0.08028153330087662                                       \n",
      "epoch 94 [7.01s]:  training loss=0.07972647994756699                                       \n",
      "epoch 95 [6.93s]: training loss=0.07891002297401428  validation ndcg@10=0.026559587959177663 [0.3s]\n",
      "epoch 96 [7.07s]:  training loss=0.07768382877111435                                       \n",
      "epoch 97 [7.09s]:  training loss=0.07998953014612198                                       \n",
      "epoch 98 [7.21s]:  training loss=0.07637243717908859                                       \n",
      "epoch 99 [6.95s]:  training loss=0.07747659087181091                                       \n",
      "epoch 100 [6.91s]: training loss=0.07810872793197632  validation ndcg@10=0.02617022250957144 [0.31s]\n",
      "epoch 101 [7.02s]:  training loss=0.07672925293445587                                      \n",
      "epoch 102 [6.96s]:  training loss=0.07676079124212265                                      \n",
      "epoch 103 [7.0s]:  training loss=0.07636842131614685                                       \n",
      "epoch 104 [6.99s]:  training loss=0.0778418555855751                                       \n",
      "epoch 105 [6.91s]: training loss=0.07449059188365936  validation ndcg@10=0.026576914346937874 [0.31s]\n",
      "epoch 106 [7.05s]:  training loss=0.07390769571065903                                      \n",
      "epoch 107 [7.13s]:  training loss=0.07352560758590698                                      \n",
      "epoch 108 [6.99s]:  training loss=0.07459438592195511                                      \n",
      "epoch 109 [7.08s]:  training loss=0.07472319155931473                                      \n",
      "epoch 110 [7.2s]: training loss=0.07437454164028168  validation ndcg@10=0.026032519416228538 [0.33s]\n",
      "epoch 111 [7.14s]:  training loss=0.07112164795398712                                      \n",
      "epoch 112 [7.12s]:  training loss=0.07073243707418442                                      \n",
      "epoch 113 [7.11s]:  training loss=0.07229829579591751                                      \n",
      "epoch 114 [7.08s]:  training loss=0.0726081132888794                                       \n",
      "epoch 115 [7.04s]: training loss=0.07190719991922379  validation ndcg@10=0.0266962699401954 [0.33s]\n",
      "epoch 116 [6.84s]:  training loss=0.07005387544631958                                      \n",
      "epoch 117 [6.98s]:  training loss=0.06799480319023132                                      \n",
      "epoch 118 [6.94s]:  training loss=0.07082898169755936                                      \n",
      "epoch 119 [7.03s]:  training loss=0.06860554963350296                                      \n",
      "epoch 120 [7.05s]: training loss=0.06810780614614487  validation ndcg@10=0.027231703731088828 [0.33s]\n",
      "epoch 121 [7.01s]:  training loss=0.07160934805870056                                      \n",
      "epoch 122 [7.27s]:  training loss=0.06884892284870148                                      \n",
      "epoch 123 [7.26s]:  training loss=0.0698280781507492                                       \n",
      "epoch 124 [7.17s]:  training loss=0.0678573027253151                                       \n",
      "epoch 125 [7.02s]: training loss=0.0675860047340393  validation ndcg@10=0.026562740373777993 [0.31s]\n",
      "epoch 126 [7.02s]:  training loss=0.06623276323080063                                      \n",
      "epoch 127 [7.04s]:  training loss=0.06748577952384949                                      \n",
      "epoch 128 [7.02s]:  training loss=0.06628585606813431                                      \n",
      "epoch 129 [6.94s]:  training loss=0.06637505441904068                                      \n",
      "epoch 130 [7.2s]: training loss=0.0684661716222763  validation ndcg@10=0.026825359488756587 [0.32s]\n",
      "epoch 131 [7.17s]:  training loss=0.06696195900440216                                      \n",
      "epoch 132 [7.04s]:  training loss=0.06551171094179153                                      \n",
      "epoch 133 [7.25s]:  training loss=0.06572307646274567                                      \n",
      "epoch 134 [7.17s]:  training loss=0.06471021473407745                                      \n",
      "epoch 135 [7.19s]: training loss=0.06485378742218018  validation ndcg@10=0.026690552357984416 [0.32s]\n",
      "epoch 136 [6.92s]:  training loss=0.06436436623334885                                      \n",
      "epoch 137 [6.97s]:  training loss=0.06535720080137253                                      \n",
      "epoch 138 [6.96s]:  training loss=0.06404539197683334                                      \n",
      "epoch 139 [7.05s]:  training loss=0.0658397451043129                                       \n",
      "epoch 140 [6.85s]: training loss=0.06609553098678589  validation ndcg@10=0.026714992057234502 [0.32s]\n",
      "epoch 141 [8.84s]:  training loss=0.0642065480351448                                       \n",
      "epoch 142 [6.72s]:  training loss=0.063532754778862                                        \n",
      "epoch 143 [7.06s]:  training loss=0.06285429745912552                                      \n",
      "epoch 144 [6.91s]:  training loss=0.06156972423195839                                      \n",
      "epoch 145 [6.89s]: training loss=0.06149964779615402  validation ndcg@10=0.026105260818607205 [0.32s]\n",
      "epoch 1 [12.04s]:  training loss=0.473563551902771                                         \n",
      "epoch 2 [12.47s]:  training loss=0.29809802770614624                                       \n",
      "epoch 3 [12.9s]:  training loss=0.23766110837459564                                        \n",
      "epoch 4 [12.62s]:  training loss=0.20958073437213898                                       \n",
      "epoch 5 [12.68s]: training loss=0.190916046500206  validation ndcg@10=0.020207263086466087 [0.42s]\n",
      "epoch 6 [12.82s]:  training loss=0.1778501719236374                                        \n",
      "epoch 7 [12.54s]:  training loss=0.16819974780082703                                       \n",
      "epoch 8 [12.76s]:  training loss=0.1552838832139969                                        \n",
      "epoch 9 [12.11s]:  training loss=0.15386348962783813                                       \n",
      "epoch 10 [13.15s]: training loss=0.13941805064678192  validation ndcg@10=0.022128580032343765 [0.41s]\n",
      "epoch 11 [12.76s]:  training loss=0.13653835654258728                                      \n",
      "epoch 12 [12.63s]:  training loss=0.12889675796031952                                      \n",
      "epoch 13 [13.18s]:  training loss=0.12605242431163788                                      \n",
      "epoch 14 [12.76s]:  training loss=0.11911969631910324                                      \n",
      "epoch 15 [12.83s]: training loss=0.11576221138238907  validation ndcg@10=0.023813784603963925 [0.43s]\n",
      "epoch 16 [12.36s]:  training loss=0.10846774280071259                                      \n",
      "epoch 17 [12.68s]:  training loss=0.10611870884895325                                      \n",
      "epoch 18 [12.66s]:  training loss=0.1044064536690712                                       \n",
      "epoch 19 [12.72s]:  training loss=0.10309260338544846                                      \n",
      "epoch 20 [12.63s]: training loss=0.09840424358844757  validation ndcg@10=0.024586967569076492 [0.42s]\n",
      "epoch 21 [12.31s]:  training loss=0.0933883786201477                                       \n",
      "epoch 22 [12.78s]:  training loss=0.09332681447267532                                      \n",
      "epoch 23 [12.63s]:  training loss=0.09423321485519409                                      \n",
      "epoch 24 [13.01s]:  training loss=0.0871284082531929                                       \n",
      "epoch 25 [12.57s]: training loss=0.08635688573122025  validation ndcg@10=0.02511491939446903 [0.43s]\n",
      "epoch 26 [12.47s]:  training loss=0.08596543967723846                                      \n",
      "epoch 27 [12.54s]:  training loss=0.08311989158391953                                      \n",
      "epoch 28 [12.98s]:  training loss=0.08245906233787537                                      \n",
      "epoch 29 [12.55s]:  training loss=0.08008067309856415                                      \n",
      "epoch 30 [13.05s]: training loss=0.08200899511575699  validation ndcg@10=0.02618212675604488 [0.44s]\n",
      "epoch 31 [12.7s]:  training loss=0.07658526301383972                                       \n",
      "epoch 32 [12.66s]:  training loss=0.07320088148117065                                      \n",
      "epoch 33 [12.48s]:  training loss=0.07382544130086899                                      \n",
      "epoch 34 [12.55s]:  training loss=0.0755530372262001                                       \n",
      "epoch 35 [12.29s]: training loss=0.07294881343841553  validation ndcg@10=0.0264384101412879 [0.41s]\n",
      "epoch 36 [12.95s]:  training loss=0.06965696811676025                                      \n",
      "epoch 37 [12.91s]:  training loss=0.07105396687984467                                      \n",
      "epoch 38 [12.5s]:  training loss=0.06945380568504333                                       \n",
      "epoch 39 [12.62s]:  training loss=0.06775037944316864                                      \n",
      "epoch 40 [12.52s]: training loss=0.06620385497808456  validation ndcg@10=0.025794586783751506 [0.4s]\n",
      "epoch 41 [12.6s]:  training loss=0.0669175237417221                                        \n",
      "epoch 42 [12.28s]:  training loss=0.06675183773040771                                      \n",
      "epoch 43 [12.69s]:  training loss=0.06474629789590836                                      \n",
      "epoch 44 [12.68s]:  training loss=0.06471283733844757                                      \n",
      "epoch 45 [12.6s]: training loss=0.06016295403242111  validation ndcg@10=0.02582025205334857 [0.41s]\n",
      "epoch 46 [12.65s]:  training loss=0.06115175783634186                                      \n",
      "epoch 47 [12.37s]:  training loss=0.05933759734034538                                      \n",
      "epoch 48 [12.73s]:  training loss=0.060761939734220505                                     \n",
      "epoch 49 [12.2s]:  training loss=0.060967303812503815                                      \n",
      "epoch 50 [12.33s]: training loss=0.059522904455661774  validation ndcg@10=0.026527186526382432 [0.48s]\n",
      "epoch 51 [12.93s]:  training loss=0.05782292038202286                                      \n",
      "epoch 52 [12.64s]:  training loss=0.05783150717616081                                      \n",
      "epoch 53 [13.33s]:  training loss=0.056480761617422104                                     \n",
      "epoch 54 [12.61s]:  training loss=0.05692387372255325                                      \n",
      "epoch 55 [12.79s]: training loss=0.057551123201847076  validation ndcg@10=0.026185924805967035 [0.38s]\n",
      "epoch 56 [14.54s]:  training loss=0.05360762029886246                                      \n",
      "epoch 57 [12.29s]:  training loss=0.054565075784921646                                     \n",
      "epoch 58 [12.5s]:  training loss=0.05301443859934807                                       \n",
      "epoch 59 [12.16s]:  training loss=0.05189600586891174                                      \n",
      "epoch 60 [12.66s]: training loss=0.050922691822052  validation ndcg@10=0.026411003436740133 [0.4s]\n",
      "epoch 61 [12.74s]:  training loss=0.05285889655351639                                      \n",
      "epoch 62 [12.19s]:  training loss=0.05133020132780075                                      \n",
      "epoch 63 [12.33s]:  training loss=0.05177547410130501                                      \n",
      "epoch 64 [12.3s]:  training loss=0.052362244576215744                                      \n",
      "epoch 65 [12.5s]: training loss=0.050369519740343094  validation ndcg@10=0.025904842121574706 [0.44s]\n",
      "epoch 66 [12.74s]:  training loss=0.0493227057158947                                       \n",
      "epoch 67 [12.37s]:  training loss=0.04926922172307968                                      \n",
      "epoch 68 [12.6s]:  training loss=0.050022948533296585                                      \n",
      "epoch 69 [12.27s]:  training loss=0.049247439950704575                                     \n",
      "epoch 70 [12.58s]: training loss=0.04811985418200493  validation ndcg@10=0.027159679965225485 [0.43s]\n",
      "epoch 71 [12.24s]:  training loss=0.048529837280511856                                     \n",
      "epoch 72 [12.76s]:  training loss=0.04787946864962578                                      \n",
      "epoch 73 [13.25s]:  training loss=0.0457087941467762                                       \n",
      "epoch 74 [12.43s]:  training loss=0.048263922333717346                                     \n",
      "epoch 75 [12.78s]: training loss=0.04631911963224411  validation ndcg@10=0.025708774274657023 [0.43s]\n",
      "epoch 76 [12.52s]:  training loss=0.046688299626111984                                     \n",
      "epoch 77 [12.55s]:  training loss=0.047376543283462524                                     \n",
      "epoch 78 [12.68s]:  training loss=0.046205442398786545                                     \n",
      "epoch 79 [12.63s]:  training loss=0.045869775116443634                                     \n",
      "epoch 80 [12.53s]: training loss=0.045622970908880234  validation ndcg@10=0.02670667535178482 [0.41s]\n",
      "epoch 81 [12.6s]:  training loss=0.043900907039642334                                      \n",
      "epoch 82 [12.56s]:  training loss=0.04414362832903862                                      \n",
      "epoch 83 [12.62s]:  training loss=0.04249151423573494                                      \n",
      "epoch 84 [12.14s]:  training loss=0.04612449184060097                                      \n",
      "epoch 85 [12.79s]: training loss=0.044062674045562744  validation ndcg@10=0.026217275953754626 [0.43s]\n",
      "epoch 86 [12.86s]:  training loss=0.04307688772678375                                      \n",
      "epoch 87 [12.24s]:  training loss=0.044333238154649734                                     \n",
      "epoch 88 [13.11s]:  training loss=0.04181484133005142                                      \n",
      "epoch 89 [12.83s]:  training loss=0.04155965894460678                                      \n",
      "epoch 90 [12.62s]: training loss=0.04243367537856102  validation ndcg@10=0.025611029161280687 [0.41s]\n",
      "epoch 91 [12.6s]:  training loss=0.04356076940894127                                       \n",
      "epoch 92 [12.64s]:  training loss=0.04214337095618248                                      \n",
      "epoch 93 [12.35s]:  training loss=0.04255994036793709                                      \n",
      "epoch 94 [12.75s]:  training loss=0.04250026121735573                                      \n",
      "epoch 95 [12.23s]: training loss=0.04042914882302284  validation ndcg@10=0.025274877173681955 [0.41s]\n",
      "epoch 1 [15.62s]:  training loss=0.451174259185791                                         \n",
      "epoch 2 [16.1s]:  training loss=0.28151968121528625                                        \n",
      "epoch 3 [15.66s]:  training loss=0.22394250333309174                                       \n",
      "epoch 4 [15.83s]:  training loss=0.1978853940963745                                        \n",
      "epoch 5 [15.91s]: training loss=0.1748160421848297  validation ndcg@10=0.02000243142191648 [0.44s]\n",
      "epoch 6 [15.78s]:  training loss=0.17141850292682648                                       \n",
      "epoch 7 [15.94s]:  training loss=0.16061627864837646                                       \n",
      "epoch 8 [16.15s]:  training loss=0.1483822911977768                                        \n",
      "epoch 9 [16.2s]:  training loss=0.13930107653141022                                        \n",
      "epoch 10 [15.52s]: training loss=0.1306922286748886  validation ndcg@10=0.022350021410314773 [0.46s]\n",
      "epoch 11 [16.0s]:  training loss=0.12734971940517426                                       \n",
      "epoch 12 [16.26s]:  training loss=0.12103357911109924                                      \n",
      "epoch 13 [16.11s]:  training loss=0.12001489847898483                                      \n",
      "epoch 14 [16.11s]:  training loss=0.11006128787994385                                      \n",
      "epoch 15 [15.89s]: training loss=0.10464707762002945  validation ndcg@10=0.023797184181858312 [0.47s]\n",
      "epoch 16 [17.32s]:  training loss=0.10544312000274658                                      \n",
      "epoch 17 [16.73s]:  training loss=0.10000301897525787                                      \n",
      "epoch 18 [16.22s]:  training loss=0.09706302732229233                                      \n",
      "epoch 19 [15.88s]:  training loss=0.09392144531011581                                      \n",
      "epoch 20 [16.14s]: training loss=0.09147656708955765  validation ndcg@10=0.02454757497079941 [0.48s]\n",
      "epoch 21 [15.86s]:  training loss=0.09124650061130524                                      \n",
      "epoch 22 [15.7s]:  training loss=0.08655951917171478                                       \n",
      "epoch 23 [15.4s]:  training loss=0.08597471565008163                                       \n",
      "epoch 24 [15.94s]:  training loss=0.07947496324777603                                      \n",
      "epoch 25 [15.95s]: training loss=0.08099886029958725  validation ndcg@10=0.024889906076610263 [0.47s]\n",
      "epoch 26 [16.12s]:  training loss=0.07924686372280121                                      \n",
      "epoch 27 [15.82s]:  training loss=0.07892434298992157                                      \n",
      "epoch 28 [15.96s]:  training loss=0.07602673768997192                                      \n",
      "epoch 29 [15.74s]:  training loss=0.07225154340267181                                      \n",
      "epoch 30 [16.32s]: training loss=0.07309240847826004  validation ndcg@10=0.02432142835052557 [0.47s]\n",
      "epoch 31 [15.91s]:  training loss=0.07015394419431686                                      \n",
      "epoch 32 [15.92s]:  training loss=0.07135286182165146                                      \n",
      "epoch 33 [16.12s]:  training loss=0.0687701404094696                                       \n",
      "epoch 34 [16.26s]:  training loss=0.06800900399684906                                      \n",
      "epoch 35 [16.47s]: training loss=0.06793491542339325  validation ndcg@10=0.024939075857538774 [0.43s]\n",
      "epoch 36 [16.48s]:  training loss=0.06700228154659271                                      \n",
      "epoch 37 [16.55s]:  training loss=0.06257893890142441                                      \n",
      "epoch 38 [15.93s]:  training loss=0.061953675001859665                                     \n",
      "epoch 39 [16.2s]:  training loss=0.06246323511004448                                       \n",
      "epoch 40 [16.31s]: training loss=0.06299886107444763  validation ndcg@10=0.025103565501508253 [0.46s]\n",
      "epoch 41 [16.44s]:  training loss=0.06081576645374298                                      \n",
      "epoch 42 [16.1s]:  training loss=0.05779599770903587                                       \n",
      "epoch 43 [16.4s]:  training loss=0.061815522611141205                                      \n",
      "epoch 44 [16.02s]:  training loss=0.057890769094228745                                     \n",
      "epoch 45 [16.12s]: training loss=0.056638650596141815  validation ndcg@10=0.025039557254376964 [0.47s]\n",
      "epoch 46 [15.9s]:  training loss=0.05576787516474724                                       \n",
      "epoch 47 [15.99s]:  training loss=0.05539161339402199                                      \n",
      "epoch 48 [16.19s]:  training loss=0.0553765743970871                                       \n",
      "epoch 49 [15.72s]:  training loss=0.0565958134829998                                       \n",
      "epoch 50 [16.42s]: training loss=0.052852969616651535  validation ndcg@10=0.026244900865256018 [0.45s]\n",
      "epoch 51 [16.41s]:  training loss=0.05204583331942558                                      \n",
      "epoch 52 [16.62s]:  training loss=0.05675651505589485                                      \n",
      "epoch 53 [16.01s]:  training loss=0.052931420505046844                                     \n",
      "epoch 54 [16.16s]:  training loss=0.05083221197128296                                      \n",
      "epoch 55 [15.62s]: training loss=0.051378283649683  validation ndcg@10=0.024811753876736318 [0.46s]\n",
      "epoch 56 [16.03s]:  training loss=0.049694642424583435                                     \n",
      "epoch 57 [15.72s]:  training loss=0.05161536857485771                                      \n",
      "epoch 58 [16.08s]:  training loss=0.05037277191877365                                      \n",
      "epoch 59 [15.81s]:  training loss=0.04835351184010506                                      \n",
      "epoch 60 [16.07s]: training loss=0.04961686208844185  validation ndcg@10=0.025552663206388354 [0.49s]\n",
      "epoch 61 [15.69s]:  training loss=0.04784170538187027                                      \n",
      "epoch 62 [17.22s]:  training loss=0.046683527529239655                                     \n",
      "epoch 63 [15.49s]:  training loss=0.04754333570599556                                      \n",
      "epoch 64 [15.68s]:  training loss=0.04553660750389099                                      \n",
      "epoch 65 [15.59s]: training loss=0.04574716091156006  validation ndcg@10=0.02425411600025217 [0.44s]\n",
      "epoch 66 [15.79s]:  training loss=0.043300505727529526                                     \n",
      "epoch 67 [15.99s]:  training loss=0.045198358595371246                                     \n",
      "epoch 68 [16.06s]:  training loss=0.04436658322811127                                      \n",
      "epoch 69 [16.09s]:  training loss=0.04673335701227188                                      \n",
      "epoch 70 [15.93s]: training loss=0.04462326690554619  validation ndcg@10=0.025108692260202285 [0.47s]\n",
      "epoch 71 [16.05s]:  training loss=0.04396960511803627                                      \n",
      "epoch 72 [16.5s]:  training loss=0.042450930923223495                                      \n",
      "epoch 73 [16.29s]:  training loss=0.04356595128774643                                      \n",
      "epoch 74 [15.98s]:  training loss=0.045144736766815186                                     \n",
      "epoch 75 [16.63s]: training loss=0.044258736073970795  validation ndcg@10=0.02592363467294126 [0.45s]\n",
      "epoch 1 [15.12s]:  training loss=0.3062094748020172                                        \n",
      "epoch 2 [14.83s]:  training loss=0.2120964229106903                                        \n",
      "epoch 3 [14.79s]:  training loss=0.1924603432416916                                        \n",
      "epoch 4 [14.57s]:  training loss=0.1889844536781311                                        \n",
      "epoch 5 [15.59s]: training loss=0.18631736934185028  validation ndcg@10=0.018227332307674815 [0.43s]\n",
      "epoch 6 [14.88s]:  training loss=0.1778510957956314                                        \n",
      "epoch 7 [14.71s]:  training loss=0.1758531779050827                                        \n",
      "epoch 8 [14.73s]:  training loss=0.18247780203819275                                       \n",
      "epoch 9 [15.19s]:  training loss=0.17112594842910767                                       \n",
      "epoch 10 [15.07s]: training loss=0.1790347844362259  validation ndcg@10=0.017517671489498718 [0.44s]\n",
      "epoch 11 [14.63s]:  training loss=0.17538417875766754                                      \n",
      "epoch 12 [14.79s]:  training loss=0.18237964808940887                                      \n",
      "epoch 13 [15.07s]:  training loss=0.17774496972560883                                      \n",
      "epoch 14 [14.86s]:  training loss=0.17672884464263916                                      \n",
      "epoch 15 [14.51s]: training loss=0.17451556026935577  validation ndcg@10=0.018782218900820193 [0.44s]\n",
      "epoch 16 [14.53s]:  training loss=0.1793489307165146                                       \n",
      "epoch 17 [14.74s]:  training loss=0.1715276837348938                                       \n",
      "epoch 18 [15.34s]:  training loss=0.17625920474529266                                      \n",
      "epoch 19 [14.78s]:  training loss=0.17719586193561554                                      \n",
      "epoch 20 [14.89s]: training loss=0.18444547057151794  validation ndcg@10=0.018181846687795596 [0.46s]\n",
      "epoch 21 [15.19s]:  training loss=0.1777312308549881                                       \n",
      "epoch 22 [15.19s]:  training loss=0.17385199666023254                                      \n",
      "epoch 23 [15.07s]:  training loss=0.19479303061962128                                      \n",
      "epoch 24 [15.25s]:  training loss=0.19205603003501892                                      \n",
      "epoch 25 [15.35s]: training loss=0.18556010723114014  validation ndcg@10=0.016827288879015596 [0.45s]\n",
      "epoch 26 [14.9s]:  training loss=0.19181287288665771                                       \n",
      "epoch 27 [15.01s]:  training loss=0.19270673394203186                                      \n",
      "epoch 28 [15.07s]:  training loss=0.18510542809963226                                      \n",
      "epoch 29 [15.98s]:  training loss=0.178379625082016                                        \n",
      "epoch 30 [15.58s]: training loss=0.1895223706960678  validation ndcg@10=0.017569317825610647 [0.45s]\n",
      "epoch 31 [15.63s]:  training loss=0.18493250012397766                                      \n",
      "epoch 32 [15.7s]:  training loss=0.183708056807518                                         \n",
      "epoch 33 [15.23s]:  training loss=0.19142062962055206                                      \n",
      "epoch 34 [15.38s]:  training loss=0.19370469450950623                                      \n",
      "epoch 35 [16.64s]: training loss=0.17857854068279266  validation ndcg@10=0.015772816682880216 [0.42s]\n",
      "epoch 36 [14.72s]:  training loss=0.20325545966625214                                      \n",
      "epoch 37 [15.17s]:  training loss=0.20422744750976562                                      \n",
      "epoch 38 [15.18s]:  training loss=0.196136936545372                                        \n",
      "epoch 39 [15.17s]:  training loss=0.2108500897884369                                       \n",
      "epoch 40 [14.79s]: training loss=0.1932794451713562  validation ndcg@10=0.016655184087100865 [0.43s]\n",
      "epoch 1 [14.69s]:  training loss=0.3395841121673584                                        \n",
      "epoch 2 [14.35s]:  training loss=0.20186428725719452                                       \n",
      "epoch 3 [14.61s]:  training loss=0.1681670993566513                                        \n",
      "epoch 4 [14.76s]:  training loss=0.14359793066978455                                       \n",
      "epoch 5 [14.58s]: training loss=0.13072486221790314  validation ndcg@10=0.024103840044610362 [0.48s]\n",
      "epoch 6 [14.69s]:  training loss=0.11959468573331833                                       \n",
      "epoch 7 [13.81s]:  training loss=0.1089346632361412                                        \n",
      "epoch 8 [14.23s]:  training loss=0.10306351631879807                                       \n",
      "epoch 9 [14.14s]:  training loss=0.09884434193372726                                       \n",
      "epoch 10 [14.46s]: training loss=0.09179585427045822  validation ndcg@10=0.022558928377666574 [0.44s]\n",
      "epoch 11 [14.43s]:  training loss=0.0876547247171402                                       \n",
      "epoch 12 [14.46s]:  training loss=0.08416273444890976                                      \n",
      "epoch 13 [13.88s]:  training loss=0.08484256267547607                                      \n",
      "epoch 14 [14.29s]:  training loss=0.08194776624441147                                      \n",
      "epoch 15 [14.02s]: training loss=0.07733131945133209  validation ndcg@10=0.023372946719888762 [0.44s]\n",
      "epoch 16 [14.01s]:  training loss=0.07649670541286469                                      \n",
      "epoch 17 [14.16s]:  training loss=0.07370997220277786                                      \n",
      "epoch 18 [14.05s]:  training loss=0.07453569769859314                                      \n",
      "epoch 19 [13.99s]:  training loss=0.06941330432891846                                      \n",
      "epoch 20 [13.93s]: training loss=0.06808038055896759  validation ndcg@10=0.023166902648143216 [0.4s]\n",
      "epoch 21 [14.62s]:  training loss=0.06689304858446121                                      \n",
      "epoch 22 [14.41s]:  training loss=0.06986155360937119                                      \n",
      "epoch 23 [14.33s]:  training loss=0.06465756148099899                                      \n",
      "epoch 24 [13.92s]:  training loss=0.06125288084149361                                      \n",
      "epoch 25 [14.46s]: training loss=0.061681948602199554  validation ndcg@10=0.022474653397304506 [0.44s]\n",
      "epoch 26 [14.26s]:  training loss=0.06202699616551399                                      \n",
      "epoch 27 [14.22s]:  training loss=0.06283828616142273                                      \n",
      "epoch 28 [14.04s]:  training loss=0.059418078511953354                                     \n",
      "epoch 29 [13.93s]:  training loss=0.06107645109295845                                      \n",
      "epoch 30 [14.16s]: training loss=0.05721422657370567  validation ndcg@10=0.024089695303583025 [0.45s]\n",
      "epoch 1 [13.31s]:  training loss=0.4010121524333954                                        \n",
      "epoch 2 [13.55s]:  training loss=0.24362318217754364                                     \n",
      "epoch 3 [13.81s]:  training loss=0.19873113930225372                                     \n",
      "epoch 4 [13.39s]:  training loss=0.17695920169353485                                     \n",
      "epoch 5 [13.82s]: training loss=0.1598651111125946  validation ndcg@10=0.0225917888485136 [0.4s]\n",
      "epoch 6 [13.4s]:  training loss=0.14391633868217468                                      \n",
      "epoch 7 [13.57s]:  training loss=0.13320496678352356                                     \n",
      "epoch 8 [13.44s]:  training loss=0.12430936843156815                                     \n",
      "epoch 9 [13.62s]:  training loss=0.11857075244188309                                     \n",
      "epoch 10 [13.8s]: training loss=0.112491175532341  validation ndcg@10=0.02378783500486871 [0.45s]\n",
      "epoch 11 [13.62s]:  training loss=0.10411239415407181                                    \n",
      "epoch 12 [13.38s]:  training loss=0.10363160073757172                                    \n",
      "epoch 13 [14.11s]:  training loss=0.09663817286491394                                    \n",
      "epoch 14 [14.21s]:  training loss=0.09568791091442108                                    \n",
      "epoch 15 [13.63s]: training loss=0.09049855172634125  validation ndcg@10=0.02381558841073158 [0.41s]\n",
      "epoch 16 [13.59s]:  training loss=0.0866210088133812                                     \n",
      "epoch 17 [15.49s]:  training loss=0.0827997624874115                                     \n",
      "epoch 18 [13.65s]:  training loss=0.08278876543045044                                    \n",
      "epoch 19 [13.5s]:  training loss=0.07947045564651489                                     \n",
      "epoch 20 [13.17s]: training loss=0.07497477531433105  validation ndcg@10=0.02537339908942732 [0.43s]\n",
      "epoch 21 [13.7s]:  training loss=0.07511935383081436                                     \n",
      "epoch 22 [13.59s]:  training loss=0.0726415365934372                                     \n",
      "epoch 23 [13.83s]:  training loss=0.07066220790147781                                    \n",
      "epoch 24 [13.63s]:  training loss=0.07139459997415543                                    \n",
      "epoch 25 [13.51s]: training loss=0.06638356298208237  validation ndcg@10=0.02463264838866507 [0.39s]\n",
      "epoch 26 [13.53s]:  training loss=0.06540547311306                                       \n",
      "epoch 27 [13.57s]:  training loss=0.06579546630382538                                    \n",
      "epoch 28 [13.79s]:  training loss=0.06517434120178223                                    \n",
      "epoch 29 [13.68s]:  training loss=0.06399457156658173                                    \n",
      "epoch 30 [13.41s]: training loss=0.06159753352403641  validation ndcg@10=0.025317290052816587 [0.4s]\n",
      "epoch 31 [13.59s]:  training loss=0.06008020043373108                                    \n",
      "epoch 32 [13.94s]:  training loss=0.05926420912146568                                    \n",
      "epoch 33 [14.2s]:  training loss=0.05796605721116066                                     \n",
      "epoch 34 [13.86s]:  training loss=0.05728225037455559                                    \n",
      "epoch 35 [13.82s]: training loss=0.05848923325538635  validation ndcg@10=0.02471412447258849 [0.43s]\n",
      "epoch 36 [14.01s]:  training loss=0.055503178387880325                                   \n",
      "epoch 37 [14.01s]:  training loss=0.055526234209537506                                   \n",
      "epoch 38 [13.98s]:  training loss=0.0534789115190506                                     \n",
      "epoch 39 [14.06s]:  training loss=0.053340569138526917                                   \n",
      "epoch 40 [13.99s]: training loss=0.053153179585933685  validation ndcg@10=0.02358314843364757 [0.44s]\n",
      "epoch 41 [13.91s]:  training loss=0.0528406985104084                                     \n",
      "epoch 42 [13.29s]:  training loss=0.05254427343606949                                    \n",
      "epoch 43 [13.39s]:  training loss=0.05081498622894287                                    \n",
      "epoch 44 [13.44s]:  training loss=0.05108905956149101                                    \n",
      "epoch 45 [13.6s]: training loss=0.05100204795598984  validation ndcg@10=0.025635394974214454 [0.42s]\n",
      "epoch 46 [13.56s]:  training loss=0.04999707639217377                                    \n",
      "epoch 47 [13.66s]:  training loss=0.05120140686631203                                    \n",
      "epoch 48 [13.51s]:  training loss=0.04845977574586868                                    \n",
      "epoch 49 [13.5s]:  training loss=0.045502740889787674                                    \n",
      "epoch 50 [13.97s]: training loss=0.044480517506599426  validation ndcg@10=0.024599662275096974 [0.44s]\n",
      "epoch 51 [13.87s]:  training loss=0.04448616877198219                                    \n",
      "epoch 52 [13.36s]:  training loss=0.04603105038404465                                    \n",
      "epoch 53 [13.48s]:  training loss=0.045236337929964066                                   \n",
      "epoch 54 [14.51s]:  training loss=0.04723170027136803                                    \n",
      "epoch 55 [13.68s]: training loss=0.0438111238181591  validation ndcg@10=0.02400300472488801 [0.43s]\n",
      "epoch 56 [14.09s]:  training loss=0.04573248699307442                                    \n",
      "epoch 57 [14.14s]:  training loss=0.04388108104467392                                    \n",
      "epoch 58 [13.83s]:  training loss=0.04390023648738861                                    \n",
      "epoch 59 [14.14s]:  training loss=0.04162606596946716                                    \n",
      "epoch 60 [13.6s]: training loss=0.04298924282193184  validation ndcg@10=0.02394224277889515 [0.42s]\n",
      "epoch 61 [13.78s]:  training loss=0.04399595037102699                                    \n",
      "epoch 62 [14.38s]:  training loss=0.042516518384218216                                   \n",
      "epoch 63 [13.62s]:  training loss=0.047261737287044525                                   \n",
      "epoch 64 [13.87s]:  training loss=0.042032260447740555                                   \n",
      "epoch 65 [13.99s]: training loss=0.04106353595852852  validation ndcg@10=0.022085015268108545 [0.4s]\n",
      "epoch 66 [13.57s]:  training loss=0.04103337228298187                                    \n",
      "epoch 67 [13.86s]:  training loss=0.039167724549770355                                   \n",
      "epoch 68 [13.67s]:  training loss=0.040864601731300354                                   \n",
      "epoch 69 [13.23s]:  training loss=0.04224502667784691                                    \n",
      "epoch 70 [13.84s]: training loss=0.03951409459114075  validation ndcg@10=0.023394966987387706 [0.43s]\n",
      "epoch 1 [15.79s]:  training loss=0.30800074338912964                                     \n",
      "epoch 2 [12.67s]:  training loss=0.1897965371608734                                      \n",
      "epoch 3 [13.17s]:  training loss=0.15415151417255402                                     \n",
      "epoch 4 [14.07s]:  training loss=0.14559748768806458                                     \n",
      "epoch 5 [13.92s]: training loss=0.13046392798423767  validation ndcg@10=0.019629516564901497 [0.4s]\n",
      "epoch 6 [13.79s]:  training loss=0.1264369636774063                                      \n",
      "epoch 7 [13.84s]:  training loss=0.11901809275150299                                     \n",
      "epoch 8 [14.05s]:  training loss=0.11788211762905121                                     \n",
      "epoch 9 [13.64s]:  training loss=0.11613502353429794                                     \n",
      "epoch 10 [13.48s]: training loss=0.10820568352937698  validation ndcg@10=0.019831927547946358 [0.43s]\n",
      "epoch 11 [13.49s]:  training loss=0.11135539412498474                                    \n",
      "epoch 12 [13.9s]:  training loss=0.1085866317152977                                      \n",
      "epoch 13 [13.53s]:  training loss=0.10834173113107681                                    \n",
      "epoch 14 [13.07s]:  training loss=0.10286712646484375                                    \n",
      "epoch 15 [13.55s]: training loss=0.1070166602730751  validation ndcg@10=0.01820469137455782 [0.45s]\n",
      "epoch 16 [13.83s]:  training loss=0.09837470203638077                                    \n",
      "epoch 17 [13.83s]:  training loss=0.10476433485746384                                    \n",
      "epoch 18 [13.84s]:  training loss=0.10001932084560394                                    \n",
      "epoch 19 [13.35s]:  training loss=0.10338768362998962                                    \n",
      "epoch 20 [13.92s]: training loss=0.10251832753419876  validation ndcg@10=0.018657124295611692 [0.47s]\n",
      "epoch 21 [14.05s]:  training loss=0.10236822813749313                                    \n",
      "epoch 22 [13.56s]:  training loss=0.09994002431631088                                    \n",
      "epoch 23 [14.06s]:  training loss=0.1038290485739708                                     \n",
      "epoch 24 [13.77s]:  training loss=0.09955942630767822                                    \n",
      "epoch 25 [13.5s]: training loss=0.10163748264312744  validation ndcg@10=0.019007598897956175 [0.42s]\n",
      "epoch 26 [13.32s]:  training loss=0.09629630297422409                                    \n",
      "epoch 27 [15.54s]:  training loss=0.10027695447206497                                    \n",
      "epoch 28 [13.15s]:  training loss=0.10136636346578598                                    \n",
      "epoch 29 [13.6s]:  training loss=0.10216431319713593                                     \n",
      "epoch 30 [13.64s]: training loss=0.10048433393239975  validation ndcg@10=0.017400871718080427 [0.43s]\n",
      "epoch 31 [13.77s]:  training loss=0.09971047192811966                                    \n",
      "epoch 32 [13.14s]:  training loss=0.10212608426809311                                    \n",
      "epoch 33 [13.73s]:  training loss=0.10368122160434723                                    \n",
      "epoch 34 [13.47s]:  training loss=0.09309779852628708                                    \n",
      "epoch 35 [13.11s]: training loss=0.10374114662408829  validation ndcg@10=0.017419379884818904 [0.4s]\n",
      "100%|██████████| 50/50 [24:28:20<00:00, 1762.01s/trial, best loss: -0.027231703731088828]\n",
      "Tuning hyperparameters of PinSage Recommender on dataset adobe_core5...\n",
      "use_text_feature=False, use_no_feature=True\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengxuan_yan/opt/miniconda3/envs/torch/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 [35.96s]:  training loss=0.9053891897201538   \n",
      "epoch 2 [39.11s]:  training loss=1.3746825456619263   \n",
      "epoch 3 [42.26s]:  training loss=1.510263204574585    \n",
      "epoch 4 [37.91s]:  training loss=1.553107738494873    \n",
      "epoch 5 [38.75s]: training loss=1.6357768774032593  validation ndcg@10=0.01221994761789056 [0.9s]\n",
      "epoch 6 [38.63s]:  training loss=1.7369637489318848   \n",
      "epoch 7 [39.29s]:  training loss=1.733157753944397    \n",
      "epoch 8 [38.35s]:  training loss=1.762195110321045    \n",
      "epoch 9 [39.48s]:  training loss=1.8380528688430786   \n",
      "epoch 10 [38.11s]: training loss=1.8715980052947998  validation ndcg@10=0.014503616012785053 [0.93s]\n",
      "epoch 11 [38.93s]:  training loss=1.8492015600204468  \n",
      "epoch 12 [40.49s]:  training loss=2.0136160850524902  \n",
      "epoch 13 [38.91s]:  training loss=2.0256567001342773  \n",
      "epoch 14 [39.81s]:  training loss=2.0235257148742676  \n",
      "epoch 15 [38.09s]: training loss=2.0244855880737305  validation ndcg@10=0.015503290323800316 [0.86s]\n",
      "epoch 16 [39.6s]:  training loss=1.9915460348129272   \n",
      "epoch 17 [38.57s]:  training loss=2.284846067428589   \n",
      "epoch 18 [39.54s]:  training loss=2.174952507019043   \n",
      "epoch 19 [38.07s]:  training loss=2.312516450881958   \n",
      "epoch 20 [39.6s]: training loss=2.2454309463500977  validation ndcg@10=0.017189725733889362 [2.63s]\n",
      "epoch 21 [38.37s]:  training loss=2.262256145477295   \n",
      "epoch 22 [39.42s]:  training loss=2.3632376194000244  \n",
      "epoch 23 [37.52s]:  training loss=2.4099302291870117  \n",
      "epoch 24 [39.23s]:  training loss=2.387544631958008   \n",
      "epoch 25 [38.35s]: training loss=2.477419853210449  validation ndcg@10=0.01673687783105338 [0.82s]\n",
      "epoch 26 [39.56s]:  training loss=2.6094777584075928  \n",
      "epoch 27 [38.41s]:  training loss=2.4532980918884277  \n",
      "epoch 28 [38.89s]:  training loss=2.4930059909820557  \n",
      "epoch 29 [40.96s]:  training loss=2.6530888080596924  \n",
      "epoch 30 [38.74s]: training loss=2.461895704269409  validation ndcg@10=0.01829350808419576 [0.88s]\n",
      "epoch 31 [40.9s]:  training loss=2.553743600845337    \n",
      "epoch 32 [37.6s]:  training loss=2.4841079711914062   \n",
      "epoch 33 [39.96s]:  training loss=2.5231573581695557  \n",
      "epoch 34 [38.83s]:  training loss=2.7321224212646484  \n",
      "epoch 35 [39.75s]: training loss=2.6485538482666016  validation ndcg@10=0.016867200029865206 [0.86s]\n",
      "epoch 36 [38.49s]:  training loss=2.6391141414642334  \n",
      "epoch 37 [39.11s]:  training loss=2.6753363609313965  \n",
      "epoch 38 [40.99s]:  training loss=2.6418442726135254  \n",
      "epoch 39 [39.11s]:  training loss=2.6617894172668457  \n",
      "epoch 40 [39.02s]: training loss=2.5748331546783447  validation ndcg@10=0.01654186620255349 [0.82s]\n",
      "epoch 41 [39.45s]:  training loss=2.7642643451690674  \n",
      "epoch 42 [39.15s]:  training loss=2.780421018600464   \n",
      "epoch 43 [39.32s]:  training loss=2.746141195297241   \n",
      "epoch 44 [39.82s]:  training loss=2.7603471279144287  \n",
      "epoch 45 [40.53s]: training loss=2.9146687984466553  validation ndcg@10=0.017657347973980952 [0.87s]\n",
      "epoch 46 [40.9s]:  training loss=2.789567708969116    \n",
      "epoch 47 [39.43s]:  training loss=2.9601051807403564  \n",
      "epoch 48 [38.05s]:  training loss=2.9706594944000244  \n",
      "epoch 49 [38.93s]:  training loss=2.8789162635803223  \n",
      "epoch 50 [39.75s]: training loss=3.0161776542663574  validation ndcg@10=0.017855843811421385 [0.89s]\n",
      "epoch 51 [39.19s]:  training loss=2.9322776794433594  \n",
      "epoch 52 [38.27s]:  training loss=3.036322593688965   \n",
      "epoch 53 [38.8s]:  training loss=2.8972671031951904   \n",
      "epoch 54 [38.49s]:  training loss=3.0463359355926514  \n",
      "epoch 55 [41.02s]: training loss=3.138779401779175  validation ndcg@10=0.01654233023174719 [0.85s]\n",
      "epoch 1 [8.5s]:  training loss=0.4683801531791687                                      \n",
      "epoch 2 [8.33s]:  training loss=0.23300573229789734                                    \n",
      "epoch 3 [8.65s]:  training loss=0.18883030116558075                                    \n",
      "epoch 4 [8.48s]:  training loss=0.16752122342586517                                    \n",
      "epoch 5 [8.47s]: training loss=0.15772663056850433  validation ndcg@10=0.012467403102268635 [0.4s]\n",
      "epoch 6 [8.54s]:  training loss=0.15678073465824127                                    \n",
      "epoch 7 [8.47s]:  training loss=0.15169860422611237                                    \n",
      "epoch 8 [8.51s]:  training loss=0.14917388558387756                                    \n",
      "epoch 9 [8.37s]:  training loss=0.14249171316623688                                    \n",
      "epoch 10 [8.05s]: training loss=0.14767953753471375  validation ndcg@10=0.010146220852513802 [0.38s]\n",
      "epoch 11 [8.19s]:  training loss=0.15253111720085144                                   \n",
      "epoch 12 [8.83s]:  training loss=0.1515694558620453                                    \n",
      "epoch 13 [8.49s]:  training loss=0.14402547478675842                                   \n",
      "epoch 14 [9.17s]:  training loss=0.14828667044639587                                   \n",
      "epoch 15 [8.76s]: training loss=0.15108054876327515  validation ndcg@10=0.01016874845106992 [0.39s]\n",
      "epoch 16 [8.25s]:  training loss=0.15020783245563507                                   \n",
      "epoch 17 [7.96s]:  training loss=0.15190936625003815                                   \n",
      "epoch 18 [8.41s]:  training loss=0.15488773584365845                                   \n",
      "epoch 19 [8.23s]:  training loss=0.1519976556301117                                    \n",
      "epoch 20 [8.14s]: training loss=0.15322184562683105  validation ndcg@10=0.007175967858061836 [0.38s]\n",
      "epoch 21 [8.01s]:  training loss=0.15491005778312683                                   \n",
      "epoch 22 [8.23s]:  training loss=0.15241116285324097                                   \n",
      "epoch 23 [8.11s]:  training loss=0.1558636724948883                                    \n",
      "epoch 24 [8.03s]:  training loss=0.15423232316970825                                   \n",
      "epoch 25 [7.91s]: training loss=0.15112406015396118  validation ndcg@10=0.006770533673087205 [0.38s]\n",
      "epoch 26 [8.21s]:  training loss=0.1490209400653839                                    \n",
      "epoch 27 [7.97s]:  training loss=0.15481187403202057                                   \n",
      "epoch 28 [8.37s]:  training loss=0.15311700105667114                                   \n",
      "epoch 29 [8.13s]:  training loss=0.15425921976566315                                   \n",
      "epoch 30 [8.14s]: training loss=0.14885731041431427  validation ndcg@10=0.0058628132390822095 [0.4s]\n",
      "epoch 1 [40.59s]:  training loss=0.7444356083869934                                    \n",
      "epoch 2 [43.21s]:  training loss=0.5523183941841125                                    \n",
      "epoch 3 [40.53s]:  training loss=0.4470243453979492                                    \n",
      "epoch 4 [40.9s]:  training loss=0.3488790690898895                                     \n",
      "epoch 5 [40.54s]: training loss=0.2739222049713135  validation ndcg@10=0.02095018273780637 [0.79s]\n",
      "epoch 6 [41.23s]:  training loss=0.22324535250663757                                   \n",
      "epoch 7 [41.3s]:  training loss=0.1851871758699417                                     \n",
      "epoch 8 [41.11s]:  training loss=0.1562238335609436                                    \n",
      "epoch 9 [41.27s]:  training loss=0.1338358074426651                                    \n",
      "epoch 10 [43.34s]: training loss=0.11251246929168701  validation ndcg@10=0.024599014548965358 [0.91s]\n",
      "epoch 11 [41.12s]:  training loss=0.10361473262310028                                  \n",
      "epoch 12 [41.39s]:  training loss=0.09746909141540527                                  \n",
      "epoch 13 [41.3s]:  training loss=0.08733928203582764                                   \n",
      "epoch 14 [41.37s]:  training loss=0.0798761248588562                                   \n",
      "epoch 15 [40.54s]: training loss=0.07698460668325424  validation ndcg@10=0.02521216004461934 [0.8s]\n",
      "epoch 16 [41.26s]:  training loss=0.07019559293985367                                  \n",
      "epoch 17 [40.36s]:  training loss=0.0661783292889595                                   \n",
      "epoch 18 [41.13s]:  training loss=0.06451771408319473                                  \n",
      "epoch 19 [43.16s]:  training loss=0.06109902635216713                                  \n",
      "epoch 20 [40.9s]: training loss=0.058240294456481934  validation ndcg@10=0.02374948303156287 [0.8s]\n",
      "epoch 21 [41.04s]:  training loss=0.055489275604486465                                 \n",
      "epoch 22 [40.83s]:  training loss=0.05677773058414459                                  \n",
      "epoch 23 [41.04s]:  training loss=0.05272692069411278                                  \n",
      "epoch 24 [41.05s]:  training loss=0.050885144621133804                                 \n",
      "epoch 25 [40.96s]: training loss=0.05003786459565163  validation ndcg@10=0.022865624835637754 [0.85s]\n",
      "epoch 26 [40.88s]:  training loss=0.04928474873304367                                  \n",
      "epoch 27 [42.2s]:  training loss=0.047116395086050034                                  \n",
      "epoch 28 [40.71s]:  training loss=0.04635309800505638                                  \n",
      "epoch 29 [41.14s]:  training loss=0.04461992159485817                                  \n",
      "epoch 30 [40.59s]: training loss=0.04272005334496498  validation ndcg@10=0.023116827602336413 [0.79s]\n",
      "epoch 31 [41.05s]:  training loss=0.04186886176466942                                    \n",
      "epoch 32 [41.34s]:  training loss=0.043413396924734116                                   \n",
      "epoch 33 [41.2s]:  training loss=0.040187589824199677                                    \n",
      "epoch 34 [40.88s]:  training loss=0.04059535637497902                                    \n",
      "epoch 35 [43.4s]: training loss=0.03721541166305542  validation ndcg@10=0.023338536123525138 [0.9s]\n",
      "epoch 36 [39.87s]:  training loss=0.037294551730155945                                   \n",
      "epoch 37 [42.4s]:  training loss=0.03767029196023941                                     \n",
      "epoch 38 [42.3s]:  training loss=0.038068607449531555                                    \n",
      "epoch 39 [41.74s]:  training loss=0.03814605623483658                                    \n",
      "epoch 40 [43.06s]: training loss=0.0364551916718483  validation ndcg@10=0.02214082745936954 [0.81s]\n",
      "epoch 1 [22.57s]:  training loss=0.8493251800537109                                      \n",
      "epoch 2 [24.14s]:  training loss=0.823293924331665                                       \n",
      "epoch 3 [23.87s]:  training loss=0.8090721368789673                                      \n",
      "epoch 4 [23.74s]:  training loss=0.800278902053833                                       \n",
      "epoch 5 [23.64s]: training loss=0.7952448129653931  validation ndcg@10=0.0009366925100110341 [0.63s]\n",
      "epoch 6 [23.4s]:  training loss=0.7904145121574402                                       \n",
      "epoch 7 [23.41s]:  training loss=0.7850356101989746                                      \n",
      "epoch 8 [22.9s]:  training loss=0.7837774157524109                                       \n",
      "epoch 9 [22.07s]:  training loss=0.7761421799659729                                      \n",
      "epoch 10 [21.76s]: training loss=0.772760272026062  validation ndcg@10=0.0012815976401762141 [0.57s]\n",
      "epoch 11 [21.56s]:  training loss=0.7715145349502563                                     \n",
      "epoch 12 [21.38s]:  training loss=0.7683020830154419                                     \n",
      "epoch 13 [22.72s]:  training loss=0.7617005109786987                                     \n",
      "epoch 14 [19.84s]:  training loss=0.7583124041557312                                     \n",
      "epoch 15 [19.6s]: training loss=0.7531183362007141  validation ndcg@10=0.003953289883094648 [0.54s]\n",
      "epoch 16 [19.48s]:  training loss=0.7500115633010864                                     \n",
      "epoch 17 [19.41s]:  training loss=0.7400078773498535                                     \n",
      "epoch 18 [19.89s]:  training loss=0.7337136268615723                                     \n",
      "epoch 19 [20.18s]:  training loss=0.7247399687767029                                     \n",
      "epoch 20 [19.53s]: training loss=0.7123755812644958  validation ndcg@10=0.013843048524809526 [0.57s]\n",
      "epoch 21 [19.6s]:  training loss=0.7047300338745117                                      \n",
      "epoch 22 [19.54s]:  training loss=0.6970794796943665                                     \n",
      "epoch 23 [19.53s]:  training loss=0.6844241619110107                                     \n",
      "epoch 24 [19.58s]:  training loss=0.6737841367721558                                     \n",
      "epoch 25 [19.47s]: training loss=0.6723461151123047  validation ndcg@10=0.01680221661447464 [0.56s]\n",
      "epoch 26 [19.53s]:  training loss=0.6633923649787903                                     \n",
      "epoch 27 [19.74s]:  training loss=0.6594818830490112                                     \n",
      "epoch 28 [19.59s]:  training loss=0.6530805826187134                                     \n",
      "epoch 29 [19.53s]:  training loss=0.6463585495948792                                     \n",
      "epoch 30 [19.57s]: training loss=0.6380501985549927  validation ndcg@10=0.018344009054588845 [0.57s]\n",
      "epoch 31 [19.59s]:  training loss=0.6357086300849915                                     \n",
      "epoch 32 [19.52s]:  training loss=0.6315074563026428                                     \n",
      "epoch 33 [19.5s]:  training loss=0.6266859769821167                                      \n",
      "epoch 34 [19.49s]:  training loss=0.6269473433494568                                     \n",
      "epoch 35 [19.48s]: training loss=0.6235790848731995  validation ndcg@10=0.019047342233329934 [0.58s]\n",
      "epoch 36 [19.6s]:  training loss=0.6183351278305054                                      \n",
      "epoch 37 [21.36s]:  training loss=0.6132014393806458                                     \n",
      "epoch 38 [19.65s]:  training loss=0.6099485158920288                                     \n",
      "epoch 39 [19.58s]:  training loss=0.6107502579689026                                     \n",
      "epoch 40 [19.57s]: training loss=0.6035795211791992  validation ndcg@10=0.019387204985187218 [0.54s]\n",
      "epoch 41 [19.66s]:  training loss=0.6040270924568176                                     \n",
      "epoch 42 [19.49s]:  training loss=0.6004186272621155                                     \n",
      "epoch 43 [19.5s]:  training loss=0.5952305793762207                                      \n",
      "epoch 44 [19.5s]:  training loss=0.5939605832099915                                      \n",
      "epoch 45 [19.5s]: training loss=0.5921434760093689  validation ndcg@10=0.019619211240817436 [0.58s]\n",
      "epoch 46 [20.09s]:  training loss=0.5892673134803772                                     \n",
      "epoch 47 [19.83s]:  training loss=0.5871084332466125                                     \n",
      "epoch 48 [19.61s]:  training loss=0.5832600593566895                                     \n",
      "epoch 49 [19.47s]:  training loss=0.5806421041488647                                     \n",
      "epoch 50 [19.46s]: training loss=0.5812320709228516  validation ndcg@10=0.019340669181210746 [0.55s]\n",
      "epoch 51 [19.49s]:  training loss=0.5753604173660278                                     \n",
      "epoch 52 [19.52s]:  training loss=0.5697228312492371                                     \n",
      "epoch 53 [19.59s]:  training loss=0.5733678340911865                                     \n",
      "epoch 54 [19.44s]:  training loss=0.5660833716392517                                     \n",
      "epoch 55 [19.37s]: training loss=0.5679677724838257  validation ndcg@10=0.019645641686688184 [0.53s]\n",
      "epoch 56 [19.5s]:  training loss=0.5633691549301147                                      \n",
      "epoch 57 [19.63s]:  training loss=0.5635766983032227                                     \n",
      "epoch 58 [19.59s]:  training loss=0.5597534775733948                                     \n",
      "epoch 59 [19.59s]:  training loss=0.5586144328117371                                     \n",
      "epoch 60 [19.68s]: training loss=0.5585167407989502  validation ndcg@10=0.019464171605158564 [0.51s]\n",
      "epoch 61 [21.37s]:  training loss=0.5525357127189636                                     \n",
      "epoch 62 [19.53s]:  training loss=0.5516579151153564                                     \n",
      "epoch 63 [19.68s]:  training loss=0.550614058971405                                      \n",
      "epoch 64 [19.5s]:  training loss=0.5436193346977234                                      \n",
      "epoch 65 [19.48s]: training loss=0.544363260269165  validation ndcg@10=0.019244513477232092 [0.5s]\n",
      "epoch 66 [19.61s]:  training loss=0.5427325963973999                                     \n",
      "epoch 67 [19.53s]:  training loss=0.5398768782615662                                     \n",
      "epoch 68 [19.59s]:  training loss=0.5375006794929504                                     \n",
      "epoch 69 [19.55s]:  training loss=0.5369631052017212                                     \n",
      "epoch 70 [19.47s]: training loss=0.5303769707679749  validation ndcg@10=0.018718709619289516 [0.56s]\n",
      "epoch 71 [19.51s]:  training loss=0.5360707640647888                                     \n",
      "epoch 72 [19.66s]:  training loss=0.5278019905090332                                     \n",
      "epoch 73 [20.13s]:  training loss=0.5309401154518127                                     \n",
      "epoch 74 [20.33s]:  training loss=0.5255990028381348                                     \n",
      "epoch 75 [19.74s]: training loss=0.5240966081619263  validation ndcg@10=0.01854077946446081 [0.5s]\n",
      "epoch 76 [19.49s]:  training loss=0.5208390951156616                                     \n",
      "epoch 77 [19.56s]:  training loss=0.52219158411026                                       \n",
      "epoch 78 [19.39s]:  training loss=0.5168358683586121                                     \n",
      "epoch 79 [19.55s]:  training loss=0.5164975523948669                                     \n",
      "epoch 80 [19.53s]: training loss=0.5086129903793335  validation ndcg@10=0.018175673644151975 [0.54s]\n",
      "epoch 1 [11.11s]:  training loss=0.48688775300979614                                     \n",
      "epoch 2 [11.58s]:  training loss=0.1934322863817215                                      \n",
      "epoch 3 [11.94s]:  training loss=0.12378422915935516                                     \n",
      "epoch 4 [11.88s]:  training loss=0.09999213367700577                                     \n",
      "epoch 5 [11.54s]: training loss=0.08479490876197815  validation ndcg@10=0.022175290284879853 [0.42s]\n",
      "epoch 6 [11.9s]:  training loss=0.07732231914997101                                      \n",
      "epoch 7 [11.6s]:  training loss=0.0721101388335228                                       \n",
      "epoch 8 [12.18s]:  training loss=0.06717105209827423                                     \n",
      "epoch 9 [13.22s]:  training loss=0.06627102941274643                                     \n",
      "epoch 10 [11.62s]: training loss=0.0632905587553978  validation ndcg@10=0.018617774604218748 [0.4s]\n",
      "epoch 11 [12.11s]:  training loss=0.06383135914802551                                    \n",
      "epoch 12 [12.06s]:  training loss=0.05900369957089424                                    \n",
      "epoch 13 [11.48s]:  training loss=0.06113313511013985                                    \n",
      "epoch 14 [11.9s]:  training loss=0.06311120092868805                                     \n",
      "epoch 15 [12.24s]: training loss=0.06122300401329994  validation ndcg@10=0.018232409116646274 [0.4s]\n",
      "epoch 16 [11.82s]:  training loss=0.06548044085502625                                    \n",
      "epoch 17 [11.72s]:  training loss=0.0650038793683052                                     \n",
      "epoch 18 [11.9s]:  training loss=0.06484513729810715                                     \n",
      "epoch 19 [11.8s]:  training loss=0.06230225786566734                                     \n",
      "epoch 20 [11.86s]: training loss=0.062214285135269165  validation ndcg@10=0.01752463059088967 [0.43s]\n",
      "epoch 21 [11.83s]:  training loss=0.06400932371616364                                    \n",
      "epoch 22 [11.87s]:  training loss=0.06535002589225769                                    \n",
      "epoch 23 [11.78s]:  training loss=0.06382814049720764                                    \n",
      "epoch 24 [12.12s]:  training loss=0.06220119073987007                                    \n",
      "epoch 25 [11.77s]: training loss=0.06394420564174652  validation ndcg@10=0.018719257342526257 [0.41s]\n",
      "epoch 26 [11.86s]:  training loss=0.062427766621112823                                   \n",
      "epoch 27 [12.08s]:  training loss=0.061197202652692795                                   \n",
      "epoch 28 [11.91s]:  training loss=0.06231649965047836                                    \n",
      "epoch 29 [11.68s]:  training loss=0.06514544785022736                                    \n",
      "epoch 30 [11.65s]: training loss=0.06688369810581207  validation ndcg@10=0.02002794042924988 [0.46s]\n",
      "epoch 1 [27.21s]:  training loss=0.8316785097122192                                      \n",
      "epoch 2 [29.65s]:  training loss=0.8035423755645752                                      \n",
      "epoch 3 [29.33s]:  training loss=0.7938172817230225                                      \n",
      "epoch 4 [28.79s]:  training loss=0.790035605430603                                       \n",
      "epoch 5 [29.18s]: training loss=0.7866833806037903  validation ndcg@10=0.0009457923233162363 [0.61s]\n",
      "epoch 6 [29.64s]:  training loss=0.7821170091629028                                      \n",
      "epoch 7 [28.65s]:  training loss=0.7778930068016052                                      \n",
      "epoch 8 [31.07s]:  training loss=0.7712977528572083                                      \n",
      "epoch 9 [28.48s]:  training loss=0.7644743323326111                                      \n",
      "epoch 10 [28.33s]: training loss=0.7617714405059814  validation ndcg@10=0.0032484506924052633 [0.62s]\n",
      "epoch 11 [29.17s]:  training loss=0.7563340067863464                                     \n",
      "epoch 12 [29.12s]:  training loss=0.7506128549575806                                     \n",
      "epoch 13 [28.89s]:  training loss=0.7406517267227173                                     \n",
      "epoch 14 [29.2s]:  training loss=0.7288323044776917                                      \n",
      "epoch 15 [28.73s]: training loss=0.7145426869392395  validation ndcg@10=0.018249134207998976 [0.64s]\n",
      "epoch 16 [28.6s]:  training loss=0.6914392709732056                                      \n",
      "epoch 17 [29.23s]:  training loss=0.6749571561813354                                     \n",
      "epoch 18 [29.4s]:  training loss=0.6570300459861755                                      \n",
      "epoch 19 [29.36s]:  training loss=0.6475673913955688                                     \n",
      "epoch 20 [29.23s]: training loss=0.6394540071487427  validation ndcg@10=0.02120986619129447 [0.67s]\n",
      "epoch 21 [29.74s]:  training loss=0.6287950873374939                                     \n",
      "epoch 22 [28.74s]:  training loss=0.6216282844543457                                     \n",
      "epoch 23 [29.2s]:  training loss=0.6183129549026489                                      \n",
      "epoch 24 [28.68s]:  training loss=0.6129435896873474                                     \n",
      "epoch 25 [30.59s]: training loss=0.6091787815093994  validation ndcg@10=0.02156038395187413 [0.63s]\n",
      "epoch 26 [28.55s]:  training loss=0.6018025875091553                                     \n",
      "epoch 27 [29.38s]:  training loss=0.597558856010437                                      \n",
      "epoch 28 [29.31s]:  training loss=0.5976065397262573                                     \n",
      "epoch 29 [29.26s]:  training loss=0.5935103893280029                                     \n",
      "epoch 30 [29.01s]: training loss=0.5863736867904663  validation ndcg@10=0.021349173704559098 [0.65s]\n",
      "epoch 31 [29.1s]:  training loss=0.5864925384521484                                      \n",
      "epoch 32 [28.45s]:  training loss=0.5786415338516235                                     \n",
      "epoch 33 [28.4s]:  training loss=0.5756276249885559                                      \n",
      "epoch 34 [28.26s]:  training loss=0.5750035047531128                                     \n",
      "epoch 35 [28.24s]: training loss=0.5707277655601501  validation ndcg@10=0.021168390141376146 [0.6s]\n",
      "epoch 36 [28.71s]:  training loss=0.5665181875228882                                     \n",
      "epoch 37 [29.14s]:  training loss=0.5613752603530884                                     \n",
      "epoch 38 [29.09s]:  training loss=0.5577277541160583                                     \n",
      "epoch 39 [28.91s]:  training loss=0.553792417049408                                      \n",
      "epoch 40 [29.14s]: training loss=0.5538948774337769  validation ndcg@10=0.021444477690690963 [0.65s]\n",
      "epoch 41 [31.17s]:  training loss=0.5480383634567261                                     \n",
      "epoch 42 [28.9s]:  training loss=0.5442567467689514                                      \n",
      "epoch 43 [28.77s]:  training loss=0.5410709381103516                                     \n",
      "epoch 44 [27.96s]:  training loss=0.5368298292160034                                     \n",
      "epoch 45 [28.6s]: training loss=0.5349795818328857  validation ndcg@10=0.021423866521204902 [0.62s]\n",
      "epoch 46 [29.22s]:  training loss=0.5320987105369568                                     \n",
      "epoch 47 [29.67s]:  training loss=0.5287344455718994                                     \n",
      "epoch 48 [29.84s]:  training loss=0.5231839418411255                                     \n",
      "epoch 49 [29.71s]:  training loss=0.5228475332260132                                     \n",
      "epoch 50 [29.53s]: training loss=0.5204238295555115  validation ndcg@10=0.02099789127599941 [0.61s]\n",
      "epoch 1 [18.45s]:  training loss=0.8102533221244812                                      \n",
      "epoch 2 [18.1s]:  training loss=0.7875852584838867                                       \n",
      "epoch 3 [18.43s]:  training loss=0.7796716094017029                                      \n",
      "epoch 4 [18.62s]:  training loss=0.7654155492782593                                      \n",
      "epoch 5 [17.85s]: training loss=0.7567988038063049  validation ndcg@10=0.012959740594005717 [0.53s]\n",
      "epoch 6 [18.31s]:  training loss=0.7438861727714539                                      \n",
      "epoch 7 [17.75s]:  training loss=0.7237429022789001                                      \n",
      "epoch 8 [18.33s]:  training loss=0.680561363697052                                       \n",
      "epoch 9 [18.1s]:  training loss=0.6375333070755005                                       \n",
      "epoch 10 [18.42s]: training loss=0.6160266399383545  validation ndcg@10=0.02088255851843037 [0.62s]\n",
      "epoch 11 [17.86s]:  training loss=0.6046251654624939                                     \n",
      "epoch 12 [19.88s]:  training loss=0.5972478985786438                                     \n",
      "epoch 13 [18.35s]:  training loss=0.5822691917419434                                     \n",
      "epoch 14 [18.68s]:  training loss=0.5715200901031494                                     \n",
      "epoch 15 [18.51s]: training loss=0.5640424489974976  validation ndcg@10=0.021070227334255166 [0.54s]\n",
      "epoch 16 [18.42s]:  training loss=0.5580590963363647                                     \n",
      "epoch 17 [18.21s]:  training loss=0.5455306172370911                                     \n",
      "epoch 18 [18.04s]:  training loss=0.5382015705108643                                     \n",
      "epoch 19 [18.21s]:  training loss=0.5275628566741943                                     \n",
      "epoch 20 [18.48s]: training loss=0.5197906494140625  validation ndcg@10=0.021117026929369096 [0.54s]\n",
      "epoch 21 [18.25s]:  training loss=0.5079088807106018                                     \n",
      "epoch 22 [18.2s]:  training loss=0.5019423961639404                                      \n",
      "epoch 23 [18.37s]:  training loss=0.4917028844356537                                     \n",
      "epoch 24 [18.66s]:  training loss=0.4855775237083435                                     \n",
      "epoch 25 [18.23s]: training loss=0.4789556562900543  validation ndcg@10=0.02152009009172688 [0.5s]\n",
      "epoch 26 [18.17s]:  training loss=0.4714703857898712                                     \n",
      "epoch 27 [18.35s]:  training loss=0.46095648407936096                                    \n",
      "epoch 28 [18.06s]:  training loss=0.4505558907985687                                     \n",
      "epoch 29 [17.92s]:  training loss=0.44166645407676697                                    \n",
      "epoch 30 [18.08s]: training loss=0.43416211009025574  validation ndcg@10=0.021626828437311924 [0.51s]\n",
      "epoch 31 [18.21s]:  training loss=0.42693448066711426                                    \n",
      "epoch 32 [18.23s]:  training loss=0.4125119149684906                                     \n",
      "epoch 33 [18.25s]:  training loss=0.4046836793422699                                     \n",
      "epoch 34 [18.17s]:  training loss=0.39627546072006226                                    \n",
      "epoch 35 [18.05s]: training loss=0.39140212535858154  validation ndcg@10=0.022273237686550148 [0.49s]\n",
      "epoch 36 [18.45s]:  training loss=0.38448113203048706                                    \n",
      "epoch 37 [18.0s]:  training loss=0.3774043619632721                                      \n",
      "epoch 38 [19.56s]:  training loss=0.36904552578926086                                    \n",
      "epoch 39 [16.76s]:  training loss=0.3643135130405426                                     \n",
      "epoch 40 [17.71s]: training loss=0.3575761020183563  validation ndcg@10=0.024100529837434652 [0.51s]\n",
      "epoch 41 [17.41s]:  training loss=0.3506929576396942                                     \n",
      "epoch 42 [17.69s]:  training loss=0.34824422001838684                                    \n",
      "epoch 43 [17.09s]:  training loss=0.34436726570129395                                    \n",
      "epoch 44 [18.14s]:  training loss=0.3320549726486206                                     \n",
      "epoch 45 [17.33s]: training loss=0.33261463046073914  validation ndcg@10=0.023335157372990505 [0.5s]\n",
      "epoch 46 [16.9s]:  training loss=0.3277137279510498                                      \n",
      "epoch 47 [17.63s]:  training loss=0.3203407824039459                                     \n",
      "epoch 48 [17.34s]:  training loss=0.3148763179779053                                     \n",
      "epoch 49 [17.15s]:  training loss=0.31032949686050415                                    \n",
      "epoch 50 [17.66s]: training loss=0.3094693422317505  validation ndcg@10=0.023955214652710513 [0.5s]\n",
      "epoch 51 [17.52s]:  training loss=0.30120769143104553                                    \n",
      "epoch 52 [17.35s]:  training loss=0.29909178614616394                                    \n",
      "epoch 53 [17.12s]:  training loss=0.2922663688659668                                     \n",
      "epoch 54 [17.82s]:  training loss=0.2885424494743347                                     \n",
      "epoch 55 [17.51s]: training loss=0.2855616807937622  validation ndcg@10=0.02433232590848136 [0.49s]\n",
      "epoch 56 [17.45s]:  training loss=0.28088000416755676                                    \n",
      "epoch 57 [19.24s]:  training loss=0.27614152431488037                                    \n",
      "epoch 58 [17.9s]:  training loss=0.27251437306404114                                     \n",
      "epoch 59 [17.87s]:  training loss=0.2682967483997345                                     \n",
      "epoch 60 [17.75s]: training loss=0.2662697732448578  validation ndcg@10=0.023956569296994423 [0.48s]\n",
      "epoch 61 [17.39s]:  training loss=0.26319748163223267                                    \n",
      "epoch 62 [17.41s]:  training loss=0.2575588524341583                                     \n",
      "epoch 63 [17.76s]:  training loss=0.2531750798225403                                     \n",
      "epoch 64 [17.6s]:  training loss=0.2538769841194153                                      \n",
      "epoch 65 [17.62s]: training loss=0.24529792368412018  validation ndcg@10=0.024580975170732832 [0.49s]\n",
      "epoch 66 [17.32s]:  training loss=0.24349786341190338                                    \n",
      "epoch 67 [17.7s]:  training loss=0.24033106863498688                                     \n",
      "epoch 68 [17.33s]:  training loss=0.23634083569049835                                    \n",
      "epoch 69 [17.77s]:  training loss=0.23316268622875214                                    \n",
      "epoch 70 [17.33s]: training loss=0.23028792440891266  validation ndcg@10=0.024681048420958387 [0.5s]\n",
      "epoch 71 [17.34s]:  training loss=0.22508202493190765                                    \n",
      "epoch 72 [17.32s]:  training loss=0.22357559204101562                                    \n",
      "epoch 73 [17.18s]:  training loss=0.22107689082622528                                    \n",
      "epoch 74 [17.45s]:  training loss=0.2171122431755066                                     \n",
      "epoch 75 [19.51s]: training loss=0.2121727615594864  validation ndcg@10=0.025029387808356764 [0.48s]\n",
      "epoch 76 [17.16s]:  training loss=0.2110322266817093                                     \n",
      "epoch 77 [17.34s]:  training loss=0.20758189260959625                                    \n",
      "epoch 78 [17.43s]:  training loss=0.20204907655715942                                    \n",
      "epoch 79 [17.58s]:  training loss=0.19816596806049347                                    \n",
      "epoch 80 [17.56s]: training loss=0.19917620718479156  validation ndcg@10=0.0253888029810987 [0.52s]\n",
      "epoch 81 [17.53s]:  training loss=0.191383495926857                                      \n",
      "epoch 82 [17.95s]:  training loss=0.19069911539554596                                    \n",
      "epoch 83 [17.59s]:  training loss=0.18964838981628418                                    \n",
      "epoch 84 [17.44s]:  training loss=0.18558470904827118                                    \n",
      "epoch 85 [17.39s]: training loss=0.18383948504924774  validation ndcg@10=0.025772780360958276 [0.48s]\n",
      "epoch 86 [17.86s]:  training loss=0.18291021883487701                                    \n",
      "epoch 87 [17.84s]:  training loss=0.18085280060768127                                    \n",
      "epoch 88 [17.27s]:  training loss=0.17579315602779388                                    \n",
      "epoch 89 [17.68s]:  training loss=0.17749883234500885                                    \n",
      "epoch 90 [17.43s]: training loss=0.17114943265914917  validation ndcg@10=0.025582075128282908 [0.54s]\n",
      "epoch 91 [18.17s]:  training loss=0.16949445009231567                                    \n",
      "epoch 92 [18.75s]:  training loss=0.1688796430826187                                     \n",
      "epoch 93 [17.33s]:  training loss=0.16681548953056335                                    \n",
      "epoch 94 [17.62s]:  training loss=0.16423764824867249                                    \n",
      "epoch 95 [17.3s]: training loss=0.16158126294612885  validation ndcg@10=0.02580496671738204 [0.48s]\n",
      "epoch 96 [17.72s]:  training loss=0.15840277075767517                                    \n",
      "epoch 97 [17.57s]:  training loss=0.15399204194545746                                    \n",
      "epoch 98 [17.54s]:  training loss=0.15408490598201752                                    \n",
      "epoch 99 [17.27s]:  training loss=0.14998775720596313                                    \n",
      "epoch 100 [18.09s]: training loss=0.1504335254430771  validation ndcg@10=0.02612033054732362 [0.5s]\n",
      "epoch 101 [17.84s]:  training loss=0.14807644486427307                                   \n",
      "epoch 102 [17.67s]:  training loss=0.1479240208864212                                    \n",
      "epoch 103 [17.41s]:  training loss=0.1425706446170807                                    \n",
      "epoch 104 [17.57s]:  training loss=0.14262939989566803                                   \n",
      "epoch 105 [17.73s]: training loss=0.14208678901195526  validation ndcg@10=0.026056465444337873 [0.52s]\n",
      "epoch 106 [17.66s]:  training loss=0.13678817451000214                                   \n",
      "epoch 107 [17.28s]:  training loss=0.1404155045747757                                    \n",
      "epoch 108 [17.46s]:  training loss=0.13594011962413788                                   \n",
      "epoch 109 [17.28s]:  training loss=0.13377512991428375                                   \n",
      "epoch 110 [19.44s]: training loss=0.13165190815925598  validation ndcg@10=0.025901006229901248 [0.48s]\n",
      "epoch 111 [17.38s]:  training loss=0.1299661248922348                                    \n",
      "epoch 112 [17.6s]:  training loss=0.13015024363994598                                    \n",
      "epoch 113 [17.03s]:  training loss=0.12771114706993103                                   \n",
      "epoch 114 [17.23s]:  training loss=0.1272788941860199                                    \n",
      "epoch 115 [17.06s]: training loss=0.12443792819976807  validation ndcg@10=0.02664973948439339 [0.48s]\n",
      "epoch 116 [17.27s]:  training loss=0.12341306358575821                                   \n",
      "epoch 117 [17.45s]:  training loss=0.1244078204035759                                    \n",
      "epoch 118 [17.29s]:  training loss=0.1192210465669632                                    \n",
      "epoch 119 [17.38s]:  training loss=0.12008057534694672                                   \n",
      "epoch 120 [17.6s]: training loss=0.11840325593948364  validation ndcg@10=0.027189716350585296 [0.49s]\n",
      "epoch 121 [17.34s]:  training loss=0.11460395902395248                                   \n",
      "epoch 122 [17.22s]:  training loss=0.1146743968129158                                    \n",
      "epoch 123 [17.9s]:  training loss=0.11265558004379272                                    \n",
      "epoch 124 [17.34s]:  training loss=0.11585204303264618                                   \n",
      "epoch 125 [17.86s]: training loss=0.11055397242307663  validation ndcg@10=0.027366613948497813 [0.56s]\n",
      "epoch 126 [17.74s]:  training loss=0.11243210732936859                                   \n",
      "epoch 127 [17.6s]:  training loss=0.11054352670907974                                    \n",
      "epoch 128 [19.16s]:  training loss=0.10817219316959381                                   \n",
      "epoch 129 [17.55s]:  training loss=0.10649676620960236                                   \n",
      "epoch 130 [17.03s]: training loss=0.10520912706851959  validation ndcg@10=0.02717014188957983 [0.55s]\n",
      "epoch 131 [17.1s]:  training loss=0.10639128088951111                                    \n",
      "epoch 132 [17.39s]:  training loss=0.10308755934238434                                   \n",
      "epoch 133 [17.56s]:  training loss=0.10186253488063812                                   \n",
      "epoch 134 [17.53s]:  training loss=0.10219037532806396                                   \n",
      "epoch 135 [17.53s]: training loss=0.10127613693475723  validation ndcg@10=0.02772510629337284 [0.48s]\n",
      "epoch 136 [17.89s]:  training loss=0.09631022810935974                                   \n",
      "epoch 137 [17.48s]:  training loss=0.09712892025709152                                   \n",
      "epoch 138 [17.29s]:  training loss=0.09649258852005005                                   \n",
      "epoch 139 [17.67s]:  training loss=0.09432293474674225                                   \n",
      "epoch 140 [17.29s]: training loss=0.093643918633461  validation ndcg@10=0.027687524792059215 [0.48s]\n",
      "epoch 141 [17.24s]:  training loss=0.09446634352207184                                   \n",
      "epoch 142 [17.73s]:  training loss=0.0931834802031517                                    \n",
      "epoch 143 [17.62s]:  training loss=0.09144586324691772                                   \n",
      "epoch 144 [17.47s]:  training loss=0.09065220504999161                                   \n",
      "epoch 145 [17.59s]: training loss=0.0928814709186554  validation ndcg@10=0.02707514163921685 [0.48s]\n",
      "epoch 146 [20.16s]:  training loss=0.09114699810743332                                   \n",
      "epoch 147 [17.9s]:  training loss=0.09200921654701233                                    \n",
      "epoch 148 [17.43s]:  training loss=0.08935233950614929                                   \n",
      "epoch 149 [17.49s]:  training loss=0.08707726001739502                                   \n",
      "epoch 150 [17.22s]: training loss=0.08427939563989639  validation ndcg@10=0.027604550565882423 [0.5s]\n",
      "epoch 151 [17.28s]:  training loss=0.08659207820892334                                   \n",
      "epoch 152 [17.3s]:  training loss=0.08463120460510254                                    \n",
      "epoch 153 [17.22s]:  training loss=0.0835019201040268                                    \n",
      "epoch 154 [17.32s]:  training loss=0.08383821696043015                                   \n",
      "epoch 155 [17.0s]: training loss=0.08301091939210892  validation ndcg@10=0.027201302200312586 [0.48s]\n",
      "epoch 156 [17.72s]:  training loss=0.08254872262477875                                   \n",
      "epoch 157 [17.8s]:  training loss=0.08381340652704239                                    \n",
      "epoch 158 [17.53s]:  training loss=0.08223195374011993                                   \n",
      "epoch 159 [17.58s]:  training loss=0.08038460463285446                                   \n",
      "epoch 160 [17.66s]: training loss=0.08085325360298157  validation ndcg@10=0.027550444526861284 [0.5s]\n",
      "epoch 1 [4.97s]:  training loss=0.7387885451316833                                       \n",
      "epoch 2 [5.05s]:  training loss=0.568724513053894                                        \n",
      "epoch 3 [4.82s]:  training loss=0.480527400970459                                        \n",
      "epoch 4 [4.89s]:  training loss=0.40378695726394653                                      \n",
      "epoch 5 [4.88s]: training loss=0.3400517702102661  validation ndcg@10=0.020491215874087372 [0.29s]\n",
      "epoch 6 [4.69s]:  training loss=0.2951788604259491                                       \n",
      "epoch 7 [4.59s]:  training loss=0.2588982582092285                                       \n",
      "epoch 8 [4.73s]:  training loss=0.22982989251613617                                      \n",
      "epoch 9 [4.75s]:  training loss=0.20797224342823029                                      \n",
      "epoch 10 [6.65s]: training loss=0.18642623722553253  validation ndcg@10=0.019925920380197415 [0.3s]\n",
      "epoch 11 [4.9s]:  training loss=0.17590129375457764                                      \n",
      "epoch 12 [4.34s]:  training loss=0.16262272000312805                                     \n",
      "epoch 13 [4.38s]:  training loss=0.14987587928771973                                     \n",
      "epoch 14 [4.38s]:  training loss=0.1436607986688614                                      \n",
      "epoch 15 [4.54s]: training loss=0.13392357528209686  validation ndcg@10=0.02091894343323028 [0.27s]\n",
      "epoch 16 [4.74s]:  training loss=0.1271183341741562                                      \n",
      "epoch 17 [4.78s]:  training loss=0.12187322974205017                                     \n",
      "epoch 18 [4.69s]:  training loss=0.1134534403681755                                      \n",
      "epoch 19 [4.71s]:  training loss=0.11136427521705627                                     \n",
      "epoch 20 [4.59s]: training loss=0.10631097853183746  validation ndcg@10=0.020220015172309782 [0.28s]\n",
      "epoch 21 [4.36s]:  training loss=0.10129950940608978                                     \n",
      "epoch 22 [4.36s]:  training loss=0.09627033025026321                                     \n",
      "epoch 23 [4.42s]:  training loss=0.09082795679569244                                     \n",
      "epoch 24 [4.72s]:  training loss=0.09122736752033234                                     \n",
      "epoch 25 [5.08s]: training loss=0.08598654717206955  validation ndcg@10=0.020187010992911206 [0.3s]\n",
      "epoch 26 [4.84s]:  training loss=0.0830821618437767                                      \n",
      "epoch 27 [4.6s]:  training loss=0.08122055232524872                                      \n",
      "epoch 28 [4.69s]:  training loss=0.07752713561058044                                     \n",
      "epoch 29 [4.67s]:  training loss=0.07460090517997742                                     \n",
      "epoch 30 [4.72s]: training loss=0.07378747314214706  validation ndcg@10=0.021892130373175254 [0.27s]\n",
      "epoch 31 [4.61s]:  training loss=0.06930907815694809                                     \n",
      "epoch 32 [4.73s]:  training loss=0.0698661208152771                                      \n",
      "epoch 33 [4.45s]:  training loss=0.06741542369127274                                     \n",
      "epoch 34 [4.59s]:  training loss=0.06614953279495239                                     \n",
      "epoch 35 [4.65s]: training loss=0.06380137801170349  validation ndcg@10=0.019457187423773264 [0.27s]\n",
      "epoch 36 [4.65s]:  training loss=0.06298399716615677                                     \n",
      "epoch 37 [4.66s]:  training loss=0.06002650409936905                                     \n",
      "epoch 38 [4.78s]:  training loss=0.05897987633943558                                     \n",
      "epoch 39 [4.67s]:  training loss=0.06062515079975128                                     \n",
      "epoch 40 [4.68s]: training loss=0.05887049064040184  validation ndcg@10=0.018469781005717376 [0.3s]\n",
      "epoch 41 [4.61s]:  training loss=0.05748820677399635                                     \n",
      "epoch 42 [4.73s]:  training loss=0.054849281907081604                                    \n",
      "epoch 43 [4.52s]:  training loss=0.05500662326812744                                     \n",
      "epoch 44 [4.67s]:  training loss=0.055175743997097015                                    \n",
      "epoch 45 [4.62s]: training loss=0.052924465388059616  validation ndcg@10=0.019005456736909215 [0.28s]\n",
      "epoch 46 [4.65s]:  training loss=0.051010459661483765                                    \n",
      "epoch 47 [4.39s]:  training loss=0.05327723175287247                                     \n",
      "epoch 48 [4.56s]:  training loss=0.050741273909807205                                    \n",
      "epoch 49 [4.61s]:  training loss=0.048979610204696655                                    \n",
      "epoch 50 [4.59s]: training loss=0.05059109255671501  validation ndcg@10=0.017443967896542144 [0.29s]\n",
      "epoch 51 [4.45s]:  training loss=0.048798684030771255                                    \n",
      "epoch 52 [4.61s]:  training loss=0.04905787482857704                                     \n",
      "epoch 53 [4.48s]:  training loss=0.04821344465017319                                     \n",
      "epoch 54 [4.52s]:  training loss=0.04615293815732002                                     \n",
      "epoch 55 [4.42s]: training loss=0.04772442579269409  validation ndcg@10=0.018523149635728 [0.26s]\n",
      "epoch 1 [5.99s]:  training loss=0.5983400940895081                                       \n",
      "epoch 2 [5.92s]:  training loss=0.31493327021598816                                      \n",
      "epoch 3 [5.85s]:  training loss=0.19725079834461212                                      \n",
      "epoch 4 [5.86s]:  training loss=0.14998596906661987                                      \n",
      "epoch 5 [5.9s]: training loss=0.12429583817720413  validation ndcg@10=0.019845202337904204 [0.34s]\n",
      "epoch 6 [5.98s]:  training loss=0.10585135221481323                                      \n",
      "epoch 7 [6.04s]:  training loss=0.09975030273199081                                      \n",
      "epoch 8 [6.04s]:  training loss=0.09348678588867188                                      \n",
      "epoch 9 [6.1s]:  training loss=0.08812002837657928                                       \n",
      "epoch 10 [5.92s]: training loss=0.08450251072645187  validation ndcg@10=0.019507074726182235 [0.31s]\n",
      "epoch 11 [6.45s]:  training loss=0.08059815317392349                                     \n",
      "epoch 12 [6.59s]:  training loss=0.07715515047311783                                     \n",
      "epoch 13 [6.4s]:  training loss=0.07450741529464722                                      \n",
      "epoch 14 [6.1s]:  training loss=0.07692493498325348                                      \n",
      "epoch 15 [6.04s]: training loss=0.07620303332805634  validation ndcg@10=0.01850628242624132 [0.31s]\n",
      "epoch 16 [6.04s]:  training loss=0.07527298480272293                                     \n",
      "epoch 17 [6.17s]:  training loss=0.07593147456645966                                     \n",
      "epoch 18 [5.95s]:  training loss=0.07424532622098923                                     \n",
      "epoch 19 [6.35s]:  training loss=0.07389044016599655                                     \n",
      "epoch 20 [6.04s]: training loss=0.0712130069732666  validation ndcg@10=0.017123417353728933 [0.32s]\n",
      "epoch 21 [5.96s]:  training loss=0.0728466734290123                                      \n",
      "epoch 22 [6.12s]:  training loss=0.07106185704469681                                     \n",
      "epoch 23 [6.05s]:  training loss=0.07280480116605759                                     \n",
      "epoch 24 [6.02s]:  training loss=0.07058015465736389                                     \n",
      "epoch 25 [6.1s]: training loss=0.07139955461025238  validation ndcg@10=0.01591005731858326 [0.35s]\n",
      "epoch 26 [5.88s]:  training loss=0.07358809560537338                                     \n",
      "epoch 27 [6.22s]:  training loss=0.07027579098939896                                     \n",
      "epoch 28 [6.27s]:  training loss=0.06938475370407104                                     \n",
      "epoch 29 [6.3s]:  training loss=0.07315786182880402                                      \n",
      "epoch 30 [6.12s]: training loss=0.0727979987859726  validation ndcg@10=0.016109835285440258 [0.34s]\n",
      "epoch 1 [9.47s]:  training loss=0.37655824422836304                                      \n",
      "epoch 2 [9.44s]:  training loss=0.31393951177597046                                     \n",
      "epoch 3 [9.25s]:  training loss=0.34898513555526733                                     \n",
      "epoch 4 [9.56s]:  training loss=0.3748805820941925                                      \n",
      "epoch 5 [9.2s]: training loss=0.3841223418712616  validation ndcg@10=0.013143094183588108 [0.36s]\n",
      "epoch 6 [9.64s]:  training loss=0.41627082228660583                                     \n",
      "epoch 7 [9.72s]:  training loss=0.44918447732925415                                     \n",
      "epoch 8 [9.69s]:  training loss=0.4952758550643921                                      \n",
      "epoch 9 [9.72s]:  training loss=0.4679412841796875                                      \n",
      "epoch 10 [9.48s]: training loss=0.5024009943008423  validation ndcg@10=0.014845384236754015 [0.37s]\n",
      "epoch 11 [9.56s]:  training loss=0.5084072947502136                                     \n",
      "epoch 12 [9.91s]:  training loss=0.5004919171333313                                     \n",
      "epoch 13 [9.73s]:  training loss=0.5577418804168701                                     \n",
      "epoch 14 [9.91s]:  training loss=0.5484636425971985                                     \n",
      "epoch 15 [9.69s]: training loss=0.5772082805633545  validation ndcg@10=0.013658352603892884 [0.4s]\n",
      "epoch 16 [9.67s]:  training loss=0.5466197729110718                                     \n",
      "epoch 17 [9.51s]:  training loss=0.5684158205986023                                     \n",
      "epoch 18 [9.57s]:  training loss=0.5879992842674255                                     \n",
      "epoch 19 [9.56s]:  training loss=0.6054148077964783                                     \n",
      "epoch 20 [9.63s]: training loss=0.5828610062599182  validation ndcg@10=0.013684586808012371 [0.35s]\n",
      "epoch 21 [9.73s]:  training loss=0.64127117395401                                       \n",
      "epoch 22 [11.32s]:  training loss=0.6332250237464905                                    \n",
      "epoch 23 [10.08s]:  training loss=0.6583462953567505                                    \n",
      "epoch 24 [9.78s]:  training loss=0.6222723722457886                                     \n",
      "epoch 25 [9.49s]: training loss=0.6406047940254211  validation ndcg@10=0.016983520059830254 [0.35s]\n",
      "epoch 26 [9.52s]:  training loss=0.6872553825378418                                     \n",
      "epoch 27 [9.36s]:  training loss=0.6466602683067322                                     \n",
      "epoch 28 [9.46s]:  training loss=0.6525734663009644                                     \n",
      "epoch 29 [9.61s]:  training loss=0.6985533237457275                                     \n",
      "epoch 30 [9.33s]: training loss=0.7048138380050659  validation ndcg@10=0.01452356753381584 [0.37s]\n",
      "epoch 31 [9.48s]:  training loss=0.6978644728660583                                     \n",
      "epoch 32 [9.66s]:  training loss=0.6828678846359253                                     \n",
      "epoch 33 [9.54s]:  training loss=0.6525675058364868                                     \n",
      "epoch 34 [9.36s]:  training loss=0.7222357392311096                                     \n",
      "epoch 35 [9.23s]: training loss=0.6998664140701294  validation ndcg@10=0.014634608508457467 [0.38s]\n",
      "epoch 36 [9.49s]:  training loss=0.6907982230186462                                     \n",
      "epoch 37 [9.77s]:  training loss=0.7060428857803345                                     \n",
      "epoch 38 [9.55s]:  training loss=0.7621111273765564                                     \n",
      "epoch 39 [9.61s]:  training loss=0.7557067275047302                                     \n",
      "epoch 40 [9.51s]: training loss=0.7585457563400269  validation ndcg@10=0.013535246954846997 [0.36s]\n",
      "epoch 41 [9.62s]:  training loss=0.6926416754722595                                     \n",
      "epoch 42 [9.26s]:  training loss=0.7345252633094788                                     \n",
      "epoch 43 [9.76s]:  training loss=0.805888831615448                                      \n",
      "epoch 44 [9.7s]:  training loss=0.7560495734214783                                      \n",
      "epoch 45 [9.91s]: training loss=0.7680192589759827  validation ndcg@10=0.015575768020242483 [0.41s]\n",
      "epoch 46 [9.66s]:  training loss=0.7851423621177673                                     \n",
      "epoch 47 [9.88s]:  training loss=0.7689259648323059                                     \n",
      "epoch 48 [9.65s]:  training loss=0.732265055179596                                      \n",
      "epoch 49 [10.14s]:  training loss=0.7515897154808044                                    \n",
      "epoch 50 [9.85s]: training loss=0.8272672891616821  validation ndcg@10=0.015705285191167078 [0.39s]\n",
      "epoch 1 [57.33s]:  training loss=0.3501571714878082                                     \n",
      "epoch 2 [53.51s]:  training loss=0.17939284443855286                                    \n",
      "epoch 3 [52.14s]:  training loss=0.1569075733423233                                     \n",
      "epoch 4 [50.54s]:  training loss=0.15558938682079315                                    \n",
      "epoch 5 [49.91s]: training loss=0.1498165875673294  validation ndcg@10=0.013737133999818133 [1.03s]\n",
      "epoch 6 [50.32s]:  training loss=0.15248286724090576                                    \n",
      "epoch 7 [46.34s]:  training loss=0.15397919714450836                                    \n",
      "epoch 8 [46.49s]:  training loss=0.15004247426986694                                    \n",
      "epoch 9 [46.4s]:  training loss=0.15729467570781708                                     \n",
      "epoch 10 [46.36s]: training loss=0.15750207006931305  validation ndcg@10=0.012880474423109281 [1.0s]\n",
      "epoch 11 [46.38s]:  training loss=0.15466660261154175                                   \n",
      "epoch 12 [46.26s]:  training loss=0.1539730727672577                                    \n",
      "epoch 13 [46.49s]:  training loss=0.1550251692533493                                    \n",
      "epoch 14 [46.35s]:  training loss=0.17175398766994476                                   \n",
      "epoch 15 [46.5s]: training loss=0.15871362388134003  validation ndcg@10=0.013505634131411528 [1.0s]\n",
      "epoch 16 [46.44s]:  training loss=0.1710468977689743                                    \n",
      "epoch 17 [47.35s]:  training loss=0.17028258740901947                                   \n",
      "epoch 18 [48.64s]:  training loss=0.1829986721277237                                    \n",
      "epoch 19 [46.64s]:  training loss=0.16276796162128448                                   \n",
      "epoch 20 [46.68s]: training loss=0.16578048467636108  validation ndcg@10=0.010749476536741878 [1.01s]\n",
      "epoch 21 [46.4s]:  training loss=0.17579418420791626                                    \n",
      "epoch 22 [46.28s]:  training loss=0.17594106495380402                                   \n",
      "epoch 23 [46.15s]:  training loss=0.18724261224269867                                   \n",
      "epoch 24 [46.28s]:  training loss=0.17734350264072418                                   \n",
      "epoch 25 [46.44s]: training loss=0.17858895659446716  validation ndcg@10=0.012140611427503644 [1.0s]\n",
      "epoch 26 [46.22s]:  training loss=0.17529037594795227                                   \n",
      "epoch 27 [46.47s]:  training loss=0.17925769090652466                                   \n",
      "epoch 28 [46.63s]:  training loss=0.1811119168996811                                    \n",
      "epoch 29 [47.11s]:  training loss=0.18341629207134247                                   \n",
      "epoch 30 [48.55s]: training loss=0.17521727085113525  validation ndcg@10=0.013163404817059786 [1.08s]\n",
      "epoch 1 [8.9s]:  training loss=0.7842522263526917                                        \n",
      "epoch 2 [9.15s]:  training loss=0.6932240128517151                                       \n",
      "epoch 3 [9.18s]:  training loss=0.5727730989456177                                       \n",
      "epoch 4 [9.23s]:  training loss=0.4873186945915222                                       \n",
      "epoch 5 [9.07s]: training loss=0.43805113434791565  validation ndcg@10=0.02081146229654924 [0.38s]\n",
      "epoch 6 [9.3s]:  training loss=0.39598608016967773                                       \n",
      "epoch 7 [9.37s]:  training loss=0.35623791813850403                                      \n",
      "epoch 8 [9.34s]:  training loss=0.31879130005836487                                      \n",
      "epoch 9 [9.15s]:  training loss=0.27975964546203613                                      \n",
      "epoch 10 [9.6s]: training loss=0.24638549983501434  validation ndcg@10=0.025046890068305013 [0.39s]\n",
      "epoch 11 [9.32s]:  training loss=0.2140752077102661                                      \n",
      "epoch 12 [9.45s]:  training loss=0.19462522864341736                                     \n",
      "epoch 13 [9.31s]:  training loss=0.17528656125068665                                     \n",
      "epoch 14 [9.31s]:  training loss=0.16455571353435516                                     \n",
      "epoch 15 [9.44s]: training loss=0.14793799817562103  validation ndcg@10=0.022554163531976944 [0.42s]\n",
      "epoch 16 [9.52s]:  training loss=0.13822229206562042                                     \n",
      "epoch 17 [9.44s]:  training loss=0.13005200028419495                                     \n",
      "epoch 18 [9.26s]:  training loss=0.12085157632827759                                     \n",
      "epoch 19 [9.26s]:  training loss=0.11707102507352829                                     \n",
      "epoch 20 [9.38s]: training loss=0.10728488117456436  validation ndcg@10=0.024505866427956456 [0.4s]\n",
      "epoch 21 [9.38s]:  training loss=0.10308347642421722                                     \n",
      "epoch 22 [9.46s]:  training loss=0.09750036895275116                                     \n",
      "epoch 23 [9.47s]:  training loss=0.09349200874567032                                     \n",
      "epoch 24 [9.56s]:  training loss=0.08772462606430054                                     \n",
      "epoch 25 [9.2s]: training loss=0.08596009761095047  validation ndcg@10=0.02393571738896411 [0.39s]\n",
      "epoch 26 [9.47s]:  training loss=0.08320201933383942                                     \n",
      "epoch 27 [9.38s]:  training loss=0.07830551266670227                                     \n",
      "epoch 28 [9.45s]:  training loss=0.07478784769773483                                     \n",
      "epoch 29 [9.49s]:  training loss=0.07568231970071793                                     \n",
      "epoch 30 [9.65s]: training loss=0.0730690285563469  validation ndcg@10=0.0247114096743872 [0.41s]\n",
      "epoch 31 [9.95s]:  training loss=0.07417667657136917                                     \n",
      "epoch 32 [9.76s]:  training loss=0.07004828751087189                                     \n",
      "epoch 33 [9.47s]:  training loss=0.06842930614948273                                     \n",
      "epoch 34 [9.59s]:  training loss=0.06619207561016083                                     \n",
      "epoch 35 [9.47s]: training loss=0.06288106739521027  validation ndcg@10=0.02509857797572732 [0.39s]\n",
      "epoch 36 [9.94s]:  training loss=0.06272461265325546                                     \n",
      "epoch 37 [9.54s]:  training loss=0.062470827251672745                                    \n",
      "epoch 38 [9.63s]:  training loss=0.06019575521349907                                     \n",
      "epoch 39 [9.56s]:  training loss=0.05794793739914894                                     \n",
      "epoch 40 [9.64s]: training loss=0.05882434546947479  validation ndcg@10=0.024217509066557164 [0.38s]\n",
      "epoch 41 [9.3s]:  training loss=0.05824590474367142                                      \n",
      "epoch 42 [9.44s]:  training loss=0.055264394730329514                                    \n",
      "epoch 43 [9.49s]:  training loss=0.05462464317679405                                     \n",
      "epoch 44 [9.23s]:  training loss=0.053525034338235855                                    \n",
      "epoch 45 [9.73s]: training loss=0.052261121571063995  validation ndcg@10=0.023944350052052893 [0.39s]\n",
      "epoch 46 [9.48s]:  training loss=0.05293349549174309                                     \n",
      "epoch 47 [9.5s]:  training loss=0.050790585577487946                                     \n",
      "epoch 48 [9.35s]:  training loss=0.05121802166104317                                     \n",
      "epoch 49 [9.68s]:  training loss=0.050477802753448486                                    \n",
      "epoch 50 [9.76s]: training loss=0.05216760188341141  validation ndcg@10=0.024840561246224333 [0.41s]\n",
      "epoch 51 [9.42s]:  training loss=0.05061611533164978                                     \n",
      "epoch 52 [10.93s]:  training loss=0.04880805313587189                                    \n",
      "epoch 53 [9.42s]:  training loss=0.046259805560112                                       \n",
      "epoch 54 [9.66s]:  training loss=0.04684736952185631                                     \n",
      "epoch 55 [9.46s]: training loss=0.04762426018714905  validation ndcg@10=0.024277638224417267 [0.43s]\n",
      "epoch 56 [9.33s]:  training loss=0.04642842710018158                                     \n",
      "epoch 57 [9.51s]:  training loss=0.046007271856069565                                    \n",
      "epoch 58 [9.65s]:  training loss=0.04459790140390396                                     \n",
      "epoch 59 [9.58s]:  training loss=0.045011553913354874                                    \n",
      "epoch 60 [9.43s]: training loss=0.046805888414382935  validation ndcg@10=0.02410804555167741 [0.41s]\n",
      "epoch 1 [16.2s]:  training loss=0.8387611508369446                                       \n",
      "epoch 2 [16.16s]:  training loss=0.8088653683662415                                     \n",
      "epoch 3 [16.31s]:  training loss=0.7946892380714417                                     \n",
      "epoch 4 [16.49s]:  training loss=0.7897891402244568                                     \n",
      "epoch 5 [15.9s]: training loss=0.7863547801971436  validation ndcg@10=0.0012020478756042435 [0.43s]\n",
      "epoch 6 [15.83s]:  training loss=0.7787054181098938                                     \n",
      "epoch 7 [15.88s]:  training loss=0.7765167951583862                                     \n",
      "epoch 8 [16.07s]:  training loss=0.7708120346069336                                     \n",
      "epoch 9 [15.65s]:  training loss=0.7660204172134399                                     \n",
      "epoch 10 [15.67s]: training loss=0.7606936693191528  validation ndcg@10=0.001643032283651365 [0.42s]\n",
      "epoch 11 [15.55s]:  training loss=0.7581626176834106                                    \n",
      "epoch 12 [15.2s]:  training loss=0.7536718249320984                                     \n",
      "epoch 13 [14.85s]:  training loss=0.7476852536201477                                    \n",
      "epoch 14 [15.01s]:  training loss=0.7410283088684082                                    \n",
      "epoch 15 [14.63s]: training loss=0.7365021109580994  validation ndcg@10=0.010398222504096763 [0.46s]\n",
      "epoch 16 [15.09s]:  training loss=0.723844051361084                                     \n",
      "epoch 17 [15.0s]:  training loss=0.7077678442001343                                     \n",
      "epoch 18 [14.44s]:  training loss=0.6942117214202881                                    \n",
      "epoch 19 [14.45s]:  training loss=0.6731378436088562                                    \n",
      "epoch 20 [13.94s]: training loss=0.6524905562400818  validation ndcg@10=0.017178842942932218 [0.4s]\n",
      "epoch 21 [13.76s]:  training loss=0.6476934552192688                                    \n",
      "epoch 22 [13.83s]:  training loss=0.6413407921791077                                    \n",
      "epoch 23 [14.06s]:  training loss=0.6301941275596619                                    \n",
      "epoch 24 [14.04s]:  training loss=0.6239026784896851                                    \n",
      "epoch 25 [13.9s]: training loss=0.6199663281440735  validation ndcg@10=0.019387035235840504 [0.45s]\n",
      "epoch 26 [14.13s]:  training loss=0.6135954260826111                                    \n",
      "epoch 27 [14.02s]:  training loss=0.6100409030914307                                    \n",
      "epoch 28 [13.87s]:  training loss=0.6084088087081909                                    \n",
      "epoch 29 [14.35s]:  training loss=0.6036399006843567                                    \n",
      "epoch 30 [14.52s]: training loss=0.5990707278251648  validation ndcg@10=0.020050963724951826 [0.41s]\n",
      "epoch 31 [15.67s]:  training loss=0.5923385620117188                                    \n",
      "epoch 32 [13.81s]:  training loss=0.5894989371299744                                    \n",
      "epoch 33 [13.78s]:  training loss=0.5830259323120117                                    \n",
      "epoch 34 [14.09s]:  training loss=0.5801960229873657                                    \n",
      "epoch 35 [13.89s]: training loss=0.5779311656951904  validation ndcg@10=0.02014905347373078 [0.45s]\n",
      "epoch 36 [13.7s]:  training loss=0.5754637718200684                                     \n",
      "epoch 37 [13.86s]:  training loss=0.5712876319885254                                    \n",
      "epoch 38 [13.99s]:  training loss=0.5699275732040405                                    \n",
      "epoch 39 [13.95s]:  training loss=0.5672083497047424                                    \n",
      "epoch 40 [13.8s]: training loss=0.5598488450050354  validation ndcg@10=0.020230576890901974 [0.4s]\n",
      "epoch 41 [14.01s]:  training loss=0.5587387084960938                                    \n",
      "epoch 42 [14.09s]:  training loss=0.5488556027412415                                    \n",
      "epoch 43 [13.81s]:  training loss=0.5495517253875732                                    \n",
      "epoch 44 [14.0s]:  training loss=0.5464898943901062                                     \n",
      "epoch 45 [13.92s]: training loss=0.5442360043525696  validation ndcg@10=0.020031006578770327 [0.4s]\n",
      "epoch 46 [14.1s]:  training loss=0.5364546179771423                                     \n",
      "epoch 47 [13.96s]:  training loss=0.5321624875068665                                    \n",
      "epoch 48 [14.19s]:  training loss=0.5316563248634338                                    \n",
      "epoch 49 [14.14s]:  training loss=0.5246344804763794                                    \n",
      "epoch 50 [14.06s]: training loss=0.5224132537841797  validation ndcg@10=0.019995787116346957 [0.45s]\n",
      "epoch 51 [13.9s]:  training loss=0.5168114900588989                                     \n",
      "epoch 52 [13.88s]:  training loss=0.5148624181747437                                    \n",
      "epoch 53 [14.39s]:  training loss=0.5115869045257568                                    \n",
      "epoch 54 [14.45s]:  training loss=0.5104313492774963                                    \n",
      "epoch 55 [14.21s]: training loss=0.5044153332710266  validation ndcg@10=0.020092108612420626 [0.41s]\n",
      "epoch 56 [14.6s]:  training loss=0.5013946890830994                                     \n",
      "epoch 57 [14.42s]:  training loss=0.4993654191493988                                    \n",
      "epoch 58 [14.63s]:  training loss=0.49803343415260315                                   \n",
      "epoch 59 [14.38s]:  training loss=0.4921274185180664                                    \n",
      "epoch 60 [14.34s]: training loss=0.48697152733802795  validation ndcg@10=0.02016414979945969 [0.42s]\n",
      "epoch 61 [14.07s]:  training loss=0.4826330542564392                                    \n",
      "epoch 62 [14.43s]:  training loss=0.48521676659584045                                   \n",
      "epoch 63 [14.17s]:  training loss=0.47945377230644226                                   \n",
      "epoch 64 [14.42s]:  training loss=0.47429347038269043                                   \n",
      "epoch 65 [14.37s]: training loss=0.4711601436138153  validation ndcg@10=0.020223819434212913 [0.43s]\n",
      "epoch 1 [9.21s]:  training loss=0.862392783164978                                       \n",
      "epoch 2 [9.21s]:  training loss=0.8368894457817078                                      \n",
      "epoch 3 [9.3s]:  training loss=0.8239585757255554                                       \n",
      "epoch 4 [9.29s]:  training loss=0.8157377243041992                                      \n",
      "epoch 5 [9.71s]: training loss=0.8077947497367859  validation ndcg@10=0.0007660955138664601 [0.37s]\n",
      "epoch 6 [11.69s]:  training loss=0.8034549355506897                                     \n",
      "epoch 7 [9.07s]:  training loss=0.7992156147956848                                      \n",
      "epoch 8 [9.09s]:  training loss=0.7953895330429077                                      \n",
      "epoch 9 [9.07s]:  training loss=0.7903262376785278                                      \n",
      "epoch 10 [9.0s]: training loss=0.7875335812568665  validation ndcg@10=0.0007222553815387716 [0.38s]\n",
      "epoch 11 [9.17s]:  training loss=0.7835666537284851                                     \n",
      "epoch 12 [9.26s]:  training loss=0.782414436340332                                      \n",
      "epoch 13 [9.0s]:  training loss=0.7769823670387268                                      \n",
      "epoch 14 [9.06s]:  training loss=0.7749096155166626                                     \n",
      "epoch 15 [9.27s]: training loss=0.7763614654541016  validation ndcg@10=0.00037625546369291114 [0.37s]\n",
      "epoch 16 [8.83s]:  training loss=0.7727695107460022                                     \n",
      "epoch 17 [9.13s]:  training loss=0.7657411098480225                                     \n",
      "epoch 18 [9.22s]:  training loss=0.7674092054367065                                     \n",
      "epoch 19 [9.09s]:  training loss=0.7632421255111694                                     \n",
      "epoch 20 [9.03s]: training loss=0.7617785930633545  validation ndcg@10=0.0010722985155563558 [0.39s]\n",
      "epoch 21 [8.97s]:  training loss=0.7605580687522888                                     \n",
      "epoch 22 [9.23s]:  training loss=0.7573118209838867                                     \n",
      "epoch 23 [9.12s]:  training loss=0.757391631603241                                      \n",
      "epoch 24 [9.05s]:  training loss=0.7535760402679443                                     \n",
      "epoch 25 [9.24s]: training loss=0.7494888305664062  validation ndcg@10=0.0016628976232759447 [0.36s]\n",
      "epoch 26 [9.05s]:  training loss=0.7505656480789185                                     \n",
      "epoch 27 [9.08s]:  training loss=0.7461358308792114                                     \n",
      "epoch 28 [9.1s]:  training loss=0.7439217567443848                                      \n",
      "epoch 29 [9.06s]:  training loss=0.7418920993804932                                     \n",
      "epoch 30 [9.18s]: training loss=0.7397847771644592  validation ndcg@10=0.0035852802867289424 [0.35s]\n",
      "epoch 31 [8.97s]:  training loss=0.7372830510139465                                     \n",
      "epoch 32 [9.02s]:  training loss=0.7332913279533386                                     \n",
      "epoch 33 [8.85s]:  training loss=0.7301316857337952                                     \n",
      "epoch 34 [9.11s]:  training loss=0.7314319610595703                                     \n",
      "epoch 35 [9.19s]: training loss=0.7273796200752258  validation ndcg@10=0.0018066783491428 [0.36s]\n",
      "epoch 36 [8.94s]:  training loss=0.7270405292510986                                     \n",
      "epoch 37 [9.01s]:  training loss=0.7248286604881287                                     \n",
      "epoch 38 [9.01s]:  training loss=0.7198956608772278                                     \n",
      "epoch 39 [9.13s]:  training loss=0.7175787687301636                                     \n",
      "epoch 40 [8.88s]: training loss=0.7180768251419067  validation ndcg@10=0.004458917280420009 [0.35s]\n",
      "epoch 41 [8.91s]:  training loss=0.7124106884002686                                     \n",
      "epoch 42 [9.24s]:  training loss=0.7111548185348511                                     \n",
      "epoch 43 [9.24s]:  training loss=0.7042376399040222                                     \n",
      "epoch 44 [9.08s]:  training loss=0.7025153636932373                                     \n",
      "epoch 45 [9.29s]: training loss=0.7016616463661194  validation ndcg@10=0.004869796875228405 [0.38s]\n",
      "epoch 46 [9.03s]:  training loss=0.6999608874320984                                     \n",
      "epoch 47 [9.43s]:  training loss=0.6963041424751282                                     \n",
      "epoch 48 [9.06s]:  training loss=0.6972235441207886                                     \n",
      "epoch 49 [8.98s]:  training loss=0.693653404712677                                      \n",
      "epoch 50 [9.33s]: training loss=0.6889815330505371  validation ndcg@10=0.008653178198118809 [0.36s]\n",
      "epoch 51 [9.08s]:  training loss=0.6836711168289185                                     \n",
      "epoch 52 [9.2s]:  training loss=0.6843786239624023                                      \n",
      "epoch 53 [8.86s]:  training loss=0.6790204644203186                                     \n",
      "epoch 54 [9.34s]:  training loss=0.6730161905288696                                     \n",
      "epoch 55 [9.15s]: training loss=0.6705621480941772  validation ndcg@10=0.014199196452180051 [0.36s]\n",
      "epoch 56 [9.41s]:  training loss=0.6645385026931763                                     \n",
      "epoch 57 [9.11s]:  training loss=0.6577728986740112                                     \n",
      "epoch 58 [8.75s]:  training loss=0.6521412134170532                                     \n",
      "epoch 59 [9.32s]:  training loss=0.6487894654273987                                     \n",
      "epoch 60 [9.03s]: training loss=0.6407626867294312  validation ndcg@10=0.015659656464434252 [0.4s]\n",
      "epoch 61 [8.8s]:  training loss=0.6334530115127563                                      \n",
      "epoch 62 [8.93s]:  training loss=0.6263161897659302                                     \n",
      "epoch 63 [9.21s]:  training loss=0.6251174211502075                                     \n",
      "epoch 64 [12.04s]:  training loss=0.619176983833313                                     \n",
      "epoch 65 [9.28s]: training loss=0.6131618618965149  validation ndcg@10=0.016464981062933908 [0.38s]\n",
      "epoch 66 [9.09s]:  training loss=0.609304666519165                                      \n",
      "epoch 67 [9.08s]:  training loss=0.603809118270874                                      \n",
      "epoch 68 [8.91s]:  training loss=0.6011186838150024                                     \n",
      "epoch 69 [8.9s]:  training loss=0.5970851182937622                                      \n",
      "epoch 70 [9.0s]: training loss=0.5933523774147034  validation ndcg@10=0.016824442426627235 [0.35s]\n",
      "epoch 71 [9.13s]:  training loss=0.590813398361206                                      \n",
      "epoch 72 [9.0s]:  training loss=0.5902742743492126                                      \n",
      "epoch 73 [8.82s]:  training loss=0.5849343538284302                                     \n",
      "epoch 74 [8.79s]:  training loss=0.5843505859375                                        \n",
      "epoch 75 [9.08s]: training loss=0.5834653973579407  validation ndcg@10=0.01760963796724087 [0.4s]\n",
      "epoch 76 [9.3s]:  training loss=0.5799471735954285                                      \n",
      "epoch 77 [8.93s]:  training loss=0.5771055221557617                                     \n",
      "epoch 78 [9.0s]:  training loss=0.5744538307189941                                      \n",
      "epoch 79 [9.14s]:  training loss=0.5726017951965332                                     \n",
      "epoch 80 [9.03s]: training loss=0.5702892541885376  validation ndcg@10=0.017387165505514324 [0.37s]\n",
      "epoch 81 [9.04s]:  training loss=0.5678961277008057                                     \n",
      "epoch 82 [9.1s]:  training loss=0.5640019178390503                                      \n",
      "epoch 83 [9.07s]:  training loss=0.5648177862167358                                     \n",
      "epoch 84 [9.26s]:  training loss=0.5632904767990112                                     \n",
      "epoch 85 [9.19s]: training loss=0.5575228333473206  validation ndcg@10=0.018434589925991327 [0.38s]\n",
      "epoch 86 [9.42s]:  training loss=0.5567992925643921                                     \n",
      "epoch 87 [9.24s]:  training loss=0.5571689605712891                                     \n",
      "epoch 88 [8.83s]:  training loss=0.5541432499885559                                     \n",
      "epoch 89 [9.05s]:  training loss=0.5553596019744873                                     \n",
      "epoch 90 [8.98s]: training loss=0.5484143495559692  validation ndcg@10=0.01859780109584309 [0.42s]\n",
      "epoch 91 [9.52s]:  training loss=0.5486280918121338                                     \n",
      "epoch 92 [9.63s]:  training loss=0.5492990612983704                                     \n",
      "epoch 93 [9.34s]:  training loss=0.5469425916671753                                     \n",
      "epoch 94 [9.14s]:  training loss=0.5461348295211792                                     \n",
      "epoch 95 [9.18s]: training loss=0.5419788360595703  validation ndcg@10=0.018320247440049484 [0.37s]\n",
      "epoch 96 [9.27s]:  training loss=0.5412830114364624                                     \n",
      "epoch 97 [9.27s]:  training loss=0.5412184000015259                                     \n",
      "epoch 98 [8.82s]:  training loss=0.5360119342803955                                     \n",
      "epoch 99 [9.33s]:  training loss=0.5374436378479004                                     \n",
      "epoch 100 [9.24s]: training loss=0.5328830480575562  validation ndcg@10=0.018549699964907997 [0.4s]\n",
      "epoch 101 [9.14s]:  training loss=0.5338987112045288                                    \n",
      "epoch 102 [9.09s]:  training loss=0.5354306101799011                                    \n",
      "epoch 103 [9.21s]:  training loss=0.5308840870857239                                    \n",
      "epoch 104 [9.13s]:  training loss=0.5303792357444763                                    \n",
      "epoch 105 [9.18s]: training loss=0.5328083634376526  validation ndcg@10=0.018881370945472712 [0.41s]\n",
      "epoch 106 [9.29s]:  training loss=0.5245372653007507                                    \n",
      "epoch 107 [9.19s]:  training loss=0.5261425375938416                                    \n",
      "epoch 108 [9.08s]:  training loss=0.5198882222175598                                    \n",
      "epoch 109 [9.07s]:  training loss=0.5219247341156006                                    \n",
      "epoch 110 [9.18s]: training loss=0.5208704471588135  validation ndcg@10=0.01845884373139025 [0.37s]\n",
      "epoch 111 [9.16s]:  training loss=0.517257809638977                                     \n",
      "epoch 112 [9.13s]:  training loss=0.5165361166000366                                    \n",
      "epoch 113 [8.99s]:  training loss=0.5166886448860168                                    \n",
      "epoch 114 [9.12s]:  training loss=0.5170282125473022                                    \n",
      "epoch 115 [9.33s]: training loss=0.5130475163459778  validation ndcg@10=0.01862785223561811 [0.39s]\n",
      "epoch 116 [9.17s]:  training loss=0.5122817158699036                                    \n",
      "epoch 117 [9.11s]:  training loss=0.512141764163971                                     \n",
      "epoch 118 [9.2s]:  training loss=0.5093770623207092                                     \n",
      "epoch 119 [9.24s]:  training loss=0.5092594623565674                                    \n",
      "epoch 120 [9.25s]: training loss=0.5091078877449036  validation ndcg@10=0.018392109970626926 [0.36s]\n",
      "epoch 121 [9.28s]:  training loss=0.5079255700111389                                    \n",
      "epoch 122 [11.27s]:  training loss=0.5085762739181519                                   \n",
      "epoch 123 [9.37s]:  training loss=0.505175769329071                                     \n",
      "epoch 124 [9.46s]:  training loss=0.5024843811988831                                    \n",
      "epoch 125 [9.35s]: training loss=0.49955832958221436  validation ndcg@10=0.018160429358273354 [0.36s]\n",
      "epoch 126 [9.49s]:  training loss=0.49771416187286377                                   \n",
      "epoch 127 [9.43s]:  training loss=0.49706828594207764                                   \n",
      "epoch 128 [9.11s]:  training loss=0.5007184743881226                                    \n",
      "epoch 129 [9.12s]:  training loss=0.49446988105773926                                   \n",
      "epoch 130 [9.23s]: training loss=0.49825987219810486  validation ndcg@10=0.01848231921305464 [0.36s]\n",
      "epoch 1 [8.3s]:  training loss=0.800326943397522                                        \n",
      "epoch 2 [8.24s]:  training loss=0.7696630358695984                                      \n",
      "epoch 3 [8.28s]:  training loss=0.7289458513259888                                      \n",
      "epoch 4 [8.11s]:  training loss=0.6490743160247803                                      \n",
      "epoch 5 [8.52s]: training loss=0.6117103695869446  validation ndcg@10=0.019647970287694046 [0.34s]\n",
      "epoch 6 [8.38s]:  training loss=0.5844373106956482                                      \n",
      "epoch 7 [8.35s]:  training loss=0.5589228272438049                                      \n",
      "epoch 8 [8.47s]:  training loss=0.5339831709861755                                      \n",
      "epoch 9 [8.26s]:  training loss=0.5147444009780884                                      \n",
      "epoch 10 [8.51s]: training loss=0.49616295099258423  validation ndcg@10=0.020223160715247636 [0.37s]\n",
      "epoch 11 [8.53s]:  training loss=0.47381097078323364                                    \n",
      "epoch 12 [8.33s]:  training loss=0.4541083574295044                                     \n",
      "epoch 13 [8.61s]:  training loss=0.4305321276187897                                     \n",
      "epoch 14 [8.7s]:  training loss=0.4151976704597473                                      \n",
      "epoch 15 [8.64s]: training loss=0.3950977921485901  validation ndcg@10=0.021658446964003927 [0.36s]\n",
      "epoch 16 [8.41s]:  training loss=0.3822079300880432                                     \n",
      "epoch 17 [8.78s]:  training loss=0.3709353506565094                                     \n",
      "epoch 18 [8.55s]:  training loss=0.3545781672000885                                     \n",
      "epoch 19 [8.72s]:  training loss=0.34445393085479736                                    \n",
      "epoch 20 [8.42s]: training loss=0.33000749349594116  validation ndcg@10=0.022260208464551415 [0.34s]\n",
      "epoch 21 [8.71s]:  training loss=0.3186216652393341                                     \n",
      "epoch 22 [8.7s]:  training loss=0.3081303834915161                                      \n",
      "epoch 23 [8.67s]:  training loss=0.2932531237602234                                     \n",
      "epoch 24 [8.71s]:  training loss=0.28191468119621277                                    \n",
      "epoch 25 [8.68s]: training loss=0.27316397428512573  validation ndcg@10=0.022775623548671835 [0.35s]\n",
      "epoch 26 [8.31s]:  training loss=0.26679879426956177                                    \n",
      "epoch 27 [8.29s]:  training loss=0.2573300898075104                                     \n",
      "epoch 28 [8.4s]:  training loss=0.24496804177761078                                     \n",
      "epoch 29 [8.36s]:  training loss=0.23516497015953064                                    \n",
      "epoch 30 [8.3s]: training loss=0.23092354834079742  validation ndcg@10=0.022369513955252475 [0.36s]\n",
      "epoch 31 [8.5s]:  training loss=0.21917618811130524                                     \n",
      "epoch 32 [8.76s]:  training loss=0.21226531267166138                                    \n",
      "epoch 33 [8.82s]:  training loss=0.20606794953346252                                    \n",
      "epoch 34 [8.82s]:  training loss=0.19827155768871307                                    \n",
      "epoch 35 [8.44s]: training loss=0.19168008863925934  validation ndcg@10=0.02387917822375685 [0.34s]\n",
      "epoch 36 [8.56s]:  training loss=0.18293100595474243                                    \n",
      "epoch 37 [8.24s]:  training loss=0.17963118851184845                                    \n",
      "epoch 38 [8.59s]:  training loss=0.17023304104804993                                    \n",
      "epoch 39 [8.4s]:  training loss=0.16704383492469788                                     \n",
      "epoch 40 [8.55s]: training loss=0.1640135496854782  validation ndcg@10=0.024179034185506258 [0.34s]\n",
      "epoch 41 [8.69s]:  training loss=0.1594374179840088                                     \n",
      "epoch 42 [8.51s]:  training loss=0.15279699862003326                                    \n",
      "epoch 43 [8.49s]:  training loss=0.14973504841327667                                    \n",
      "epoch 44 [8.47s]:  training loss=0.1440681368112564                                     \n",
      "epoch 45 [8.65s]: training loss=0.14239616692066193  validation ndcg@10=0.023852659785071523 [0.36s]\n",
      "epoch 46 [8.39s]:  training loss=0.13474349677562714                                    \n",
      "epoch 47 [8.59s]:  training loss=0.1343020498752594                                     \n",
      "epoch 48 [8.43s]:  training loss=0.13159514963626862                                    \n",
      "epoch 49 [8.6s]:  training loss=0.12541475892066956                                     \n",
      "epoch 50 [8.54s]: training loss=0.12556476891040802  validation ndcg@10=0.024491546746872508 [0.4s]\n",
      "epoch 51 [8.92s]:  training loss=0.12198830395936966                                    \n",
      "epoch 52 [8.53s]:  training loss=0.11536306142807007                                    \n",
      "epoch 53 [8.39s]:  training loss=0.11796911060810089                                    \n",
      "epoch 54 [10.34s]:  training loss=0.1139746680855751                                    \n",
      "epoch 55 [8.92s]: training loss=0.1112632229924202  validation ndcg@10=0.02415788488080504 [0.34s]\n",
      "epoch 56 [9.03s]:  training loss=0.10736492276191711                                    \n",
      "epoch 57 [8.78s]:  training loss=0.10790517926216125                                    \n",
      "epoch 58 [8.46s]:  training loss=0.10665702074766159                                    \n",
      "epoch 59 [8.28s]:  training loss=0.10211002081632614                                    \n",
      "epoch 60 [8.47s]: training loss=0.10025510936975479  validation ndcg@10=0.024460660569597184 [0.35s]\n",
      "epoch 61 [8.4s]:  training loss=0.09858180582523346                                     \n",
      "epoch 62 [8.46s]:  training loss=0.09687389433383942                                    \n",
      "epoch 63 [8.54s]:  training loss=0.09343421459197998                                    \n",
      "epoch 64 [8.67s]:  training loss=0.09244964271783829                                    \n",
      "epoch 65 [8.49s]: training loss=0.09070548415184021  validation ndcg@10=0.025104593316809587 [0.39s]\n",
      "epoch 66 [8.34s]:  training loss=0.08804099261760712                                    \n",
      "epoch 67 [8.36s]:  training loss=0.08841617405414581                                    \n",
      "epoch 68 [8.35s]:  training loss=0.08621113747358322                                    \n",
      "epoch 69 [8.46s]:  training loss=0.08555848896503448                                    \n",
      "epoch 70 [8.29s]: training loss=0.08158303797245026  validation ndcg@10=0.02581776201260731 [0.37s]\n",
      "epoch 71 [8.64s]:  training loss=0.08198206126689911                                    \n",
      "epoch 72 [8.44s]:  training loss=0.07969235628843307                                    \n",
      "epoch 73 [8.46s]:  training loss=0.079994335770607                                      \n",
      "epoch 74 [8.54s]:  training loss=0.07592292129993439                                    \n",
      "epoch 75 [8.64s]: training loss=0.07671160250902176  validation ndcg@10=0.024942185165915193 [0.34s]\n",
      "epoch 76 [8.65s]:  training loss=0.0764370933175087                                     \n",
      "epoch 77 [8.4s]:  training loss=0.07410246133804321                                     \n",
      "epoch 78 [8.58s]:  training loss=0.07357736676931381                                    \n",
      "epoch 79 [8.38s]:  training loss=0.07406286895275116                                    \n",
      "epoch 80 [8.52s]: training loss=0.07032544165849686  validation ndcg@10=0.02516842094547627 [0.36s]\n",
      "epoch 81 [8.36s]:  training loss=0.06996607035398483                                    \n",
      "epoch 82 [8.62s]:  training loss=0.07054296880960464                                    \n",
      "epoch 83 [8.44s]:  training loss=0.06611979752779007                                    \n",
      "epoch 84 [8.39s]:  training loss=0.06678298860788345                                    \n",
      "epoch 85 [8.64s]: training loss=0.06533155590295792  validation ndcg@10=0.02426485420754551 [0.36s]\n",
      "epoch 86 [8.56s]:  training loss=0.06498264521360397                                    \n",
      "epoch 87 [8.82s]:  training loss=0.06421419978141785                                    \n",
      "epoch 88 [8.34s]:  training loss=0.06412170082330704                                    \n",
      "epoch 89 [8.32s]:  training loss=0.06237397342920303                                    \n",
      "epoch 90 [8.77s]: training loss=0.06084326282143593  validation ndcg@10=0.02489866712863035 [0.38s]\n",
      "epoch 91 [8.38s]:  training loss=0.06253061443567276                                    \n",
      "epoch 92 [8.72s]:  training loss=0.05906284227967262                                    \n",
      "epoch 93 [8.45s]:  training loss=0.06020031496882439                                    \n",
      "epoch 94 [8.43s]:  training loss=0.057859670370817184                                   \n",
      "epoch 95 [8.35s]: training loss=0.05883418768644333  validation ndcg@10=0.025246297036108693 [0.36s]\n",
      "epoch 1 [13.39s]:  training loss=0.5788965821266174                                     \n",
      "epoch 2 [14.07s]:  training loss=0.7604978680610657                                     \n",
      "epoch 3 [13.94s]:  training loss=0.8925293684005737                                     \n",
      "epoch 4 [14.13s]:  training loss=1.0280427932739258                                     \n",
      "epoch 5 [14.45s]: training loss=1.0963307619094849  validation ndcg@10=0.012652943862338532 [0.5s]\n",
      "epoch 6 [14.38s]:  training loss=1.1078832149505615                                     \n",
      "epoch 7 [14.49s]:  training loss=1.1267513036727905                                     \n",
      "epoch 8 [14.69s]:  training loss=1.2086101770401                                        \n",
      "epoch 9 [14.49s]:  training loss=1.220252513885498                                      \n",
      "epoch 10 [14.4s]: training loss=1.226934790611267  validation ndcg@10=0.01703715871308681 [0.52s]\n",
      "epoch 11 [14.7s]:  training loss=1.3266029357910156                                     \n",
      "epoch 12 [14.65s]:  training loss=1.296837568283081                                     \n",
      "epoch 13 [15.82s]:  training loss=1.2813979387283325                                    \n",
      "epoch 14 [15.09s]:  training loss=1.334234595298767                                     \n",
      "epoch 15 [14.5s]: training loss=1.4015324115753174  validation ndcg@10=0.01520342388371074 [0.53s]\n",
      "epoch 16 [14.55s]:  training loss=1.4012722969055176                                    \n",
      "epoch 17 [14.45s]:  training loss=1.5010887384414673                                    \n",
      "epoch 18 [14.15s]:  training loss=1.466554880142212                                     \n",
      "epoch 19 [14.04s]:  training loss=1.4025495052337646                                    \n",
      "epoch 20 [14.02s]: training loss=1.4769290685653687  validation ndcg@10=0.016960444709594368 [0.48s]\n",
      "epoch 21 [14.22s]:  training loss=1.4903684854507446                                    \n",
      "epoch 22 [14.14s]:  training loss=1.5818169116973877                                    \n",
      "epoch 23 [13.72s]:  training loss=1.5836856365203857                                    \n",
      "epoch 24 [14.23s]:  training loss=1.5459684133529663                                    \n",
      "epoch 25 [14.52s]: training loss=1.6393184661865234  validation ndcg@10=0.01690910388928396 [0.48s]\n",
      "epoch 26 [14.4s]:  training loss=1.6737867593765259                                     \n",
      "epoch 27 [14.01s]:  training loss=1.581949234008789                                     \n",
      "epoch 28 [14.32s]:  training loss=1.6340117454528809                                    \n",
      "epoch 29 [14.62s]:  training loss=1.5576242208480835                                    \n",
      "epoch 30 [14.28s]: training loss=1.6194725036621094  validation ndcg@10=0.016028967717051285 [0.51s]\n",
      "epoch 31 [13.88s]:  training loss=1.6249240636825562                                    \n",
      "epoch 32 [14.43s]:  training loss=1.7321947813034058                                    \n",
      "epoch 33 [14.38s]:  training loss=1.563533902168274                                     \n",
      "epoch 34 [14.45s]:  training loss=1.6349681615829468                                    \n",
      "epoch 35 [14.18s]: training loss=1.7899916172027588  validation ndcg@10=0.0184988874725384 [0.5s]\n",
      "epoch 36 [14.34s]:  training loss=1.7156471014022827                                    \n",
      "epoch 37 [14.53s]:  training loss=1.7656859159469604                                    \n",
      "epoch 38 [14.41s]:  training loss=1.7187137603759766                                    \n",
      "epoch 39 [14.49s]:  training loss=1.7633956670761108                                    \n",
      "epoch 40 [14.03s]: training loss=1.8304245471954346  validation ndcg@10=0.016598867007273626 [0.49s]\n",
      "epoch 41 [14.45s]:  training loss=1.880059003829956                                     \n",
      "epoch 42 [14.28s]:  training loss=1.7914763689041138                                    \n",
      "epoch 43 [14.58s]:  training loss=1.6541409492492676                                    \n",
      "epoch 44 [14.39s]:  training loss=1.8603641986846924                                    \n",
      "epoch 45 [14.27s]: training loss=1.8864470720291138  validation ndcg@10=0.016467814079917476 [0.55s]\n",
      "epoch 46 [14.09s]:  training loss=1.8068063259124756                                    \n",
      "epoch 47 [14.46s]:  training loss=1.8095451593399048                                    \n",
      "epoch 48 [14.39s]:  training loss=1.8190562725067139                                    \n",
      "epoch 49 [14.34s]:  training loss=1.8810458183288574                                    \n",
      "epoch 50 [16.54s]: training loss=1.903288722038269  validation ndcg@10=0.017555516614665388 [0.48s]\n",
      "epoch 51 [14.73s]:  training loss=1.9860590696334839                                    \n",
      "epoch 52 [15.15s]:  training loss=1.745693564414978                                     \n",
      "epoch 53 [14.3s]:  training loss=1.8840678930282593                                     \n",
      "epoch 54 [14.29s]:  training loss=2.0797111988067627                                    \n",
      "epoch 55 [14.11s]: training loss=2.0122878551483154  validation ndcg@10=0.015411264863017698 [0.51s]\n",
      "epoch 56 [14.75s]:  training loss=1.901457667350769                                     \n",
      "epoch 57 [14.4s]:  training loss=2.1221861839294434                                     \n",
      "epoch 58 [14.3s]:  training loss=1.928759217262268                                      \n",
      "epoch 59 [14.76s]:  training loss=1.893028974533081                                     \n",
      "epoch 60 [14.47s]: training loss=1.9477301836013794  validation ndcg@10=0.014799306932805378 [0.49s]\n",
      "epoch 1 [37.61s]:  training loss=0.820215106010437                                      \n",
      "epoch 2 [38.11s]:  training loss=0.7894505858421326                                     \n",
      "epoch 3 [38.12s]:  training loss=0.775650143623352                                      \n",
      "epoch 4 [38.14s]:  training loss=0.7627490758895874                                     \n",
      "epoch 5 [38.2s]: training loss=0.7473328709602356  validation ndcg@10=0.012480693815720072 [0.83s]\n",
      "epoch 6 [38.0s]:  training loss=0.719182550907135                                       \n",
      "epoch 7 [38.39s]:  training loss=0.6761556267738342                                     \n",
      "epoch 8 [38.18s]:  training loss=0.6453799605369568                                     \n",
      "epoch 9 [38.27s]:  training loss=0.6253258585929871                                     \n",
      "epoch 10 [39.34s]: training loss=0.6146288514137268  validation ndcg@10=0.021557973524524897 [1.84s]\n",
      "epoch 11 [38.16s]:  training loss=0.5985272526741028                                    \n",
      "epoch 12 [38.5s]:  training loss=0.5910752415657043                                     \n",
      "epoch 13 [37.35s]:  training loss=0.5779227614402771                                    \n",
      "epoch 14 [38.61s]:  training loss=0.5699934959411621                                    \n",
      "epoch 15 [38.05s]: training loss=0.5611745715141296  validation ndcg@10=0.021540276833528677 [0.78s]\n",
      "epoch 16 [38.92s]:  training loss=0.5549287796020508                                    \n",
      "epoch 17 [37.88s]:  training loss=0.546608030796051                                     \n",
      "epoch 18 [36.87s]:  training loss=0.5379659533500671                                    \n",
      "epoch 19 [38.13s]:  training loss=0.5283564925193787                                    \n",
      "epoch 20 [37.22s]: training loss=0.5206225514411926  validation ndcg@10=0.021150608159289968 [0.75s]\n",
      "epoch 21 [38.01s]:  training loss=0.5125347971916199                                    \n",
      "epoch 22 [37.83s]:  training loss=0.5023180842399597                                    \n",
      "epoch 23 [37.79s]:  training loss=0.4987570345401764                                    \n",
      "epoch 24 [38.18s]:  training loss=0.4897620975971222                                    \n",
      "epoch 25 [39.68s]: training loss=0.48169445991516113  validation ndcg@10=0.01913985439928605 [0.79s]\n",
      "epoch 26 [37.42s]:  training loss=0.4740891754627228                                    \n",
      "epoch 27 [37.8s]:  training loss=0.462963342666626                                      \n",
      "epoch 28 [37.74s]:  training loss=0.4562811851501465                                    \n",
      "epoch 29 [37.72s]:  training loss=0.4460875988006592                                    \n",
      "epoch 30 [38.29s]: training loss=0.43819141387939453  validation ndcg@10=0.019766136272706524 [0.78s]\n",
      "epoch 31 [38.25s]:  training loss=0.4277912378311157                                    \n",
      "epoch 32 [38.11s]:  training loss=0.4215596616268158                                    \n",
      "epoch 33 [37.81s]:  training loss=0.4178052544593811                                    \n",
      "epoch 34 [37.84s]:  training loss=0.4078967869281769                                    \n",
      "epoch 35 [38.04s]: training loss=0.39887309074401855  validation ndcg@10=0.02121847075008679 [0.78s]\n",
      "epoch 1 [17.46s]:  training loss=0.7709844708442688                                      \n",
      "epoch 2 [16.75s]:  training loss=0.6363576054573059                                      \n",
      "epoch 3 [15.44s]:  training loss=0.5479298233985901                                      \n",
      "epoch 4 [14.66s]:  training loss=0.4967431128025055                                      \n",
      "epoch 5 [14.68s]: training loss=0.4637013375759125  validation ndcg@10=0.008230276940278995 [0.5s]\n",
      "epoch 6 [14.31s]:  training loss=0.42825615406036377                                     \n",
      "epoch 7 [14.33s]:  training loss=0.40136876702308655                                     \n",
      "epoch 8 [14.16s]:  training loss=0.37191858887672424                                     \n",
      "epoch 9 [14.23s]:  training loss=0.3454064130783081                                      \n",
      "epoch 10 [15.67s]: training loss=0.3205120265483856  validation ndcg@10=0.006589189150825876 [0.49s]\n",
      "epoch 11 [13.96s]:  training loss=0.2944317162036896                                     \n",
      "epoch 12 [13.65s]:  training loss=0.2677764296531677                                     \n",
      "epoch 13 [12.79s]:  training loss=0.2582617700099945                                     \n",
      "epoch 14 [12.93s]:  training loss=0.23748841881752014                                    \n",
      "epoch 15 [12.86s]: training loss=0.22392138838768005  validation ndcg@10=0.00685909537810722 [0.46s]\n",
      "epoch 16 [12.85s]:  training loss=0.21234889328479767                                    \n",
      "epoch 17 [12.83s]:  training loss=0.20142917335033417                                    \n",
      "epoch 18 [12.92s]:  training loss=0.18914836645126343                                    \n",
      "epoch 19 [12.93s]:  training loss=0.17995023727416992                                    \n",
      "epoch 20 [12.91s]: training loss=0.17174653708934784  validation ndcg@10=0.008184352914854033 [0.47s]\n",
      "epoch 21 [12.93s]:  training loss=0.16657096147537231                                    \n",
      "epoch 22 [13.0s]:  training loss=0.15902096033096313                                     \n",
      "epoch 23 [13.01s]:  training loss=0.15247851610183716                                    \n",
      "epoch 24 [12.96s]:  training loss=0.14742660522460938                                    \n",
      "epoch 25 [12.92s]: training loss=0.13985447585582733  validation ndcg@10=0.008112334575445959 [0.45s]\n",
      "epoch 26 [12.87s]:  training loss=0.1340285837650299                                     \n",
      "epoch 27 [12.93s]:  training loss=0.133620947599411                                      \n",
      "epoch 28 [12.93s]:  training loss=0.1267513483762741                                     \n",
      "epoch 29 [13.0s]:  training loss=0.12001366168260574                                     \n",
      "epoch 30 [13.03s]: training loss=0.11627623438835144  validation ndcg@10=0.008787074835624922 [0.5s]\n",
      "epoch 31 [12.87s]:  training loss=0.11668071895837784                                    \n",
      "epoch 32 [12.88s]:  training loss=0.11046437919139862                                    \n",
      "epoch 33 [13.03s]:  training loss=0.1055898442864418                                     \n",
      "epoch 34 [12.93s]:  training loss=0.10460957884788513                                    \n",
      "epoch 35 [13.07s]: training loss=0.1029936671257019  validation ndcg@10=0.010630421550305079 [0.45s]\n",
      "epoch 36 [12.94s]:  training loss=0.10002090036869049                                    \n",
      "epoch 37 [13.04s]:  training loss=0.09641081839799881                                    \n",
      "epoch 38 [13.08s]:  training loss=0.09647592902183533                                    \n",
      "epoch 39 [13.11s]:  training loss=0.09336448460817337                                    \n",
      "epoch 40 [13.19s]: training loss=0.09102524816989899  validation ndcg@10=0.009376482887986379 [0.45s]\n",
      "epoch 41 [13.15s]:  training loss=0.08763118833303452                                    \n",
      "epoch 42 [13.22s]:  training loss=0.08205214142799377                                    \n",
      "epoch 43 [13.08s]:  training loss=0.08422048389911652                                    \n",
      "epoch 44 [13.09s]:  training loss=0.08329501748085022                                    \n",
      "epoch 45 [13.08s]: training loss=0.0815611481666565  validation ndcg@10=0.010667124097424355 [0.48s]\n",
      "epoch 46 [13.02s]:  training loss=0.08080338686704636                                    \n",
      "epoch 47 [13.2s]:  training loss=0.07780062407255173                                     \n",
      "epoch 48 [12.98s]:  training loss=0.07588399946689606                                    \n",
      "epoch 49 [13.05s]:  training loss=0.07741368561983109                                    \n",
      "epoch 50 [14.82s]: training loss=0.07495898753404617  validation ndcg@10=0.011392490150792278 [0.57s]\n",
      "epoch 51 [12.99s]:  training loss=0.07506205886602402                                    \n",
      "epoch 52 [13.38s]:  training loss=0.07349018007516861                                    \n",
      "epoch 53 [13.7s]:  training loss=0.07021801173686981                                     \n",
      "epoch 54 [13.01s]:  training loss=0.06896400451660156                                    \n",
      "epoch 55 [12.91s]: training loss=0.06934178620576859  validation ndcg@10=0.01159626778845436 [0.5s]\n",
      "epoch 56 [12.92s]:  training loss=0.0688004344701767                                     \n",
      "epoch 57 [13.03s]:  training loss=0.06733816862106323                                    \n",
      "epoch 58 [12.84s]:  training loss=0.06889799237251282                                    \n",
      "epoch 59 [12.98s]:  training loss=0.06895609945058823                                    \n",
      "epoch 60 [12.96s]: training loss=0.06938644498586655  validation ndcg@10=0.011788000298074707 [0.51s]\n",
      "epoch 61 [13.01s]:  training loss=0.06372527033090591                                    \n",
      "epoch 62 [12.95s]:  training loss=0.06538427621126175                                    \n",
      "epoch 63 [12.87s]:  training loss=0.066319540143013                                      \n",
      "epoch 64 [12.95s]:  training loss=0.06455231457948685                                    \n",
      "epoch 65 [12.95s]: training loss=0.06320568174123764  validation ndcg@10=0.012293710323647768 [0.45s]\n",
      "epoch 66 [13.01s]:  training loss=0.06070321425795555                                    \n",
      "epoch 67 [13.0s]:  training loss=0.0642065703868866                                      \n",
      "epoch 68 [12.9s]:  training loss=0.0601181760430336                                      \n",
      "epoch 69 [12.92s]:  training loss=0.0620027519762516                                     \n",
      "epoch 70 [12.94s]: training loss=0.0624547004699707  validation ndcg@10=0.011907738827987065 [0.46s]\n",
      "epoch 71 [13.06s]:  training loss=0.06262021511793137                                    \n",
      "epoch 72 [12.95s]:  training loss=0.05965789034962654                                    \n",
      "epoch 73 [13.01s]:  training loss=0.0570298470556736                                     \n",
      "epoch 74 [12.95s]:  training loss=0.05892767012119293                                    \n",
      "epoch 75 [12.93s]: training loss=0.058421194553375244  validation ndcg@10=0.012469835279879616 [0.5s]\n",
      "epoch 76 [13.01s]:  training loss=0.05891389772295952                                    \n",
      "epoch 77 [12.92s]:  training loss=0.05957330763339996                                    \n",
      "epoch 78 [12.87s]:  training loss=0.05600396543741226                                    \n",
      "epoch 79 [12.91s]:  training loss=0.057965077459812164                                   \n",
      "epoch 80 [12.95s]: training loss=0.057303279638290405  validation ndcg@10=0.012371115612361825 [0.45s]\n",
      "epoch 81 [12.96s]:  training loss=0.05768764019012451                                    \n",
      "epoch 82 [12.84s]:  training loss=0.05712634325027466                                    \n",
      "epoch 83 [12.86s]:  training loss=0.05438878759741783                                    \n",
      "epoch 84 [13.01s]:  training loss=0.05701514333486557                                    \n",
      "epoch 85 [13.01s]: training loss=0.05775483697652817  validation ndcg@10=0.012377052742365202 [0.52s]\n",
      "epoch 86 [12.98s]:  training loss=0.05453598126769066                                    \n",
      "epoch 87 [12.91s]:  training loss=0.05616842210292816                                    \n",
      "epoch 88 [12.92s]:  training loss=0.05543975532054901                                    \n",
      "epoch 89 [12.92s]:  training loss=0.054308295249938965                                   \n",
      "epoch 90 [13.09s]: training loss=0.05404311791062355  validation ndcg@10=0.011968289659444931 [0.44s]\n",
      "epoch 91 [14.82s]:  training loss=0.053723808377981186                                   \n",
      "epoch 92 [12.82s]:  training loss=0.055361825972795486                                   \n",
      "epoch 93 [12.94s]:  training loss=0.05194767192006111                                    \n",
      "epoch 94 [13.72s]:  training loss=0.05343636870384216                                    \n",
      "epoch 95 [13.16s]: training loss=0.05542737990617752  validation ndcg@10=0.013465880519683485 [0.5s]\n",
      "epoch 96 [12.93s]:  training loss=0.053182635456323624                                   \n",
      "epoch 97 [12.84s]:  training loss=0.055567286908626556                                   \n",
      "epoch 98 [12.91s]:  training loss=0.052586160600185394                                   \n",
      "epoch 99 [12.99s]:  training loss=0.05396927148103714                                    \n",
      "epoch 100 [12.94s]: training loss=0.051881927996873856  validation ndcg@10=0.012750883565276434 [0.45s]\n",
      "epoch 101 [13.02s]:  training loss=0.05217723175883293                                   \n",
      "epoch 102 [13.01s]:  training loss=0.051105696707963943                                  \n",
      "epoch 103 [13.1s]:  training loss=0.051573701202869415                                   \n",
      "epoch 104 [13.03s]:  training loss=0.05231982842087746                                   \n",
      "epoch 105 [12.95s]: training loss=0.051491063088178635  validation ndcg@10=0.012878268546774421 [0.46s]\n",
      "epoch 106 [12.9s]:  training loss=0.05025458708405495                                    \n",
      "epoch 107 [12.93s]:  training loss=0.05129401013255119                                   \n",
      "epoch 108 [12.92s]:  training loss=0.051941562443971634                                  \n",
      "epoch 109 [13.45s]:  training loss=0.05159740149974823                                   \n",
      "epoch 110 [13.62s]: training loss=0.050334904342889786  validation ndcg@10=0.013417277310662746 [0.47s]\n",
      "epoch 111 [12.88s]:  training loss=0.051553044468164444                                  \n",
      "epoch 112 [12.82s]:  training loss=0.05151531845331192                                   \n",
      "epoch 113 [12.95s]:  training loss=0.04801712930202484                                   \n",
      "epoch 114 [13.15s]:  training loss=0.049268078058958054                                  \n",
      "epoch 115 [12.95s]: training loss=0.04918167367577553  validation ndcg@10=0.013329690904369397 [0.45s]\n",
      "epoch 116 [12.89s]:  training loss=0.0502130500972271                                    \n",
      "epoch 117 [12.9s]:  training loss=0.050242532044649124                                   \n",
      "epoch 118 [12.95s]:  training loss=0.04848311468958855                                   \n",
      "epoch 119 [12.76s]:  training loss=0.04933954402804375                                   \n",
      "epoch 120 [12.85s]: training loss=0.04956643655896187  validation ndcg@10=0.013929755154532255 [0.48s]\n",
      "epoch 121 [12.77s]:  training loss=0.05163976177573204                                   \n",
      "epoch 122 [12.79s]:  training loss=0.04915238544344902                                   \n",
      "epoch 123 [12.97s]:  training loss=0.04838239774107933                                   \n",
      "epoch 124 [12.84s]:  training loss=0.050173982977867126                                  \n",
      "epoch 125 [12.83s]: training loss=0.050632864236831665  validation ndcg@10=0.013465698356119441 [0.46s]\n",
      "epoch 126 [12.83s]:  training loss=0.0485733300447464                                    \n",
      "epoch 127 [12.79s]:  training loss=0.04815815016627312                                   \n",
      "epoch 128 [13.05s]:  training loss=0.04895653948187828                                   \n",
      "epoch 129 [12.83s]:  training loss=0.048941049724817276                                  \n",
      "epoch 130 [12.91s]: training loss=0.04936995729804039  validation ndcg@10=0.01329775543473589 [0.49s]\n",
      "epoch 131 [14.77s]:  training loss=0.04911187291145325                                   \n",
      "epoch 132 [12.91s]:  training loss=0.045958176255226135                                  \n",
      "epoch 133 [12.99s]:  training loss=0.0473913811147213                                    \n",
      "epoch 134 [12.94s]:  training loss=0.048360973596572876                                  \n",
      "epoch 135 [12.98s]: training loss=0.0492408312857151  validation ndcg@10=0.012852706660391762 [0.46s]\n",
      "epoch 136 [13.73s]:  training loss=0.04746604338288307                                   \n",
      "epoch 137 [13.22s]:  training loss=0.048753347247838974                                  \n",
      "epoch 138 [12.94s]:  training loss=0.04854825511574745                                   \n",
      "epoch 139 [12.92s]:  training loss=0.05031896382570267                                   \n",
      "epoch 140 [13.0s]: training loss=0.04652193933725357  validation ndcg@10=0.013127591599242898 [0.47s]\n",
      "epoch 141 [12.89s]:  training loss=0.0471549928188324                                    \n",
      "epoch 142 [13.05s]:  training loss=0.048661183565855026                                  \n",
      "epoch 143 [12.98s]:  training loss=0.05031697079539299                                   \n",
      "epoch 144 [12.77s]:  training loss=0.04919104650616646                                   \n",
      "epoch 145 [12.97s]: training loss=0.04647378996014595  validation ndcg@10=0.014115307791818632 [0.49s]\n",
      "epoch 146 [12.84s]:  training loss=0.04956745356321335                                   \n",
      "epoch 147 [13.01s]:  training loss=0.0471830740571022                                    \n",
      "epoch 148 [12.97s]:  training loss=0.04704003036022186                                   \n",
      "epoch 149 [12.95s]:  training loss=0.047814395278692245                                  \n",
      "epoch 150 [12.92s]: training loss=0.044379860162734985  validation ndcg@10=0.012725862484359494 [0.51s]\n",
      "epoch 151 [12.92s]:  training loss=0.04800073057413101                                   \n",
      "epoch 152 [12.95s]:  training loss=0.04968857392668724                                   \n",
      "epoch 153 [12.88s]:  training loss=0.04747522622346878                                   \n",
      "epoch 154 [12.89s]:  training loss=0.04698629677295685                                   \n",
      "epoch 155 [12.82s]: training loss=0.049382053315639496  validation ndcg@10=0.012705632809251147 [0.57s]\n",
      "epoch 156 [12.99s]:  training loss=0.046664368361234665                                  \n",
      "epoch 157 [12.95s]:  training loss=0.04517684876918793                                   \n",
      "epoch 158 [12.91s]:  training loss=0.04755083844065666                                   \n",
      "epoch 159 [12.97s]:  training loss=0.04648029804229736                                   \n",
      "epoch 160 [12.87s]: training loss=0.04654859006404877  validation ndcg@10=0.012720212686596863 [0.52s]\n",
      "epoch 161 [12.93s]:  training loss=0.04740481078624725                                   \n",
      "epoch 162 [12.96s]:  training loss=0.04757561534643173                                   \n",
      "epoch 163 [12.83s]:  training loss=0.04783123359084129                                   \n",
      "epoch 164 [12.93s]:  training loss=0.04481294006109238                                   \n",
      "epoch 165 [12.87s]: training loss=0.04765461012721062  validation ndcg@10=0.012252869305768358 [0.48s]\n",
      "epoch 166 [12.96s]:  training loss=0.04709213972091675                                   \n",
      "epoch 167 [12.93s]:  training loss=0.04487883672118187                                   \n",
      "epoch 168 [12.87s]:  training loss=0.04679098725318909                                   \n",
      "epoch 169 [12.98s]:  training loss=0.04625943303108215                                   \n",
      "epoch 170 [12.99s]: training loss=0.04539390653371811  validation ndcg@10=0.012079081518737255 [0.45s]\n",
      "epoch 1 [22.25s]:  training loss=0.7943795919418335                                       \n",
      "epoch 2 [20.24s]:  training loss=0.7538653612136841                                       \n",
      "epoch 3 [20.32s]:  training loss=0.6938927173614502                                       \n",
      "epoch 4 [20.39s]:  training loss=0.6199175715446472                                       \n",
      "epoch 5 [20.98s]: training loss=0.586128294467926  validation ndcg@10=0.020512412719942672 [0.6s]\n",
      "epoch 6 [20.52s]:  training loss=0.5531439185142517                                       \n",
      "epoch 7 [20.5s]:  training loss=0.5291035771369934                                        \n",
      "epoch 8 [20.41s]:  training loss=0.5015856027603149                                       \n",
      "epoch 9 [20.56s]:  training loss=0.46949607133865356                                      \n",
      "epoch 10 [20.51s]: training loss=0.44722434878349304  validation ndcg@10=0.020387486788060905 [0.52s]\n",
      "epoch 11 [20.5s]:  training loss=0.42535024881362915                                      \n",
      "epoch 12 [20.51s]:  training loss=0.40070053935050964                                     \n",
      "epoch 13 [20.46s]:  training loss=0.3806256055831909                                      \n",
      "epoch 14 [20.49s]:  training loss=0.36509230732917786                                     \n",
      "epoch 15 [20.47s]: training loss=0.3521445691585541  validation ndcg@10=0.02030892038361637 [0.52s]\n",
      "epoch 16 [20.6s]:  training loss=0.33732911944389343                                      \n",
      "epoch 17 [20.43s]:  training loss=0.31854110956192017                                     \n",
      "epoch 18 [20.47s]:  training loss=0.30305352807044983                                     \n",
      "epoch 19 [20.68s]:  training loss=0.289268434047699                                       \n",
      "epoch 20 [20.5s]: training loss=0.27112460136413574  validation ndcg@10=0.02153826959168755 [0.52s]\n",
      "epoch 21 [20.41s]:  training loss=0.2588099539279938                                      \n",
      "epoch 22 [20.47s]:  training loss=0.24351780116558075                                     \n",
      "epoch 23 [20.42s]:  training loss=0.2299872785806656                                      \n",
      "epoch 24 [20.36s]:  training loss=0.21466569602489471                                     \n",
      "epoch 25 [20.36s]: training loss=0.2033344805240631  validation ndcg@10=0.021678723786332902 [0.58s]\n",
      "epoch 26 [20.4s]:  training loss=0.1967622935771942                                       \n",
      "epoch 27 [20.29s]:  training loss=0.1828833669424057                                      \n",
      "epoch 28 [21.27s]:  training loss=0.17597711086273193                                     \n",
      "epoch 29 [21.3s]:  training loss=0.16459648311138153                                      \n",
      "epoch 30 [20.33s]: training loss=0.15812210738658905  validation ndcg@10=0.023667701571057884 [0.55s]\n",
      "epoch 31 [20.95s]:  training loss=0.15184931457042694                                     \n",
      "epoch 32 [20.43s]:  training loss=0.1471506804227829                                      \n",
      "epoch 33 [20.39s]:  training loss=0.13829155266284943                                     \n",
      "epoch 34 [20.35s]:  training loss=0.13330818712711334                                     \n",
      "epoch 35 [20.34s]: training loss=0.12943069636821747  validation ndcg@10=0.023556692892057505 [0.55s]\n",
      "epoch 36 [20.33s]:  training loss=0.12359828501939774                                     \n",
      "epoch 37 [20.44s]:  training loss=0.12116415053606033                                     \n",
      "epoch 38 [20.48s]:  training loss=0.11605425179004669                                     \n",
      "epoch 39 [20.5s]:  training loss=0.11004526168107986                                      \n",
      "epoch 40 [20.47s]: training loss=0.10856667906045914  validation ndcg@10=0.023844348338193894 [0.53s]\n",
      "epoch 41 [20.37s]:  training loss=0.10521399229764938                                     \n",
      "epoch 42 [20.45s]:  training loss=0.10395105928182602                                     \n",
      "epoch 43 [20.38s]:  training loss=0.09822342544794083                                     \n",
      "epoch 44 [20.35s]:  training loss=0.09591025114059448                                     \n",
      "epoch 45 [20.35s]: training loss=0.09356514364480972  validation ndcg@10=0.02488262388456358 [0.58s]\n",
      "epoch 46 [20.59s]:  training loss=0.09007067233324051                                     \n",
      "epoch 47 [20.37s]:  training loss=0.089417465031147                                       \n",
      "epoch 48 [20.38s]:  training loss=0.08744233101606369                                     \n",
      "epoch 49 [20.53s]:  training loss=0.08487354964017868                                     \n",
      "epoch 50 [20.48s]: training loss=0.08286431431770325  validation ndcg@10=0.025120204755892998 [0.53s]\n",
      "epoch 51 [20.5s]:  training loss=0.08089053630828857                                      \n",
      "epoch 52 [20.58s]:  training loss=0.07737068086862564                                     \n",
      "epoch 53 [20.41s]:  training loss=0.07909281551837921                                     \n",
      "epoch 54 [20.49s]:  training loss=0.0768362358212471                                      \n",
      "epoch 55 [20.46s]: training loss=0.07722511142492294  validation ndcg@10=0.023821440474319378 [0.53s]\n",
      "epoch 56 [21.41s]:  training loss=0.07366180419921875                                     \n",
      "epoch 57 [19.85s]:  training loss=0.07205193489789963                                     \n",
      "epoch 58 [21.07s]:  training loss=0.06842327117919922                                     \n",
      "epoch 59 [20.49s]:  training loss=0.06737654656171799                                     \n",
      "epoch 60 [20.45s]: training loss=0.0704968199133873  validation ndcg@10=0.024921356863905675 [0.59s]\n",
      "epoch 61 [20.5s]:  training loss=0.06914859265089035                                      \n",
      "epoch 62 [20.51s]:  training loss=0.06503435969352722                                     \n",
      "epoch 63 [20.76s]:  training loss=0.06420568376779556                                     \n",
      "epoch 64 [20.37s]:  training loss=0.06209269538521767                                     \n",
      "epoch 65 [20.96s]: training loss=0.061417605727910995  validation ndcg@10=0.02495041967781151 [0.54s]\n",
      "epoch 66 [20.47s]:  training loss=0.06002747640013695                                     \n",
      "epoch 67 [20.48s]:  training loss=0.06145104765892029                                     \n",
      "epoch 68 [20.48s]:  training loss=0.05943033844232559                                     \n",
      "epoch 69 [20.48s]:  training loss=0.057820942252874374                                    \n",
      "epoch 70 [20.53s]: training loss=0.056733388453722  validation ndcg@10=0.024117553887589925 [0.54s]\n",
      "epoch 71 [20.53s]:  training loss=0.058128662407398224                                    \n",
      "epoch 72 [20.51s]:  training loss=0.05681018531322479                                     \n",
      "epoch 73 [20.5s]:  training loss=0.0529964454472065                                       \n",
      "epoch 74 [20.51s]:  training loss=0.05753132700920105                                     \n",
      "epoch 75 [20.45s]: training loss=0.05197520554065704  validation ndcg@10=0.025133105448287767 [0.58s]\n",
      "epoch 76 [20.48s]:  training loss=0.05252097174525261                                     \n",
      "epoch 77 [20.44s]:  training loss=0.05215770751237869                                     \n",
      "epoch 78 [20.41s]:  training loss=0.05104759708046913                                     \n",
      "epoch 79 [20.46s]:  training loss=0.05044739693403244                                     \n",
      "epoch 80 [20.38s]: training loss=0.04715929925441742  validation ndcg@10=0.023949206800065386 [0.53s]\n",
      "epoch 81 [20.73s]:  training loss=0.04956473037600517                                     \n",
      "epoch 82 [22.04s]:  training loss=0.04929732903838158                                     \n",
      "epoch 83 [20.56s]:  training loss=0.04839096963405609                                     \n",
      "epoch 84 [20.94s]:  training loss=0.04755309596657753                                     \n",
      "epoch 85 [21.0s]: training loss=0.048012591898441315  validation ndcg@10=0.02430371972513028 [0.57s]\n",
      "epoch 86 [20.45s]:  training loss=0.04646534472703934                                     \n",
      "epoch 87 [20.35s]:  training loss=0.04635961726307869                                     \n",
      "epoch 88 [20.6s]:  training loss=0.045911628752946854                                     \n",
      "epoch 89 [20.5s]:  training loss=0.04547187313437462                                      \n",
      "epoch 90 [20.44s]: training loss=0.04424600303173065  validation ndcg@10=0.025030943792325584 [0.55s]\n",
      "epoch 91 [20.6s]:  training loss=0.04359932616353035                                      \n",
      "epoch 92 [20.49s]:  training loss=0.04479914903640747                                     \n",
      "epoch 93 [20.58s]:  training loss=0.042584408074617386                                    \n",
      "epoch 94 [20.62s]:  training loss=0.043118931353092194                                    \n",
      "epoch 95 [20.58s]: training loss=0.04190710559487343  validation ndcg@10=0.024375699946702998 [0.59s]\n",
      "epoch 96 [20.66s]:  training loss=0.040121786296367645                                    \n",
      "epoch 97 [20.58s]:  training loss=0.0398235023021698                                      \n",
      "epoch 98 [20.44s]:  training loss=0.04081684723496437                                     \n",
      "epoch 99 [20.63s]:  training loss=0.0405699722468853                                      \n",
      "epoch 100 [20.55s]: training loss=0.04004165157675743  validation ndcg@10=0.02375873271940465 [0.58s]\n",
      "epoch 1 [20.7s]:  training loss=0.4671727418899536                                        \n",
      "epoch 2 [23.08s]:  training loss=0.18726034462451935                                      \n",
      "epoch 3 [22.87s]:  training loss=0.12841704487800598                                      \n",
      "epoch 4 [22.8s]:  training loss=0.10968925058841705                                       \n",
      "epoch 5 [22.81s]: training loss=0.09878456592559814  validation ndcg@10=0.017538638628771332 [0.53s]\n",
      "epoch 6 [24.8s]:  training loss=0.09519757330417633                                       \n",
      "epoch 7 [22.77s]:  training loss=0.09107635915279388                                      \n",
      "epoch 8 [23.02s]:  training loss=0.08493704348802567                                      \n",
      "epoch 9 [23.42s]:  training loss=0.08528359979391098                                      \n",
      "epoch 10 [23.73s]: training loss=0.08508674800395966  validation ndcg@10=0.014584171803247057 [0.51s]\n",
      "epoch 11 [23.3s]:  training loss=0.08344490826129913                                      \n",
      "epoch 12 [22.95s]:  training loss=0.08714274317026138                                     \n",
      "epoch 13 [23.39s]:  training loss=0.08154952526092529                                     \n",
      "epoch 14 [23.69s]:  training loss=0.08394678682088852                                     \n",
      "epoch 15 [23.37s]: training loss=0.08129166811704636  validation ndcg@10=0.01677250811176776 [0.54s]\n",
      "epoch 16 [23.44s]:  training loss=0.08662354946136475                                     \n",
      "epoch 17 [23.17s]:  training loss=0.08201886713504791                                     \n",
      "epoch 18 [23.27s]:  training loss=0.08565955609083176                                     \n",
      "epoch 19 [22.92s]:  training loss=0.0793742910027504                                      \n",
      "epoch 20 [23.47s]: training loss=0.07957050204277039  validation ndcg@10=0.012940627218063471 [0.53s]\n",
      "epoch 21 [23.17s]:  training loss=0.0869515985250473                                      \n",
      "epoch 22 [22.16s]:  training loss=0.08900956809520721                                     \n",
      "epoch 23 [23.98s]:  training loss=0.08572299033403397                                     \n",
      "epoch 24 [23.61s]:  training loss=0.08093549311161041                                     \n",
      "epoch 25 [23.57s]: training loss=0.08395696431398392  validation ndcg@10=0.013824397315128946 [0.54s]\n",
      "epoch 26 [23.95s]:  training loss=0.08032374083995819                                     \n",
      "epoch 27 [25.55s]:  training loss=0.0858752653002739                                      \n",
      "epoch 28 [24.02s]:  training loss=0.08188311010599136                                     \n",
      "epoch 29 [23.78s]:  training loss=0.08998969942331314                                     \n",
      "epoch 30 [23.66s]: training loss=0.08715254068374634  validation ndcg@10=0.013184972731267806 [0.58s]\n",
      "epoch 1 [17.34s]:  training loss=0.8121078014373779                                       \n",
      "epoch 2 [17.82s]:  training loss=0.7872016429901123                                       \n",
      "epoch 3 [18.29s]:  training loss=0.7797150611877441                                       \n",
      "epoch 4 [17.86s]:  training loss=0.7624569535255432                                       \n",
      "epoch 5 [17.49s]: training loss=0.7541781663894653  validation ndcg@10=0.009455264203632558 [0.48s]\n",
      "epoch 6 [17.94s]:  training loss=0.7367260456085205                                       \n",
      "epoch 7 [17.86s]:  training loss=0.7238184809684753                                       \n",
      "epoch 8 [17.78s]:  training loss=0.694553017616272                                        \n",
      "epoch 9 [18.41s]:  training loss=0.6390213966369629                                       \n",
      "epoch 10 [18.57s]: training loss=0.6106220483779907  validation ndcg@10=0.01872374763629732 [0.5s]\n",
      "epoch 11 [17.93s]:  training loss=0.5949490666389465                                      \n",
      "epoch 12 [17.43s]:  training loss=0.5802492499351501                                      \n",
      "epoch 13 [17.58s]:  training loss=0.5700047016143799                                      \n",
      "epoch 14 [17.97s]:  training loss=0.5605308413505554                                      \n",
      "epoch 15 [18.17s]: training loss=0.5457208156585693  validation ndcg@10=0.019844857773586532 [0.53s]\n",
      "epoch 16 [17.24s]:  training loss=0.5342766642570496                                      \n",
      "epoch 17 [17.38s]:  training loss=0.5231078267097473                                      \n",
      "epoch 18 [17.37s]:  training loss=0.5132561922073364                                      \n",
      "epoch 19 [17.39s]:  training loss=0.5014438033103943                                      \n",
      "epoch 20 [17.4s]: training loss=0.49548590183258057  validation ndcg@10=0.02050484892073968 [0.51s]\n",
      "epoch 21 [17.85s]:  training loss=0.480069637298584                                       \n",
      "epoch 22 [17.83s]:  training loss=0.46883827447891235                                     \n",
      "epoch 23 [18.02s]:  training loss=0.45578840374946594                                     \n",
      "epoch 24 [19.12s]:  training loss=0.4427153468132019                                      \n",
      "epoch 25 [17.74s]: training loss=0.4329926669597626  validation ndcg@10=0.02159033240386637 [0.48s]\n",
      "epoch 26 [18.32s]:  training loss=0.42068225145339966                                     \n",
      "epoch 27 [17.78s]:  training loss=0.4123135805130005                                      \n",
      "epoch 28 [18.0s]:  training loss=0.40587419271469116                                      \n",
      "epoch 29 [17.97s]:  training loss=0.3951636254787445                                      \n",
      "epoch 30 [17.73s]: training loss=0.38958218693733215  validation ndcg@10=0.02392239783839712 [0.53s]\n",
      "epoch 31 [17.68s]:  training loss=0.3815939128398895                                      \n",
      "epoch 32 [17.95s]:  training loss=0.3721145987510681                                      \n",
      "epoch 33 [17.9s]:  training loss=0.36920005083084106                                      \n",
      "epoch 34 [18.17s]:  training loss=0.3621788024902344                                      \n",
      "epoch 35 [17.88s]: training loss=0.35503798723220825  validation ndcg@10=0.02360899861979629 [0.5s]\n",
      "epoch 36 [18.31s]:  training loss=0.3465282618999481                                      \n",
      "epoch 37 [17.88s]:  training loss=0.3422142565250397                                      \n",
      "epoch 38 [17.87s]:  training loss=0.33798643946647644                                     \n",
      "epoch 39 [17.4s]:  training loss=0.3269435167312622                                       \n",
      "epoch 40 [17.69s]: training loss=0.31647759675979614  validation ndcg@10=0.023861705375783093 [0.48s]\n",
      "epoch 41 [17.36s]:  training loss=0.31396380066871643                                     \n",
      "epoch 42 [17.81s]:  training loss=0.30756261944770813                                     \n",
      "epoch 43 [17.82s]:  training loss=0.30757325887680054                                     \n",
      "epoch 44 [17.66s]:  training loss=0.29892221093177795                                     \n",
      "epoch 45 [17.61s]: training loss=0.29424285888671875  validation ndcg@10=0.023437305684027188 [0.51s]\n",
      "epoch 46 [17.82s]:  training loss=0.2916109561920166                                      \n",
      "epoch 47 [17.54s]:  training loss=0.28508204221725464                                     \n",
      "epoch 48 [17.65s]:  training loss=0.28259947896003723                                     \n",
      "epoch 49 [17.42s]:  training loss=0.2804203927516937                                      \n",
      "epoch 50 [17.52s]: training loss=0.27372848987579346  validation ndcg@10=0.023913278308696502 [0.5s]\n",
      "epoch 51 [17.94s]:  training loss=0.2683868706226349                                      \n",
      "epoch 52 [17.7s]:  training loss=0.2658560276031494                                       \n",
      "epoch 53 [19.12s]:  training loss=0.265882670879364                                       \n",
      "epoch 54 [17.74s]:  training loss=0.2541061341762543                                      \n",
      "epoch 55 [17.79s]: training loss=0.2510327994823456  validation ndcg@10=0.024766345482214625 [0.51s]\n",
      "epoch 56 [18.03s]:  training loss=0.2427937388420105                                      \n",
      "epoch 57 [17.32s]:  training loss=0.2410639375448227                                      \n",
      "epoch 58 [18.01s]:  training loss=0.2409033328294754                                      \n",
      "epoch 59 [18.06s]:  training loss=0.23440338671207428                                     \n",
      "epoch 60 [17.94s]: training loss=0.23399250209331512  validation ndcg@10=0.02453332780236629 [0.56s]\n",
      "epoch 61 [17.67s]:  training loss=0.22760772705078125                                     \n",
      "epoch 62 [17.62s]:  training loss=0.22557079792022705                                     \n",
      "epoch 63 [18.19s]:  training loss=0.2250014692544937                                      \n",
      "epoch 64 [18.17s]:  training loss=0.2169744372367859                                      \n",
      "epoch 65 [17.87s]: training loss=0.21478542685508728  validation ndcg@10=0.02470001344253923 [0.5s]\n",
      "epoch 66 [17.66s]:  training loss=0.20855233073234558                                     \n",
      "epoch 67 [17.65s]:  training loss=0.20494170486927032                                     \n",
      "epoch 68 [17.64s]:  training loss=0.1994890719652176                                      \n",
      "epoch 69 [17.79s]:  training loss=0.19970649480819702                                     \n",
      "epoch 70 [17.74s]: training loss=0.19766706228256226  validation ndcg@10=0.024317046684357326 [0.52s]\n",
      "epoch 71 [18.94s]:  training loss=0.19306930899620056                                     \n",
      "epoch 72 [17.83s]:  training loss=0.18992647528648376                                     \n",
      "epoch 73 [18.14s]:  training loss=0.1870880126953125                                      \n",
      "epoch 74 [17.89s]:  training loss=0.18645845353603363                                     \n",
      "epoch 75 [18.27s]: training loss=0.17924341559410095  validation ndcg@10=0.024776735109052966 [0.51s]\n",
      "epoch 76 [17.85s]:  training loss=0.1790236234664917                                      \n",
      "epoch 77 [17.45s]:  training loss=0.17666412889957428                                     \n",
      "epoch 78 [17.45s]:  training loss=0.17438188195228577                                     \n",
      "epoch 79 [18.52s]:  training loss=0.17101852595806122                                     \n",
      "epoch 80 [17.92s]: training loss=0.1685856282711029  validation ndcg@10=0.024582446808030442 [0.53s]\n",
      "epoch 81 [19.06s]:  training loss=0.16245310008525848                                     \n",
      "epoch 82 [17.69s]:  training loss=0.16211692988872528                                     \n",
      "epoch 83 [18.14s]:  training loss=0.16030849516391754                                     \n",
      "epoch 84 [17.5s]:  training loss=0.156922847032547                                        \n",
      "epoch 85 [18.18s]: training loss=0.15499766170978546  validation ndcg@10=0.02569669546864128 [0.53s]\n",
      "epoch 86 [17.75s]:  training loss=0.15240713953971863                                     \n",
      "epoch 87 [18.01s]:  training loss=0.15328122675418854                                     \n",
      "epoch 88 [17.97s]:  training loss=0.14900977909564972                                     \n",
      "epoch 89 [17.58s]:  training loss=0.1444503217935562                                      \n",
      "epoch 90 [18.08s]: training loss=0.14110778272151947  validation ndcg@10=0.025399718905950143 [0.49s]\n",
      "epoch 91 [17.91s]:  training loss=0.1410032957792282                                      \n",
      "epoch 92 [17.76s]:  training loss=0.13911445438861847                                     \n",
      "epoch 93 [17.9s]:  training loss=0.13814286887645721                                      \n",
      "epoch 94 [18.09s]:  training loss=0.1366327852010727                                      \n",
      "epoch 95 [18.13s]: training loss=0.13070929050445557  validation ndcg@10=0.025251039975693227 [0.5s]\n",
      "epoch 96 [18.23s]:  training loss=0.13328917324543                                        \n",
      "epoch 97 [17.38s]:  training loss=0.12797436118125916                                     \n",
      "epoch 98 [18.0s]:  training loss=0.12973855435848236                                      \n",
      "epoch 99 [17.43s]:  training loss=0.12681542336940765                                     \n",
      "epoch 100 [17.66s]: training loss=0.12478241324424744  validation ndcg@10=0.025591922639372103 [0.49s]\n",
      "epoch 101 [17.84s]:  training loss=0.12327558547258377                                    \n",
      "epoch 102 [17.49s]:  training loss=0.12284287810325623                                    \n",
      "epoch 103 [18.03s]:  training loss=0.12147354334592819                                    \n",
      "epoch 104 [17.52s]:  training loss=0.11740785092115402                                    \n",
      "epoch 105 [17.58s]: training loss=0.11817660927772522  validation ndcg@10=0.025434282218318807 [0.5s]\n",
      "epoch 106 [17.47s]:  training loss=0.11521457880735397                                    \n",
      "epoch 107 [17.73s]:  training loss=0.1151154637336731                                     \n",
      "epoch 108 [17.65s]:  training loss=0.11448168009519577                                    \n",
      "epoch 109 [19.36s]:  training loss=0.10879798233509064                                    \n",
      "epoch 110 [17.28s]: training loss=0.11122724413871765  validation ndcg@10=0.025648395244663533 [0.51s]\n",
      "epoch 1 [16.42s]:  training loss=0.8525753617286682                                       \n",
      "epoch 2 [16.03s]:  training loss=0.8369266390800476                                       \n",
      "epoch 3 [16.28s]:  training loss=0.8303385376930237                                       \n",
      "epoch 4 [15.92s]:  training loss=0.8231707215309143                                       \n",
      "epoch 5 [16.48s]: training loss=0.8189733624458313  validation ndcg@10=0.0007181770858331644 [0.52s]\n",
      "epoch 6 [16.26s]:  training loss=0.8146110773086548                                       \n",
      "epoch 7 [15.87s]:  training loss=0.806980311870575                                        \n",
      "epoch 8 [16.26s]:  training loss=0.8098561763763428                                       \n",
      "epoch 9 [16.7s]:  training loss=0.8029977083206177                                        \n",
      "epoch 10 [16.41s]: training loss=0.80548095703125  validation ndcg@10=0.0007997997348701869 [0.47s]\n",
      "epoch 11 [16.06s]:  training loss=0.8016311526298523                                      \n",
      "epoch 12 [16.14s]:  training loss=0.7988646626472473                                      \n",
      "epoch 13 [16.05s]:  training loss=0.795560359954834                                       \n",
      "epoch 14 [16.35s]:  training loss=0.7983065247535706                                      \n",
      "epoch 15 [16.24s]: training loss=0.7988634705543518  validation ndcg@10=0.000735393536735335 [0.49s]\n",
      "epoch 16 [16.57s]:  training loss=0.7949661612510681                                      \n",
      "epoch 17 [16.48s]:  training loss=0.8000772595405579                                      \n",
      "epoch 18 [15.82s]:  training loss=0.7924274206161499                                      \n",
      "epoch 19 [15.91s]:  training loss=0.7937007546424866                                      \n",
      "epoch 20 [16.57s]: training loss=0.7952495217323303  validation ndcg@10=0.0009015472582178358 [0.52s]\n",
      "epoch 21 [16.42s]:  training loss=0.7912381887435913                                      \n",
      "epoch 22 [16.72s]:  training loss=0.7895404100418091                                      \n",
      "epoch 23 [16.45s]:  training loss=0.7891842126846313                                      \n",
      "epoch 24 [16.51s]:  training loss=0.7862845659255981                                      \n",
      "epoch 25 [16.09s]: training loss=0.7889785766601562  validation ndcg@10=0.0013046913731084972 [0.46s]\n",
      "epoch 26 [16.11s]:  training loss=0.7865108251571655                                      \n",
      "epoch 27 [16.23s]:  training loss=0.7882902026176453                                      \n",
      "epoch 28 [16.18s]:  training loss=0.7869819402694702                                      \n",
      "epoch 29 [16.51s]:  training loss=0.785200834274292                                       \n",
      "epoch 30 [18.17s]: training loss=0.7824356555938721  validation ndcg@10=0.0014685105796078523 [0.48s]\n",
      "epoch 31 [14.88s]:  training loss=0.7835233807563782                                      \n",
      "epoch 32 [15.72s]:  training loss=0.784613847732544                                       \n",
      "epoch 33 [17.31s]:  training loss=0.7846312522888184                                      \n",
      "epoch 34 [16.43s]:  training loss=0.784687340259552                                       \n",
      "epoch 35 [16.49s]: training loss=0.7830193042755127  validation ndcg@10=0.001088571674671115 [0.5s]\n",
      "epoch 36 [16.42s]:  training loss=0.7814303040504456                                      \n",
      "epoch 37 [16.33s]:  training loss=0.7821541428565979                                      \n",
      "epoch 38 [16.52s]:  training loss=0.7804277539253235                                      \n",
      "epoch 39 [16.55s]:  training loss=0.7794361114501953                                      \n",
      "epoch 40 [16.26s]: training loss=0.778109073638916  validation ndcg@10=0.001587030659326382 [0.48s]\n",
      "epoch 41 [16.01s]:  training loss=0.7780811786651611                                      \n",
      "epoch 42 [16.49s]:  training loss=0.779300332069397                                       \n",
      "epoch 43 [16.34s]:  training loss=0.776228129863739                                       \n",
      "epoch 44 [16.62s]:  training loss=0.7781867384910583                                      \n",
      "epoch 45 [16.59s]: training loss=0.7727565169334412  validation ndcg@10=0.002311697362376759 [0.5s]\n",
      "epoch 46 [16.29s]:  training loss=0.7770023345947266                                      \n",
      "epoch 47 [16.42s]:  training loss=0.7720402479171753                                      \n",
      "epoch 48 [16.29s]:  training loss=0.7745266556739807                                      \n",
      "epoch 49 [17.09s]:  training loss=0.7758558988571167                                      \n",
      "epoch 50 [16.52s]: training loss=0.7718005776405334  validation ndcg@10=0.0034151965801651855 [0.5s]\n",
      "epoch 51 [16.4s]:  training loss=0.7713443636894226                                       \n",
      "epoch 52 [16.16s]:  training loss=0.7715620398521423                                      \n",
      "epoch 53 [16.9s]:  training loss=0.7706690430641174                                       \n",
      "epoch 54 [16.05s]:  training loss=0.7724106311798096                                      \n",
      "epoch 55 [16.59s]: training loss=0.7714517712593079  validation ndcg@10=0.0033315635701682174 [0.49s]\n",
      "epoch 56 [16.77s]:  training loss=0.7697221040725708                                      \n",
      "epoch 57 [16.55s]:  training loss=0.7663910984992981                                      \n",
      "epoch 58 [16.21s]:  training loss=0.7680800557136536                                      \n",
      "epoch 59 [16.64s]:  training loss=0.7659614682197571                                      \n",
      "epoch 60 [16.35s]: training loss=0.7668927907943726  validation ndcg@10=0.004118602141151243 [0.5s]\n",
      "epoch 61 [16.65s]:  training loss=0.7649685740470886                                      \n",
      "epoch 62 [16.72s]:  training loss=0.7667420506477356                                      \n",
      "epoch 63 [17.39s]:  training loss=0.7619513869285583                                      \n",
      "epoch 64 [16.86s]:  training loss=0.7633580565452576                                      \n",
      "epoch 65 [16.49s]: training loss=0.7628710865974426  validation ndcg@10=0.005197270871893842 [0.49s]\n",
      "epoch 66 [16.4s]:  training loss=0.7648759484291077                                       \n",
      "epoch 67 [17.04s]:  training loss=0.7619084119796753                                      \n",
      "epoch 68 [16.8s]:  training loss=0.7623819708824158                                       \n",
      "epoch 69 [16.64s]:  training loss=0.7607294321060181                                      \n",
      "epoch 70 [16.73s]: training loss=0.7606868147850037  validation ndcg@10=0.0054814512697395875 [0.47s]\n",
      "epoch 71 [17.01s]:  training loss=0.7585906982421875                                      \n",
      "epoch 72 [17.2s]:  training loss=0.7599332928657532                                       \n",
      "epoch 73 [17.03s]:  training loss=0.7595398426055908                                      \n",
      "epoch 74 [16.22s]:  training loss=0.756957471370697                                       \n",
      "epoch 75 [16.76s]: training loss=0.7574533224105835  validation ndcg@10=0.00570529947481573 [0.5s]\n",
      "epoch 76 [16.69s]:  training loss=0.7565801739692688                                      \n",
      "epoch 77 [16.45s]:  training loss=0.7562639117240906                                      \n",
      "epoch 78 [18.19s]:  training loss=0.7551503777503967                                      \n",
      "epoch 79 [16.19s]:  training loss=0.7556764483451843                                      \n",
      "epoch 80 [16.58s]: training loss=0.7552517652511597  validation ndcg@10=0.007612143881464998 [0.49s]\n",
      "epoch 81 [16.4s]:  training loss=0.7550240159034729                                       \n",
      "epoch 82 [16.89s]:  training loss=0.7558751702308655                                      \n",
      "epoch 83 [16.42s]:  training loss=0.7529494166374207                                      \n",
      "epoch 84 [16.19s]:  training loss=0.7508436441421509                                      \n",
      "epoch 85 [16.3s]: training loss=0.750800371170044  validation ndcg@10=0.00736847492325525 [0.51s]\n",
      "epoch 86 [16.19s]:  training loss=0.7499509453773499                                      \n",
      "epoch 87 [16.44s]:  training loss=0.7488859295845032                                      \n",
      "epoch 88 [16.88s]:  training loss=0.747035801410675                                       \n",
      "epoch 89 [16.54s]:  training loss=0.7501311898231506                                      \n",
      "epoch 90 [16.92s]: training loss=0.7451988458633423  validation ndcg@10=0.009321691005361043 [0.5s]\n",
      "epoch 91 [16.11s]:  training loss=0.7463487982749939                                      \n",
      "epoch 92 [16.66s]:  training loss=0.7470008134841919                                      \n",
      "epoch 93 [16.39s]:  training loss=0.7457934617996216                                      \n",
      "epoch 94 [16.12s]:  training loss=0.7454280257225037                                      \n",
      "epoch 95 [16.6s]: training loss=0.7436686158180237  validation ndcg@10=0.011136862432873224 [0.5s]\n",
      "epoch 96 [16.8s]:  training loss=0.7428900003433228                                       \n",
      "epoch 97 [16.89s]:  training loss=0.7412099242210388                                      \n",
      "epoch 98 [16.85s]:  training loss=0.7400397062301636                                      \n",
      "epoch 99 [16.32s]:  training loss=0.7414909601211548                                      \n",
      "epoch 100 [17.16s]: training loss=0.7388253211975098  validation ndcg@10=0.013944807290212684 [0.53s]\n",
      "epoch 101 [17.14s]:  training loss=0.7382611632347107                                     \n",
      "epoch 102 [17.09s]:  training loss=0.7399197816848755                                     \n",
      "epoch 103 [16.67s]:  training loss=0.7374904155731201                                     \n",
      "epoch 104 [16.87s]:  training loss=0.7365978360176086                                     \n",
      "epoch 105 [16.7s]: training loss=0.7340248227119446  validation ndcg@10=0.014239405736393547 [0.47s]\n",
      "epoch 106 [16.45s]:  training loss=0.7314947247505188                                     \n",
      "epoch 107 [17.07s]:  training loss=0.732048749923706                                      \n",
      "epoch 108 [16.42s]:  training loss=0.7346460223197937                                     \n",
      "epoch 109 [16.51s]:  training loss=0.7315641641616821                                     \n",
      "epoch 110 [16.21s]: training loss=0.7290820479393005  validation ndcg@10=0.016231751042678974 [0.53s]\n",
      "epoch 111 [16.81s]:  training loss=0.7271343469619751                                     \n",
      "epoch 112 [16.99s]:  training loss=0.7292028069496155                                     \n",
      "epoch 113 [16.64s]:  training loss=0.7255315184593201                                     \n",
      "epoch 114 [17.24s]:  training loss=0.7245674729347229                                     \n",
      "epoch 115 [17.07s]: training loss=0.7231467962265015  validation ndcg@10=0.018692314049746157 [0.5s]\n",
      "epoch 116 [16.9s]:  training loss=0.7226265668869019                                      \n",
      "epoch 117 [16.67s]:  training loss=0.7196246981620789                                     \n",
      "epoch 118 [16.47s]:  training loss=0.7201201915740967                                     \n",
      "epoch 119 [16.76s]:  training loss=0.7158726453781128                                     \n",
      "epoch 120 [16.18s]: training loss=0.7164014577865601  validation ndcg@10=0.019660047645458298 [0.48s]\n",
      "epoch 121 [16.72s]:  training loss=0.7175844311714172                                     \n",
      "epoch 122 [17.1s]:  training loss=0.7146614193916321                                      \n",
      "epoch 123 [17.44s]:  training loss=0.7099964022636414                                     \n",
      "epoch 124 [16.66s]:  training loss=0.7107006907463074                                     \n",
      "epoch 125 [17.09s]: training loss=0.7055732607841492  validation ndcg@10=0.019291809508007598 [0.53s]\n",
      "epoch 126 [17.58s]:  training loss=0.7065035104751587                                     \n",
      "epoch 127 [16.79s]:  training loss=0.7023171186447144                                     \n",
      "epoch 128 [18.09s]:  training loss=0.6995044350624084                                     \n",
      "epoch 129 [17.06s]:  training loss=0.6970886588096619                                     \n",
      "epoch 130 [16.66s]: training loss=0.6945869326591492  validation ndcg@10=0.01965977169540505 [0.54s]\n",
      "epoch 131 [16.87s]:  training loss=0.6895128488540649                                     \n",
      "epoch 132 [17.15s]:  training loss=0.6894088387489319                                     \n",
      "epoch 133 [16.79s]:  training loss=0.6836247444152832                                     \n",
      "epoch 134 [16.75s]:  training loss=0.6826627850532532                                     \n",
      "epoch 135 [16.19s]: training loss=0.6794388294219971  validation ndcg@10=0.01961705986130644 [0.55s]\n",
      "epoch 136 [17.6s]:  training loss=0.6780681014060974                                      \n",
      "epoch 137 [17.13s]:  training loss=0.6754732728004456                                     \n",
      "epoch 138 [16.58s]:  training loss=0.670760989189148                                      \n",
      "epoch 139 [17.14s]:  training loss=0.6704532504081726                                     \n",
      "epoch 140 [17.35s]: training loss=0.6662464141845703  validation ndcg@10=0.019444939405177303 [0.52s]\n",
      "epoch 141 [17.16s]:  training loss=0.6609323620796204                                     \n",
      "epoch 142 [16.64s]:  training loss=0.6622962355613708                                     \n",
      "epoch 143 [16.86s]:  training loss=0.6603946685791016                                     \n",
      "epoch 144 [17.21s]:  training loss=0.6557557582855225                                     \n",
      "epoch 145 [16.79s]: training loss=0.6524828672409058  validation ndcg@10=0.01979218401560415 [0.52s]\n",
      "epoch 146 [17.12s]:  training loss=0.6559851169586182                                     \n",
      "epoch 147 [17.09s]:  training loss=0.651665985584259                                      \n",
      "epoch 148 [16.98s]:  training loss=0.650815486907959                                      \n",
      "epoch 149 [16.39s]:  training loss=0.6464818716049194                                     \n",
      "epoch 150 [16.8s]: training loss=0.6495012640953064  validation ndcg@10=0.019657625899412747 [0.51s]\n",
      "epoch 151 [17.2s]:  training loss=0.6465886235237122                                      \n",
      "epoch 152 [17.4s]:  training loss=0.6393963694572449                                      \n",
      "epoch 153 [16.56s]:  training loss=0.6419795155525208                                     \n",
      "epoch 154 [17.02s]:  training loss=0.6383897662162781                                     \n",
      "epoch 155 [16.4s]: training loss=0.6407623887062073  validation ndcg@10=0.019731873377645738 [0.54s]\n",
      "epoch 156 [16.8s]:  training loss=0.6373081803321838                                      \n",
      "epoch 157 [16.66s]:  training loss=0.6336793899536133                                     \n",
      "epoch 158 [16.82s]:  training loss=0.6338344812393188                                     \n",
      "epoch 159 [17.1s]:  training loss=0.6349571943283081                                      \n",
      "epoch 160 [17.1s]: training loss=0.6334534883499146  validation ndcg@10=0.019753189394522246 [0.55s]\n",
      "epoch 161 [16.77s]:  training loss=0.628376305103302                                      \n",
      "epoch 162 [16.78s]:  training loss=0.6297096014022827                                     \n",
      "epoch 163 [16.84s]:  training loss=0.6301254630088806                                     \n",
      "epoch 164 [16.49s]:  training loss=0.626478910446167                                      \n",
      "epoch 165 [16.76s]: training loss=0.6250092387199402  validation ndcg@10=0.02016881406545318 [0.49s]\n",
      "epoch 166 [16.62s]:  training loss=0.6230283379554749                                     \n",
      "epoch 167 [16.45s]:  training loss=0.6273710131645203                                     \n",
      "epoch 168 [16.86s]:  training loss=0.6261494755744934                                     \n",
      "epoch 169 [16.68s]:  training loss=0.6216320395469666                                     \n",
      "epoch 170 [16.61s]: training loss=0.6194542050361633  validation ndcg@10=0.020054096942371827 [0.52s]\n",
      "epoch 171 [16.73s]:  training loss=0.6211777925491333                                     \n",
      "epoch 172 [16.54s]:  training loss=0.6190755367279053                                     \n",
      "epoch 173 [16.87s]:  training loss=0.6182766556739807                                     \n",
      "epoch 174 [16.74s]:  training loss=0.6200789213180542                                     \n",
      "epoch 175 [17.05s]: training loss=0.6164596080780029  validation ndcg@10=0.02032667905754266 [0.5s]\n",
      "epoch 176 [16.78s]:  training loss=0.6185729503631592                                     \n",
      "epoch 177 [17.99s]:  training loss=0.6138032674789429                                     \n",
      "epoch 178 [17.31s]:  training loss=0.6109041571617126                                     \n",
      "epoch 179 [17.04s]:  training loss=0.6147526502609253                                     \n",
      "epoch 180 [16.64s]: training loss=0.610393226146698  validation ndcg@10=0.02056112761687369 [0.53s]\n",
      "epoch 181 [16.49s]:  training loss=0.6117523908615112                                     \n",
      "epoch 182 [16.88s]:  training loss=0.6162264943122864                                     \n",
      "epoch 183 [16.32s]:  training loss=0.6127446889877319                                     \n",
      "epoch 184 [17.04s]:  training loss=0.6115041971206665                                     \n",
      "epoch 185 [16.56s]: training loss=0.6109838485717773  validation ndcg@10=0.020449917640466532 [0.53s]\n",
      "epoch 186 [16.74s]:  training loss=0.6131775379180908                                     \n",
      "epoch 187 [17.02s]:  training loss=0.6059231162071228                                     \n",
      "epoch 188 [16.7s]:  training loss=0.6082584261894226                                      \n",
      "epoch 189 [16.76s]:  training loss=0.607215404510498                                      \n",
      "epoch 190 [16.6s]: training loss=0.606892466545105  validation ndcg@10=0.020483146212824436 [0.49s]\n",
      "epoch 191 [16.66s]:  training loss=0.6055812835693359                                     \n",
      "epoch 192 [16.69s]:  training loss=0.6056333780288696                                     \n",
      "epoch 193 [16.47s]:  training loss=0.6029449105262756                                     \n",
      "epoch 194 [16.51s]:  training loss=0.6026545763015747                                     \n",
      "epoch 195 [16.65s]: training loss=0.6038293242454529  validation ndcg@10=0.020557034461284582 [0.54s]\n",
      "epoch 196 [16.5s]:  training loss=0.5968387126922607                                      \n",
      "epoch 197 [16.94s]:  training loss=0.6002857089042664                                     \n",
      "epoch 198 [16.66s]:  training loss=0.5969282388687134                                     \n",
      "epoch 199 [17.31s]:  training loss=0.5989546179771423                                     \n",
      "epoch 200 [16.76s]: training loss=0.5969048738479614  validation ndcg@10=0.020474181871466088 [0.5s]\n",
      "epoch 1 [17.78s]:  training loss=0.8013378381729126                                       \n",
      "epoch 2 [17.59s]:  training loss=0.7682117223739624                                       \n",
      "epoch 3 [17.55s]:  training loss=0.7354421019554138                                       \n",
      "epoch 4 [17.55s]:  training loss=0.6976471543312073                                       \n",
      "epoch 5 [17.1s]: training loss=0.6069368124008179  validation ndcg@10=0.02005808120950663 [0.55s]\n",
      "epoch 6 [17.52s]:  training loss=0.5587786436080933                                       \n",
      "epoch 7 [17.02s]:  training loss=0.532389760017395                                        \n",
      "epoch 8 [17.33s]:  training loss=0.5067465305328369                                       \n",
      "epoch 9 [16.98s]:  training loss=0.4842204749584198                                       \n",
      "epoch 10 [17.93s]: training loss=0.45989683270454407  validation ndcg@10=0.019713953727902505 [0.53s]\n",
      "epoch 11 [17.16s]:  training loss=0.44252556562423706                                     \n",
      "epoch 12 [17.11s]:  training loss=0.4230164885520935                                      \n",
      "epoch 13 [17.62s]:  training loss=0.40354353189468384                                     \n",
      "epoch 14 [17.1s]:  training loss=0.3796401619911194                                       \n",
      "epoch 15 [17.42s]: training loss=0.36140087246894836  validation ndcg@10=0.02204256780765753 [0.52s]\n",
      "epoch 16 [17.03s]:  training loss=0.33980628848075867                                     \n",
      "epoch 17 [16.85s]:  training loss=0.32097530364990234                                     \n",
      "epoch 18 [17.62s]:  training loss=0.3052186965942383                                      \n",
      "epoch 19 [17.39s]:  training loss=0.29189416766166687                                     \n",
      "epoch 20 [17.11s]: training loss=0.28141993284225464  validation ndcg@10=0.023158102377902893 [0.53s]\n",
      "epoch 21 [17.36s]:  training loss=0.26678648591041565                                     \n",
      "epoch 22 [17.63s]:  training loss=0.25557175278663635                                     \n",
      "epoch 23 [17.23s]:  training loss=0.24511517584323883                                     \n",
      "epoch 24 [18.61s]:  training loss=0.23503808677196503                                     \n",
      "epoch 25 [17.84s]: training loss=0.22463272511959076  validation ndcg@10=0.02427272302910025 [0.5s]\n",
      "epoch 26 [17.3s]:  training loss=0.21469366550445557                                      \n",
      "epoch 27 [17.02s]:  training loss=0.2069886177778244                                      \n",
      "epoch 28 [17.59s]:  training loss=0.20148004591464996                                     \n",
      "epoch 29 [17.1s]:  training loss=0.1932559311389923                                       \n",
      "epoch 30 [16.99s]: training loss=0.1853812336921692  validation ndcg@10=0.024550469056133593 [0.51s]\n",
      "epoch 31 [17.29s]:  training loss=0.17752788960933685                                     \n",
      "epoch 32 [17.28s]:  training loss=0.17289477586746216                                     \n",
      "epoch 33 [17.64s]:  training loss=0.16505417227745056                                     \n",
      "epoch 34 [17.16s]:  training loss=0.16063512861728668                                     \n",
      "epoch 35 [16.73s]: training loss=0.15631262958049774  validation ndcg@10=0.024251301032893267 [0.55s]\n",
      "epoch 36 [17.14s]:  training loss=0.1509060263633728                                      \n",
      "epoch 37 [17.12s]:  training loss=0.145377978682518                                       \n",
      "epoch 38 [17.02s]:  training loss=0.14146701991558075                                     \n",
      "epoch 39 [17.24s]:  training loss=0.13581015169620514                                     \n",
      "epoch 40 [17.45s]: training loss=0.13152022659778595  validation ndcg@10=0.025507394932570055 [0.52s]\n",
      "epoch 41 [17.99s]:  training loss=0.1274135410785675                                      \n",
      "epoch 42 [17.0s]:  training loss=0.1215893104672432                                       \n",
      "epoch 43 [17.3s]:  training loss=0.12121149897575378                                      \n",
      "epoch 44 [17.54s]:  training loss=0.11789429932832718                                     \n",
      "epoch 45 [17.17s]: training loss=0.11107893288135529  validation ndcg@10=0.02591546227231006 [0.51s]\n",
      "epoch 46 [17.59s]:  training loss=0.10950426012277603                                     \n",
      "epoch 47 [17.54s]:  training loss=0.10397647321224213                                     \n",
      "epoch 48 [16.87s]:  training loss=0.10312633961439133                                     \n",
      "epoch 49 [17.17s]:  training loss=0.10122545063495636                                     \n",
      "epoch 50 [17.36s]: training loss=0.09840387105941772  validation ndcg@10=0.026066820326721158 [0.53s]\n",
      "epoch 51 [17.54s]:  training loss=0.09742821753025055                                     \n",
      "epoch 52 [17.27s]:  training loss=0.09300439059734344                                     \n",
      "epoch 53 [17.79s]:  training loss=0.0943564772605896                                      \n",
      "epoch 54 [17.06s]:  training loss=0.09089912474155426                                     \n",
      "epoch 55 [17.76s]: training loss=0.08771155774593353  validation ndcg@10=0.027077776954266886 [0.54s]\n",
      "epoch 56 [17.21s]:  training loss=0.08478710800409317                                     \n",
      "epoch 57 [17.14s]:  training loss=0.08419535309076309                                     \n",
      "epoch 58 [17.36s]:  training loss=0.08189544081687927                                     \n",
      "epoch 59 [17.14s]:  training loss=0.08226991444826126                                     \n",
      "epoch 60 [17.18s]: training loss=0.08130677044391632  validation ndcg@10=0.02670803553882518 [0.55s]\n",
      "epoch 61 [17.3s]:  training loss=0.07819534838199615                                      \n",
      "epoch 62 [17.02s]:  training loss=0.07757743448019028                                     \n",
      "epoch 63 [17.29s]:  training loss=0.07526646554470062                                     \n",
      "epoch 64 [17.07s]:  training loss=0.0760379508137703                                      \n",
      "epoch 65 [17.17s]: training loss=0.07288838177919388  validation ndcg@10=0.027757457394389333 [0.56s]\n",
      "epoch 66 [17.49s]:  training loss=0.07069191336631775                                     \n",
      "epoch 67 [17.36s]:  training loss=0.07083815336227417                                     \n",
      "epoch 68 [17.45s]:  training loss=0.07096168398857117                                     \n",
      "epoch 69 [18.79s]:  training loss=0.06554032117128372                                     \n",
      "epoch 70 [16.91s]: training loss=0.06627517193555832  validation ndcg@10=0.026971768463747218 [0.52s]\n",
      "epoch 71 [17.09s]:  training loss=0.06567904353141785                                     \n",
      "epoch 72 [17.53s]:  training loss=0.06387326866388321                                     \n",
      "epoch 73 [17.48s]:  training loss=0.0649784654378891                                      \n",
      "epoch 74 [17.23s]:  training loss=0.0634012296795845                                      \n",
      "epoch 75 [17.44s]: training loss=0.06170009821653366  validation ndcg@10=0.027496046550106915 [0.51s]\n",
      "epoch 76 [17.28s]:  training loss=0.06004536151885986                                     \n",
      "epoch 77 [17.65s]:  training loss=0.060089532285928726                                    \n",
      "epoch 78 [17.13s]:  training loss=0.060421500355005264                                    \n",
      "epoch 79 [16.91s]:  training loss=0.05839002504944801                                     \n",
      "epoch 80 [17.2s]: training loss=0.057905975729227066  validation ndcg@10=0.027709616967116203 [0.53s]\n",
      "epoch 81 [17.6s]:  training loss=0.05688441917300224                                      \n",
      "epoch 82 [17.43s]:  training loss=0.05582872033119202                                     \n",
      "epoch 83 [16.97s]:  training loss=0.0562095046043396                                      \n",
      "epoch 84 [17.52s]:  training loss=0.054148588329553604                                    \n",
      "epoch 85 [17.04s]: training loss=0.05384950339794159  validation ndcg@10=0.027788588301660436 [0.54s]\n",
      "epoch 86 [17.54s]:  training loss=0.05251273512840271                                     \n",
      "epoch 87 [17.59s]:  training loss=0.051899366080760956                                    \n",
      "epoch 88 [17.23s]:  training loss=0.05152968689799309                                     \n",
      "epoch 89 [17.26s]:  training loss=0.05256333202123642                                     \n",
      "epoch 90 [17.43s]: training loss=0.050271086394786835  validation ndcg@10=0.02815619079678832 [0.53s]\n",
      "epoch 91 [16.75s]:  training loss=0.051255401223897934                                    \n",
      "epoch 92 [17.59s]:  training loss=0.0491320863366127                                      \n",
      "epoch 93 [17.28s]:  training loss=0.048604484647512436                                    \n",
      "epoch 94 [16.98s]:  training loss=0.048640381544828415                                    \n",
      "epoch 95 [17.0s]: training loss=0.04806043580174446  validation ndcg@10=0.028762214100549317 [0.52s]\n",
      "epoch 96 [17.22s]:  training loss=0.04688601195812225                                     \n",
      "epoch 97 [17.5s]:  training loss=0.047235533595085144                                     \n",
      "epoch 98 [17.03s]:  training loss=0.044664110988378525                                    \n",
      "epoch 99 [17.42s]:  training loss=0.04620935767889023                                     \n",
      "epoch 100 [17.3s]: training loss=0.0450730100274086  validation ndcg@10=0.02809959320224947 [0.52s]\n",
      "epoch 101 [17.41s]:  training loss=0.04628390818834305                                    \n",
      "epoch 102 [17.15s]:  training loss=0.04362751170992851                                    \n",
      "epoch 103 [17.23s]:  training loss=0.044553037732839584                                   \n",
      "epoch 104 [17.62s]:  training loss=0.043684445321559906                                   \n",
      "epoch 105 [17.45s]: training loss=0.04370800405740738  validation ndcg@10=0.028581827232752714 [0.52s]\n",
      "epoch 106 [17.32s]:  training loss=0.043655138462781906                                   \n",
      "epoch 107 [16.86s]:  training loss=0.04379847273230553                                    \n",
      "epoch 108 [17.14s]:  training loss=0.04252226650714874                                    \n",
      "epoch 109 [17.32s]:  training loss=0.040839891880750656                                   \n",
      "epoch 110 [17.45s]: training loss=0.04181617125868797  validation ndcg@10=0.028285392011444883 [0.53s]\n",
      "epoch 111 [17.43s]:  training loss=0.04292066767811775                                    \n",
      "epoch 112 [17.63s]:  training loss=0.04234327748417854                                    \n",
      "epoch 113 [17.53s]:  training loss=0.04032128304243088                                    \n",
      "epoch 114 [19.22s]:  training loss=0.04043262451887131                                    \n",
      "epoch 115 [15.71s]: training loss=0.04039138928055763  validation ndcg@10=0.028120292839016462 [0.46s]\n",
      "epoch 116 [16.76s]:  training loss=0.04090341925621033                                    \n",
      "epoch 117 [17.44s]:  training loss=0.03951621800661087                                    \n",
      "epoch 118 [16.78s]:  training loss=0.04270283132791519                                    \n",
      "epoch 119 [16.99s]:  training loss=0.039031509310007095                                   \n",
      "epoch 120 [17.59s]: training loss=0.03810847923159599  validation ndcg@10=0.027917291401075028 [0.54s]\n",
      "epoch 1 [17.46s]:  training loss=0.8058041930198669                                        \n",
      "epoch 2 [17.33s]:  training loss=0.7811818718910217                                        \n",
      "epoch 3 [17.94s]:  training loss=0.7690808176994324                                        \n",
      "epoch 4 [17.23s]:  training loss=0.7491250038146973                                        \n",
      "epoch 5 [17.45s]: training loss=0.7293926477432251  validation ndcg@10=0.02047163886805407 [0.52s]\n",
      "epoch 6 [17.82s]:  training loss=0.704677402973175                                         \n",
      "epoch 7 [17.95s]:  training loss=0.6397826671600342                                        \n",
      "epoch 8 [17.78s]:  training loss=0.6018164753913879                                        \n",
      "epoch 9 [17.63s]:  training loss=0.5797432065010071                                        \n",
      "epoch 10 [18.41s]: training loss=0.5639325380325317  validation ndcg@10=0.020044327148729408 [0.56s]\n",
      "epoch 11 [17.79s]:  training loss=0.5437210202217102                                       \n",
      "epoch 12 [17.86s]:  training loss=0.5361850261688232                                       \n",
      "epoch 13 [17.7s]:  training loss=0.5218944549560547                                        \n",
      "epoch 14 [17.66s]:  training loss=0.5029390454292297                                       \n",
      "epoch 15 [18.01s]: training loss=0.49108296632766724  validation ndcg@10=0.02025949524206046 [0.52s]\n",
      "epoch 16 [18.26s]:  training loss=0.47920748591423035                                      \n",
      "epoch 17 [17.92s]:  training loss=0.4709073007106781                                       \n",
      "epoch 18 [17.92s]:  training loss=0.45791858434677124                                      \n",
      "epoch 19 [18.3s]:  training loss=0.44130849838256836                                       \n",
      "epoch 20 [17.99s]: training loss=0.4320244789123535  validation ndcg@10=0.02071127579601688 [0.52s]\n",
      "epoch 21 [18.13s]:  training loss=0.4187312424182892                                       \n",
      "epoch 22 [17.65s]:  training loss=0.40493330359458923                                      \n",
      "epoch 23 [17.6s]:  training loss=0.3926059901714325                                        \n",
      "epoch 24 [17.79s]:  training loss=0.38344690203666687                                      \n",
      "epoch 25 [17.75s]: training loss=0.3714168667793274  validation ndcg@10=0.021983367498379206 [0.54s]\n",
      "epoch 26 [17.8s]:  training loss=0.3617105484008789                                        \n",
      "epoch 27 [17.89s]:  training loss=0.35125604271888733                                      \n",
      "epoch 28 [17.31s]:  training loss=0.3391807973384857                                       \n",
      "epoch 29 [18.11s]:  training loss=0.334728866815567                                        \n",
      "epoch 30 [17.9s]: training loss=0.3227267563343048  validation ndcg@10=0.023372794842385622 [0.54s]\n",
      "epoch 31 [18.22s]:  training loss=0.31528186798095703                                      \n",
      "epoch 32 [18.24s]:  training loss=0.30953118205070496                                      \n",
      "epoch 33 [18.15s]:  training loss=0.30335232615470886                                      \n",
      "epoch 34 [18.25s]:  training loss=0.2926456928253174                                       \n",
      "epoch 35 [17.68s]: training loss=0.28398674726486206  validation ndcg@10=0.02411861824833617 [0.52s]\n",
      "epoch 36 [18.74s]:  training loss=0.2828463613986969                                       \n",
      "epoch 37 [17.88s]:  training loss=0.2726004719734192                                       \n",
      "epoch 38 [17.57s]:  training loss=0.26372846961021423                                      \n",
      "epoch 39 [17.9s]:  training loss=0.26097264885902405                                       \n",
      "epoch 40 [17.96s]: training loss=0.2549108862876892  validation ndcg@10=0.024243031845770584 [0.53s]\n",
      "epoch 41 [17.68s]:  training loss=0.2454758882522583                                       \n",
      "epoch 42 [17.51s]:  training loss=0.2417340874671936                                       \n",
      "epoch 43 [17.7s]:  training loss=0.2332008183002472                                        \n",
      "epoch 44 [18.23s]:  training loss=0.23047484457492828                                      \n",
      "epoch 45 [18.0s]: training loss=0.22045308351516724  validation ndcg@10=0.02486188265007571 [0.53s]\n",
      "epoch 46 [17.94s]:  training loss=0.21776430308818817                                      \n",
      "epoch 47 [17.45s]:  training loss=0.21371321380138397                                      \n",
      "epoch 48 [17.61s]:  training loss=0.20883561670780182                                      \n",
      "epoch 49 [17.73s]:  training loss=0.20369766652584076                                      \n",
      "epoch 50 [17.73s]: training loss=0.19941973686218262  validation ndcg@10=0.024615796218761938 [0.54s]\n",
      "epoch 51 [18.23s]:  training loss=0.1919090896844864                                       \n",
      "epoch 52 [17.86s]:  training loss=0.18896543979644775                                      \n",
      "epoch 53 [17.95s]:  training loss=0.18607626855373383                                      \n",
      "epoch 54 [17.93s]:  training loss=0.1834370493888855                                       \n",
      "epoch 55 [17.76s]: training loss=0.17493173480033875  validation ndcg@10=0.026123662195449 [0.5s]\n",
      "epoch 56 [17.66s]:  training loss=0.17364419996738434                                      \n",
      "epoch 57 [17.78s]:  training loss=0.16720156371593475                                      \n",
      "epoch 58 [17.43s]:  training loss=0.16260340809822083                                      \n",
      "epoch 59 [17.73s]:  training loss=0.1595415472984314                                       \n",
      "epoch 60 [17.64s]: training loss=0.15781401097774506  validation ndcg@10=0.025836116367444435 [0.53s]\n",
      "epoch 61 [17.75s]:  training loss=0.1531667411327362                                       \n",
      "epoch 62 [17.5s]:  training loss=0.15109847486019135                                       \n",
      "epoch 63 [17.67s]:  training loss=0.1467256247997284                                       \n",
      "epoch 64 [17.72s]:  training loss=0.1434699147939682                                       \n",
      "epoch 65 [17.89s]: training loss=0.14090995490550995  validation ndcg@10=0.02544042836459689 [0.5s]\n",
      "epoch 66 [17.66s]:  training loss=0.13960005342960358                                      \n",
      "epoch 67 [17.92s]:  training loss=0.13527356088161469                                      \n",
      "epoch 68 [17.89s]:  training loss=0.13183574378490448                                      \n",
      "epoch 69 [18.39s]:  training loss=0.1295345574617386                                       \n",
      "epoch 70 [17.93s]: training loss=0.12930607795715332  validation ndcg@10=0.025800685582926808 [0.52s]\n",
      "epoch 71 [17.74s]:  training loss=0.12695711851119995                                      \n",
      "epoch 72 [17.81s]:  training loss=0.1250501126050949                                       \n",
      "epoch 73 [17.93s]:  training loss=0.12252768129110336                                      \n",
      "epoch 74 [18.02s]:  training loss=0.12091246247291565                                      \n",
      "epoch 75 [18.52s]: training loss=0.11790847778320312  validation ndcg@10=0.026367163687074494 [0.62s]\n",
      "epoch 76 [19.01s]:  training loss=0.11512133479118347                                      \n",
      "epoch 77 [17.98s]:  training loss=0.11096963286399841                                      \n",
      "epoch 78 [17.87s]:  training loss=0.11249014735221863                                      \n",
      "epoch 79 [18.01s]:  training loss=0.11104931682348251                                      \n",
      "epoch 80 [17.38s]: training loss=0.10850159078836441  validation ndcg@10=0.026633817055907588 [0.52s]\n",
      "epoch 81 [17.5s]:  training loss=0.10804815590381622                                       \n",
      "epoch 82 [17.69s]:  training loss=0.10495907813310623                                      \n",
      "epoch 83 [17.92s]:  training loss=0.10435900837182999                                      \n",
      "epoch 84 [17.47s]:  training loss=0.101896271109581                                        \n",
      "epoch 85 [17.46s]: training loss=0.10272090137004852  validation ndcg@10=0.02686890632996475 [0.54s]\n",
      "epoch 86 [17.66s]:  training loss=0.0990479588508606                                       \n",
      "epoch 87 [17.68s]:  training loss=0.09887600690126419                                      \n",
      "epoch 88 [17.78s]:  training loss=0.09530609846115112                                      \n",
      "epoch 89 [17.71s]:  training loss=0.094213105738163                                        \n",
      "epoch 90 [17.4s]: training loss=0.09552858024835587  validation ndcg@10=0.026766128780913195 [0.52s]\n",
      "epoch 91 [17.49s]:  training loss=0.09123566001653671                                      \n",
      "epoch 92 [17.88s]:  training loss=0.09149366617202759                                      \n",
      "epoch 93 [17.66s]:  training loss=0.09004750102758408                                      \n",
      "epoch 94 [17.76s]:  training loss=0.09048299491405487                                      \n",
      "epoch 95 [18.19s]: training loss=0.08686992526054382  validation ndcg@10=0.02720255233489581 [0.51s]\n",
      "epoch 96 [18.17s]:  training loss=0.08475787937641144                                      \n",
      "epoch 97 [18.01s]:  training loss=0.08258606493473053                                      \n",
      "epoch 98 [18.09s]:  training loss=0.08267850428819656                                      \n",
      "epoch 99 [17.97s]:  training loss=0.08462566882371902                                      \n",
      "epoch 100 [17.75s]: training loss=0.08063877373933792  validation ndcg@10=0.02700805464463942 [0.5s]\n",
      "epoch 101 [17.88s]:  training loss=0.07853396981954575                                     \n",
      "epoch 102 [18.19s]:  training loss=0.08364813029766083                                     \n",
      "epoch 103 [18.09s]:  training loss=0.07895860821008682                                     \n",
      "epoch 104 [17.75s]:  training loss=0.07838525623083115                                     \n",
      "epoch 105 [17.9s]: training loss=0.07927929610013962  validation ndcg@10=0.026964900231030074 [0.53s]\n",
      "epoch 106 [17.72s]:  training loss=0.07577738165855408                                     \n",
      "epoch 107 [17.71s]:  training loss=0.07579532265663147                                     \n",
      "epoch 108 [17.67s]:  training loss=0.07598859071731567                                     \n",
      "epoch 109 [17.85s]:  training loss=0.07344796508550644                                     \n",
      "epoch 110 [17.82s]: training loss=0.07410924136638641  validation ndcg@10=0.02709267215812394 [0.53s]\n",
      "epoch 111 [17.55s]:  training loss=0.071134552359581                                       \n",
      "epoch 112 [17.79s]:  training loss=0.0711042508482933                                      \n",
      "epoch 113 [17.63s]:  training loss=0.07069391757249832                                     \n",
      "epoch 114 [17.82s]:  training loss=0.06927242875099182                                     \n",
      "epoch 115 [18.18s]: training loss=0.06804189085960388  validation ndcg@10=0.027092895971945932 [0.56s]\n",
      "epoch 116 [19.12s]:  training loss=0.06803605705499649                                     \n",
      "epoch 117 [17.27s]:  training loss=0.06604549288749695                                     \n",
      "epoch 118 [17.42s]:  training loss=0.06729499250650406                                     \n",
      "epoch 119 [18.03s]:  training loss=0.06615357100963593                                     \n",
      "epoch 120 [17.21s]: training loss=0.0654328316450119  validation ndcg@10=0.02732932934489132 [0.5s]\n",
      "epoch 121 [17.43s]:  training loss=0.06465746462345123                                     \n",
      "epoch 122 [17.54s]:  training loss=0.06343722343444824                                     \n",
      "epoch 123 [17.88s]:  training loss=0.061257243156433105                                    \n",
      "epoch 124 [17.97s]:  training loss=0.06268177926540375                                     \n",
      "epoch 125 [17.3s]: training loss=0.06129185110330582  validation ndcg@10=0.027645139374311806 [0.51s]\n",
      "epoch 126 [17.76s]:  training loss=0.061038218438625336                                    \n",
      "epoch 127 [17.74s]:  training loss=0.06013906002044678                                     \n",
      "epoch 128 [17.93s]:  training loss=0.061290692538022995                                    \n",
      "epoch 129 [18.28s]:  training loss=0.05994999408721924                                     \n",
      "epoch 130 [18.51s]: training loss=0.05858771502971649  validation ndcg@10=0.028205371493743872 [0.52s]\n",
      "epoch 131 [17.61s]:  training loss=0.058477237820625305                                    \n",
      "epoch 132 [17.74s]:  training loss=0.05797383189201355                                     \n",
      "epoch 133 [18.06s]:  training loss=0.05874691903591156                                     \n",
      "epoch 134 [18.04s]:  training loss=0.05722039192914963                                     \n",
      "epoch 135 [17.9s]: training loss=0.056715354323387146  validation ndcg@10=0.028420003077108163 [0.55s]\n",
      "epoch 136 [18.26s]:  training loss=0.05446174368262291                                     \n",
      "epoch 137 [17.97s]:  training loss=0.05481555312871933                                     \n",
      "epoch 138 [17.74s]:  training loss=0.055775512009859085                                    \n",
      "epoch 139 [17.79s]:  training loss=0.055173639208078384                                    \n",
      "epoch 140 [17.55s]: training loss=0.05393905192613602  validation ndcg@10=0.02889881617403063 [0.53s]\n",
      "epoch 141 [17.64s]:  training loss=0.053474679589271545                                    \n",
      "epoch 142 [17.89s]:  training loss=0.05236786603927612                                     \n",
      "epoch 143 [17.67s]:  training loss=0.05396129935979843                                     \n",
      "epoch 144 [17.6s]:  training loss=0.052029289305210114                                     \n",
      "epoch 145 [18.1s]: training loss=0.05090653896331787  validation ndcg@10=0.028403662459874343 [0.53s]\n",
      "epoch 146 [17.24s]:  training loss=0.05390778183937073                                     \n",
      "epoch 147 [17.9s]:  training loss=0.049825988709926605                                     \n",
      "epoch 148 [17.76s]:  training loss=0.04969412460923195                                     \n",
      "epoch 149 [17.86s]:  training loss=0.050549447536468506                                    \n",
      "epoch 150 [17.87s]: training loss=0.04958787187933922  validation ndcg@10=0.02876390346799829 [0.56s]\n",
      "epoch 151 [17.85s]:  training loss=0.05016243830323219                                     \n",
      "epoch 152 [17.6s]:  training loss=0.04815322533249855                                      \n",
      "epoch 153 [17.86s]:  training loss=0.04905132204294205                                     \n",
      "epoch 154 [17.44s]:  training loss=0.049972645938396454                                    \n",
      "epoch 155 [18.41s]: training loss=0.04859289899468422  validation ndcg@10=0.028404240966688466 [0.54s]\n",
      "epoch 156 [18.46s]:  training loss=0.04876141622662544                                     \n",
      "epoch 157 [18.97s]:  training loss=0.04795988276600838                                     \n",
      "epoch 158 [17.72s]:  training loss=0.04641902446746826                                     \n",
      "epoch 159 [17.85s]:  training loss=0.04833916947245598                                     \n",
      "epoch 160 [17.53s]: training loss=0.04723836109042168  validation ndcg@10=0.028246969724019872 [0.52s]\n",
      "epoch 161 [17.64s]:  training loss=0.04651594161987305                                     \n",
      "epoch 162 [17.78s]:  training loss=0.04733239486813545                                     \n",
      "epoch 163 [17.67s]:  training loss=0.0459667444229126                                      \n",
      "epoch 164 [17.5s]:  training loss=0.047104883939027786                                     \n",
      "epoch 165 [17.58s]: training loss=0.046009670943021774  validation ndcg@10=0.028384165365974975 [0.53s]\n",
      "epoch 1 [49.26s]:  training loss=0.7936801314353943                                        \n",
      "epoch 2 [51.2s]:  training loss=0.7531701326370239                                        \n",
      "epoch 3 [51.52s]:  training loss=0.698809802532196                                        \n",
      "epoch 4 [51.64s]:  training loss=0.5946692824363708                                       \n",
      "epoch 5 [51.72s]: training loss=0.5468246340751648  validation ndcg@10=0.019935471955703183 [1.08s]\n",
      "epoch 6 [50.07s]:  training loss=0.5161370038986206                                       \n",
      "epoch 7 [50.8s]:  training loss=0.48749274015426636                                       \n",
      "epoch 8 [50.1s]:  training loss=0.45522910356521606                                       \n",
      "epoch 9 [50.95s]:  training loss=0.4302169382572174                                       \n",
      "epoch 10 [50.52s]: training loss=0.40418046712875366  validation ndcg@10=0.021885005385473645 [1.09s]\n",
      "epoch 11 [51.22s]:  training loss=0.3752162754535675                                      \n",
      "epoch 12 [52.64s]:  training loss=0.34644725918769836                                     \n",
      "epoch 13 [50.25s]:  training loss=0.32246220111846924                                     \n",
      "epoch 14 [51.62s]:  training loss=0.3023442327976227                                      \n",
      "epoch 15 [50.83s]: training loss=0.2830589711666107  validation ndcg@10=0.023227051552959 [1.09s]\n",
      "epoch 16 [50.2s]:  training loss=0.2633722722530365                                       \n",
      "epoch 17 [50.34s]:  training loss=0.24776232242584229                                     \n",
      "epoch 18 [51.51s]:  training loss=0.2319122850894928                                      \n",
      "epoch 19 [51.78s]:  training loss=0.22187107801437378                                     \n",
      "epoch 20 [51.21s]: training loss=0.20857828855514526  validation ndcg@10=0.024887378786458712 [1.07s]\n",
      "epoch 21 [51.64s]:  training loss=0.19945043325424194                                     \n",
      "epoch 22 [49.8s]:  training loss=0.18782447278499603                                      \n",
      "epoch 23 [51.1s]:  training loss=0.18012982606887817                                      \n",
      "epoch 24 [50.3s]:  training loss=0.168247252702713                                        \n",
      "epoch 25 [51.02s]: training loss=0.16149075329303741  validation ndcg@10=0.02490379372034423 [1.07s]\n",
      "epoch 26 [50.94s]:  training loss=0.15473565459251404                                     \n",
      "epoch 27 [50.79s]:  training loss=0.14605934917926788                                     \n",
      "epoch 28 [51.43s]:  training loss=0.1418973058462143                                      \n",
      "epoch 29 [49.86s]:  training loss=0.13310515880584717                                     \n",
      "epoch 30 [51.46s]: training loss=0.12772010266780853  validation ndcg@10=0.024051348969381853 [1.11s]\n",
      "epoch 31 [50.1s]:  training loss=0.1203092634677887                                       \n",
      "epoch 32 [51.26s]:  training loss=0.11567813158035278                                     \n",
      "epoch 33 [49.81s]:  training loss=0.11562419682741165                                     \n",
      "epoch 34 [50.95s]:  training loss=0.11242735385894775                                     \n",
      "epoch 35 [50.25s]: training loss=0.10398785769939423  validation ndcg@10=0.025827187823967205 [1.11s]\n",
      "epoch 36 [51.03s]:  training loss=0.10403341799974442                                     \n",
      "epoch 37 [49.82s]:  training loss=0.09833157062530518                                     \n",
      "epoch 38 [50.87s]:  training loss=0.09572989493608475                                     \n",
      "epoch 39 [50.44s]:  training loss=0.09115145355463028                                     \n",
      "epoch 40 [52.68s]: training loss=0.08984003961086273  validation ndcg@10=0.02627045632105092 [0.98s]\n",
      "epoch 41 [50.35s]:  training loss=0.08531239628791809                                     \n",
      "epoch 42 [50.43s]:  training loss=0.08474262058734894                                      \n",
      "epoch 43 [51.94s]:  training loss=0.08276872336864471                                      \n",
      "epoch 44 [51.12s]:  training loss=0.08103416115045547                                      \n",
      "epoch 45 [49.95s]: training loss=0.07606613636016846  validation ndcg@10=0.02626487531555172 [1.12s]\n",
      "epoch 46 [50.37s]:  training loss=0.07468222826719284                                      \n",
      "epoch 47 [50.15s]:  training loss=0.07220745831727982                                      \n",
      "epoch 48 [51.01s]:  training loss=0.07235807925462723                                      \n",
      "epoch 49 [50.12s]:  training loss=0.06952089816331863                                      \n",
      "epoch 50 [50.99s]: training loss=0.06855032593011856  validation ndcg@10=0.02743515941797763 [1.07s]\n",
      "epoch 51 [50.38s]:  training loss=0.07000312954187393                                      \n",
      "epoch 52 [51.1s]:  training loss=0.06979324668645859                                       \n",
      "epoch 53 [49.72s]:  training loss=0.06494011729955673                                      \n",
      "epoch 54 [51.07s]:  training loss=0.06433531641960144                                      \n",
      "epoch 55 [52.84s]: training loss=0.06109689921140671  validation ndcg@10=0.02740087602682181 [1.12s]\n",
      "epoch 56 [49.92s]:  training loss=0.06125100329518318                                      \n",
      "epoch 57 [51.65s]:  training loss=0.05804689601063728                                      \n",
      "epoch 58 [51.0s]:  training loss=0.05682901293039322                                       \n",
      "epoch 59 [51.07s]:  training loss=0.05658715218305588                                      \n",
      "epoch 60 [51.47s]: training loss=0.05577104166150093  validation ndcg@10=0.027377068155520348 [1.08s]\n",
      "epoch 61 [51.22s]:  training loss=0.05572400614619255                                      \n",
      "epoch 62 [51.92s]:  training loss=0.05194081738591194                                      \n",
      "epoch 63 [49.65s]:  training loss=0.054478880017995834                                     \n",
      "epoch 64 [51.34s]:  training loss=0.05315684527158737                                      \n",
      "epoch 65 [51.67s]: training loss=0.05248174071311951  validation ndcg@10=0.026534084715963593 [1.12s]\n",
      "epoch 66 [51.82s]:  training loss=0.051958661526441574                                     \n",
      "epoch 67 [51.85s]:  training loss=0.051512327045202255                                     \n",
      "epoch 68 [53.31s]:  training loss=0.048602163791656494                                     \n",
      "epoch 69 [51.9s]:  training loss=0.048901211470365524                                      \n",
      "epoch 70 [51.97s]: training loss=0.050016291439533234  validation ndcg@10=0.026692294511798796 [1.09s]\n",
      "epoch 71 [50.02s]:  training loss=0.04884631931781769                                      \n",
      "epoch 72 [51.09s]:  training loss=0.046936292201280594                                     \n",
      "epoch 73 [50.73s]:  training loss=0.047585513442754745                                     \n",
      "epoch 74 [50.61s]:  training loss=0.04551621526479721                                      \n",
      "epoch 75 [51.43s]: training loss=0.0452079139649868  validation ndcg@10=0.026694629563514077 [1.07s]\n",
      "epoch 1 [18.93s]:  training loss=0.8517367839813232                                        \n",
      "epoch 2 [18.29s]:  training loss=0.8348023891448975                                        \n",
      "epoch 3 [17.75s]:  training loss=0.8192852735519409                                        \n",
      "epoch 4 [17.8s]:  training loss=0.8097400665283203                                         \n",
      "epoch 5 [17.82s]: training loss=0.8069589138031006  validation ndcg@10=0.0011169200138494279 [0.54s]\n",
      "epoch 6 [17.25s]:  training loss=0.8036970496177673                                        \n",
      "epoch 7 [17.6s]:  training loss=0.8008911609649658                                         \n",
      "epoch 8 [17.92s]:  training loss=0.7957910299301147                                        \n",
      "epoch 9 [17.42s]:  training loss=0.7975773811340332                                        \n",
      "epoch 10 [17.64s]: training loss=0.7916247844696045  validation ndcg@10=0.000945784607305124 [0.53s]\n",
      "epoch 11 [17.72s]:  training loss=0.7939069867134094                                       \n",
      "epoch 12 [17.61s]:  training loss=0.7892261743545532                                       \n",
      "epoch 13 [17.61s]:  training loss=0.7895777821540833                                       \n",
      "epoch 14 [16.97s]:  training loss=0.787350058555603                                        \n",
      "epoch 15 [17.33s]: training loss=0.78642737865448  validation ndcg@10=0.000914923106590268 [0.52s]\n",
      "epoch 16 [17.19s]:  training loss=0.7854254841804504                                       \n",
      "epoch 17 [17.27s]:  training loss=0.7856143712997437                                       \n",
      "epoch 18 [17.49s]:  training loss=0.7842057347297668                                       \n",
      "epoch 19 [17.77s]:  training loss=0.7808325290679932                                       \n",
      "epoch 20 [17.9s]: training loss=0.7813960313796997  validation ndcg@10=0.001802030204219475 [0.55s]\n",
      "epoch 21 [16.96s]:  training loss=0.7816788554191589                                       \n",
      "epoch 22 [17.63s]:  training loss=0.7806437015533447                                       \n",
      "epoch 23 [17.73s]:  training loss=0.7780998349189758                                       \n",
      "epoch 24 [17.95s]:  training loss=0.7773976922035217                                       \n",
      "epoch 25 [18.05s]: training loss=0.7775382399559021  validation ndcg@10=0.002142352172539865 [0.56s]\n",
      "epoch 26 [17.45s]:  training loss=0.7748593091964722                                       \n",
      "epoch 27 [18.66s]:  training loss=0.7722561359405518                                       \n",
      "epoch 28 [17.66s]:  training loss=0.7713941335678101                                       \n",
      "epoch 29 [17.06s]:  training loss=0.7718785405158997                                       \n",
      "epoch 30 [17.62s]: training loss=0.7697274684906006  validation ndcg@10=0.0026721353583603687 [0.56s]\n",
      "epoch 31 [17.97s]:  training loss=0.766059398651123                                        \n",
      "epoch 32 [17.61s]:  training loss=0.7680814862251282                                       \n",
      "epoch 33 [17.55s]:  training loss=0.7672407627105713                                       \n",
      "epoch 34 [17.48s]:  training loss=0.764808177947998                                        \n",
      "epoch 35 [17.31s]: training loss=0.7642960548400879  validation ndcg@10=0.0041634160550518004 [0.52s]\n",
      "epoch 36 [17.63s]:  training loss=0.7638241052627563                                       \n",
      "epoch 37 [17.6s]:  training loss=0.7637065649032593                                        \n",
      "epoch 38 [17.71s]:  training loss=0.7592563033103943                                       \n",
      "epoch 39 [17.51s]:  training loss=0.7590891718864441                                       \n",
      "epoch 40 [17.58s]: training loss=0.7571502327919006  validation ndcg@10=0.0039298095403570675 [0.57s]\n",
      "epoch 41 [17.79s]:  training loss=0.7544527053833008                                       \n",
      "epoch 42 [17.78s]:  training loss=0.7550625205039978                                       \n",
      "epoch 43 [17.48s]:  training loss=0.7512692213058472                                       \n",
      "epoch 44 [17.68s]:  training loss=0.7492072582244873                                       \n",
      "epoch 45 [17.58s]: training loss=0.7496709227561951  validation ndcg@10=0.00440815444902286 [0.55s]\n",
      "epoch 46 [17.49s]:  training loss=0.7505550384521484                                       \n",
      "epoch 47 [17.68s]:  training loss=0.7491549849510193                                       \n",
      "epoch 48 [17.09s]:  training loss=0.7473785281181335                                       \n",
      "epoch 49 [17.4s]:  training loss=0.7471541166305542                                        \n",
      "epoch 50 [17.62s]: training loss=0.7430528402328491  validation ndcg@10=0.009495780918756292 [0.55s]\n",
      "epoch 51 [17.1s]:  training loss=0.7432695627212524                                        \n",
      "epoch 52 [18.17s]:  training loss=0.7416627407073975                                       \n",
      "epoch 53 [17.84s]:  training loss=0.7431138753890991                                       \n",
      "epoch 54 [17.13s]:  training loss=0.7422307133674622                                       \n",
      "epoch 55 [17.88s]: training loss=0.7368699908256531  validation ndcg@10=0.013407660121927269 [0.56s]\n",
      "epoch 56 [17.48s]:  training loss=0.7351174354553223                                       \n",
      "epoch 57 [17.66s]:  training loss=0.7348647713661194                                       \n",
      "epoch 58 [18.16s]:  training loss=0.732117235660553                                        \n",
      "epoch 59 [17.16s]:  training loss=0.7344738841056824                                       \n",
      "epoch 60 [17.89s]: training loss=0.7304136753082275  validation ndcg@10=0.01548778674858609 [0.53s]\n",
      "epoch 61 [18.27s]:  training loss=0.728981614112854                                        \n",
      "epoch 62 [17.53s]:  training loss=0.7246997356414795                                       \n",
      "epoch 63 [17.86s]:  training loss=0.7226806282997131                                       \n",
      "epoch 64 [17.65s]:  training loss=0.7192084789276123                                       \n",
      "epoch 65 [17.59s]: training loss=0.7161171436309814  validation ndcg@10=0.018749208762093034 [0.54s]\n",
      "epoch 66 [17.52s]:  training loss=0.7171427607536316                                       \n",
      "epoch 67 [18.11s]:  training loss=0.7123479843139648                                       \n",
      "epoch 68 [18.99s]:  training loss=0.7113410830497742                                       \n",
      "epoch 69 [17.58s]:  training loss=0.7064423561096191                                       \n",
      "epoch 70 [17.59s]: training loss=0.7022883296012878  validation ndcg@10=0.01941902357794848 [0.53s]\n",
      "epoch 71 [17.53s]:  training loss=0.699810802936554                                        \n",
      "epoch 72 [17.24s]:  training loss=0.6926522254943848                                       \n",
      "epoch 73 [17.84s]:  training loss=0.6876691579818726                                       \n",
      "epoch 74 [17.6s]:  training loss=0.6838846802711487                                        \n",
      "epoch 75 [16.92s]: training loss=0.675944983959198  validation ndcg@10=0.019475156097354254 [0.54s]\n",
      "epoch 76 [16.79s]:  training loss=0.6698893904685974                                       \n",
      "epoch 77 [17.98s]:  training loss=0.6645144820213318                                       \n",
      "epoch 78 [18.03s]:  training loss=0.6575076580047607                                       \n",
      "epoch 79 [17.55s]:  training loss=0.6558576226234436                                       \n",
      "epoch 80 [17.27s]: training loss=0.648768961429596  validation ndcg@10=0.019233017587475924 [0.55s]\n",
      "epoch 81 [17.68s]:  training loss=0.6456400752067566                                       \n",
      "epoch 82 [17.76s]:  training loss=0.6394675374031067                                       \n",
      "epoch 83 [17.89s]:  training loss=0.6397087574005127                                       \n",
      "epoch 84 [17.27s]:  training loss=0.6353431940078735                                       \n",
      "epoch 85 [17.35s]: training loss=0.6319411396980286  validation ndcg@10=0.019596416343765508 [0.67s]\n",
      "epoch 86 [18.03s]:  training loss=0.6266933679580688                                       \n",
      "epoch 87 [18.07s]:  training loss=0.6253599524497986                                       \n",
      "epoch 88 [17.72s]:  training loss=0.623308539390564                                        \n",
      "epoch 89 [17.49s]:  training loss=0.6219809055328369                                       \n",
      "epoch 90 [17.83s]: training loss=0.6196522116661072  validation ndcg@10=0.019406681242389038 [0.55s]\n",
      "epoch 91 [17.59s]:  training loss=0.6159522533416748                                       \n",
      "epoch 92 [18.21s]:  training loss=0.6169793009757996                                       \n",
      "epoch 93 [17.73s]:  training loss=0.6137844920158386                                       \n",
      "epoch 94 [17.96s]:  training loss=0.6107334494590759                                       \n",
      "epoch 95 [17.42s]: training loss=0.6085203886032104  validation ndcg@10=0.019551222720327763 [0.57s]\n",
      "epoch 96 [17.7s]:  training loss=0.6107896566390991                                        \n",
      "epoch 97 [17.72s]:  training loss=0.6075831055641174                                       \n",
      "epoch 98 [17.95s]:  training loss=0.6080741286277771                                       \n",
      "epoch 99 [17.83s]:  training loss=0.6031355261802673                                       \n",
      "epoch 100 [17.77s]: training loss=0.6040793061256409  validation ndcg@10=0.019766881312574338 [0.57s]\n",
      "epoch 101 [17.35s]:  training loss=0.6036697030067444                                      \n",
      "epoch 102 [17.84s]:  training loss=0.6001843214035034                                      \n",
      "epoch 103 [17.24s]:  training loss=0.6003963947296143                                      \n",
      "epoch 104 [17.82s]:  training loss=0.5993085503578186                                      \n",
      "epoch 105 [17.72s]: training loss=0.5933967232704163  validation ndcg@10=0.019562630415209482 [0.56s]\n",
      "epoch 106 [17.64s]:  training loss=0.5939874053001404                                      \n",
      "epoch 107 [17.74s]:  training loss=0.5929471850395203                                      \n",
      "epoch 108 [18.88s]:  training loss=0.5929359197616577                                      \n",
      "epoch 109 [17.4s]:  training loss=0.5904223322868347                                       \n",
      "epoch 110 [17.55s]: training loss=0.5905293822288513  validation ndcg@10=0.019711178212059928 [0.53s]\n",
      "epoch 111 [17.01s]:  training loss=0.5867953896522522                                      \n",
      "epoch 112 [17.05s]:  training loss=0.5899123549461365                                      \n",
      "epoch 113 [17.48s]:  training loss=0.5824763178825378                                      \n",
      "epoch 114 [17.51s]:  training loss=0.5822585225105286                                      \n",
      "epoch 115 [17.67s]: training loss=0.5832943320274353  validation ndcg@10=0.019472627568866988 [0.54s]\n",
      "epoch 116 [17.49s]:  training loss=0.5779514908790588                                      \n",
      "epoch 117 [17.38s]:  training loss=0.575996994972229                                       \n",
      "epoch 118 [17.75s]:  training loss=0.5777775049209595                                      \n",
      "epoch 119 [17.38s]:  training loss=0.5764867067337036                                      \n",
      "epoch 120 [17.42s]: training loss=0.5768043398857117  validation ndcg@10=0.01941473058386778 [0.53s]\n",
      "epoch 121 [17.98s]:  training loss=0.5753400921821594                                      \n",
      "epoch 122 [17.94s]:  training loss=0.5772255659103394                                      \n",
      "epoch 123 [17.41s]:  training loss=0.5736883282661438                                      \n",
      "epoch 124 [17.36s]:  training loss=0.5728296637535095                                      \n",
      "epoch 125 [17.17s]: training loss=0.5698000192642212  validation ndcg@10=0.019383882068432275 [0.55s]\n",
      "epoch 1 [14.1s]:  training loss=0.8082283735275269                                         \n",
      "epoch 2 [13.32s]:  training loss=0.7788536548614502                                        \n",
      "epoch 3 [13.65s]:  training loss=0.7570711374282837                                        \n",
      "epoch 4 [13.74s]:  training loss=0.7372235655784607                                        \n",
      "epoch 5 [13.4s]: training loss=0.7106477618217468  validation ndcg@10=0.019642332404206903 [0.48s]\n",
      "epoch 6 [13.42s]:  training loss=0.6638352274894714                                        \n",
      "epoch 7 [13.19s]:  training loss=0.5944610238075256                                        \n",
      "epoch 8 [13.34s]:  training loss=0.5672554969787598                                        \n",
      "epoch 9 [13.69s]:  training loss=0.5450409054756165                                        \n",
      "epoch 10 [13.63s]: training loss=0.5242102146148682  validation ndcg@10=0.01926959217888101 [0.46s]\n",
      "epoch 11 [13.63s]:  training loss=0.5053921341896057                                       \n",
      "epoch 12 [13.87s]:  training loss=0.48673388361930847                                      \n",
      "epoch 13 [14.07s]:  training loss=0.4780663847923279                                       \n",
      "epoch 14 [13.81s]:  training loss=0.46343928575515747                                      \n",
      "epoch 15 [13.67s]: training loss=0.44815027713775635  validation ndcg@10=0.019930081171132895 [0.51s]\n",
      "epoch 16 [13.8s]:  training loss=0.4345155954360962                                        \n",
      "epoch 17 [13.83s]:  training loss=0.4168754816055298                                       \n",
      "epoch 18 [13.38s]:  training loss=0.40802210569381714                                      \n",
      "epoch 19 [13.3s]:  training loss=0.3891884982585907                                        \n",
      "epoch 20 [13.89s]: training loss=0.38088560104370117  validation ndcg@10=0.021431064451132277 [0.46s]\n",
      "epoch 21 [13.73s]:  training loss=0.3726133108139038                                       \n",
      "epoch 22 [13.59s]:  training loss=0.3575185239315033                                       \n",
      "epoch 23 [13.44s]:  training loss=0.346271812915802                                        \n",
      "epoch 24 [13.23s]:  training loss=0.3279452323913574                                       \n",
      "epoch 25 [13.43s]: training loss=0.3119726777076721  validation ndcg@10=0.023235542973455352 [0.46s]\n",
      "epoch 26 [13.38s]:  training loss=0.3012186884880066                                       \n",
      "epoch 27 [13.76s]:  training loss=0.28717485070228577                                      \n",
      "epoch 28 [13.59s]:  training loss=0.2726418673992157                                       \n",
      "epoch 29 [13.45s]:  training loss=0.2654261291027069                                       \n",
      "epoch 30 [13.49s]: training loss=0.25683411955833435  validation ndcg@10=0.023176713745329144 [0.46s]\n",
      "epoch 31 [13.68s]:  training loss=0.2455645054578781                                       \n",
      "epoch 32 [14.87s]:  training loss=0.2384965866804123                                       \n",
      "epoch 33 [13.45s]:  training loss=0.22744515538215637                                      \n",
      "epoch 34 [13.5s]:  training loss=0.22127492725849152                                       \n",
      "epoch 35 [13.27s]: training loss=0.21132786571979523  validation ndcg@10=0.024296471589440018 [0.48s]\n",
      "epoch 36 [13.61s]:  training loss=0.20835933089256287                                      \n",
      "epoch 37 [13.51s]:  training loss=0.2011955827474594                                       \n",
      "epoch 38 [13.06s]:  training loss=0.1944299340248108                                       \n",
      "epoch 39 [13.75s]:  training loss=0.19141513109207153                                      \n",
      "epoch 40 [13.99s]: training loss=0.18180304765701294  validation ndcg@10=0.024285688274780205 [0.51s]\n",
      "epoch 41 [13.55s]:  training loss=0.17563091218471527                                      \n",
      "epoch 42 [13.69s]:  training loss=0.17294245958328247                                      \n",
      "epoch 43 [13.63s]:  training loss=0.1683192104101181                                       \n",
      "epoch 44 [13.76s]:  training loss=0.16322888433933258                                      \n",
      "epoch 45 [13.86s]: training loss=0.1590878814458847  validation ndcg@10=0.023648333934739387 [0.46s]\n",
      "epoch 46 [13.84s]:  training loss=0.15286198258399963                                      \n",
      "epoch 47 [13.64s]:  training loss=0.1491590440273285                                       \n",
      "epoch 48 [13.7s]:  training loss=0.14694267511367798                                       \n",
      "epoch 49 [14.11s]:  training loss=0.14243337512016296                                      \n",
      "epoch 50 [13.37s]: training loss=0.14033059775829315  validation ndcg@10=0.025127354255167697 [0.5s]\n",
      "epoch 51 [13.48s]:  training loss=0.13499794900417328                                      \n",
      "epoch 52 [13.54s]:  training loss=0.1309502273797989                                       \n",
      "epoch 53 [13.46s]:  training loss=0.12445972114801407                                      \n",
      "epoch 54 [13.4s]:  training loss=0.1263565719127655                                        \n",
      "epoch 55 [13.68s]: training loss=0.11902730166912079  validation ndcg@10=0.02519561431751472 [0.48s]\n",
      "epoch 56 [13.41s]:  training loss=0.11879526823759079                                      \n",
      "epoch 57 [13.6s]:  training loss=0.11753245443105698                                       \n",
      "epoch 58 [13.18s]:  training loss=0.11400789022445679                                      \n",
      "epoch 59 [13.62s]:  training loss=0.11357196420431137                                      \n",
      "epoch 60 [13.91s]: training loss=0.10642757266759872  validation ndcg@10=0.025440606514091232 [0.49s]\n",
      "epoch 61 [13.78s]:  training loss=0.10367131233215332                                      \n",
      "epoch 62 [13.49s]:  training loss=0.103721484541893                                        \n",
      "epoch 63 [13.45s]:  training loss=0.101061150431633                                        \n",
      "epoch 64 [13.53s]:  training loss=0.09786640852689743                                      \n",
      "epoch 65 [13.5s]: training loss=0.09533187747001648  validation ndcg@10=0.025445481828724834 [0.46s]\n",
      "epoch 66 [13.51s]:  training loss=0.09615115076303482                                      \n",
      "epoch 67 [13.56s]:  training loss=0.09007041156291962                                      \n",
      "epoch 68 [13.21s]:  training loss=0.0912289097905159                                       \n",
      "epoch 69 [13.76s]:  training loss=0.090329609811306                                        \n",
      "epoch 70 [13.81s]: training loss=0.08653488755226135  validation ndcg@10=0.025487538745535793 [0.49s]\n",
      "epoch 71 [13.89s]:  training loss=0.08731745183467865                                      \n",
      "epoch 72 [14.03s]:  training loss=0.08786717802286148                                      \n",
      "epoch 73 [13.85s]:  training loss=0.0845421552658081                                       \n",
      "epoch 74 [13.34s]:  training loss=0.08241400122642517                                      \n",
      "epoch 75 [13.69s]: training loss=0.083256296813488  validation ndcg@10=0.02510773712103346 [0.44s]\n",
      "epoch 76 [14.35s]:  training loss=0.0796460211277008                                       \n",
      "epoch 77 [13.73s]:  training loss=0.07971571385860443                                      \n",
      "epoch 78 [13.69s]:  training loss=0.07499349117279053                                      \n",
      "epoch 79 [13.8s]:  training loss=0.07688667625188828                                       \n",
      "epoch 80 [14.13s]: training loss=0.07625920325517654  validation ndcg@10=0.026021651999179267 [0.52s]\n",
      "epoch 81 [14.01s]:  training loss=0.07394091784954071                                      \n",
      "epoch 82 [13.8s]:  training loss=0.07348693162202835                                       \n",
      "epoch 83 [13.72s]:  training loss=0.07228189706802368                                      \n",
      "epoch 84 [13.73s]:  training loss=0.07122838497161865                                      \n",
      "epoch 85 [14.87s]: training loss=0.07008139044046402  validation ndcg@10=0.026475047648639637 [0.46s]\n",
      "epoch 86 [13.27s]:  training loss=0.06877288222312927                                      \n",
      "epoch 87 [13.6s]:  training loss=0.06742317974567413                                       \n",
      "epoch 88 [13.46s]:  training loss=0.06613931804895401                                      \n",
      "epoch 89 [13.56s]:  training loss=0.0650460422039032                                       \n",
      "epoch 90 [13.3s]: training loss=0.06520163267850876  validation ndcg@10=0.026625025818021734 [0.45s]\n",
      "epoch 91 [13.11s]:  training loss=0.0643019899725914                                       \n",
      "epoch 92 [13.74s]:  training loss=0.06312720477581024                                      \n",
      "epoch 93 [13.76s]:  training loss=0.0646691545844078                                       \n",
      "epoch 94 [13.73s]:  training loss=0.06249956786632538                                      \n",
      "epoch 95 [13.63s]: training loss=0.06101544573903084  validation ndcg@10=0.026242113687401803 [0.47s]\n",
      "epoch 96 [13.44s]:  training loss=0.059379033744335175                                     \n",
      "epoch 97 [13.67s]:  training loss=0.06129550561308861                                      \n",
      "epoch 98 [13.4s]:  training loss=0.06092609837651253                                       \n",
      "epoch 99 [13.3s]:  training loss=0.057011041790246964                                      \n",
      "epoch 100 [13.53s]: training loss=0.05634333938360214  validation ndcg@10=0.02595399114844881 [0.47s]\n",
      "epoch 101 [13.5s]:  training loss=0.05620620399713516                                      \n",
      "epoch 102 [13.5s]:  training loss=0.05695537477731705                                      \n",
      "epoch 103 [13.21s]:  training loss=0.05726096034049988                                     \n",
      "epoch 104 [13.39s]:  training loss=0.05641228333115578                                     \n",
      "epoch 105 [13.43s]: training loss=0.05471375584602356  validation ndcg@10=0.027070151299678893 [0.49s]\n",
      "epoch 106 [13.6s]:  training loss=0.054695796221494675                                     \n",
      "epoch 107 [13.49s]:  training loss=0.05190027132630348                                     \n",
      "epoch 108 [13.42s]:  training loss=0.05399809405207634                                     \n",
      "epoch 109 [13.4s]:  training loss=0.05327950045466423                                      \n",
      "epoch 110 [13.39s]: training loss=0.051691535860300064  validation ndcg@10=0.026975791010685337 [0.52s]\n",
      "epoch 111 [13.27s]:  training loss=0.050971273332834244                                    \n",
      "epoch 112 [13.25s]:  training loss=0.05300414189696312                                     \n",
      "epoch 113 [13.29s]:  training loss=0.05288984254002571                                     \n",
      "epoch 114 [13.14s]:  training loss=0.05054381862282753                                     \n",
      "epoch 115 [13.41s]: training loss=0.04956303536891937  validation ndcg@10=0.026890839672515984 [0.47s]\n",
      "epoch 116 [13.71s]:  training loss=0.04912182316184044                                     \n",
      "epoch 117 [13.77s]:  training loss=0.04952755570411682                                     \n",
      "epoch 118 [13.49s]:  training loss=0.04777153208851814                                     \n",
      "epoch 119 [13.23s]:  training loss=0.04858371987938881                                     \n",
      "epoch 120 [13.68s]: training loss=0.0499008372426033  validation ndcg@10=0.027152773256382278 [0.49s]\n",
      "epoch 121 [13.6s]:  training loss=0.04642845317721367                                      \n",
      "epoch 122 [13.64s]:  training loss=0.047973234206438065                                    \n",
      "epoch 123 [13.52s]:  training loss=0.04589543491601944                                     \n",
      "epoch 124 [13.38s]:  training loss=0.04729302227497101                                     \n",
      "epoch 125 [13.44s]: training loss=0.04682792350649834  validation ndcg@10=0.02776303506415521 [0.47s]\n",
      "epoch 126 [13.44s]:  training loss=0.04510343447327614                                     \n",
      "epoch 127 [13.13s]:  training loss=0.04514236003160477                                     \n",
      "epoch 128 [13.18s]:  training loss=0.04604974016547203                                     \n",
      "epoch 129 [13.36s]:  training loss=0.04444802179932594                                     \n",
      "epoch 130 [13.49s]: training loss=0.04357167333364487  validation ndcg@10=0.027696667584992264 [0.5s]\n",
      "epoch 131 [13.11s]:  training loss=0.042484164237976074                                    \n",
      "epoch 132 [13.73s]:  training loss=0.04406534135341644                                     \n",
      "epoch 133 [13.78s]:  training loss=0.04336676374077797                                     \n",
      "epoch 134 [13.02s]:  training loss=0.04395235702395439                                     \n",
      "epoch 135 [13.18s]: training loss=0.04451090842485428  validation ndcg@10=0.027309685661399234 [0.45s]\n",
      "epoch 136 [13.82s]:  training loss=0.04563825577497482                                     \n",
      "epoch 137 [13.13s]:  training loss=0.043086741119623184                                    \n",
      "epoch 138 [14.73s]:  training loss=0.04161360487341881                                     \n",
      "epoch 139 [11.54s]:  training loss=0.0424310639500618                                      \n",
      "epoch 140 [12.42s]: training loss=0.04118838906288147  validation ndcg@10=0.027329892520573225 [0.49s]\n",
      "epoch 141 [12.88s]:  training loss=0.04203583300113678                                     \n",
      "epoch 142 [12.61s]:  training loss=0.04156546667218208                                     \n",
      "epoch 143 [12.92s]:  training loss=0.04146222025156021                                     \n",
      "epoch 144 [13.01s]:  training loss=0.04137488082051277                                     \n",
      "epoch 145 [12.6s]: training loss=0.039736486971378326  validation ndcg@10=0.027361324831426344 [0.45s]\n",
      "epoch 146 [12.7s]:  training loss=0.03990183398127556                                      \n",
      "epoch 147 [12.59s]:  training loss=0.039765506982803345                                    \n",
      "epoch 148 [12.95s]:  training loss=0.038424551486968994                                    \n",
      "epoch 149 [12.92s]:  training loss=0.03767528757452965                                     \n",
      "epoch 150 [13.04s]: training loss=0.0373355858027935  validation ndcg@10=0.027455431315148816 [0.47s]\n",
      "epoch 1 [17.45s]:  training loss=0.7379971146583557                                        \n",
      "epoch 2 [18.01s]:  training loss=0.5153834819793701                                        \n",
      "epoch 3 [17.9s]:  training loss=0.37279748916625977                                        \n",
      "epoch 4 [18.08s]:  training loss=0.2778303325176239                                        \n",
      "epoch 5 [18.13s]: training loss=0.210891991853714  validation ndcg@10=0.02429587229448878 [0.57s]\n",
      "epoch 6 [18.45s]:  training loss=0.16352078318595886                                       \n",
      "epoch 7 [18.44s]:  training loss=0.14069823920726776                                       \n",
      "epoch 8 [17.95s]:  training loss=0.11980511248111725                                       \n",
      "epoch 9 [19.85s]:  training loss=0.1068895012140274                                        \n",
      "epoch 10 [18.11s]: training loss=0.09443832188844681  validation ndcg@10=0.027054121796878213 [0.56s]\n",
      "epoch 11 [18.72s]:  training loss=0.08776238560676575                                      \n",
      "epoch 12 [18.03s]:  training loss=0.08135075867176056                                      \n",
      "epoch 13 [18.07s]:  training loss=0.0793052390217781                                       \n",
      "epoch 14 [18.33s]:  training loss=0.07217878103256226                                      \n",
      "epoch 15 [17.92s]: training loss=0.0667455643415451  validation ndcg@10=0.027138550922925272 [0.58s]\n",
      "epoch 16 [18.57s]:  training loss=0.06271197646856308                                      \n",
      "epoch 17 [18.68s]:  training loss=0.05967031791806221                                      \n",
      "epoch 18 [18.21s]:  training loss=0.056759271770715714                                     \n",
      "epoch 19 [18.28s]:  training loss=0.055634014308452606                                     \n",
      "epoch 20 [18.5s]: training loss=0.05382656678557396  validation ndcg@10=0.02759146180661369 [0.53s]\n",
      "epoch 21 [18.21s]:  training loss=0.05056210234761238                                      \n",
      "epoch 22 [17.96s]:  training loss=0.05120218172669411                                      \n",
      "epoch 23 [18.2s]:  training loss=0.04820598289370537                                       \n",
      "epoch 24 [17.88s]:  training loss=0.04790395125746727                                      \n",
      "epoch 25 [18.42s]: training loss=0.046175360679626465  validation ndcg@10=0.025983654241939918 [0.54s]\n",
      "epoch 26 [20.1s]:  training loss=0.04584839567542076                                       \n",
      "epoch 27 [18.28s]:  training loss=0.043775271624326706                                     \n",
      "epoch 28 [18.41s]:  training loss=0.041928716003894806                                     \n",
      "epoch 29 [18.51s]:  training loss=0.04033450409770012                                      \n",
      "epoch 30 [18.52s]: training loss=0.039758067578077316  validation ndcg@10=0.026750190930231163 [0.56s]\n",
      "epoch 31 [18.12s]:  training loss=0.038730379194021225                                     \n",
      "epoch 32 [18.46s]:  training loss=0.040015146136283875                                     \n",
      "epoch 33 [18.56s]:  training loss=0.03809250146150589                                      \n",
      "epoch 34 [18.34s]:  training loss=0.03689020499587059                                      \n",
      "epoch 35 [18.25s]: training loss=0.037927623838186264  validation ndcg@10=0.02667710100190256 [0.5s]\n",
      "epoch 36 [18.07s]:  training loss=0.03835533931851387                                      \n",
      "epoch 37 [18.15s]:  training loss=0.03509931638836861                                      \n",
      "epoch 38 [18.01s]:  training loss=0.035480041056871414                                     \n",
      "epoch 39 [18.21s]:  training loss=0.03541795536875725                                      \n",
      "epoch 40 [18.55s]: training loss=0.03472523391246796  validation ndcg@10=0.026766222653251907 [0.51s]\n",
      "epoch 41 [18.16s]:  training loss=0.03545839339494705                                      \n",
      "epoch 42 [18.19s]:  training loss=0.0346781425178051                                       \n",
      "epoch 43 [19.83s]:  training loss=0.03508776053786278                                      \n",
      "epoch 44 [18.09s]:  training loss=0.03462325781583786                                      \n",
      "epoch 45 [18.24s]: training loss=0.0340745709836483  validation ndcg@10=0.02663087479580758 [0.54s]\n",
      "epoch 1 [48.3s]:  training loss=5.703469276428223                                          \n",
      "epoch 2 [50.2s]:  training loss=9.767003059387207                                          \n",
      "epoch 3 [48.51s]:  training loss=11.237886428833008                                        \n",
      "epoch 4 [50.24s]:  training loss=11.711260795593262                                        \n",
      "epoch 5 [48.73s]: training loss=12.423030853271484  validation ndcg@10=0.014424406505833922 [1.13s]\n",
      "epoch 6 [51.48s]:  training loss=13.259628295898438                                        \n",
      "epoch 7 [49.15s]:  training loss=13.665422439575195                                        \n",
      "epoch 8 [49.98s]:  training loss=14.661898612976074                                        \n",
      "epoch 9 [49.46s]:  training loss=15.533636093139648                                        \n",
      "epoch 10 [49.65s]: training loss=15.956562995910645  validation ndcg@10=0.01808546189594086 [1.16s]\n",
      "epoch 11 [49.9s]:  training loss=15.025083541870117                                        \n",
      "epoch 12 [51.09s]:  training loss=15.484074592590332                                       \n",
      "epoch 13 [49.86s]:  training loss=16.040660858154297                                       \n",
      "epoch 14 [49.29s]:  training loss=16.007444381713867                                       \n",
      "epoch 15 [50.79s]: training loss=17.19144058227539  validation ndcg@10=0.01810542776885616 [1.14s]\n",
      "epoch 16 [49.7s]:  training loss=17.110414505004883                                        \n",
      "epoch 17 [50.02s]:  training loss=17.726165771484375                                       \n",
      "epoch 18 [52.75s]:  training loss=18.042728424072266                                       \n",
      "epoch 19 [49.25s]:  training loss=17.176801681518555                                       \n",
      "epoch 20 [49.64s]: training loss=19.408824920654297  validation ndcg@10=0.017572951349736153 [1.15s]\n",
      "epoch 21 [49.85s]:  training loss=19.027359008789062                                       \n",
      "epoch 22 [50.33s]:  training loss=19.0589656829834                                         \n",
      "epoch 23 [51.01s]:  training loss=19.913042068481445                                       \n",
      "epoch 24 [52.07s]:  training loss=21.97508430480957                                        \n",
      "epoch 25 [49.68s]: training loss=19.386741638183594  validation ndcg@10=0.01795218793872614 [1.18s]\n",
      "epoch 26 [49.83s]:  training loss=20.300973892211914                                       \n",
      "epoch 27 [49.57s]:  training loss=20.159183502197266                                       \n",
      "epoch 28 [49.63s]:  training loss=21.27852439880371                                        \n",
      "epoch 29 [50.91s]:  training loss=19.69923210144043                                        \n",
      "epoch 30 [52.85s]: training loss=20.658248901367188  validation ndcg@10=0.017324189405366605 [1.14s]\n",
      "epoch 31 [49.45s]:  training loss=20.091697692871094                                       \n",
      "epoch 32 [50.54s]:  training loss=20.00766372680664                                        \n",
      "epoch 33 [50.23s]:  training loss=21.2723331451416                                         \n",
      "epoch 34 [50.59s]:  training loss=22.210006713867188                                       \n",
      "epoch 35 [50.57s]: training loss=22.904460906982422  validation ndcg@10=0.016688256623805933 [1.14s]\n",
      "epoch 36 [51.73s]:  training loss=21.98150634765625                                        \n",
      "epoch 37 [48.42s]:  training loss=22.56806182861328                                        \n",
      "epoch 38 [49.82s]:  training loss=22.76986312866211                                        \n",
      "epoch 39 [49.37s]:  training loss=21.97304344177246                                        \n",
      "epoch 40 [50.29s]: training loss=22.070045471191406  validation ndcg@10=0.017759149367776814 [1.17s]\n",
      "epoch 1 [13.77s]:  training loss=0.8372626900672913                                        \n",
      "epoch 2 [13.81s]:  training loss=0.8012180924415588                                        \n",
      "epoch 3 [13.76s]:  training loss=0.7929407954216003                                        \n",
      "epoch 4 [13.68s]:  training loss=0.7853582501411438                                        \n",
      "epoch 5 [13.77s]: training loss=0.7783379554748535  validation ndcg@10=0.004024459189170917 [0.52s]\n",
      "epoch 6 [13.43s]:  training loss=0.7692875266075134                                        \n",
      "epoch 7 [15.57s]:  training loss=0.7633846998214722                                        \n",
      "epoch 8 [13.87s]:  training loss=0.7552016377449036                                        \n",
      "epoch 9 [13.38s]:  training loss=0.7551763653755188                                        \n",
      "epoch 10 [13.62s]: training loss=0.742324709892273  validation ndcg@10=0.01078333083861938 [0.49s]\n",
      "epoch 11 [13.66s]:  training loss=0.7372893691062927                                       \n",
      "epoch 12 [13.51s]:  training loss=0.7283036112785339                                       \n",
      "epoch 13 [13.39s]:  training loss=0.7138910889625549                                       \n",
      "epoch 14 [12.9s]:  training loss=0.6895621418952942                                        \n",
      "epoch 15 [12.98s]: training loss=0.6614242196083069  validation ndcg@10=0.016967061951920936 [0.54s]\n",
      "epoch 16 [13.35s]:  training loss=0.6403069496154785                                       \n",
      "epoch 17 [13.34s]:  training loss=0.6264603734016418                                       \n",
      "epoch 18 [13.29s]:  training loss=0.6180420517921448                                       \n",
      "epoch 19 [13.43s]:  training loss=0.6067860126495361                                       \n",
      "epoch 20 [13.26s]: training loss=0.5995362997055054  validation ndcg@10=0.018024947280819433 [0.5s]\n",
      "epoch 21 [13.45s]:  training loss=0.5961434245109558                                       \n",
      "epoch 22 [13.14s]:  training loss=0.5872808694839478                                       \n",
      "epoch 23 [13.55s]:  training loss=0.5807750225067139                                       \n",
      "epoch 24 [13.49s]:  training loss=0.5725417137145996                                       \n",
      "epoch 25 [13.52s]: training loss=0.5712575316429138  validation ndcg@10=0.018381274680785445 [0.54s]\n",
      "epoch 26 [13.93s]:  training loss=0.5670411586761475                                       \n",
      "epoch 27 [13.77s]:  training loss=0.5558229684829712                                       \n",
      "epoch 28 [13.74s]:  training loss=0.5547740459442139                                       \n",
      "epoch 29 [15.01s]:  training loss=0.5498988032341003                                       \n",
      "epoch 30 [13.92s]: training loss=0.5440543293952942  validation ndcg@10=0.019074217085921052 [0.48s]\n",
      "epoch 31 [13.73s]:  training loss=0.5402181148529053                                       \n",
      "epoch 32 [13.65s]:  training loss=0.5349082350730896                                       \n",
      "epoch 33 [13.7s]:  training loss=0.5281569361686707                                        \n",
      "epoch 34 [13.34s]:  training loss=0.5254183411598206                                       \n",
      "epoch 35 [13.37s]: training loss=0.5157950520515442  validation ndcg@10=0.019037590375997976 [0.53s]\n",
      "epoch 36 [13.47s]:  training loss=0.5168018937110901                                       \n",
      "epoch 37 [13.77s]:  training loss=0.509006917476654                                        \n",
      "epoch 38 [13.47s]:  training loss=0.507228434085846                                        \n",
      "epoch 39 [13.5s]:  training loss=0.5036687254905701                                        \n",
      "epoch 40 [13.03s]: training loss=0.49619466066360474  validation ndcg@10=0.019390397359927085 [0.48s]\n",
      "epoch 41 [13.42s]:  training loss=0.49018585681915283                                      \n",
      "epoch 42 [13.82s]:  training loss=0.48715755343437195                                      \n",
      "epoch 43 [13.71s]:  training loss=0.4830935299396515                                       \n",
      "epoch 44 [13.87s]:  training loss=0.47973182797431946                                      \n",
      "epoch 45 [13.76s]: training loss=0.4735158681869507  validation ndcg@10=0.020280431154715794 [0.51s]\n",
      "epoch 46 [13.58s]:  training loss=0.4650639295578003                                       \n",
      "epoch 47 [13.71s]:  training loss=0.4670623242855072                                       \n",
      "epoch 48 [13.33s]:  training loss=0.4590666890144348                                       \n",
      "epoch 49 [13.35s]:  training loss=0.4552563428878784                                       \n",
      "epoch 50 [13.56s]: training loss=0.44890978932380676  validation ndcg@10=0.021000994622425908 [0.51s]\n",
      "epoch 51 [15.26s]:  training loss=0.4479660391807556                                       \n",
      "epoch 52 [13.19s]:  training loss=0.4371900260448456                                       \n",
      "epoch 53 [13.59s]:  training loss=0.432574987411499                                        \n",
      "epoch 54 [13.32s]:  training loss=0.43057459592819214                                      \n",
      "epoch 55 [13.28s]: training loss=0.42932167649269104  validation ndcg@10=0.02118087920317051 [0.54s]\n",
      "epoch 56 [13.3s]:  training loss=0.42241156101226807                                       \n",
      "epoch 57 [13.25s]:  training loss=0.4159262776374817                                       \n",
      "epoch 58 [13.38s]:  training loss=0.40428245067596436                                      \n",
      "epoch 59 [13.79s]:  training loss=0.4080251157283783                                       \n",
      "epoch 60 [13.62s]: training loss=0.4028658866882324  validation ndcg@10=0.022853104087189435 [0.56s]\n",
      "epoch 61 [13.73s]:  training loss=0.4044744670391083                                       \n",
      "epoch 62 [13.5s]:  training loss=0.39523211121559143                                       \n",
      "epoch 63 [13.25s]:  training loss=0.39338353276252747                                      \n",
      "epoch 64 [13.44s]:  training loss=0.391463041305542                                        \n",
      "epoch 65 [13.22s]: training loss=0.3871406018733978  validation ndcg@10=0.023191578081665085 [0.49s]\n",
      "epoch 66 [13.62s]:  training loss=0.38625311851501465                                      \n",
      "epoch 67 [13.35s]:  training loss=0.386630654335022                                        \n",
      "epoch 68 [13.35s]:  training loss=0.3803704082965851                                       \n",
      "epoch 69 [13.42s]:  training loss=0.3762100040912628                                       \n",
      "epoch 70 [13.31s]: training loss=0.3697093725204468  validation ndcg@10=0.024167820931389236 [0.48s]\n",
      "epoch 71 [13.24s]:  training loss=0.3739899694919586                                       \n",
      "epoch 72 [13.41s]:  training loss=0.36947131156921387                                      \n",
      "epoch 73 [15.33s]:  training loss=0.36535564064979553                                      \n",
      "epoch 74 [13.44s]:  training loss=0.36783260107040405                                      \n",
      "epoch 75 [13.43s]: training loss=0.360821008682251  validation ndcg@10=0.02467567150962252 [0.5s]\n",
      "epoch 76 [13.34s]:  training loss=0.3566739857196808                                       \n",
      "epoch 77 [13.78s]:  training loss=0.3517761826515198                                       \n",
      "epoch 78 [13.74s]:  training loss=0.34788978099823                                         \n",
      "epoch 79 [13.49s]:  training loss=0.35110536217689514                                      \n",
      "epoch 80 [13.36s]: training loss=0.34476396441459656  validation ndcg@10=0.02306247370212963 [0.49s]\n",
      "epoch 81 [13.32s]:  training loss=0.3437487483024597                                       \n",
      "epoch 82 [13.2s]:  training loss=0.3367992341518402                                        \n",
      "epoch 83 [13.38s]:  training loss=0.335369735956192                                        \n",
      "epoch 84 [13.13s]:  training loss=0.33137694001197815                                      \n",
      "epoch 85 [13.57s]: training loss=0.33164986968040466  validation ndcg@10=0.02306138574921534 [0.51s]\n",
      "epoch 86 [13.7s]:  training loss=0.329459011554718                                         \n",
      "epoch 87 [13.41s]:  training loss=0.325912207365036                                        \n",
      "epoch 88 [13.49s]:  training loss=0.32676488161087036                                      \n",
      "epoch 89 [13.48s]:  training loss=0.32168376445770264                                      \n",
      "epoch 90 [13.61s]: training loss=0.3174918293952942  validation ndcg@10=0.023477139094203892 [0.5s]\n",
      "epoch 91 [13.24s]:  training loss=0.3142055869102478                                       \n",
      "epoch 92 [13.64s]:  training loss=0.31305062770843506                                      \n",
      "epoch 93 [13.4s]:  training loss=0.31251999735832214                                       \n",
      "epoch 94 [14.38s]:  training loss=0.3061947524547577                                       \n",
      "epoch 95 [14.77s]: training loss=0.30218708515167236  validation ndcg@10=0.02388198795792463 [0.52s]\n",
      "epoch 96 [13.19s]:  training loss=0.30318760871887207                                      \n",
      "epoch 97 [13.34s]:  training loss=0.3004191219806671                                       \n",
      "epoch 98 [13.18s]:  training loss=0.2993917465209961                                       \n",
      "epoch 99 [13.2s]:  training loss=0.29707008600234985                                       \n",
      "epoch 100 [13.19s]: training loss=0.2943258583545685  validation ndcg@10=0.022375376315236083 [0.48s]\n",
      "epoch 1 [28.18s]:  training loss=0.8551181554794312                                        \n",
      "epoch 2 [27.74s]:  training loss=0.8339492678642273                                       \n",
      "epoch 3 [27.7s]:  training loss=0.8223320841789246                                        \n",
      "epoch 4 [27.47s]:  training loss=0.8117738366127014                                       \n",
      "epoch 5 [28.44s]: training loss=0.8081979751586914  validation ndcg@10=0.0008222392273833256 [0.64s]\n",
      "epoch 6 [27.03s]:  training loss=0.805513858795166                                        \n",
      "epoch 7 [27.03s]:  training loss=0.8016388416290283                                       \n",
      "epoch 8 [27.84s]:  training loss=0.7993374466896057                                       \n",
      "epoch 9 [29.0s]:  training loss=0.7973707914352417                                        \n",
      "epoch 10 [27.08s]: training loss=0.7980552911758423  validation ndcg@10=0.0008615610118051606 [0.63s]\n",
      "epoch 11 [27.64s]:  training loss=0.7964445352554321                                      \n",
      "epoch 12 [27.44s]:  training loss=0.7918213605880737                                      \n",
      "epoch 13 [27.68s]:  training loss=0.7918596863746643                                      \n",
      "epoch 14 [27.9s]:  training loss=0.791256308555603                                        \n",
      "epoch 15 [27.9s]: training loss=0.790473461151123  validation ndcg@10=0.0009714668446846763 [0.66s]\n",
      "epoch 16 [28.57s]:  training loss=0.7921998500823975                                      \n",
      "epoch 17 [28.37s]:  training loss=0.7895894050598145                                      \n",
      "epoch 18 [28.38s]:  training loss=0.7865097522735596                                      \n",
      "epoch 19 [28.05s]:  training loss=0.7829689383506775                                      \n",
      "epoch 20 [29.26s]: training loss=0.7832255959510803  validation ndcg@10=0.0009168210662067465 [0.67s]\n",
      "epoch 21 [27.58s]:  training loss=0.7844443321228027                                      \n",
      "epoch 22 [28.11s]:  training loss=0.7820440530776978                                      \n",
      "epoch 23 [28.07s]:  training loss=0.7824235558509827                                      \n",
      "epoch 24 [28.34s]:  training loss=0.778113067150116                                       \n",
      "epoch 25 [27.67s]: training loss=0.7814608812332153  validation ndcg@10=0.0011114692496321382 [0.67s]\n",
      "epoch 26 [27.72s]:  training loss=0.7780367136001587                                      \n",
      "epoch 27 [27.89s]:  training loss=0.7762578725814819                                      \n",
      "epoch 28 [28.27s]:  training loss=0.7768976092338562                                      \n",
      "epoch 29 [28.18s]:  training loss=0.7765378952026367                                      \n",
      "epoch 30 [27.46s]: training loss=0.7761871814727783  validation ndcg@10=0.0014282656215466592 [0.64s]\n",
      "epoch 31 [29.9s]:  training loss=0.7743878960609436                                       \n",
      "epoch 32 [27.49s]:  training loss=0.773094892501831                                       \n",
      "epoch 33 [28.95s]:  training loss=0.7718327641487122                                      \n",
      "epoch 34 [28.69s]:  training loss=0.7708833813667297                                      \n",
      "epoch 35 [28.84s]: training loss=0.7703215479850769  validation ndcg@10=0.0012224143242424046 [0.68s]\n",
      "epoch 36 [29.03s]:  training loss=0.7687721848487854                                      \n",
      "epoch 37 [29.2s]:  training loss=0.769012451171875                                        \n",
      "epoch 38 [29.17s]:  training loss=0.7671271562576294                                      \n",
      "epoch 39 [29.61s]:  training loss=0.7672674655914307                                      \n",
      "epoch 40 [29.75s]: training loss=0.7677514553070068  validation ndcg@10=0.0011364297039030318 [0.7s]\n",
      "epoch 41 [29.66s]:  training loss=0.7642167806625366                                      \n",
      "epoch 42 [29.03s]:  training loss=0.76470947265625                                        \n",
      "epoch 43 [28.97s]:  training loss=0.7597482800483704                                      \n",
      "epoch 44 [28.73s]:  training loss=0.7619946599006653                                      \n",
      "epoch 45 [29.19s]: training loss=0.762291431427002  validation ndcg@10=0.0018618747054852682 [0.7s]\n",
      "epoch 46 [28.86s]:  training loss=0.7601690292358398                                      \n",
      "epoch 47 [28.95s]:  training loss=0.7601941227912903                                      \n",
      "epoch 48 [28.8s]:  training loss=0.7569881081581116                                       \n",
      "epoch 49 [29.11s]:  training loss=0.7590444087982178                                      \n",
      "epoch 50 [28.67s]: training loss=0.7562282681465149  validation ndcg@10=0.0029698879727880346 [0.68s]\n",
      "epoch 51 [29.28s]:  training loss=0.7553458213806152                                      \n",
      "epoch 52 [28.81s]:  training loss=0.7534094452857971                                      \n",
      "epoch 53 [29.51s]:  training loss=0.7519545555114746                                      \n",
      "epoch 54 [28.76s]:  training loss=0.7527168989181519                                      \n",
      "epoch 55 [30.78s]: training loss=0.7532392144203186  validation ndcg@10=0.005204952465464009 [0.61s]\n",
      "epoch 56 [28.85s]:  training loss=0.7517790794372559                                      \n",
      "epoch 57 [29.05s]:  training loss=0.748303234577179                                       \n",
      "epoch 58 [28.98s]:  training loss=0.7477744221687317                                      \n",
      "epoch 59 [28.49s]:  training loss=0.748464822769165                                       \n",
      "epoch 60 [28.99s]: training loss=0.7450920939445496  validation ndcg@10=0.008853648142700255 [0.67s]\n",
      "epoch 61 [28.84s]:  training loss=0.7430059909820557                                      \n",
      "epoch 62 [28.97s]:  training loss=0.745909571647644                                       \n",
      "epoch 63 [29.09s]:  training loss=0.7414779663085938                                      \n",
      "epoch 64 [28.35s]:  training loss=0.740760326385498                                       \n",
      "epoch 65 [28.71s]: training loss=0.7405934929847717  validation ndcg@10=0.011885110116677259 [0.66s]\n",
      "epoch 66 [29.55s]:  training loss=0.7380285859107971                                      \n",
      "epoch 67 [28.5s]:  training loss=0.7394201755523682                                       \n",
      "epoch 68 [29.81s]:  training loss=0.7361409664154053                                      \n",
      "epoch 69 [29.11s]:  training loss=0.7351906299591064                                      \n",
      "epoch 70 [29.48s]: training loss=0.7309476733207703  validation ndcg@10=0.016012274875337688 [0.7s]\n",
      "epoch 71 [28.95s]:  training loss=0.7331030964851379                                      \n",
      "epoch 72 [29.08s]:  training loss=0.7282539010047913                                      \n",
      "epoch 73 [28.71s]:  training loss=0.7306101322174072                                      \n",
      "epoch 74 [28.61s]:  training loss=0.7261117696762085                                      \n",
      "epoch 75 [28.83s]: training loss=0.7256664633750916  validation ndcg@10=0.01751840913452917 [0.71s]\n",
      "epoch 76 [28.91s]:  training loss=0.7243791222572327                                      \n",
      "epoch 77 [28.94s]:  training loss=0.7203331589698792                                      \n",
      "epoch 78 [28.75s]:  training loss=0.7158462405204773                                      \n",
      "epoch 79 [30.02s]:  training loss=0.7163413166999817                                      \n",
      "epoch 80 [29.74s]: training loss=0.7159066200256348  validation ndcg@10=0.01866492815954616 [0.71s]\n",
      "epoch 81 [28.59s]:  training loss=0.7101671099662781                                      \n",
      "epoch 82 [29.52s]:  training loss=0.7056004405021667                                      \n",
      "epoch 83 [29.12s]:  training loss=0.7012199759483337                                      \n",
      "epoch 84 [29.76s]:  training loss=0.6995170712471008                                      \n",
      "epoch 85 [29.76s]: training loss=0.695216715335846  validation ndcg@10=0.018907920461854574 [0.7s]\n",
      "epoch 86 [29.58s]:  training loss=0.6909864544868469                                      \n",
      "epoch 87 [28.48s]:  training loss=0.6895579695701599                                      \n",
      "epoch 88 [29.22s]:  training loss=0.6806613802909851                                      \n",
      "epoch 89 [29.64s]:  training loss=0.6803922057151794                                      \n",
      "epoch 90 [29.1s]: training loss=0.6744406819343567  validation ndcg@10=0.01893146632506341 [0.68s]\n",
      "epoch 91 [29.61s]:  training loss=0.6701107025146484                                      \n",
      "epoch 92 [30.06s]:  training loss=0.6675903797149658                                      \n",
      "epoch 93 [29.16s]:  training loss=0.6631708741188049                                      \n",
      "epoch 94 [29.3s]:  training loss=0.659790575504303                                        \n",
      "epoch 95 [29.86s]: training loss=0.6570743918418884  validation ndcg@10=0.019358255003976185 [0.7s]\n",
      "epoch 96 [29.55s]:  training loss=0.6535096168518066                                      \n",
      "epoch 97 [29.21s]:  training loss=0.6510640978813171                                      \n",
      "epoch 98 [28.76s]:  training loss=0.6476816534996033                                      \n",
      "epoch 99 [28.47s]:  training loss=0.6447533369064331                                      \n",
      "epoch 100 [28.34s]: training loss=0.6394526362419128  validation ndcg@10=0.019770920806828556 [0.71s]\n",
      "epoch 101 [30.99s]:  training loss=0.6441915035247803                                     \n",
      "epoch 102 [28.19s]:  training loss=0.6399931907653809                                     \n",
      "epoch 103 [29.35s]:  training loss=0.6385104656219482                                     \n",
      "epoch 104 [29.11s]:  training loss=0.6364879012107849                                     \n",
      "epoch 105 [29.5s]: training loss=0.633872926235199  validation ndcg@10=0.019632632288654524 [0.71s]\n",
      "epoch 106 [29.36s]:  training loss=0.627865195274353                                      \n",
      "epoch 107 [29.07s]:  training loss=0.6297950148582458                                     \n",
      "epoch 108 [29.55s]:  training loss=0.6270506978034973                                     \n",
      "epoch 109 [28.91s]:  training loss=0.6272289752960205                                     \n",
      "epoch 110 [28.59s]: training loss=0.6258736848831177  validation ndcg@10=0.020045255533476483 [0.68s]\n",
      "epoch 111 [28.92s]:  training loss=0.6228312849998474                                     \n",
      "epoch 112 [29.15s]:  training loss=0.623704731464386                                      \n",
      "epoch 113 [29.53s]:  training loss=0.6202371716499329                                     \n",
      "epoch 114 [29.54s]:  training loss=0.6196807026863098                                     \n",
      "epoch 115 [29.76s]: training loss=0.6184222102165222  validation ndcg@10=0.020159021262523967 [0.69s]\n",
      "epoch 116 [29.19s]:  training loss=0.6124128699302673                                     \n",
      "epoch 117 [28.91s]:  training loss=0.6135224103927612                                     \n",
      "epoch 118 [29.61s]:  training loss=0.6174456477165222                                     \n",
      "epoch 119 [28.9s]:  training loss=0.6134049296379089                                      \n",
      "epoch 120 [29.4s]: training loss=0.616149365901947  validation ndcg@10=0.02031857579281074 [0.71s]\n",
      "epoch 121 [29.73s]:  training loss=0.6116600632667542                                     \n",
      "epoch 122 [28.97s]:  training loss=0.6090017557144165                                     \n",
      "epoch 123 [29.09s]:  training loss=0.6101378798484802                                     \n",
      "epoch 124 [30.61s]:  training loss=0.6060436367988586                                     \n",
      "epoch 125 [28.82s]: training loss=0.6074779629707336  validation ndcg@10=0.020609761508452427 [0.68s]\n",
      "epoch 126 [29.22s]:  training loss=0.6061928272247314                                     \n",
      "epoch 127 [28.68s]:  training loss=0.6009156703948975                                     \n",
      "epoch 128 [29.48s]:  training loss=0.6062096357345581                                     \n",
      "epoch 129 [28.65s]:  training loss=0.604973316192627                                      \n",
      "epoch 130 [29.34s]: training loss=0.6044586896896362  validation ndcg@10=0.020659988407053517 [0.7s]\n",
      "epoch 131 [29.48s]:  training loss=0.6002381443977356                                     \n",
      "epoch 132 [28.85s]:  training loss=0.599358081817627                                      \n",
      "epoch 133 [28.67s]:  training loss=0.6010864973068237                                     \n",
      "epoch 134 [28.59s]:  training loss=0.5983074307441711                                     \n",
      "epoch 135 [28.54s]: training loss=0.5933834910392761  validation ndcg@10=0.020721241643028413 [0.69s]\n",
      "epoch 136 [28.9s]:  training loss=0.5971853137016296                                      \n",
      "epoch 137 [29.11s]:  training loss=0.5949521660804749                                     \n",
      "epoch 138 [28.76s]:  training loss=0.593483030796051                                      \n",
      "epoch 139 [29.0s]:  training loss=0.5929961800575256                                      \n",
      "epoch 140 [30.41s]: training loss=0.592628538608551  validation ndcg@10=0.02032064289459968 [0.7s]\n",
      "epoch 141 [29.73s]:  training loss=0.5899775624275208                                     \n",
      "epoch 142 [29.18s]:  training loss=0.5899015069007874                                     \n",
      "epoch 143 [30.0s]:  training loss=0.5896077156066895                                      \n",
      "epoch 144 [29.71s]:  training loss=0.5883267521858215                                     \n",
      "epoch 145 [29.26s]: training loss=0.5848687887191772  validation ndcg@10=0.020323038455023472 [0.67s]\n",
      "epoch 146 [28.51s]:  training loss=0.587256908416748                                      \n",
      "epoch 147 [30.35s]:  training loss=0.5866295695304871                                     \n",
      "epoch 148 [27.94s]:  training loss=0.5830581784248352                                     \n",
      "epoch 149 [28.35s]:  training loss=0.5835450291633606                                     \n",
      "epoch 150 [28.36s]: training loss=0.5822586417198181  validation ndcg@10=0.02046225844491535 [0.65s]\n",
      "epoch 151 [28.35s]:  training loss=0.5816144347190857                                     \n",
      "epoch 152 [28.65s]:  training loss=0.578743577003479                                      \n",
      "epoch 153 [29.19s]:  training loss=0.582350492477417                                      \n",
      "epoch 154 [29.24s]:  training loss=0.5792037844657898                                     \n",
      "epoch 155 [28.35s]: training loss=0.5774113535881042  validation ndcg@10=0.020418728134292467 [0.7s]\n",
      "epoch 156 [28.32s]:  training loss=0.5766611099243164                                     \n",
      "epoch 157 [29.39s]:  training loss=0.5761598348617554                                     \n",
      "epoch 158 [28.64s]:  training loss=0.5744483470916748                                     \n",
      "epoch 159 [29.8s]:  training loss=0.5772408843040466                                      \n",
      "epoch 160 [28.74s]: training loss=0.5703211426734924  validation ndcg@10=0.02046815229813784 [0.67s]\n",
      "epoch 1 [14.7s]:  training loss=0.8058338165283203                                         \n",
      "epoch 2 [15.03s]:  training loss=0.7689752578735352                                        \n",
      "epoch 3 [14.51s]:  training loss=0.7418724894523621                                        \n",
      "epoch 4 [14.58s]:  training loss=0.6959421634674072                                        \n",
      "epoch 5 [14.8s]: training loss=0.604625940322876  validation ndcg@10=0.02143824724818978 [0.5s]\n",
      "epoch 6 [14.49s]:  training loss=0.5563071966171265                                        \n",
      "epoch 7 [14.5s]:  training loss=0.532555341720581                                          \n",
      "epoch 8 [14.69s]:  training loss=0.5082297325134277                                        \n",
      "epoch 9 [14.52s]:  training loss=0.4901614487171173                                        \n",
      "epoch 10 [14.95s]: training loss=0.4679965674877167  validation ndcg@10=0.0198130547593407 [0.46s]\n",
      "epoch 11 [14.57s]:  training loss=0.4514711797237396                                       \n",
      "epoch 12 [15.03s]:  training loss=0.43364715576171875                                      \n",
      "epoch 13 [14.99s]:  training loss=0.4133767783641815                                       \n",
      "epoch 14 [14.92s]:  training loss=0.39946356415748596                                      \n",
      "epoch 15 [14.81s]: training loss=0.3819618821144104  validation ndcg@10=0.022661661506437017 [0.52s]\n",
      "epoch 16 [14.8s]:  training loss=0.3663094639778137                                        \n",
      "epoch 17 [14.92s]:  training loss=0.3473474383354187                                       \n",
      "epoch 18 [14.56s]:  training loss=0.32687655091285706                                      \n",
      "epoch 19 [14.76s]:  training loss=0.30880600214004517                                      \n",
      "epoch 20 [16.42s]: training loss=0.28855767846107483  validation ndcg@10=0.024518828275263183 [0.53s]\n",
      "epoch 21 [14.39s]:  training loss=0.27874594926834106                                      \n",
      "epoch 22 [14.68s]:  training loss=0.26478421688079834                                      \n",
      "epoch 23 [14.95s]:  training loss=0.2529188096523285                                       \n",
      "epoch 24 [14.96s]:  training loss=0.2442636936903                                          \n",
      "epoch 25 [14.75s]: training loss=0.23125021159648895  validation ndcg@10=0.024040184867621252 [0.5s]\n",
      "epoch 26 [15.07s]:  training loss=0.21710945665836334                                      \n",
      "epoch 27 [15.54s]:  training loss=0.20910674333572388                                      \n",
      "epoch 28 [15.47s]:  training loss=0.2023419886827469                                       \n",
      "epoch 29 [15.46s]:  training loss=0.19227401912212372                                      \n",
      "epoch 30 [14.88s]: training loss=0.1866743415594101  validation ndcg@10=0.024313911509298403 [0.53s]\n",
      "epoch 31 [15.43s]:  training loss=0.1785060465335846                                       \n",
      "epoch 32 [14.86s]:  training loss=0.1730930507183075                                       \n",
      "epoch 33 [15.42s]:  training loss=0.16586247086524963                                      \n",
      "epoch 34 [15.05s]:  training loss=0.15931355953216553                                      \n",
      "epoch 35 [14.93s]: training loss=0.15519975125789642  validation ndcg@10=0.025951169596691425 [0.5s]\n",
      "epoch 36 [15.03s]:  training loss=0.148586243391037                                        \n",
      "epoch 37 [15.09s]:  training loss=0.1436944156885147                                       \n",
      "epoch 38 [15.1s]:  training loss=0.14109382033348083                                       \n",
      "epoch 39 [14.6s]:  training loss=0.13562530279159546                                       \n",
      "epoch 40 [14.76s]: training loss=0.13083481788635254  validation ndcg@10=0.025742050872097424 [0.5s]\n",
      "epoch 41 [14.92s]:  training loss=0.12601104378700256                                      \n",
      "epoch 42 [14.83s]:  training loss=0.12237183004617691                                      \n",
      "epoch 43 [15.21s]:  training loss=0.1194891631603241                                       \n",
      "epoch 44 [14.64s]:  training loss=0.11752083897590637                                      \n",
      "epoch 45 [14.68s]: training loss=0.11301590502262115  validation ndcg@10=0.025812742705857174 [0.49s]\n",
      "epoch 46 [14.33s]:  training loss=0.108286052942276                                        \n",
      "epoch 47 [15.0s]:  training loss=0.1093282625079155                                        \n",
      "epoch 48 [14.84s]:  training loss=0.10328640788793564                                      \n",
      "epoch 49 [14.99s]:  training loss=0.10244059562683105                                      \n",
      "epoch 50 [15.0s]: training loss=0.09643275290727615  validation ndcg@10=0.026497392876459008 [0.53s]\n",
      "epoch 51 [14.33s]:  training loss=0.09525591135025024                                      \n",
      "epoch 52 [14.96s]:  training loss=0.09337692707777023                                      \n",
      "epoch 53 [14.91s]:  training loss=0.09288676083087921                                      \n",
      "epoch 54 [14.83s]:  training loss=0.0894157886505127                                       \n",
      "epoch 55 [14.7s]: training loss=0.08903338760137558  validation ndcg@10=0.027263698036898647 [0.5s]\n",
      "epoch 56 [15.21s]:  training loss=0.08745451271533966                                      \n",
      "epoch 57 [15.13s]:  training loss=0.08513687551021576                                      \n",
      "epoch 58 [15.04s]:  training loss=0.08301063627004623                                      \n",
      "epoch 59 [15.05s]:  training loss=0.08188482373952866                                      \n",
      "epoch 60 [14.87s]: training loss=0.07923253625631332  validation ndcg@10=0.02694374828939782 [0.52s]\n",
      "epoch 61 [14.96s]:  training loss=0.07869508862495422                                      \n",
      "epoch 62 [14.9s]:  training loss=0.07711035758256912                                       \n",
      "epoch 63 [14.66s]:  training loss=0.0745706781744957                                       \n",
      "epoch 64 [15.12s]:  training loss=0.07418149709701538                                      \n",
      "epoch 65 [16.05s]: training loss=0.07171785086393356  validation ndcg@10=0.02644046327455891 [0.52s]\n",
      "epoch 66 [14.61s]:  training loss=0.07116067409515381                                      \n",
      "epoch 67 [14.59s]:  training loss=0.0690794587135315                                       \n",
      "epoch 68 [15.11s]:  training loss=0.06961889564990997                                      \n",
      "epoch 69 [15.29s]:  training loss=0.06686273962259293                                      \n",
      "epoch 70 [15.39s]: training loss=0.06387041509151459  validation ndcg@10=0.02805103046770863 [0.53s]\n",
      "epoch 71 [15.01s]:  training loss=0.06352754682302475                                      \n",
      "epoch 72 [14.99s]:  training loss=0.06375553458929062                                      \n",
      "epoch 73 [15.03s]:  training loss=0.0630820244550705                                       \n",
      "epoch 74 [14.31s]:  training loss=0.06264474242925644                                      \n",
      "epoch 75 [14.78s]: training loss=0.06303433328866959  validation ndcg@10=0.027895570909329048 [0.51s]\n",
      "epoch 76 [14.31s]:  training loss=0.05944805592298508                                      \n",
      "epoch 77 [14.62s]:  training loss=0.0587991327047348                                       \n",
      "epoch 78 [14.93s]:  training loss=0.05985799431800842                                      \n",
      "epoch 79 [15.15s]:  training loss=0.05783655494451523                                      \n",
      "epoch 80 [15.68s]: training loss=0.05651232227683067  validation ndcg@10=0.027129237972497677 [0.51s]\n",
      "epoch 81 [15.27s]:  training loss=0.05737116560339928                                      \n",
      "epoch 82 [14.77s]:  training loss=0.05711638554930687                                      \n",
      "epoch 83 [14.98s]:  training loss=0.0550747774541378                                       \n",
      "epoch 84 [15.02s]:  training loss=0.05386100709438324                                      \n",
      "epoch 85 [15.04s]: training loss=0.0526089109480381  validation ndcg@10=0.02686174656111942 [0.53s]\n",
      "epoch 86 [14.94s]:  training loss=0.054076164960861206                                     \n",
      "epoch 87 [15.41s]:  training loss=0.05340413376688957                                      \n",
      "epoch 88 [15.01s]:  training loss=0.052304819226264954                                     \n",
      "epoch 89 [14.83s]:  training loss=0.050207074731588364                                     \n",
      "epoch 90 [15.17s]: training loss=0.05059758946299553  validation ndcg@10=0.027524369606796575 [0.53s]\n",
      "epoch 91 [14.89s]:  training loss=0.050406262278556824                                     \n",
      "epoch 92 [15.1s]:  training loss=0.05022595822811127                                       \n",
      "epoch 93 [15.01s]:  training loss=0.048703353852033615                                     \n",
      "epoch 94 [14.84s]:  training loss=0.04865971580147743                                      \n",
      "epoch 95 [15.11s]: training loss=0.04835093393921852  validation ndcg@10=0.026751115930792935 [0.53s]\n",
      "epoch 1 [17.2s]:  training loss=0.7741873860359192                                         \n",
      "epoch 2 [16.49s]:  training loss=0.658289909362793                                         \n",
      "epoch 3 [16.88s]:  training loss=0.4999493658542633                                        \n",
      "epoch 4 [16.94s]:  training loss=0.4186769723892212                                        \n",
      "epoch 5 [17.21s]: training loss=0.35201960802078247  validation ndcg@10=0.021956738269076386 [0.53s]\n",
      "epoch 6 [16.77s]:  training loss=0.2848595678806305                                        \n",
      "epoch 7 [16.33s]:  training loss=0.23429511487483978                                       \n",
      "epoch 8 [16.27s]:  training loss=0.19986456632614136                                       \n",
      "epoch 9 [16.73s]:  training loss=0.1720173954963684                                        \n",
      "epoch 10 [16.66s]: training loss=0.15379102528095245  validation ndcg@10=0.02382859157704529 [0.55s]\n",
      "epoch 11 [16.42s]:  training loss=0.1380445808172226                                       \n",
      "epoch 12 [16.58s]:  training loss=0.1231648325920105                                       \n",
      "epoch 13 [17.78s]:  training loss=0.11410016566514969                                      \n",
      "epoch 14 [14.35s]:  training loss=0.10434117913246155                                      \n",
      "epoch 15 [16.21s]: training loss=0.09759733825922012  validation ndcg@10=0.026171903451431784 [0.49s]\n",
      "epoch 16 [16.35s]:  training loss=0.09020604938268661                                      \n",
      "epoch 17 [16.16s]:  training loss=0.08666928112506866                                      \n",
      "epoch 18 [16.28s]:  training loss=0.07887531071901321                                      \n",
      "epoch 19 [16.25s]:  training loss=0.07938303053379059                                      \n",
      "epoch 20 [16.37s]: training loss=0.07160602509975433  validation ndcg@10=0.025541552446462418 [0.49s]\n",
      "epoch 21 [15.85s]:  training loss=0.07010100781917572                                      \n",
      "epoch 22 [16.09s]:  training loss=0.06561709940433502                                      \n",
      "epoch 23 [16.4s]:  training loss=0.06585301458835602                                       \n",
      "epoch 24 [16.53s]:  training loss=0.0614139698445797                                       \n",
      "epoch 25 [16.39s]: training loss=0.059478893876075745  validation ndcg@10=0.025953416594534415 [0.52s]\n",
      "epoch 26 [16.46s]:  training loss=0.05891141667962074                                      \n",
      "epoch 27 [16.03s]:  training loss=0.05544959381222725                                      \n",
      "epoch 28 [16.19s]:  training loss=0.0525486096739769                                       \n",
      "epoch 29 [15.87s]:  training loss=0.05450280010700226                                      \n",
      "epoch 30 [16.1s]: training loss=0.04975266754627228  validation ndcg@10=0.026916826764638403 [0.51s]\n",
      "epoch 31 [16.23s]:  training loss=0.0495016984641552                                       \n",
      "epoch 32 [16.5s]:  training loss=0.050368960946798325                                      \n",
      "epoch 33 [15.95s]:  training loss=0.04895933344960213                                      \n",
      "epoch 34 [16.32s]:  training loss=0.04615185037255287                                      \n",
      "epoch 35 [16.16s]: training loss=0.04548289626836777  validation ndcg@10=0.02654544514097087 [0.53s]\n",
      "epoch 36 [16.08s]:  training loss=0.04549393802881241                                      \n",
      "epoch 37 [16.02s]:  training loss=0.0446297749876976                                       \n",
      "epoch 38 [15.68s]:  training loss=0.04438869282603264                                      \n",
      "epoch 39 [16.31s]:  training loss=0.04184182360768318                                      \n",
      "epoch 40 [16.82s]: training loss=0.041092902421951294  validation ndcg@10=0.02753271765563941 [0.51s]\n",
      "epoch 41 [15.95s]:  training loss=0.04208196699619293                                      \n",
      "epoch 42 [16.41s]:  training loss=0.04114387556910515                                      \n",
      "epoch 43 [16.9s]:  training loss=0.0408485122025013                                        \n",
      "epoch 44 [16.37s]:  training loss=0.039583005011081696                                     \n",
      "epoch 45 [16.27s]: training loss=0.037884391844272614  validation ndcg@10=0.026787196336753853 [0.53s]\n",
      "epoch 46 [16.38s]:  training loss=0.03738022595643997                                      \n",
      "epoch 47 [15.91s]:  training loss=0.03932385891675949                                      \n",
      "epoch 48 [16.82s]:  training loss=0.036562494933605194                                     \n",
      "epoch 49 [15.97s]:  training loss=0.03569050878286362                                      \n",
      "epoch 50 [16.64s]: training loss=0.03601457178592682  validation ndcg@10=0.027348493662807165 [0.53s]\n",
      "epoch 51 [16.27s]:  training loss=0.03583710640668869                                      \n",
      "epoch 52 [16.75s]:  training loss=0.03692689165472984                                      \n",
      "epoch 53 [16.41s]:  training loss=0.03458370640873909                                      \n",
      "epoch 54 [16.63s]:  training loss=0.03529752418398857                                      \n",
      "epoch 55 [16.4s]: training loss=0.0348571352660656  validation ndcg@10=0.026223839881564976 [0.54s]\n",
      "epoch 56 [16.23s]:  training loss=0.03385687619447708                                      \n",
      "epoch 57 [16.25s]:  training loss=0.032727133482694626                                     \n",
      "epoch 58 [16.36s]:  training loss=0.03269559517502785                                      \n",
      "epoch 59 [16.52s]:  training loss=0.033545129001140594                                     \n",
      "epoch 60 [17.59s]: training loss=0.03532000258564949  validation ndcg@10=0.026266240308494122 [0.54s]\n",
      "epoch 61 [16.02s]:  training loss=0.032574914395809174                                     \n",
      "epoch 62 [16.39s]:  training loss=0.030813150107860565                                     \n",
      "epoch 63 [16.08s]:  training loss=0.033102963119745255                                     \n",
      "epoch 64 [16.11s]:  training loss=0.03320512920618057                                      \n",
      "epoch 65 [15.96s]: training loss=0.032257914543151855  validation ndcg@10=0.026631917751720875 [0.5s]\n",
      "epoch 1 [50.4s]:  training loss=0.8402749300003052                                         \n",
      "epoch 2 [52.04s]:  training loss=0.8146008253097534                                       \n",
      "epoch 3 [52.71s]:  training loss=0.802448034286499                                        \n",
      "epoch 4 [52.74s]:  training loss=0.8004298806190491                                       \n",
      "epoch 5 [52.52s]: training loss=0.7913346886634827  validation ndcg@10=0.00081048984544143 [1.08s]\n",
      "epoch 6 [52.18s]:  training loss=0.7928301692008972                                       \n",
      "epoch 7 [52.09s]:  training loss=0.7887711524963379                                       \n",
      "epoch 8 [51.73s]:  training loss=0.7847690582275391                                       \n",
      "epoch 9 [52.45s]:  training loss=0.7820000648498535                                       \n",
      "epoch 10 [51.76s]: training loss=0.7801279425621033  validation ndcg@10=0.0008609678615704378 [1.11s]\n",
      "epoch 11 [51.33s]:  training loss=0.7773580551147461                                      \n",
      "epoch 12 [52.51s]:  training loss=0.7749536037445068                                      \n",
      "epoch 13 [52.07s]:  training loss=0.7709733247756958                                      \n",
      "epoch 14 [54.55s]:  training loss=0.7735363245010376                                      \n",
      "epoch 15 [52.04s]: training loss=0.7660349011421204  validation ndcg@10=0.003521981290851521 [1.14s]\n",
      "epoch 16 [52.22s]:  training loss=0.7648490071296692                                      \n",
      "epoch 17 [52.61s]:  training loss=0.7638767957687378                                      \n",
      "epoch 18 [52.32s]:  training loss=0.7601555585861206                                      \n",
      "epoch 19 [52.6s]:  training loss=0.7560787200927734                                       \n",
      "epoch 20 [52.36s]: training loss=0.7549753189086914  validation ndcg@10=0.005582469046670794 [1.1s]\n",
      "epoch 21 [52.25s]:  training loss=0.7501957416534424                                      \n",
      "epoch 22 [52.14s]:  training loss=0.7491455078125                                         \n",
      "epoch 23 [52.37s]:  training loss=0.7463692426681519                                      \n",
      "epoch 24 [52.3s]:  training loss=0.739870548248291                                        \n",
      "epoch 25 [51.69s]: training loss=0.7382614612579346  validation ndcg@10=0.014069899128900715 [1.08s]\n",
      "epoch 26 [51.7s]:  training loss=0.7311636209487915                                       \n",
      "epoch 27 [52.03s]:  training loss=0.7322327494621277                                      \n",
      "epoch 28 [52.8s]:  training loss=0.7266843914985657                                       \n",
      "epoch 29 [53.98s]:  training loss=0.7218531966209412                                      \n",
      "epoch 30 [52.33s]: training loss=0.7174239754676819  validation ndcg@10=0.019648342678736587 [1.12s]\n",
      "epoch 31 [52.87s]:  training loss=0.7134911417961121                                      \n",
      "epoch 32 [51.99s]:  training loss=0.7025667428970337                                      \n",
      "epoch 33 [52.08s]:  training loss=0.6956315636634827                                      \n",
      "epoch 34 [52.3s]:  training loss=0.6841487884521484                                       \n",
      "epoch 35 [52.11s]: training loss=0.6720097064971924  validation ndcg@10=0.019482352567622152 [1.13s]\n",
      "epoch 36 [52.48s]:  training loss=0.6620194911956787                                      \n",
      "epoch 37 [52.17s]:  training loss=0.6506175398826599                                      \n",
      "epoch 38 [53.08s]:  training loss=0.6404576301574707                                      \n",
      "epoch 39 [52.83s]:  training loss=0.631140410900116                                       \n",
      "epoch 40 [52.66s]: training loss=0.6238574981689453  validation ndcg@10=0.019548016463933112 [1.1s]\n",
      "epoch 41 [51.31s]:  training loss=0.6211071014404297                                      \n",
      "epoch 42 [51.62s]:  training loss=0.6183090209960938                                      \n",
      "epoch 43 [52.03s]:  training loss=0.6100365519523621                                      \n",
      "epoch 44 [52.98s]:  training loss=0.6052259802818298                                      \n",
      "epoch 45 [52.23s]: training loss=0.6032888889312744  validation ndcg@10=0.019720019265140463 [1.14s]\n",
      "epoch 46 [52.24s]:  training loss=0.5975147485733032                                      \n",
      "epoch 47 [51.68s]:  training loss=0.5973541736602783                                      \n",
      "epoch 48 [51.75s]:  training loss=0.589030385017395                                       \n",
      "epoch 49 [51.44s]:  training loss=0.5886716246604919                                      \n",
      "epoch 50 [53.15s]: training loss=0.5882878303527832  validation ndcg@10=0.019610583933614905 [1.1s]\n",
      "epoch 51 [51.63s]:  training loss=0.5826350450515747                                      \n",
      "epoch 52 [52.26s]:  training loss=0.5814684629440308                                      \n",
      "epoch 53 [51.15s]:  training loss=0.5771550536155701                                      \n",
      "epoch 54 [52.65s]:  training loss=0.5741845965385437                                      \n",
      "epoch 55 [51.95s]: training loss=0.5719837546348572  validation ndcg@10=0.019646889858966465 [1.11s]\n",
      "epoch 56 [51.91s]:  training loss=0.5722249150276184                                      \n",
      "epoch 57 [52.25s]:  training loss=0.5693382620811462                                      \n",
      "epoch 58 [51.24s]:  training loss=0.5655756592750549                                      \n",
      "epoch 59 [53.75s]:  training loss=0.5598269701004028                                      \n",
      "epoch 60 [50.79s]: training loss=0.5585337281227112  validation ndcg@10=0.019593495115999275 [1.08s]\n",
      "epoch 61 [49.7s]:  training loss=0.5583123564720154                                       \n",
      "epoch 62 [54.37s]:  training loss=0.5541110634803772                                      \n",
      "epoch 63 [54.68s]:  training loss=0.5514656901359558                                      \n",
      "epoch 64 [54.3s]:  training loss=0.5510735511779785                                       \n",
      "epoch 65 [54.67s]: training loss=0.5477319955825806  validation ndcg@10=0.01952896829798487 [1.13s]\n",
      "epoch 66 [54.93s]:  training loss=0.544748067855835                                       \n",
      "epoch 67 [54.2s]:  training loss=0.5450180768966675                                       \n",
      "epoch 68 [54.65s]:  training loss=0.543841540813446                                       \n",
      "epoch 69 [54.89s]:  training loss=0.5367259979248047                                      \n",
      "epoch 70 [54.69s]: training loss=0.5387253761291504  validation ndcg@10=0.019420728109839122 [1.12s]\n",
      "epoch 1 [10.41s]:  training loss=0.8453163504600525                                        \n",
      "epoch 2 [10.44s]:  training loss=0.8129560947418213                                        \n",
      "epoch 3 [10.6s]:  training loss=0.801261842250824                                          \n",
      "epoch 4 [10.66s]:  training loss=0.7913525700569153                                        \n",
      "epoch 5 [10.48s]: training loss=0.7844144701957703  validation ndcg@10=0.0008394123036286274 [0.38s]\n",
      "epoch 6 [10.65s]:  training loss=0.7764829993247986                                        \n",
      "epoch 7 [10.59s]:  training loss=0.7643705606460571                                        \n",
      "epoch 8 [10.21s]:  training loss=0.7563291192054749                                        \n",
      "epoch 9 [9.75s]:  training loss=0.7383888959884644                                         \n",
      "epoch 10 [10.06s]: training loss=0.7203385233879089  validation ndcg@10=0.011333542794201094 [0.45s]\n",
      "epoch 11 [10.16s]:  training loss=0.7026079297065735                                       \n",
      "epoch 12 [10.25s]:  training loss=0.6898354887962341                                       \n",
      "epoch 13 [11.65s]:  training loss=0.6728065013885498                                       \n",
      "epoch 14 [9.99s]:  training loss=0.6607372760772705                                        \n",
      "epoch 15 [10.05s]: training loss=0.6554841995239258  validation ndcg@10=0.016185085827020485 [0.38s]\n",
      "epoch 16 [10.27s]:  training loss=0.6483117938041687                                       \n",
      "epoch 17 [9.65s]:  training loss=0.638883113861084                                         \n",
      "epoch 18 [9.75s]:  training loss=0.6357747912406921                                        \n",
      "epoch 19 [9.6s]:  training loss=0.6227036118507385                                         \n",
      "epoch 20 [10.14s]: training loss=0.6211814284324646  validation ndcg@10=0.017522304377063987 [0.37s]\n",
      "epoch 21 [10.15s]:  training loss=0.6177850961685181                                       \n",
      "epoch 22 [9.95s]:  training loss=0.6130136251449585                                        \n",
      "epoch 23 [10.16s]:  training loss=0.6070342659950256                                       \n",
      "epoch 24 [9.68s]:  training loss=0.6030986905097961                                        \n",
      "epoch 25 [10.02s]: training loss=0.5984269380569458  validation ndcg@10=0.018461829590816024 [0.38s]\n",
      "epoch 26 [9.66s]:  training loss=0.5919592380523682                                        \n",
      "epoch 27 [9.87s]:  training loss=0.5906705260276794                                        \n",
      "epoch 28 [9.79s]:  training loss=0.5853301286697388                                        \n",
      "epoch 29 [9.78s]:  training loss=0.5822045803070068                                        \n",
      "epoch 30 [9.72s]: training loss=0.5746622085571289  validation ndcg@10=0.019255953861794912 [0.38s]\n",
      "epoch 31 [9.7s]:  training loss=0.5734854340553284                                         \n",
      "epoch 32 [9.9s]:  training loss=0.5687493681907654                                         \n",
      "epoch 33 [9.94s]:  training loss=0.5618771910667419                                        \n",
      "epoch 34 [9.53s]:  training loss=0.5574178099632263                                        \n",
      "epoch 35 [9.54s]: training loss=0.5549056529998779  validation ndcg@10=0.019487907366143517 [0.38s]\n",
      "epoch 36 [9.63s]:  training loss=0.5495186448097229                                        \n",
      "epoch 37 [9.75s]:  training loss=0.5487840175628662                                        \n",
      "epoch 38 [9.26s]:  training loss=0.5428733229637146                                        \n",
      "epoch 39 [7.63s]:  training loss=0.5378017425537109                                        \n",
      "epoch 40 [7.62s]: training loss=0.5328608751296997  validation ndcg@10=0.01999850194905202 [0.34s]\n",
      "epoch 41 [7.73s]:  training loss=0.5251466035842896                                        \n",
      "epoch 42 [7.7s]:  training loss=0.5259108543395996                                         \n",
      "epoch 43 [7.8s]:  training loss=0.5202151536941528                                         \n",
      "epoch 44 [7.74s]:  training loss=0.5167773962020874                                        \n",
      "epoch 45 [7.67s]: training loss=0.5117276906967163  validation ndcg@10=0.019987122900096226 [0.36s]\n",
      "epoch 46 [7.77s]:  training loss=0.5061306357383728                                        \n",
      "epoch 47 [7.75s]:  training loss=0.5032439231872559                                        \n",
      "epoch 48 [7.65s]:  training loss=0.49891549348831177                                       \n",
      "epoch 49 [7.72s]:  training loss=0.4951443076133728                                        \n",
      "epoch 50 [7.67s]: training loss=0.48917698860168457  validation ndcg@10=0.020246208705681064 [0.35s]\n",
      "epoch 51 [7.69s]:  training loss=0.4874780774116516                                        \n",
      "epoch 52 [7.61s]:  training loss=0.4845440983772278                                        \n",
      "epoch 53 [7.69s]:  training loss=0.4740169048309326                                        \n",
      "epoch 54 [7.76s]:  training loss=0.47531694173812866                                       \n",
      "epoch 55 [7.6s]: training loss=0.4689745306968689  validation ndcg@10=0.019929863914787033 [0.33s]\n",
      "epoch 56 [7.72s]:  training loss=0.46855613589286804                                       \n",
      "epoch 57 [7.61s]:  training loss=0.4634852707386017                                        \n",
      "epoch 58 [7.59s]:  training loss=0.46151024103164673                                       \n",
      "epoch 59 [7.63s]:  training loss=0.45322301983833313                                       \n",
      "epoch 60 [7.67s]: training loss=0.45455557107925415  validation ndcg@10=0.019806285846311642 [0.35s]\n",
      "epoch 61 [7.57s]:  training loss=0.4486442804336548                                        \n",
      "epoch 62 [7.61s]:  training loss=0.44685590267181396                                       \n",
      "epoch 63 [7.6s]:  training loss=0.44518205523490906                                        \n",
      "epoch 64 [7.59s]:  training loss=0.44195878505706787                                       \n",
      "epoch 65 [7.56s]: training loss=0.4417053163051605  validation ndcg@10=0.0200941948628841 [0.35s]\n",
      "epoch 66 [7.49s]:  training loss=0.4333440065383911                                        \n",
      "epoch 67 [7.57s]:  training loss=0.42879292368888855                                       \n",
      "epoch 68 [7.54s]:  training loss=0.42802637815475464                                       \n",
      "epoch 69 [7.55s]:  training loss=0.4220852553844452                                        \n",
      "epoch 70 [7.6s]: training loss=0.4195605218410492  validation ndcg@10=0.019320554108392996 [0.33s]\n",
      "epoch 71 [7.47s]:  training loss=0.4198157489299774                                        \n",
      "epoch 72 [7.72s]:  training loss=0.4145975112915039                                        \n",
      "epoch 73 [7.6s]:  training loss=0.4121726155281067                                         \n",
      "epoch 74 [7.86s]:  training loss=0.40920382738113403                                       \n",
      "epoch 75 [7.83s]: training loss=0.40006914734840393  validation ndcg@10=0.018907657487011124 [0.35s]\n",
      "epoch 1 [6.33s]:  training loss=0.6495802998542786                                         \n",
      "epoch 2 [6.27s]:  training loss=0.4439679980278015                                        \n",
      "epoch 3 [7.0s]:  training loss=0.35594645142555237                                        \n",
      "epoch 4 [6.68s]:  training loss=0.29179203510284424                                       \n",
      "epoch 5 [6.69s]: training loss=0.24010999500751495  validation ndcg@10=0.006303509186253539 [0.46s]\n",
      "epoch 6 [7.04s]:  training loss=0.20569580793380737                                       \n",
      "epoch 7 [6.63s]:  training loss=0.18636658787727356                                       \n",
      "epoch 8 [6.45s]:  training loss=0.1746593713760376                                        \n",
      "epoch 9 [7.03s]:  training loss=0.16376525163650513                                       \n",
      "epoch 10 [6.68s]: training loss=0.1551181972026825  validation ndcg@10=0.005893006763618849 [0.43s]\n",
      "epoch 11 [7.15s]:  training loss=0.14772550761699677                                      \n",
      "epoch 12 [6.84s]:  training loss=0.14429442584514618                                      \n",
      "epoch 13 [6.87s]:  training loss=0.141916424036026                                        \n",
      "epoch 14 [6.77s]:  training loss=0.1356048285961151                                       \n",
      "epoch 15 [6.74s]: training loss=0.13410593569278717  validation ndcg@10=0.005626065055588113 [0.42s]\n",
      "epoch 16 [6.82s]:  training loss=0.13348975777626038                                      \n",
      "epoch 17 [6.69s]:  training loss=0.12798574566841125                                      \n",
      "epoch 18 [7.11s]:  training loss=0.1317324936389923                                       \n",
      "epoch 19 [6.51s]:  training loss=0.12631738185882568                                      \n",
      "epoch 20 [7.0s]: training loss=0.12659282982349396  validation ndcg@10=0.004606822998969362 [0.42s]\n",
      "epoch 21 [6.84s]:  training loss=0.1282128393650055                                       \n",
      "epoch 22 [6.93s]:  training loss=0.12459715455770493                                      \n",
      "epoch 23 [6.72s]:  training loss=0.12367987632751465                                      \n",
      "epoch 24 [6.38s]:  training loss=0.11976327002048492                                      \n",
      "epoch 25 [6.81s]: training loss=0.1190827414393425  validation ndcg@10=0.0036810165818561493 [0.43s]\n",
      "epoch 26 [6.73s]:  training loss=0.1194787248969078                                       \n",
      "epoch 27 [6.8s]:  training loss=0.12003399431705475                                       \n",
      "epoch 28 [7.32s]:  training loss=0.12293177843093872                                      \n",
      "epoch 29 [6.94s]:  training loss=0.11867965012788773                                      \n",
      "epoch 30 [7.01s]: training loss=0.1194465383887291  validation ndcg@10=0.003998887661106489 [0.46s]\n",
      "epoch 1 [18.88s]:  training loss=0.7816901206970215                                       \n",
      "epoch 2 [17.15s]:  training loss=0.6926278471946716                                       \n",
      "epoch 3 [18.05s]:  training loss=0.5402275919914246                                       \n",
      "epoch 4 [17.75s]:  training loss=0.4589253067970276                                       \n",
      "epoch 5 [18.77s]: training loss=0.4013562500476837  validation ndcg@10=0.023208089843491063 [0.54s]\n",
      "epoch 6 [17.42s]:  training loss=0.3479576110839844                                       \n",
      "epoch 7 [17.34s]:  training loss=0.2929072678089142                                       \n",
      "epoch 8 [17.85s]:  training loss=0.24791018664836884                                      \n",
      "epoch 9 [18.66s]:  training loss=0.21422508358955383                                      \n",
      "epoch 10 [18.1s]: training loss=0.18430949747562408  validation ndcg@10=0.024103627397572203 [0.54s]\n",
      "epoch 11 [18.05s]:  training loss=0.16835033893585205                                     \n",
      "epoch 12 [18.15s]:  training loss=0.14925698935985565                                     \n",
      "epoch 13 [18.03s]:  training loss=0.1404464840888977                                      \n",
      "epoch 14 [17.98s]:  training loss=0.1284807324409485                                      \n",
      "epoch 15 [17.83s]: training loss=0.11960815638303757  validation ndcg@10=0.025683594835718027 [0.5s]\n",
      "epoch 16 [18.12s]:  training loss=0.11129412055015564                                     \n",
      "epoch 17 [18.11s]:  training loss=0.10358066111803055                                     \n",
      "epoch 18 [17.73s]:  training loss=0.09608291834592819                                     \n",
      "epoch 19 [17.73s]:  training loss=0.09238091111183167                                     \n",
      "epoch 20 [17.55s]: training loss=0.08800578862428665  validation ndcg@10=0.027209254278608694 [0.49s]\n",
      "epoch 21 [17.84s]:  training loss=0.08322980254888535                                     \n",
      "epoch 22 [17.9s]:  training loss=0.07948651909828186                                      \n",
      "epoch 23 [18.14s]:  training loss=0.07719795405864716                                     \n",
      "epoch 24 [17.57s]:  training loss=0.0716690868139267                                      \n",
      "epoch 25 [17.61s]: training loss=0.07037033885717392  validation ndcg@10=0.027784672609492817 [0.5s]\n",
      "epoch 26 [17.49s]:  training loss=0.06835411489009857                                     \n",
      "epoch 27 [17.31s]:  training loss=0.06485169380903244                                     \n",
      "epoch 28 [17.46s]:  training loss=0.061792291700839996                                    \n",
      "epoch 29 [18.37s]:  training loss=0.06012945622205734                                     \n",
      "epoch 30 [17.96s]: training loss=0.05977373570203781  validation ndcg@10=0.027812791530561296 [0.51s]\n",
      "epoch 31 [17.88s]:  training loss=0.05801454186439514                                     \n",
      "epoch 32 [17.2s]:  training loss=0.05654969811439514                                      \n",
      "epoch 33 [17.38s]:  training loss=0.055450111627578735                                    \n",
      "epoch 34 [17.68s]:  training loss=0.05440523102879524                                     \n",
      "epoch 35 [17.76s]: training loss=0.05024450272321701  validation ndcg@10=0.027864676038000562 [0.49s]\n",
      "epoch 36 [17.34s]:  training loss=0.05117592588067055                                     \n",
      "epoch 37 [17.56s]:  training loss=0.04787323251366615                                     \n",
      "epoch 38 [17.62s]:  training loss=0.04807260259985924                                     \n",
      "epoch 39 [17.39s]:  training loss=0.04717369005084038                                     \n",
      "epoch 40 [17.74s]: training loss=0.04675988107919693  validation ndcg@10=0.028618992192645715 [0.5s]\n",
      "epoch 41 [17.35s]:  training loss=0.04565242677927017                                     \n",
      "epoch 42 [17.64s]:  training loss=0.04345232993364334                                     \n",
      "epoch 43 [17.96s]:  training loss=0.04364316910505295                                     \n",
      "epoch 44 [17.83s]:  training loss=0.043450728058815                                       \n",
      "epoch 45 [19.26s]: training loss=0.040954336524009705  validation ndcg@10=0.027326145701864694 [0.49s]\n",
      "epoch 46 [17.3s]:  training loss=0.04236925393342972                                      \n",
      "epoch 47 [17.52s]:  training loss=0.042278043925762177                                    \n",
      "epoch 48 [17.49s]:  training loss=0.03975359722971916                                     \n",
      "epoch 49 [17.5s]:  training loss=0.040086694061756134                                     \n",
      "epoch 50 [17.46s]: training loss=0.03902456536889076  validation ndcg@10=0.02819087534141622 [0.5s]\n",
      "epoch 51 [17.8s]:  training loss=0.040413178503513336                                     \n",
      "epoch 52 [17.56s]:  training loss=0.037644192576408386                                    \n",
      "epoch 53 [17.3s]:  training loss=0.037735745310783386                                     \n",
      "epoch 54 [17.29s]:  training loss=0.0376216322183609                                      \n",
      "epoch 55 [17.5s]: training loss=0.036463383585214615  validation ndcg@10=0.02709118199881565 [0.49s]\n",
      "epoch 56 [17.41s]:  training loss=0.03717717528343201                                     \n",
      "epoch 57 [17.46s]:  training loss=0.03505044803023338                                     \n",
      "epoch 58 [17.5s]:  training loss=0.036040596663951874                                     \n",
      "epoch 59 [17.84s]:  training loss=0.03606712073087692                                     \n",
      "epoch 60 [17.62s]: training loss=0.0339520163834095  validation ndcg@10=0.027296762090730137 [0.51s]\n",
      "epoch 61 [18.12s]:  training loss=0.03425954282283783                                     \n",
      "epoch 62 [17.33s]:  training loss=0.03465713560581207                                     \n",
      "epoch 63 [17.86s]:  training loss=0.03370382636785507                                     \n",
      "epoch 64 [17.4s]:  training loss=0.033131230622529984                                     \n",
      "epoch 65 [18.15s]: training loss=0.035122305154800415  validation ndcg@10=0.026640652832648517 [0.51s]\n",
      "epoch 1 [50.36s]:  training loss=0.7926390767097473                                       \n",
      "epoch 2 [51.42s]:  training loss=0.7522704005241394                                       \n",
      "epoch 3 [50.93s]:  training loss=0.7035841941833496                                       \n",
      "epoch 4 [51.29s]:  training loss=0.5921642780303955                                       \n",
      "epoch 5 [50.68s]: training loss=0.5513870120048523  validation ndcg@10=0.020240316674458417 [1.02s]\n",
      "epoch 6 [51.94s]:  training loss=0.5141628980636597                                       \n",
      "epoch 7 [50.34s]:  training loss=0.4826207756996155                                       \n",
      "epoch 8 [52.47s]:  training loss=0.4561592936515808                                       \n",
      "epoch 9 [56.02s]:  training loss=0.4291499853134155                                       \n",
      "epoch 10 [49.78s]: training loss=0.4083612859249115  validation ndcg@10=0.02142148184185213 [1.01s]\n",
      "epoch 11 [51.54s]:  training loss=0.3776318430900574                                      \n",
      "epoch 12 [52.86s]:  training loss=0.3458305597305298                                      \n",
      "epoch 13 [52.85s]:  training loss=0.32350435853004456                                     \n",
      "epoch 14 [52.81s]:  training loss=0.302118182182312                                       \n",
      "epoch 15 [53.31s]: training loss=0.2858661711215973  validation ndcg@10=0.022961394989051524 [1.1s]\n",
      "epoch 16 [52.7s]:  training loss=0.27117419242858887                                      \n",
      "epoch 17 [53.61s]:  training loss=0.25094494223594666                                     \n",
      "epoch 18 [53.06s]:  training loss=0.24099795520305634                                     \n",
      "epoch 19 [53.05s]:  training loss=0.22480882704257965                                     \n",
      "epoch 20 [53.05s]: training loss=0.21410724520683289  validation ndcg@10=0.023410238812292813 [1.11s]\n",
      "epoch 21 [54.84s]:  training loss=0.2025773972272873                                      \n",
      "epoch 22 [54.63s]:  training loss=0.1969291865825653                                      \n",
      "epoch 23 [55.73s]:  training loss=0.18461954593658447                                     \n",
      "epoch 24 [54.56s]:  training loss=0.17811109125614166                                     \n",
      "epoch 25 [54.56s]: training loss=0.16661112010478973  validation ndcg@10=0.023978294209631275 [1.15s]\n",
      "epoch 26 [55.36s]:  training loss=0.1601914018392563                                      \n",
      "epoch 27 [54.1s]:  training loss=0.15077419579029083                                      \n",
      "epoch 28 [54.41s]:  training loss=0.1460481584072113                                      \n",
      "epoch 29 [54.48s]:  training loss=0.13873904943466187                                     \n",
      "epoch 30 [55.93s]: training loss=0.1323482096195221  validation ndcg@10=0.02547935992243884 [1.14s]\n",
      "epoch 31 [54.68s]:  training loss=0.13013696670532227                                     \n",
      "epoch 32 [55.29s]:  training loss=0.12539894878864288                                     \n",
      "epoch 33 [54.54s]:  training loss=0.11995865404605865                                     \n",
      "epoch 34 [54.81s]:  training loss=0.11402831226587296                                     \n",
      "epoch 35 [54.82s]: training loss=0.10791010409593582  validation ndcg@10=0.02478068859920511 [1.14s]\n",
      "epoch 36 [55.12s]:  training loss=0.10700355470180511                                     \n",
      "epoch 37 [53.6s]:  training loss=0.10451534390449524                                      \n",
      "epoch 38 [53.23s]:  training loss=0.09733470529317856                                     \n",
      "epoch 39 [54.38s]:  training loss=0.0971851721405983                                      \n",
      "epoch 40 [53.92s]: training loss=0.09467199444770813  validation ndcg@10=0.025984093563773503 [1.15s]\n",
      "epoch 41 [53.95s]:  training loss=0.09270717948675156                                     \n",
      "epoch 42 [54.24s]:  training loss=0.08822141587734222                                     \n",
      "epoch 43 [53.94s]:  training loss=0.08690913021564484                                     \n",
      "epoch 44 [54.01s]:  training loss=0.0828348770737648                                      \n",
      "epoch 45 [54.05s]: training loss=0.0807640478014946  validation ndcg@10=0.027966652286574755 [1.07s]\n",
      "epoch 46 [54.02s]:  training loss=0.07808221131563187                                     \n",
      "epoch 47 [53.48s]:  training loss=0.0778549313545227                                      \n",
      "epoch 48 [54.05s]:  training loss=0.075568288564682                                       \n",
      "epoch 49 [54.11s]:  training loss=0.07237608730792999                                     \n",
      "epoch 50 [53.98s]: training loss=0.07074792683124542  validation ndcg@10=0.028608352742963927 [1.01s]\n",
      "epoch 51 [52.16s]:  training loss=0.06892812252044678                                     \n",
      "epoch 52 [53.27s]:  training loss=0.06944016367197037                                     \n",
      "epoch 53 [53.29s]:  training loss=0.06759560853242874                                     \n",
      "epoch 54 [53.46s]:  training loss=0.06592637300491333                                     \n",
      "epoch 55 [53.79s]: training loss=0.06683207303285599  validation ndcg@10=0.02887472319130747 [1.08s]\n",
      "epoch 56 [53.98s]:  training loss=0.06482381373643875                                     \n",
      "epoch 57 [54.07s]:  training loss=0.06305667757987976                                     \n",
      "epoch 58 [54.19s]:  training loss=0.060054194182157516                                    \n",
      "epoch 59 [55.42s]:  training loss=0.05876072868704796                                     \n",
      "epoch 60 [54.28s]: training loss=0.058740537613630295  validation ndcg@10=0.028153584062061456 [1.07s]\n",
      "epoch 61 [53.79s]:  training loss=0.05821138992905617                                     \n",
      "epoch 62 [53.31s]:  training loss=0.05827263370156288                                     \n",
      "epoch 63 [54.17s]:  training loss=0.056974876672029495                                    \n",
      "epoch 64 [54.52s]:  training loss=0.05617902800440788                                     \n",
      "epoch 65 [52.78s]: training loss=0.054220642894506454  validation ndcg@10=0.028391501147139613 [1.03s]\n",
      "epoch 66 [52.46s]:  training loss=0.053690142929553986                                    \n",
      "epoch 67 [53.93s]:  training loss=0.05134805291891098                                     \n",
      "epoch 68 [53.83s]:  training loss=0.05078425630927086                                     \n",
      "epoch 69 [54.43s]:  training loss=0.05058886110782623                                     \n",
      "epoch 70 [54.76s]: training loss=0.050333861261606216  validation ndcg@10=0.026829475340969054 [1.04s]\n",
      "epoch 71 [53.7s]:  training loss=0.048903971910476685                                     \n",
      "epoch 72 [54.76s]:  training loss=0.05021090805530548                                     \n",
      "epoch 73 [52.97s]:  training loss=0.049109652638435364                                    \n",
      "epoch 74 [53.2s]:  training loss=0.04829446226358414                                      \n",
      "epoch 75 [54.74s]: training loss=0.046243179589509964  validation ndcg@10=0.027761492606669273 [1.08s]\n",
      "epoch 76 [55.21s]:  training loss=0.045109137892723083                                    \n",
      "epoch 77 [53.65s]:  training loss=0.045286111533641815                                    \n",
      "epoch 78 [54.19s]:  training loss=0.04643239453434944                                     \n",
      "epoch 79 [52.05s]:  training loss=0.04512515664100647                                     \n",
      "epoch 80 [53.62s]: training loss=0.04392888396978378  validation ndcg@10=0.028209406270464504 [1.01s]\n",
      "epoch 1 [53.14s]:  training loss=0.8254152536392212                                       \n",
      "epoch 2 [53.03s]:  training loss=0.7995158433914185                                       \n",
      "epoch 3 [54.03s]:  training loss=0.7912464737892151                                       \n",
      "epoch 4 [53.35s]:  training loss=0.7858012318611145                                       \n",
      "epoch 5 [54.6s]: training loss=0.7801515460014343  validation ndcg@10=0.0012668878774261387 [1.11s]\n",
      "epoch 6 [53.55s]:  training loss=0.7730125188827515                                       \n",
      "epoch 7 [54.09s]:  training loss=0.7713263034820557                                       \n",
      "epoch 8 [53.21s]:  training loss=0.7636620402336121                                       \n",
      "epoch 9 [53.49s]:  training loss=0.7629008293151855                                       \n",
      "epoch 10 [53.5s]: training loss=0.7543092966079712  validation ndcg@10=0.005887824609569731 [1.06s]\n",
      "epoch 11 [54.39s]:  training loss=0.750788152217865                                       \n",
      "epoch 12 [55.04s]:  training loss=0.7483664155006409                                      \n",
      "epoch 13 [52.93s]:  training loss=0.7398772835731506                                      \n",
      "epoch 14 [53.16s]:  training loss=0.735603392124176                                       \n",
      "epoch 15 [52.63s]: training loss=0.7258577346801758  validation ndcg@10=0.017537095328102253 [1.04s]\n",
      "epoch 16 [52.88s]:  training loss=0.7174140810966492                                      \n",
      "epoch 17 [53.77s]:  training loss=0.7020354270935059                                      \n",
      "epoch 18 [52.85s]:  training loss=0.681006669998169                                       \n",
      "epoch 19 [53.86s]:  training loss=0.658586859703064                                       \n",
      "epoch 20 [54.83s]: training loss=0.640713095664978  validation ndcg@10=0.018641450953651578 [1.18s]\n",
      "epoch 21 [54.53s]:  training loss=0.6290364861488342                                      \n",
      "epoch 22 [53.78s]:  training loss=0.6152345538139343                                      \n",
      "epoch 23 [53.46s]:  training loss=0.6076542139053345                                      \n",
      "epoch 24 [53.22s]:  training loss=0.6004652380943298                                      \n",
      "epoch 25 [52.85s]: training loss=0.5952587723731995  validation ndcg@10=0.01867107973124509 [1.02s]\n",
      "epoch 26 [54.25s]:  training loss=0.5887587666511536                                      \n",
      "epoch 27 [52.14s]:  training loss=0.5846038460731506                                      \n",
      "epoch 28 [53.4s]:  training loss=0.5770410299301147                                       \n",
      "epoch 29 [53.02s]:  training loss=0.5739274621009827                                      \n",
      "epoch 30 [52.84s]: training loss=0.5701768398284912  validation ndcg@10=0.01900989585384964 [1.09s]\n",
      "epoch 31 [53.8s]:  training loss=0.5634381175041199                                       \n",
      "epoch 32 [53.47s]:  training loss=0.5592057108879089                                      \n",
      "epoch 33 [53.44s]:  training loss=0.5570999979972839                                      \n",
      "epoch 34 [52.74s]:  training loss=0.5507619380950928                                      \n",
      "epoch 35 [52.63s]: training loss=0.544204831123352  validation ndcg@10=0.019383687500043382 [1.04s]\n",
      "epoch 36 [53.54s]:  training loss=0.5454633235931396                                      \n",
      "epoch 37 [53.27s]:  training loss=0.5365216135978699                                      \n",
      "epoch 38 [52.3s]:  training loss=0.5336779952049255                                       \n",
      "epoch 39 [52.51s]:  training loss=0.5299217104911804                                      \n",
      "epoch 40 [54.16s]: training loss=0.5219352841377258  validation ndcg@10=0.019639749752303474 [1.02s]\n",
      "epoch 41 [52.72s]:  training loss=0.5210343599319458                                      \n",
      "epoch 42 [52.68s]:  training loss=0.5205211639404297                                      \n",
      "epoch 43 [53.21s]:  training loss=0.5144210457801819                                      \n",
      "epoch 44 [52.53s]:  training loss=0.5101314783096313                                      \n",
      "epoch 45 [53.55s]: training loss=0.5066038966178894  validation ndcg@10=0.019520301456376635 [1.07s]\n",
      "epoch 46 [52.57s]:  training loss=0.501006007194519                                       \n",
      "epoch 47 [53.33s]:  training loss=0.49969857931137085                                     \n",
      "epoch 48 [52.87s]:  training loss=0.49167484045028687                                     \n",
      "epoch 49 [52.76s]:  training loss=0.4921124577522278                                      \n",
      "epoch 50 [52.77s]: training loss=0.48737022280693054  validation ndcg@10=0.019740617580557913 [1.0s]\n",
      "epoch 51 [54.37s]:  training loss=0.4819067120552063                                      \n",
      "epoch 52 [53.93s]:  training loss=0.4789140522480011                                      \n",
      "epoch 53 [53.19s]:  training loss=0.4711063504219055                                      \n",
      "epoch 54 [54.46s]:  training loss=0.4747651517391205                                      \n",
      "epoch 55 [51.87s]: training loss=0.4683595895767212  validation ndcg@10=0.019440121890201053 [1.05s]\n",
      "epoch 56 [52.44s]:  training loss=0.465310662984848                                       \n",
      "epoch 57 [55.48s]:  training loss=0.45591142773628235                                     \n",
      "epoch 58 [50.17s]:  training loss=0.4540925920009613                                      \n",
      "epoch 59 [50.94s]:  training loss=0.44950950145721436                                     \n",
      "epoch 60 [53.51s]: training loss=0.4500599503517151  validation ndcg@10=0.020229792293069822 [1.19s]\n",
      "epoch 61 [50.49s]:  training loss=0.4462064802646637                                      \n",
      "epoch 62 [52.29s]:  training loss=0.44032180309295654                                     \n",
      "epoch 63 [53.59s]:  training loss=0.4340108335018158                                      \n",
      "epoch 64 [54.33s]:  training loss=0.4317746162414551                                      \n",
      "epoch 65 [53.25s]: training loss=0.4313829243183136  validation ndcg@10=0.02074852133958156 [1.04s]\n",
      "epoch 66 [53.78s]:  training loss=0.42551469802856445                                     \n",
      "epoch 67 [53.13s]:  training loss=0.4180217981338501                                      \n",
      "epoch 68 [53.84s]:  training loss=0.41328656673431396                                     \n",
      "epoch 69 [55.73s]:  training loss=0.40985438227653503                                     \n",
      "epoch 70 [55.34s]: training loss=0.4062819480895996  validation ndcg@10=0.02101796205316111 [1.04s]\n",
      "epoch 71 [55.26s]:  training loss=0.3997834622859955                                      \n",
      "epoch 72 [54.22s]:  training loss=0.4007018804550171                                      \n",
      "epoch 73 [54.85s]:  training loss=0.3954601585865021                                      \n",
      "epoch 74 [55.18s]:  training loss=0.3925707936286926                                      \n",
      "epoch 75 [54.44s]: training loss=0.38907063007354736  validation ndcg@10=0.021880576880110844 [1.04s]\n",
      "epoch 76 [55.13s]:  training loss=0.38902273774147034                                     \n",
      "epoch 77 [54.79s]:  training loss=0.38325321674346924                                     \n",
      "epoch 78 [55.02s]:  training loss=0.3809435963630676                                      \n",
      "epoch 79 [54.77s]:  training loss=0.3786323368549347                                      \n",
      "epoch 80 [54.79s]: training loss=0.37268227338790894  validation ndcg@10=0.021787709003779476 [1.11s]\n",
      "epoch 81 [56.01s]:  training loss=0.36969614028930664                                     \n",
      "epoch 82 [56.08s]:  training loss=0.3685583472251892                                      \n",
      "epoch 83 [54.78s]:  training loss=0.3670039474964142                                      \n",
      "epoch 84 [52.51s]:  training loss=0.36124324798583984                                     \n",
      "epoch 85 [51.99s]: training loss=0.36043548583984375  validation ndcg@10=0.022018290912858635 [1.03s]\n",
      "epoch 86 [51.75s]:  training loss=0.35461312532424927                                     \n",
      "epoch 87 [53.29s]:  training loss=0.35584282875061035                                     \n",
      "epoch 88 [52.48s]:  training loss=0.347296804189682                                       \n",
      "epoch 89 [50.09s]:  training loss=0.3452266454696655                                      \n",
      "epoch 90 [51.69s]: training loss=0.3482625484466553  validation ndcg@10=0.022081180919640602 [1.09s]\n",
      "epoch 91 [52.77s]:  training loss=0.34072157740592957                                     \n",
      "epoch 92 [51.45s]:  training loss=0.3381502628326416                                      \n",
      "epoch 93 [54.24s]:  training loss=0.33959540724754333                                     \n",
      "epoch 94 [54.42s]:  training loss=0.3341827094554901                                      \n",
      "epoch 95 [60.32s]: training loss=0.331368625164032  validation ndcg@10=0.02276868842174755 [1.36s]\n",
      "epoch 96 [58.87s]:  training loss=0.3290158808231354                                      \n",
      "epoch 97 [56.83s]:  training loss=0.3262096047401428                                      \n",
      "epoch 98 [62.78s]:  training loss=0.32315167784690857                                     \n",
      "epoch 99 [61.21s]:  training loss=0.3276542127132416                                      \n",
      "epoch 100 [61.74s]: training loss=0.3200969398021698  validation ndcg@10=0.023073262844534745 [1.33s]\n",
      "epoch 101 [62.57s]:  training loss=0.31296876072883606                                    \n",
      "epoch 102 [67.06s]:  training loss=0.31522369384765625                                    \n",
      "epoch 103 [71.59s]:  training loss=0.310946524143219                                      \n",
      "epoch 104 [74.78s]:  training loss=0.3044394850730896                                     \n",
      "epoch 105 [77.45s]: training loss=0.3086559474468231  validation ndcg@10=0.0230300507689499 [1.58s]\n",
      "epoch 106 [73.04s]:  training loss=0.30467119812965393                                    \n",
      "epoch 107 [68.4s]:  training loss=0.302484393119812                                       \n",
      "epoch 108 [67.99s]:  training loss=0.29885995388031006                                    \n",
      "epoch 109 [67.81s]:  training loss=0.2976393401622772                                     \n",
      "epoch 110 [68.46s]: training loss=0.2946285307407379  validation ndcg@10=0.022732413358454023 [1.26s]\n",
      "epoch 111 [70.94s]:  training loss=0.2899831235408783                                     \n",
      "epoch 112 [74.35s]:  training loss=0.2888760566711426                                     \n",
      "epoch 113 [66.86s]:  training loss=0.29409512877464294                                    \n",
      "epoch 114 [61.4s]:  training loss=0.2861657738685608                                      \n",
      "epoch 115 [63.17s]: training loss=0.2847902178764343  validation ndcg@10=0.023107728878887835 [1.21s]\n",
      "epoch 116 [66.94s]:  training loss=0.2853853404521942                                     \n",
      "epoch 117 [67.7s]:  training loss=0.2841430604457855                                      \n",
      "epoch 118 [63.53s]:  training loss=0.2805139720439911                                     \n",
      "epoch 119 [60.57s]:  training loss=0.27637535333633423                                    \n",
      "epoch 120 [68.46s]: training loss=0.27640604972839355  validation ndcg@10=0.023164553391092853 [1.38s]\n",
      "epoch 121 [63.46s]:  training loss=0.27525997161865234                                    \n",
      "epoch 122 [62.56s]:  training loss=0.2689133286476135                                     \n",
      "epoch 123 [64.23s]:  training loss=0.2681201994419098                                     \n",
      "epoch 124 [75.03s]:  training loss=0.2678874135017395                                     \n",
      "epoch 125 [80.69s]: training loss=0.26839569211006165  validation ndcg@10=0.023344022052753686 [1.65s]\n",
      "epoch 126 [87.95s]:  training loss=0.2631244659423828                                     \n",
      "epoch 127 [62.27s]:  training loss=0.26075315475463867                                    \n",
      "epoch 128 [61.74s]:  training loss=0.2597595751285553                                     \n",
      "epoch 129 [66.07s]:  training loss=0.2583710253238678                                     \n",
      "epoch 130 [71.98s]: training loss=0.25509336590766907  validation ndcg@10=0.023475018074890185 [1.33s]\n",
      "epoch 131 [79.88s]:  training loss=0.2553038001060486                                     \n",
      "epoch 132 [87.9s]:  training loss=0.25248950719833374                                     \n",
      "epoch 133 [90.47s]:  training loss=0.25518497824668884                                    \n",
      "epoch 134 [86.75s]:  training loss=0.25083741545677185                                    \n",
      "epoch 135 [81.88s]: training loss=0.2484177052974701  validation ndcg@10=0.023474990524567505 [1.48s]\n",
      "epoch 136 [60.54s]:  training loss=0.24710534512996674                                    \n",
      "epoch 137 [67.09s]:  training loss=0.2490307092666626                                     \n",
      "epoch 138 [73.55s]:  training loss=0.2431449592113495                                     \n",
      "epoch 139 [61.43s]:  training loss=0.2451690286397934                                     \n",
      "epoch 140 [64.86s]: training loss=0.24135424196720123  validation ndcg@10=0.023340014355290078 [1.44s]\n",
      "epoch 141 [74.77s]:  training loss=0.2385806143283844                                     \n",
      "epoch 142 [79.07s]:  training loss=0.23569828271865845                                    \n",
      "epoch 143 [93.03s]:  training loss=0.23599471151828766                                    \n",
      "epoch 144 [92.7s]:  training loss=0.23418250679969788                                     \n",
      "epoch 145 [82.46s]: training loss=0.23136956989765167  validation ndcg@10=0.023854076444585925 [1.56s]\n",
      "epoch 146 [81.95s]:  training loss=0.229394793510437                                      \n",
      "epoch 147 [80.1s]:  training loss=0.22923678159713745                                     \n",
      "epoch 148 [79.13s]:  training loss=0.2266150265932083                                     \n",
      "epoch 149 [78.24s]:  training loss=0.2291768193244934                                     \n",
      "epoch 150 [78.86s]: training loss=0.22644619643688202  validation ndcg@10=0.02375234147835386 [1.69s]\n",
      "epoch 151 [78.2s]:  training loss=0.2196112424135208                                      \n",
      "epoch 152 [75.53s]:  training loss=0.2230559140443802                                     \n",
      "epoch 153 [75.96s]:  training loss=0.21910592913627625                                    \n",
      "epoch 154 [75.5s]:  training loss=0.22006139159202576                                     \n",
      "epoch 155 [78.38s]: training loss=0.218288391828537  validation ndcg@10=0.023764624293389564 [1.43s]\n",
      "epoch 156 [76.81s]:  training loss=0.21608121693134308                                    \n",
      "epoch 157 [78.03s]:  training loss=0.21682313084602356                                    \n",
      "epoch 158 [76.99s]:  training loss=0.21262121200561523                                    \n",
      "epoch 159 [76.07s]:  training loss=0.21034061908721924                                    \n",
      "epoch 160 [78.47s]: training loss=0.21001018583774567  validation ndcg@10=0.0237728309936391 [1.68s]\n",
      "epoch 161 [76.84s]:  training loss=0.21194280683994293                                    \n",
      "epoch 162 [76.81s]:  training loss=0.208778515458107                                      \n",
      "epoch 163 [76.85s]:  training loss=0.20761738717556                                       \n",
      "epoch 164 [78.46s]:  training loss=0.2063726782798767                                     \n",
      "epoch 165 [78.33s]: training loss=0.20170584321022034  validation ndcg@10=0.02314076752667341 [1.4s]\n",
      "epoch 166 [78.23s]:  training loss=0.20384269952774048                                    \n",
      "epoch 167 [77.96s]:  training loss=0.20208092033863068                                    \n",
      "epoch 168 [75.7s]:  training loss=0.2003815919160843                                      \n",
      "epoch 169 [77.33s]:  training loss=0.19644635915756226                                    \n",
      "epoch 170 [79.18s]: training loss=0.19461500644683838  validation ndcg@10=0.023793345460914456 [1.56s]\n",
      "epoch 1 [23.68s]:  training loss=0.8007184863090515                                        \n",
      "epoch 2 [28.26s]:  training loss=0.7679020166397095                                        \n",
      "epoch 3 [32.26s]:  training loss=0.7129504680633545                                        \n",
      "epoch 4 [29.39s]:  training loss=0.6509833931922913                                        \n",
      "epoch 5 [26.63s]: training loss=0.6209908127784729  validation ndcg@10=0.01930769361080378 [0.71s]\n",
      "epoch 6 [24.82s]:  training loss=0.6009622812271118                                        \n",
      "epoch 7 [25.4s]:  training loss=0.5826209187507629                                         \n",
      "epoch 8 [23.86s]:  training loss=0.5611185431480408                                        \n",
      "epoch 9 [24.56s]:  training loss=0.539600670337677                                         \n",
      "epoch 10 [25.21s]: training loss=0.5194759964942932  validation ndcg@10=0.02049390394223223 [0.67s]\n",
      "epoch 11 [24.65s]:  training loss=0.49777084589004517                                      \n",
      "epoch 12 [24.38s]:  training loss=0.4770311713218689                                       \n",
      "epoch 13 [24.42s]:  training loss=0.4548828899860382                                       \n",
      "epoch 14 [24.48s]:  training loss=0.4333137273788452                                       \n",
      "epoch 15 [25.5s]: training loss=0.4165368378162384  validation ndcg@10=0.021845438895899044 [0.68s]\n",
      "epoch 16 [24.48s]:  training loss=0.3991975784301758                                       \n",
      "epoch 17 [26.25s]:  training loss=0.3838357925415039                                       \n",
      "epoch 18 [28.08s]:  training loss=0.3715764284133911                                       \n",
      "epoch 19 [26.71s]:  training loss=0.3522062599658966                                       \n",
      "epoch 20 [27.55s]: training loss=0.34398823976516724  validation ndcg@10=0.02198061743952776 [0.56s]\n",
      "epoch 21 [25.79s]:  training loss=0.3295575976371765                                       \n",
      "epoch 22 [25.64s]:  training loss=0.3155728280544281                                       \n",
      "epoch 23 [25.12s]:  training loss=0.3046925663948059                                       \n",
      "epoch 24 [25.65s]:  training loss=0.29323622584342957                                      \n",
      "epoch 25 [25.99s]: training loss=0.2844841480255127  validation ndcg@10=0.022325012253311453 [0.52s]\n",
      "epoch 26 [24.19s]:  training loss=0.274230420589447                                        \n",
      "epoch 27 [25.23s]:  training loss=0.2613888084888458                                       \n",
      "epoch 28 [25.3s]:  training loss=0.2538127899169922                                        \n",
      "epoch 29 [24.19s]:  training loss=0.2461337000131607                                       \n",
      "epoch 30 [24.59s]: training loss=0.23677490651607513  validation ndcg@10=0.022961904890867448 [0.89s]\n",
      "epoch 31 [24.48s]:  training loss=0.22989627718925476                                      \n",
      "epoch 32 [24.19s]:  training loss=0.223934605717659                                        \n",
      "epoch 33 [24.57s]:  training loss=0.21590358018875122                                      \n",
      "epoch 34 [24.51s]:  training loss=0.21096472442150116                                      \n",
      "epoch 35 [24.02s]: training loss=0.20065946877002716  validation ndcg@10=0.02266257614376763 [0.52s]\n",
      "epoch 36 [24.54s]:  training loss=0.195460245013237                                        \n",
      "epoch 37 [28.48s]:  training loss=0.19176946580410004                                      \n",
      "epoch 38 [31.33s]:  training loss=0.18711407482624054                                      \n",
      "epoch 39 [30.09s]:  training loss=0.17992819845676422                                      \n",
      "epoch 40 [27.91s]: training loss=0.17418022453784943  validation ndcg@10=0.023256673364306264 [0.68s]\n",
      "epoch 41 [24.72s]:  training loss=0.17230302095413208                                      \n",
      "epoch 42 [30.37s]:  training loss=0.16480551660060883                                      \n",
      "epoch 43 [28.2s]:  training loss=0.16209937632083893                                       \n",
      "epoch 44 [23.23s]:  training loss=0.15786592662334442                                      \n",
      "epoch 45 [22.96s]: training loss=0.15601398050785065  validation ndcg@10=0.023681124558536146 [0.58s]\n",
      "epoch 46 [21.57s]:  training loss=0.15138891339302063                                      \n",
      "epoch 47 [15.97s]:  training loss=0.1485242247581482                                       \n",
      "epoch 48 [14.15s]:  training loss=0.14483751356601715                                      \n",
      "epoch 49 [14.74s]:  training loss=0.14249230921268463                                      \n",
      "epoch 50 [14.65s]: training loss=0.1404062956571579  validation ndcg@10=0.023696336671675002 [0.45s]\n",
      "epoch 51 [14.53s]:  training loss=0.13910232484340668                                      \n",
      "epoch 52 [14.67s]:  training loss=0.13533002138137817                                      \n",
      "epoch 53 [14.52s]:  training loss=0.12880206108093262                                      \n",
      "epoch 54 [14.78s]:  training loss=0.12982887029647827                                      \n",
      "epoch 55 [14.69s]: training loss=0.12551189959049225  validation ndcg@10=0.023887688869594643 [0.42s]\n",
      "epoch 56 [14.48s]:  training loss=0.12441446632146835                                      \n",
      "epoch 57 [14.48s]:  training loss=0.12035710364580154                                      \n",
      "epoch 58 [14.63s]:  training loss=0.11939578503370285                                      \n",
      "epoch 59 [14.36s]:  training loss=0.11670488119125366                                      \n",
      "epoch 60 [14.71s]: training loss=0.1148071140050888  validation ndcg@10=0.024483778688804816 [0.42s]\n",
      "epoch 61 [14.4s]:  training loss=0.11324264854192734                                       \n",
      "epoch 62 [14.77s]:  training loss=0.11142899096012115                                      \n",
      "epoch 63 [14.53s]:  training loss=0.10972066968679428                                      \n",
      "epoch 64 [14.75s]:  training loss=0.10743215680122375                                      \n",
      "epoch 65 [14.31s]: training loss=0.10594626516103745  validation ndcg@10=0.02471962473997348 [0.44s]\n",
      "epoch 66 [14.81s]:  training loss=0.10316075384616852                                      \n",
      "epoch 67 [14.38s]:  training loss=0.10342946648597717                                      \n",
      "epoch 68 [14.43s]:  training loss=0.10248373448848724                                      \n",
      "epoch 69 [14.67s]:  training loss=0.09975534677505493                                      \n",
      "epoch 70 [14.56s]: training loss=0.1010129377245903  validation ndcg@10=0.024722848781765 [0.45s]\n",
      "epoch 71 [14.71s]:  training loss=0.09696822613477707                                      \n",
      "epoch 72 [14.37s]:  training loss=0.09559878706932068                                      \n",
      "epoch 73 [14.66s]:  training loss=0.09496962279081345                                      \n",
      "epoch 74 [14.33s]:  training loss=0.09321317821741104                                      \n",
      "epoch 75 [14.71s]: training loss=0.09204897284507751  validation ndcg@10=0.024014869158915614 [0.42s]\n",
      "epoch 76 [14.78s]:  training loss=0.09049081057310104                                      \n",
      "epoch 77 [14.55s]:  training loss=0.0884413942694664                                       \n",
      "epoch 78 [14.55s]:  training loss=0.08920686691999435                                      \n",
      "epoch 79 [14.78s]:  training loss=0.08936450630426407                                      \n",
      "epoch 80 [15.07s]: training loss=0.08720435202121735  validation ndcg@10=0.024218350988135847 [0.45s]\n",
      "epoch 81 [14.67s]:  training loss=0.08698228001594543                                      \n",
      "epoch 82 [14.63s]:  training loss=0.08371610194444656                                      \n",
      "epoch 83 [14.65s]:  training loss=0.08345422893762589                                      \n",
      "epoch 84 [14.47s]:  training loss=0.08218175172805786                                      \n",
      "epoch 85 [14.75s]: training loss=0.08328482508659363  validation ndcg@10=0.023979637940656377 [0.43s]\n",
      "epoch 86 [14.83s]:  training loss=0.08073904365301132                                      \n",
      "epoch 87 [14.49s]:  training loss=0.07913516461849213                                      \n",
      "epoch 88 [14.76s]:  training loss=0.07662922888994217                                      \n",
      "epoch 89 [14.75s]:  training loss=0.07757880538702011                                      \n",
      "epoch 90 [14.68s]: training loss=0.07899407297372818  validation ndcg@10=0.023121095130520138 [0.44s]\n",
      "epoch 91 [16.21s]:  training loss=0.0763065442442894                                       \n",
      "epoch 92 [14.76s]:  training loss=0.07637535035610199                                      \n",
      "epoch 93 [14.39s]:  training loss=0.07429224997758865                                      \n",
      "epoch 94 [14.42s]:  training loss=0.07589414715766907                                      \n",
      "epoch 95 [14.66s]: training loss=0.07325880229473114  validation ndcg@10=0.023829416396012715 [0.44s]\n",
      "epoch 1 [40.69s]:  training loss=0.8136471509933472                                        \n",
      "epoch 2 [43.42s]:  training loss=0.7890139222145081                                        \n",
      "epoch 3 [41.92s]:  training loss=0.7786976099014282                                        \n",
      "epoch 4 [43.54s]:  training loss=0.7693132758140564                                        \n",
      "epoch 5 [41.65s]: training loss=0.7571523785591125  validation ndcg@10=0.0020616384958237572 [0.98s]\n",
      "epoch 6 [42.38s]:  training loss=0.7459156513214111                                        \n",
      "epoch 7 [41.84s]:  training loss=0.7347358465194702                                        \n",
      "epoch 8 [43.32s]:  training loss=0.7189649343490601                                        \n",
      "epoch 9 [42.35s]:  training loss=0.7057907581329346                                        \n",
      "epoch 10 [41.84s]: training loss=0.6801887154579163  validation ndcg@10=0.021002741917964318 [1.01s]\n",
      "epoch 11 [42.47s]:  training loss=0.6353281140327454                                       \n",
      "epoch 12 [41.56s]:  training loss=0.6036593914031982                                       \n",
      "epoch 13 [42.4s]:  training loss=0.5844411253929138                                        \n",
      "epoch 14 [42.11s]:  training loss=0.572265088558197                                        \n",
      "epoch 15 [42.35s]: training loss=0.5611461997032166  validation ndcg@10=0.020360629737207217 [0.99s]\n",
      "epoch 16 [42.19s]:  training loss=0.5519915223121643                                       \n",
      "epoch 17 [43.76s]:  training loss=0.5431203246116638                                       \n",
      "epoch 18 [42.3s]:  training loss=0.5311263799667358                                        \n",
      "epoch 19 [42.05s]:  training loss=0.5207647085189819                                       \n",
      "epoch 20 [42.44s]: training loss=0.5142139196395874  validation ndcg@10=0.020205391599493427 [0.98s]\n",
      "epoch 21 [42.13s]:  training loss=0.5040913224220276                                       \n",
      "epoch 22 [42.34s]:  training loss=0.497814416885376                                        \n",
      "epoch 23 [43.07s]:  training loss=0.4864952862262726                                       \n",
      "epoch 24 [42.0s]:  training loss=0.47944799065589905                                       \n",
      "epoch 25 [41.87s]: training loss=0.46736738085746765  validation ndcg@10=0.020418080543359064 [0.97s]\n",
      "epoch 26 [42.23s]:  training loss=0.46548914909362793                                      \n",
      "epoch 27 [41.99s]:  training loss=0.4525965750217438                                       \n",
      "epoch 28 [42.1s]:  training loss=0.4474014341831207                                        \n",
      "epoch 29 [41.7s]:  training loss=0.43919724225997925                                       \n",
      "epoch 30 [42.02s]: training loss=0.43141430616378784  validation ndcg@10=0.02096449542984696 [0.98s]\n",
      "epoch 31 [41.81s]:  training loss=0.4191376566886902                                       \n",
      "epoch 32 [41.92s]:  training loss=0.41194286942481995                                      \n",
      "epoch 33 [42.28s]:  training loss=0.4068456292152405                                       \n",
      "epoch 34 [42.7s]:  training loss=0.40093550086021423                                       \n",
      "epoch 35 [44.09s]: training loss=0.3937646448612213  validation ndcg@10=0.02132329832471508 [1.0s]\n",
      "epoch 36 [41.74s]:  training loss=0.3839075267314911                                       \n",
      "epoch 37 [42.41s]:  training loss=0.37859442830085754                                      \n",
      "epoch 38 [42.08s]:  training loss=0.37384164333343506                                      \n",
      "epoch 39 [43.52s]:  training loss=0.36710262298583984                                      \n",
      "epoch 40 [41.99s]: training loss=0.3632267713546753  validation ndcg@10=0.021572340862551557 [1.0s]\n",
      "epoch 41 [41.81s]:  training loss=0.35564738512039185                                      \n",
      "epoch 42 [41.4s]:  training loss=0.3479986786842346                                        \n",
      "epoch 43 [43.69s]:  training loss=0.3497551679611206                                       \n",
      "epoch 44 [42.12s]:  training loss=0.3405943512916565                                       \n",
      "epoch 45 [41.57s]: training loss=0.33776119351387024  validation ndcg@10=0.02215696433748712 [1.01s]\n",
      "epoch 46 [42.33s]:  training loss=0.32905736565589905                                      \n",
      "epoch 47 [42.35s]:  training loss=0.32543131709098816                                      \n",
      "epoch 48 [42.09s]:  training loss=0.3172573149204254                                       \n",
      "epoch 49 [42.51s]:  training loss=0.31316694617271423                                      \n",
      "epoch 50 [43.05s]: training loss=0.3101993799209595  validation ndcg@10=0.02242820454085562 [0.99s]\n",
      "epoch 51 [42.92s]:  training loss=0.30152323842048645                                      \n",
      "epoch 52 [43.33s]:  training loss=0.2994590699672699                                       \n",
      "epoch 53 [43.4s]:  training loss=0.2941926419734955                                        \n",
      "epoch 54 [43.18s]:  training loss=0.2897031307220459                                       \n",
      "epoch 55 [44.8s]: training loss=0.28599634766578674  validation ndcg@10=0.02295519635216165 [0.96s]\n",
      "epoch 56 [43.06s]:  training loss=0.2791125178337097                                       \n",
      "epoch 57 [42.58s]:  training loss=0.27295681834220886                                      \n",
      "epoch 58 [43.12s]:  training loss=0.2772042453289032                                       \n",
      "epoch 59 [44.27s]:  training loss=0.27000394463539124                                      \n",
      "epoch 60 [43.22s]: training loss=0.26634812355041504  validation ndcg@10=0.024302971469851716 [1.0s]\n",
      "epoch 61 [43.67s]:  training loss=0.2604343593120575                                       \n",
      "epoch 62 [43.47s]:  training loss=0.253499835729599                                        \n",
      "epoch 63 [42.39s]:  training loss=0.2562016546726227                                       \n",
      "epoch 64 [43.48s]:  training loss=0.24453462660312653                                      \n",
      "epoch 65 [43.68s]: training loss=0.2398388385772705  validation ndcg@10=0.023617323928831525 [1.0s]\n",
      "epoch 66 [43.11s]:  training loss=0.2379245012998581                                       \n",
      "epoch 67 [42.71s]:  training loss=0.23047146201133728                                      \n",
      "epoch 68 [43.25s]:  training loss=0.22656159102916718                                      \n",
      "epoch 69 [42.54s]:  training loss=0.22305001318454742                                      \n",
      "epoch 70 [44.24s]: training loss=0.22118064761161804  validation ndcg@10=0.023755571981612295 [1.0s]\n",
      "epoch 71 [42.7s]:  training loss=0.21986491978168488                                       \n",
      "epoch 72 [43.6s]:  training loss=0.21541714668273926                                       \n",
      "epoch 73 [44.43s]:  training loss=0.21238285303115845                                      \n",
      "epoch 74 [42.25s]:  training loss=0.20520082116127014                                      \n",
      "epoch 75 [43.32s]: training loss=0.2035003900527954  validation ndcg@10=0.024990659532516103 [1.01s]\n",
      "epoch 76 [42.15s]:  training loss=0.19798994064331055                                      \n",
      "epoch 77 [42.95s]:  training loss=0.19061805307865143                                      \n",
      "epoch 78 [43.86s]:  training loss=0.18979306519031525                                      \n",
      "epoch 79 [42.89s]:  training loss=0.1852675825357437                                       \n",
      "epoch 80 [42.93s]: training loss=0.18417270481586456  validation ndcg@10=0.02456190403592459 [1.17s]\n",
      "epoch 81 [42.11s]:  training loss=0.17675520479679108                                      \n",
      "epoch 82 [42.85s]:  training loss=0.17689105868339539                                      \n",
      "epoch 83 [42.0s]:  training loss=0.17364121973514557                                       \n",
      "epoch 84 [42.45s]:  training loss=0.16953745484352112                                      \n",
      "epoch 85 [42.2s]: training loss=0.1671220064163208  validation ndcg@10=0.025736010004666285 [1.0s]\n",
      "epoch 86 [42.77s]:  training loss=0.16316582262516022                                      \n",
      "epoch 87 [42.8s]:  training loss=0.16237695515155792                                       \n",
      "epoch 88 [41.76s]:  training loss=0.15939484536647797                                      \n",
      "epoch 89 [43.27s]:  training loss=0.1606626659631729                                       \n",
      "epoch 90 [42.72s]: training loss=0.15547583997249603  validation ndcg@10=0.02623325606785645 [1.01s]\n",
      "epoch 91 [44.12s]:  training loss=0.1540420949459076                                       \n",
      "epoch 92 [43.21s]:  training loss=0.1518310308456421                                       \n",
      "epoch 93 [42.28s]:  training loss=0.14937427639961243                                      \n",
      "epoch 94 [44.03s]:  training loss=0.14660722017288208                                      \n",
      "epoch 95 [42.96s]: training loss=0.14176712930202484  validation ndcg@10=0.02547403664926258 [0.97s]\n",
      "epoch 96 [44.16s]:  training loss=0.14497464895248413                                      \n",
      "epoch 97 [44.07s]:  training loss=0.14410392940044403                                      \n",
      "epoch 98 [43.69s]:  training loss=0.13934378325939178                                      \n",
      "epoch 99 [41.97s]:  training loss=0.13822424411773682                                      \n",
      "epoch 100 [42.31s]: training loss=0.13479861617088318  validation ndcg@10=0.02529058421405937 [1.0s]\n",
      "epoch 101 [41.96s]:  training loss=0.13168083131313324                                     \n",
      "epoch 102 [43.05s]:  training loss=0.13147592544555664                                     \n",
      "epoch 103 [43.54s]:  training loss=0.13262732326984406                                     \n",
      "epoch 104 [42.78s]:  training loss=0.12743298709392548                                     \n",
      "epoch 105 [42.82s]: training loss=0.1279451996088028  validation ndcg@10=0.026877325866974933 [0.97s]\n",
      "epoch 106 [42.9s]:  training loss=0.12497910857200623                                      \n",
      "epoch 107 [43.03s]:  training loss=0.12525880336761475                                     \n",
      "epoch 108 [42.78s]:  training loss=0.12234807759523392                                     \n",
      "epoch 109 [42.87s]:  training loss=0.12145338952541351                                     \n",
      "epoch 110 [43.67s]: training loss=0.12112875282764435  validation ndcg@10=0.026133643593821237 [1.02s]\n",
      "epoch 111 [43.08s]:  training loss=0.11939942091703415                                     \n",
      "epoch 112 [42.96s]:  training loss=0.11692093312740326                                     \n",
      "epoch 113 [43.61s]:  training loss=0.1138286218047142                                      \n",
      "epoch 114 [42.79s]:  training loss=0.1161307543516159                                      \n",
      "epoch 115 [44.03s]: training loss=0.11461295187473297  validation ndcg@10=0.026152801921974552 [0.95s]\n",
      "epoch 116 [42.19s]:  training loss=0.11322257667779922                                     \n",
      "epoch 117 [43.78s]:  training loss=0.11193303018808365                                     \n",
      "epoch 118 [43.21s]:  training loss=0.10918673127889633                                     \n",
      "epoch 119 [43.29s]:  training loss=0.10902874171733856                                     \n",
      "epoch 120 [43.56s]: training loss=0.11028043925762177  validation ndcg@10=0.026318418389392875 [1.01s]\n",
      "epoch 121 [42.02s]:  training loss=0.10458651185035706                                     \n",
      "epoch 122 [42.32s]:  training loss=0.10631813853979111                                     \n",
      "epoch 123 [42.95s]:  training loss=0.1038312017917633                                      \n",
      "epoch 124 [41.27s]:  training loss=0.10354159772396088                                     \n",
      "epoch 125 [41.36s]: training loss=0.10255812108516693  validation ndcg@10=0.026826647196927844 [0.95s]\n",
      "epoch 126 [41.6s]:  training loss=0.09999819099903107                                      \n",
      "epoch 127 [41.33s]:  training loss=0.09904462099075317                                     \n",
      "epoch 128 [41.77s]:  training loss=0.09904535114765167                                     \n",
      "epoch 129 [41.41s]:  training loss=0.09604587405920029                                     \n",
      "epoch 130 [41.52s]: training loss=0.09805562347173691  validation ndcg@10=0.026518851767380773 [0.97s]\n",
      "epoch 1 [34.07s]:  training loss=0.4980621337890625                                        \n",
      "epoch 2 [34.15s]:  training loss=0.19588807225227356                                       \n",
      "epoch 3 [36.78s]:  training loss=0.12200938165187836                                       \n",
      "epoch 4 [34.75s]:  training loss=0.09691623598337173                                       \n",
      "epoch 5 [34.38s]: training loss=0.08331001549959183  validation ndcg@10=0.019276783161032512 [0.74s]\n",
      "epoch 6 [34.57s]:  training loss=0.07543384283781052                                       \n",
      "epoch 7 [35.45s]:  training loss=0.07022469490766525                                       \n",
      "epoch 8 [34.87s]:  training loss=0.06869328767061234                                       \n",
      "epoch 9 [34.22s]:  training loss=0.06428810209035873                                       \n",
      "epoch 10 [34.67s]: training loss=0.06402994692325592  validation ndcg@10=0.020845942156312525 [0.71s]\n",
      "epoch 11 [34.19s]:  training loss=0.06009974703192711                                      \n",
      "epoch 12 [34.36s]:  training loss=0.057085372507572174                                     \n",
      "epoch 13 [34.82s]:  training loss=0.05850189924240112                                      \n",
      "epoch 14 [35.17s]:  training loss=0.060184113681316376                                     \n",
      "epoch 15 [35.0s]: training loss=0.05950503796339035  validation ndcg@10=0.019791508972878277 [0.71s]\n",
      "epoch 16 [34.81s]:  training loss=0.058396533131599426                                     \n",
      "epoch 17 [34.73s]:  training loss=0.05853425711393356                                      \n",
      "epoch 18 [34.91s]:  training loss=0.054289694875478745                                     \n",
      "epoch 19 [36.43s]:  training loss=0.054721783846616745                                     \n",
      "epoch 20 [34.53s]: training loss=0.056639451533555984  validation ndcg@10=0.017611923935848726 [0.73s]\n",
      "epoch 21 [34.89s]:  training loss=0.05648728087544441                                      \n",
      "epoch 22 [35.57s]:  training loss=0.05821015313267708                                      \n",
      "epoch 23 [35.32s]:  training loss=0.05275743082165718                                      \n",
      "epoch 24 [34.83s]:  training loss=0.05355364829301834                                      \n",
      "epoch 25 [34.93s]: training loss=0.05698200687766075  validation ndcg@10=0.01602035847781287 [0.72s]\n",
      "epoch 26 [34.08s]:  training loss=0.05664477124810219                                      \n",
      "epoch 27 [35.01s]:  training loss=0.05432305485010147                                      \n",
      "epoch 28 [34.34s]:  training loss=0.055182818323373795                                     \n",
      "epoch 29 [35.03s]:  training loss=0.05508347600698471                                      \n",
      "epoch 30 [34.41s]: training loss=0.05947514623403549  validation ndcg@10=0.015696059939811196 [0.72s]\n",
      "epoch 31 [34.35s]:  training loss=0.05570739135146141                                      \n",
      "epoch 32 [34.54s]:  training loss=0.05778151750564575                                      \n",
      "epoch 33 [35.05s]:  training loss=0.056371379643678665                                     \n",
      "epoch 34 [35.71s]:  training loss=0.05447620525956154                                      \n",
      "epoch 35 [34.34s]: training loss=0.055006951093673706  validation ndcg@10=0.01767079574610721 [0.73s]\n",
      "epoch 1 [37.23s]:  training loss=0.8601428270339966                                        \n",
      "epoch 2 [35.65s]:  training loss=0.8276380896568298                                       \n",
      "epoch 3 [37.06s]:  training loss=0.8106638193130493                                       \n",
      "epoch 4 [36.3s]:  training loss=0.8037012219429016                                        \n",
      "epoch 5 [36.17s]: training loss=0.795380711555481  validation ndcg@10=0.00103011347695551 [1.04s]\n",
      "epoch 6 [36.44s]:  training loss=0.7915544509887695                                       \n",
      "epoch 7 [36.16s]:  training loss=0.7897028923034668                                       \n",
      "epoch 8 [36.48s]:  training loss=0.7871382832527161                                       \n",
      "epoch 9 [36.06s]:  training loss=0.7831602096557617                                       \n",
      "epoch 10 [36.4s]: training loss=0.7787461280822754  validation ndcg@10=0.005279712144216688 [0.95s]\n",
      "epoch 11 [36.07s]:  training loss=0.7788059711456299                                      \n",
      "epoch 12 [36.01s]:  training loss=0.7743820548057556                                      \n",
      "epoch 13 [35.47s]:  training loss=0.7714959383010864                                      \n",
      "epoch 14 [37.78s]:  training loss=0.7685102224349976                                      \n",
      "epoch 15 [35.72s]: training loss=0.7655016183853149  validation ndcg@10=0.006038364263054398 [0.94s]\n",
      "epoch 16 [36.84s]:  training loss=0.7629255056381226                                      \n",
      "epoch 17 [38.17s]:  training loss=0.759649932384491                                       \n",
      "epoch 18 [37.41s]:  training loss=0.7548294067382812                                      \n",
      "epoch 19 [37.24s]:  training loss=0.7542752027511597                                      \n",
      "epoch 20 [37.05s]: training loss=0.7488961219787598  validation ndcg@10=0.006661236516077245 [1.1s]\n",
      "epoch 21 [37.3s]:  training loss=0.74556964635849                                         \n",
      "epoch 22 [38.24s]:  training loss=0.7449871897697449                                      \n",
      "epoch 23 [37.9s]:  training loss=0.7377655506134033                                       \n",
      "epoch 24 [36.72s]:  training loss=0.7373002767562866                                      \n",
      "epoch 25 [38.54s]: training loss=0.7367537617683411  validation ndcg@10=0.007167826514136192 [1.0s]\n",
      "epoch 26 [36.14s]:  training loss=0.7315130233764648                                      \n",
      "epoch 27 [36.31s]:  training loss=0.7277563810348511                                      \n",
      "epoch 28 [35.97s]:  training loss=0.7233982086181641                                      \n",
      "epoch 29 [36.04s]:  training loss=0.7196923494338989                                      \n",
      "epoch 30 [35.91s]: training loss=0.7145135998725891  validation ndcg@10=0.010349983353678186 [1.09s]\n",
      "epoch 31 [35.8s]:  training loss=0.7135902047157288                                       \n",
      "epoch 32 [35.95s]:  training loss=0.7072148323059082                                      \n",
      "epoch 33 [36.51s]:  training loss=0.7060268521308899                                      \n",
      "epoch 34 [36.26s]:  training loss=0.6982532143592834                                      \n",
      "epoch 35 [36.26s]: training loss=0.6902500987052917  validation ndcg@10=0.01847920253188026 [0.94s]\n",
      "epoch 36 [36.71s]:  training loss=0.679966390132904                                       \n",
      "epoch 37 [36.18s]:  training loss=0.6673130393028259                                      \n",
      "epoch 38 [36.23s]:  training loss=0.6589776873588562                                      \n",
      "epoch 39 [36.37s]:  training loss=0.6456032991409302                                      \n",
      "epoch 40 [36.35s]: training loss=0.6321439146995544  validation ndcg@10=0.019114393981445696 [0.92s]\n",
      "epoch 41 [36.34s]:  training loss=0.6260756850242615                                      \n",
      "epoch 42 [35.98s]:  training loss=0.617429792881012                                       \n",
      "epoch 43 [36.39s]:  training loss=0.6108322739601135                                      \n",
      "epoch 44 [36.16s]:  training loss=0.6076493263244629                                      \n",
      "epoch 45 [37.78s]: training loss=0.6013768315315247  validation ndcg@10=0.019398936596326157 [0.91s]\n",
      "epoch 46 [36.05s]:  training loss=0.5974471569061279                                      \n",
      "epoch 47 [36.09s]:  training loss=0.5959858894348145                                      \n",
      "epoch 48 [36.13s]:  training loss=0.5932978987693787                                      \n",
      "epoch 49 [36.24s]:  training loss=0.5868818759918213                                      \n",
      "epoch 50 [36.08s]: training loss=0.5860226154327393  validation ndcg@10=0.019309389179698384 [0.99s]\n",
      "epoch 51 [36.44s]:  training loss=0.5814753770828247                                      \n",
      "epoch 52 [36.11s]:  training loss=0.5812210440635681                                      \n",
      "epoch 53 [35.92s]:  training loss=0.5749101638793945                                      \n",
      "epoch 54 [36.26s]:  training loss=0.5761010646820068                                      \n",
      "epoch 55 [36.51s]: training loss=0.571906566619873  validation ndcg@10=0.01946485394469485 [0.99s]\n",
      "epoch 56 [36.1s]:  training loss=0.5695895552635193                                       \n",
      "epoch 57 [36.81s]:  training loss=0.5642569661140442                                      \n",
      "epoch 58 [35.67s]:  training loss=0.5583855509757996                                      \n",
      "epoch 59 [35.67s]:  training loss=0.5609720349311829                                      \n",
      "epoch 60 [35.62s]: training loss=0.5565333962440491  validation ndcg@10=0.019401867154018846 [0.93s]\n",
      "epoch 61 [35.64s]:  training loss=0.5571920275688171                                      \n",
      "epoch 62 [35.91s]:  training loss=0.5532129406929016                                      \n",
      "epoch 63 [35.84s]:  training loss=0.5479291081428528                                      \n",
      "epoch 64 [35.69s]:  training loss=0.5499750971794128                                      \n",
      "epoch 65 [35.63s]: training loss=0.5508367419242859  validation ndcg@10=0.019473881709285518 [0.94s]\n",
      "epoch 66 [36.19s]:  training loss=0.5455228090286255                                      \n",
      "epoch 67 [35.77s]:  training loss=0.5419826507568359                                      \n",
      "epoch 68 [35.61s]:  training loss=0.5414467453956604                                      \n",
      "epoch 69 [35.61s]:  training loss=0.5360784530639648                                      \n",
      "epoch 70 [36.03s]: training loss=0.5344695448875427  validation ndcg@10=0.019406757685217758 [0.93s]\n",
      "epoch 71 [35.74s]:  training loss=0.5364488363265991                                      \n",
      "epoch 72 [37.59s]:  training loss=0.5314813852310181                                      \n",
      "epoch 73 [36.03s]:  training loss=0.5305274128913879                                      \n",
      "epoch 74 [35.97s]:  training loss=0.5244020223617554                                      \n",
      "epoch 75 [35.86s]: training loss=0.5219777822494507  validation ndcg@10=0.018977358956315225 [0.98s]\n",
      "epoch 76 [35.84s]:  training loss=0.5222328901290894                                      \n",
      "epoch 77 [35.74s]:  training loss=0.5232588052749634                                      \n",
      "epoch 78 [35.98s]:  training loss=0.5190684199333191                                      \n",
      "epoch 79 [35.69s]:  training loss=0.5159811973571777                                      \n",
      "epoch 80 [36.01s]: training loss=0.5163344144821167  validation ndcg@10=0.019170461136701594 [0.96s]\n",
      "epoch 81 [36.73s]:  training loss=0.5119909048080444                                      \n",
      "epoch 82 [35.8s]:  training loss=0.5134987831115723                                       \n",
      "epoch 83 [36.04s]:  training loss=0.5150401592254639                                      \n",
      "epoch 84 [35.98s]:  training loss=0.5078653693199158                                      \n",
      "epoch 85 [36.11s]: training loss=0.5093328952789307  validation ndcg@10=0.019209180806204255 [1.0s]\n",
      "epoch 86 [35.72s]:  training loss=0.502277672290802                                       \n",
      "epoch 87 [35.98s]:  training loss=0.508342444896698                                       \n",
      "epoch 88 [36.07s]:  training loss=0.5009153485298157                                      \n",
      "epoch 89 [35.86s]:  training loss=0.4991742968559265                                      \n",
      "epoch 90 [36.18s]: training loss=0.4963803291320801  validation ndcg@10=0.019209925180825174 [0.99s]\n",
      "epoch 1 [26.06s]:  training loss=0.6403664350509644                                       \n",
      "epoch 2 [26.58s]:  training loss=0.40558093786239624                                      \n",
      "epoch 3 [26.97s]:  training loss=0.2778065502643585                                       \n",
      "epoch 4 [26.52s]:  training loss=0.19844470918178558                                      \n",
      "epoch 5 [26.77s]: training loss=0.15148602426052094  validation ndcg@10=0.016894490032499116 [0.71s]\n",
      "epoch 6 [26.84s]:  training loss=0.1265425831079483                                       \n",
      "epoch 7 [27.19s]:  training loss=0.10997861623764038                                      \n",
      "epoch 8 [27.5s]:  training loss=0.09896188974380493                                       \n",
      "epoch 9 [27.23s]:  training loss=0.09025018662214279                                      \n",
      "epoch 10 [26.66s]: training loss=0.08756539970636368  validation ndcg@10=0.015413898523512477 [0.73s]\n",
      "epoch 11 [28.91s]:  training loss=0.08436181396245956                                     \n",
      "epoch 12 [26.67s]:  training loss=0.07767787575721741                                     \n",
      "epoch 13 [26.84s]:  training loss=0.07689180970191956                                     \n",
      "epoch 14 [27.24s]:  training loss=0.07495846599340439                                     \n",
      "epoch 15 [26.8s]: training loss=0.07219669222831726  validation ndcg@10=0.016905148553374313 [0.74s]\n",
      "epoch 16 [27.56s]:  training loss=0.06941290199756622                                     \n",
      "epoch 17 [26.96s]:  training loss=0.07097281515598297                                     \n",
      "epoch 18 [26.91s]:  training loss=0.06704998761415482                                     \n",
      "epoch 19 [27.37s]:  training loss=0.06535763293504715                                     \n",
      "epoch 20 [26.75s]: training loss=0.06412984430789948  validation ndcg@10=0.01595730355282899 [0.76s]\n",
      "epoch 21 [27.17s]:  training loss=0.06212537735700607                                     \n",
      "epoch 22 [26.99s]:  training loss=0.06150195002555847                                     \n",
      "epoch 23 [27.3s]:  training loss=0.06076451763510704                                      \n",
      "epoch 24 [26.78s]:  training loss=0.06256934255361557                                     \n",
      "epoch 25 [27.27s]: training loss=0.058189429342746735  validation ndcg@10=0.017833718372531667 [0.73s]\n",
      "epoch 26 [26.94s]:  training loss=0.062195952981710434                                    \n",
      "epoch 27 [26.73s]:  training loss=0.061364226043224335                                    \n",
      "epoch 28 [27.9s]:  training loss=0.058964334428310394                                     \n",
      "epoch 29 [27.37s]:  training loss=0.05884220823645592                                     \n",
      "epoch 30 [26.99s]: training loss=0.05945136025547981  validation ndcg@10=0.018018677849790516 [0.71s]\n",
      "epoch 31 [26.55s]:  training loss=0.05878983065485954                                     \n",
      "epoch 32 [27.24s]:  training loss=0.059084195643663406                                    \n",
      "epoch 33 [26.99s]:  training loss=0.0586429163813591                                      \n",
      "epoch 34 [27.49s]:  training loss=0.05764031782746315                                     \n",
      "epoch 35 [27.21s]: training loss=0.057739194482564926  validation ndcg@10=0.014665085744211543 [0.78s]\n",
      "epoch 36 [28.07s]:  training loss=0.05874326080083847                                     \n",
      "epoch 37 [28.0s]:  training loss=0.05461205169558525                                      \n",
      "epoch 38 [28.27s]:  training loss=0.055844202637672424                                    \n",
      "epoch 39 [27.74s]:  training loss=0.05654969811439514                                     \n",
      "epoch 40 [28.25s]: training loss=0.05370452627539635  validation ndcg@10=0.01605630848384799 [0.73s]\n",
      "epoch 41 [27.54s]:  training loss=0.055626459419727325                                    \n",
      "epoch 42 [27.78s]:  training loss=0.05532253906130791                                     \n",
      "epoch 43 [26.67s]:  training loss=0.0551966167986393                                      \n",
      "epoch 44 [27.29s]:  training loss=0.05916401743888855                                     \n",
      "epoch 45 [27.53s]: training loss=0.0587908960878849  validation ndcg@10=0.013981857616863694 [0.78s]\n",
      "epoch 46 [27.54s]:  training loss=0.05457698181271553                                     \n",
      "epoch 47 [28.72s]:  training loss=0.05368484929203987                                     \n",
      "epoch 48 [28.13s]:  training loss=0.053467173129320145                                    \n",
      "epoch 49 [27.34s]:  training loss=0.05635090172290802                                     \n",
      "epoch 50 [27.21s]: training loss=0.05596565082669258  validation ndcg@10=0.015631709296638405 [0.73s]\n",
      "epoch 51 [27.4s]:  training loss=0.05509025231003761                                      \n",
      "epoch 52 [27.44s]:  training loss=0.05513284355401993                                     \n",
      "epoch 53 [27.82s]:  training loss=0.054814428091049194                                    \n",
      "epoch 54 [28.43s]:  training loss=0.05497540161013603                                     \n",
      "epoch 55 [28.53s]: training loss=0.05178723856806755  validation ndcg@10=0.016020272055073795 [0.76s]\n",
      "epoch 1 [42.43s]:  training loss=0.8471207618713379                                       \n",
      "epoch 2 [41.61s]:  training loss=0.8229643106460571                                       \n",
      "epoch 3 [41.78s]:  training loss=0.8077131509780884                                       \n",
      "epoch 4 [42.22s]:  training loss=0.8018313050270081                                       \n",
      "epoch 5 [44.18s]: training loss=0.7997093796730042  validation ndcg@10=0.0009410918260562388 [0.87s]\n",
      "epoch 6 [42.9s]:  training loss=0.7984039187431335                                        \n",
      "epoch 7 [41.82s]:  training loss=0.7972794771194458                                       \n",
      "epoch 8 [42.9s]:  training loss=0.7926247119903564                                        \n",
      "epoch 9 [43.59s]:  training loss=0.7904545664787292                                       \n",
      "epoch 10 [42.45s]: training loss=0.786769688129425  validation ndcg@10=0.0009646307139330402 [0.88s]\n",
      "epoch 11 [42.96s]:  training loss=0.7869377732276917                                      \n",
      "epoch 12 [41.75s]:  training loss=0.7815905809402466                                      \n",
      "epoch 13 [41.83s]:  training loss=0.7831555604934692                                      \n",
      "epoch 14 [41.2s]:  training loss=0.7809691429138184                                       \n",
      "epoch 15 [41.29s]: training loss=0.7783871293067932  validation ndcg@10=0.0013890846552179388 [0.91s]\n",
      "epoch 16 [41.11s]:  training loss=0.7747319340705872                                      \n",
      "epoch 17 [41.38s]:  training loss=0.7734907269477844                                      \n",
      "epoch 18 [41.5s]:  training loss=0.7767829298973083                                       \n",
      "epoch 19 [41.26s]:  training loss=0.7710455656051636                                      \n",
      "epoch 20 [41.4s]: training loss=0.7710062861442566  validation ndcg@10=0.0013918958310994935 [0.89s]\n",
      "epoch 21 [42.32s]:  training loss=0.7672224044799805                                      \n",
      "epoch 22 [43.36s]:  training loss=0.7690922617912292                                      \n",
      "epoch 23 [41.46s]:  training loss=0.7667750716209412                                      \n",
      "epoch 24 [41.73s]:  training loss=0.7629056572914124                                      \n",
      "epoch 25 [41.35s]: training loss=0.7619064450263977  validation ndcg@10=0.002765063037204552 [0.86s]\n",
      "epoch 26 [41.14s]:  training loss=0.7565612196922302                                      \n",
      "epoch 27 [41.52s]:  training loss=0.7568634748458862                                      \n",
      "epoch 28 [41.83s]:  training loss=0.7542765140533447                                      \n",
      "epoch 29 [41.44s]:  training loss=0.7545686960220337                                      \n",
      "epoch 30 [41.67s]: training loss=0.7525274157524109  validation ndcg@10=0.006405193328724254 [0.88s]\n",
      "epoch 31 [40.94s]:  training loss=0.7487278580665588                                      \n",
      "epoch 32 [41.88s]:  training loss=0.748440146446228                                       \n",
      "epoch 33 [41.96s]:  training loss=0.7441806793212891                                      \n",
      "epoch 34 [41.31s]:  training loss=0.7417017817497253                                      \n",
      "epoch 35 [42.25s]: training loss=0.7422638535499573  validation ndcg@10=0.012062041352328128 [0.85s]\n",
      "epoch 36 [41.07s]:  training loss=0.7396871447563171                                      \n",
      "epoch 37 [41.67s]:  training loss=0.737982451915741                                       \n",
      "epoch 38 [41.17s]:  training loss=0.7364282608032227                                      \n",
      "epoch 39 [41.31s]:  training loss=0.7330594658851624                                      \n",
      "epoch 40 [41.44s]: training loss=0.7342920303344727  validation ndcg@10=0.016099942017218316 [0.99s]\n",
      "epoch 41 [41.02s]:  training loss=0.7274488806724548                                      \n",
      "epoch 42 [41.61s]:  training loss=0.7260772585868835                                      \n",
      "epoch 43 [40.99s]:  training loss=0.7237401604652405                                      \n",
      "epoch 44 [41.49s]:  training loss=0.7214037775993347                                      \n",
      "epoch 45 [41.35s]: training loss=0.7151904702186584  validation ndcg@10=0.01815530339273488 [0.85s]\n",
      "epoch 46 [41.13s]:  training loss=0.7150022387504578                                      \n",
      "epoch 47 [41.07s]:  training loss=0.7098099589347839                                      \n",
      "epoch 48 [41.69s]:  training loss=0.7081429362297058                                      \n",
      "epoch 49 [41.36s]:  training loss=0.6987734436988831                                      \n",
      "epoch 50 [41.09s]: training loss=0.6980020403862  validation ndcg@10=0.018797812280336442 [0.9s]\n",
      "epoch 51 [42.67s]:  training loss=0.6916567087173462                                      \n",
      "epoch 52 [41.16s]:  training loss=0.6866748332977295                                      \n",
      "epoch 53 [41.14s]:  training loss=0.6758324503898621                                      \n",
      "epoch 54 [41.41s]:  training loss=0.6683201193809509                                      \n",
      "epoch 55 [41.13s]: training loss=0.6603753566741943  validation ndcg@10=0.018789056355148822 [0.92s]\n",
      "epoch 56 [41.49s]:  training loss=0.6534019112586975                                      \n",
      "epoch 57 [41.39s]:  training loss=0.6481276750564575                                      \n",
      "epoch 58 [41.6s]:  training loss=0.6413867473602295                                       \n",
      "epoch 59 [42.56s]:  training loss=0.6342283487319946                                      \n",
      "epoch 60 [40.97s]: training loss=0.6308230757713318  validation ndcg@10=0.019200597258829086 [0.89s]\n",
      "epoch 61 [41.71s]:  training loss=0.6280931830406189                                      \n",
      "epoch 62 [41.24s]:  training loss=0.6234641075134277                                      \n",
      "epoch 63 [41.22s]:  training loss=0.6226937174797058                                      \n",
      "epoch 64 [41.29s]:  training loss=0.6179133653640747                                      \n",
      "epoch 65 [41.73s]: training loss=0.6130174994468689  validation ndcg@10=0.01966344696295749 [0.86s]\n",
      "epoch 66 [41.92s]:  training loss=0.6142022609710693                                      \n",
      "epoch 67 [41.42s]:  training loss=0.6055635213851929                                      \n",
      "epoch 68 [41.32s]:  training loss=0.6069520711898804                                      \n",
      "epoch 69 [41.73s]:  training loss=0.6034852862358093                                      \n",
      "epoch 70 [42.21s]: training loss=0.6021966338157654  validation ndcg@10=0.020148331451556045 [0.85s]\n",
      "epoch 71 [41.44s]:  training loss=0.5986089706420898                                      \n",
      "epoch 72 [42.33s]:  training loss=0.5960680246353149                                      \n",
      "epoch 73 [43.11s]:  training loss=0.5966240167617798                                      \n",
      "epoch 74 [45.67s]:  training loss=0.594114363193512                                       \n",
      "epoch 75 [45.34s]: training loss=0.5925225019454956  validation ndcg@10=0.020337245414667422 [0.91s]\n",
      "epoch 76 [44.79s]:  training loss=0.587970495223999                                       \n",
      "epoch 77 [43.39s]:  training loss=0.589989185333252                                       \n",
      "epoch 78 [44.34s]:  training loss=0.5826985836029053                                      \n",
      "epoch 79 [41.41s]:  training loss=0.5840875506401062                                      \n",
      "epoch 80 [41.69s]: training loss=0.582940936088562  validation ndcg@10=0.02042138768470666 [0.91s]\n",
      "epoch 81 [41.28s]:  training loss=0.5780816078186035                                      \n",
      "epoch 82 [41.85s]:  training loss=0.5787786245346069                                      \n",
      "epoch 83 [41.08s]:  training loss=0.5766963362693787                                      \n",
      "epoch 84 [41.46s]:  training loss=0.5779783725738525                                      \n",
      "epoch 85 [41.27s]: training loss=0.5714828372001648  validation ndcg@10=0.02065308907864215 [0.88s]\n",
      "epoch 86 [42.05s]:  training loss=0.5734806060791016                                      \n",
      "epoch 87 [42.2s]:  training loss=0.5703585743904114                                       \n",
      "epoch 88 [42.0s]:  training loss=0.567080020904541                                        \n",
      "epoch 89 [42.32s]:  training loss=0.5656435489654541                                      \n",
      "epoch 90 [41.66s]: training loss=0.562210202217102  validation ndcg@10=0.020728277896843183 [0.88s]\n",
      "epoch 91 [41.75s]:  training loss=0.5626029372215271                                      \n",
      "epoch 92 [41.73s]:  training loss=0.5639870762825012                                      \n",
      "epoch 93 [42.13s]:  training loss=0.559034526348114                                       \n",
      "epoch 94 [42.27s]:  training loss=0.5578626990318298                                      \n",
      "epoch 95 [42.4s]: training loss=0.5551615357398987  validation ndcg@10=0.02103088976123913 [0.87s]\n",
      "epoch 96 [41.66s]:  training loss=0.5543786883354187                                      \n",
      "epoch 97 [41.97s]:  training loss=0.5561636090278625                                      \n",
      "epoch 98 [43.04s]:  training loss=0.5514375567436218                                      \n",
      "epoch 99 [42.16s]:  training loss=0.5460565686225891                                      \n",
      "epoch 100 [42.2s]: training loss=0.546885073184967  validation ndcg@10=0.02091145756385188 [0.89s]\n",
      "epoch 101 [41.61s]:  training loss=0.5471416711807251                                     \n",
      "epoch 102 [41.68s]:  training loss=0.5475690960884094                                     \n",
      "epoch 103 [41.86s]:  training loss=0.5464193820953369                                     \n",
      "epoch 104 [43.19s]:  training loss=0.5411452651023865                                     \n",
      "epoch 105 [41.78s]: training loss=0.5388328433036804  validation ndcg@10=0.021049640100678186 [0.86s]\n",
      "epoch 106 [41.72s]:  training loss=0.5377897620201111                                     \n",
      "epoch 107 [43.29s]:  training loss=0.5371681451797485                                     \n",
      "epoch 108 [41.01s]:  training loss=0.5343884229660034                                     \n",
      "epoch 109 [41.17s]:  training loss=0.5352339744567871                                     \n",
      "epoch 110 [47.54s]: training loss=0.5284887552261353  validation ndcg@10=0.021047856340131295 [0.95s]\n",
      "epoch 111 [48.03s]:  training loss=0.5309066772460938                                     \n",
      "epoch 112 [45.17s]:  training loss=0.5276705622673035                                     \n",
      "epoch 113 [44.96s]:  training loss=0.5295616984367371                                     \n",
      "epoch 114 [43.72s]:  training loss=0.5291847586631775                                     \n",
      "epoch 115 [45.71s]: training loss=0.5240418910980225  validation ndcg@10=0.021079163497851314 [0.95s]\n",
      "epoch 116 [44.31s]:  training loss=0.5213639140129089                                     \n",
      "epoch 117 [43.87s]:  training loss=0.5251482129096985                                     \n",
      "epoch 118 [43.93s]:  training loss=0.5163581967353821                                     \n",
      "epoch 119 [44.0s]:  training loss=0.5144789218902588                                      \n",
      "epoch 120 [44.05s]: training loss=0.517463743686676  validation ndcg@10=0.021110027255351115 [0.94s]\n",
      "epoch 121 [43.54s]:  training loss=0.5170624256134033                                     \n",
      "epoch 122 [43.61s]:  training loss=0.5152183771133423                                     \n",
      "epoch 123 [44.24s]:  training loss=0.515700101852417                                      \n",
      "epoch 124 [43.82s]:  training loss=0.5116484761238098                                     \n",
      "epoch 125 [46.32s]: training loss=0.5092385411262512  validation ndcg@10=0.02123160736719994 [0.95s]\n",
      "epoch 126 [43.89s]:  training loss=0.5074406266212463                                     \n",
      "epoch 127 [43.9s]:  training loss=0.511154055595398                                       \n",
      "epoch 128 [45.78s]:  training loss=0.5051395297050476                                     \n",
      "epoch 129 [42.42s]:  training loss=0.5059731006622314                                     \n",
      "epoch 130 [43.72s]: training loss=0.5070798993110657  validation ndcg@10=0.021353634533008307 [0.9s]\n",
      "epoch 131 [43.19s]:  training loss=0.5028815865516663                                     \n",
      "epoch 132 [43.57s]:  training loss=0.5002316236495972                                     \n",
      "epoch 133 [43.01s]:  training loss=0.5000526309013367                                     \n",
      "epoch 134 [43.42s]:  training loss=0.49919334053993225                                    \n",
      "epoch 135 [44.03s]: training loss=0.4978821277618408  validation ndcg@10=0.02135151478795755 [0.89s]\n",
      "epoch 136 [43.36s]:  training loss=0.4952368438243866                                     \n",
      "epoch 137 [43.8s]:  training loss=0.49619314074516296                                     \n",
      "epoch 138 [44.13s]:  training loss=0.4950663149356842                                     \n",
      "epoch 139 [45.03s]:  training loss=0.49241185188293457                                    \n",
      "epoch 140 [44.26s]: training loss=0.4916873276233673  validation ndcg@10=0.021206879999690696 [0.89s]\n",
      "epoch 141 [43.36s]:  training loss=0.4911101758480072                                     \n",
      "epoch 142 [44.43s]:  training loss=0.48543980717658997                                    \n",
      "epoch 143 [44.36s]:  training loss=0.4843461215496063                                     \n",
      "epoch 144 [42.76s]:  training loss=0.4823872447013855                                     \n",
      "epoch 145 [42.38s]: training loss=0.4839614927768707  validation ndcg@10=0.021374776269714655 [0.87s]\n",
      "epoch 146 [42.15s]:  training loss=0.4816717505455017                                     \n",
      "epoch 147 [42.68s]:  training loss=0.4762741029262543                                     \n",
      "epoch 148 [42.65s]:  training loss=0.47944197058677673                                    \n",
      "epoch 149 [43.11s]:  training loss=0.47966375946998596                                    \n",
      "epoch 150 [42.33s]: training loss=0.4733438491821289  validation ndcg@10=0.0213492631322295 [0.88s]\n",
      "epoch 151 [41.86s]:  training loss=0.4760459363460541                                     \n",
      "epoch 152 [41.97s]:  training loss=0.476507306098938                                      \n",
      "epoch 153 [42.4s]:  training loss=0.47378766536712646                                     \n",
      "epoch 154 [42.02s]:  training loss=0.47077158093452454                                    \n",
      "epoch 155 [43.05s]: training loss=0.46959131956100464  validation ndcg@10=0.02130859417786778 [0.85s]\n",
      "epoch 156 [43.39s]:  training loss=0.4704700708389282                                     \n",
      "epoch 157 [41.36s]:  training loss=0.4654422998428345                                     \n",
      "epoch 158 [41.73s]:  training loss=0.4724295735359192                                     \n",
      "epoch 159 [41.58s]:  training loss=0.4680826663970947                                     \n",
      "epoch 160 [42.12s]: training loss=0.4625585675239563  validation ndcg@10=0.021186239289112774 [0.91s]\n",
      "epoch 161 [41.55s]:  training loss=0.4632972180843353                                     \n",
      "epoch 162 [42.19s]:  training loss=0.46317318081855774                                    \n",
      "epoch 163 [42.69s]:  training loss=0.46137621998786926                                    \n",
      "epoch 164 [42.42s]:  training loss=0.4573313593864441                                     \n",
      "epoch 165 [42.33s]: training loss=0.457179993391037  validation ndcg@10=0.02149645371689314 [0.88s]\n",
      "epoch 166 [41.82s]:  training loss=0.4556408226490021                                     \n",
      "epoch 167 [42.54s]:  training loss=0.4552856385707855                                     \n",
      "epoch 168 [42.19s]:  training loss=0.4529050588607788                                     \n",
      "epoch 169 [42.47s]:  training loss=0.4541453719139099                                     \n",
      "epoch 170 [42.05s]: training loss=0.4498998820781708  validation ndcg@10=0.021933461411724156 [0.86s]\n",
      "epoch 171 [41.99s]:  training loss=0.44741255044937134                                    \n",
      "epoch 172 [42.4s]:  training loss=0.450074166059494                                       \n",
      "epoch 173 [41.74s]:  training loss=0.44804778695106506                                    \n",
      "epoch 174 [41.59s]:  training loss=0.4488404095172882                                     \n",
      "epoch 175 [41.58s]: training loss=0.44473516941070557  validation ndcg@10=0.02194127425015081 [0.91s]\n",
      "epoch 176 [42.08s]:  training loss=0.44188565015792847                                    \n",
      "epoch 177 [42.01s]:  training loss=0.44283032417297363                                    \n",
      "epoch 178 [41.85s]:  training loss=0.44163790345191956                                    \n",
      "epoch 179 [42.11s]:  training loss=0.44148942828178406                                    \n",
      "epoch 180 [42.53s]: training loss=0.4374811351299286  validation ndcg@10=0.021758445507543366 [0.87s]\n",
      "epoch 181 [41.78s]:  training loss=0.4323965609073639                                     \n",
      "epoch 182 [41.81s]:  training loss=0.434637188911438                                      \n",
      "epoch 183 [41.79s]:  training loss=0.4344760775566101                                     \n",
      "epoch 184 [42.04s]:  training loss=0.4321492910385132                                     \n",
      "epoch 185 [42.05s]: training loss=0.4346163868904114  validation ndcg@10=0.02198539924105852 [0.94s]\n",
      "epoch 186 [41.81s]:  training loss=0.42833349108695984                                    \n",
      "epoch 187 [42.05s]:  training loss=0.4283715784549713                                     \n",
      "epoch 188 [41.59s]:  training loss=0.42905405163764954                                    \n",
      "epoch 189 [41.97s]:  training loss=0.42675110697746277                                    \n",
      "epoch 190 [42.59s]: training loss=0.4247911870479584  validation ndcg@10=0.022216690942255708 [0.85s]\n",
      "epoch 191 [41.23s]:  training loss=0.4232810437679291                                     \n",
      "epoch 192 [41.08s]:  training loss=0.4195825755596161                                     \n",
      "epoch 193 [41.32s]:  training loss=0.4224081039428711                                     \n",
      "epoch 194 [41.04s]:  training loss=0.418298602104187                                      \n",
      "epoch 195 [41.07s]: training loss=0.41716673970222473  validation ndcg@10=0.022028496377876017 [0.89s]\n",
      "epoch 196 [40.89s]:  training loss=0.41577884554862976                                    \n",
      "epoch 197 [41.46s]:  training loss=0.4152492880821228                                     \n",
      "epoch 198 [41.38s]:  training loss=0.4129558205604553                                     \n",
      "epoch 199 [40.97s]:  training loss=0.41513046622276306                                    \n",
      "epoch 200 [41.38s]: training loss=0.4092302620410919  validation ndcg@10=0.022281118142780058 [0.9s]\n",
      "epoch 1 [18.82s]:  training loss=0.4278774857521057                                       \n",
      "epoch 2 [18.95s]:  training loss=0.1617908477783203                                       \n",
      "epoch 3 [19.2s]:  training loss=0.11354545503854752                                       \n",
      "epoch 4 [18.62s]:  training loss=0.09351737052202225                                      \n",
      "epoch 5 [20.06s]: training loss=0.08693679422140121  validation ndcg@10=0.019758295596802533 [0.45s]\n",
      "epoch 6 [18.79s]:  training loss=0.08047764003276825                                      \n",
      "epoch 7 [18.63s]:  training loss=0.07842527329921722                                      \n",
      "epoch 8 [18.39s]:  training loss=0.0754236951470375                                       \n",
      "epoch 9 [18.99s]:  training loss=0.07689385861158371                                      \n",
      "epoch 10 [18.89s]: training loss=0.07615868747234344  validation ndcg@10=0.018221989804125073 [0.44s]\n",
      "epoch 11 [18.44s]:  training loss=0.0784250795841217                                      \n",
      "epoch 12 [18.88s]:  training loss=0.07903289049863815                                     \n",
      "epoch 13 [18.95s]:  training loss=0.07673195004463196                                     \n",
      "epoch 14 [18.37s]:  training loss=0.0773901715874672                                      \n",
      "epoch 15 [18.65s]: training loss=0.0783151164650917  validation ndcg@10=0.01596805045569986 [0.48s]\n",
      "epoch 16 [18.6s]:  training loss=0.0754738599061966                                       \n",
      "epoch 17 [18.47s]:  training loss=0.08307833224534988                                     \n",
      "epoch 18 [18.92s]:  training loss=0.07750408351421356                                     \n",
      "epoch 19 [18.5s]:  training loss=0.08623310178518295                                      \n",
      "epoch 20 [18.46s]: training loss=0.0806700736284256  validation ndcg@10=0.015737680348138813 [0.5s]\n",
      "epoch 21 [18.34s]:  training loss=0.07920405268669128                                     \n",
      "epoch 22 [18.67s]:  training loss=0.07725675404071808                                     \n",
      "epoch 23 [18.77s]:  training loss=0.07661496102809906                                     \n",
      "epoch 24 [18.45s]:  training loss=0.08339937031269073                                     \n",
      "epoch 25 [18.63s]: training loss=0.07998791337013245  validation ndcg@10=0.013896946320123003 [0.5s]\n",
      "epoch 26 [18.61s]:  training loss=0.07846098393201828                                     \n",
      "epoch 27 [18.25s]:  training loss=0.08274193853139877                                     \n",
      "epoch 28 [18.92s]:  training loss=0.07803597301244736                                     \n",
      "epoch 29 [18.91s]:  training loss=0.07904351502656937                                     \n",
      "epoch 30 [18.44s]: training loss=0.08457638323307037  validation ndcg@10=0.014121950078543007 [0.45s]\n",
      "epoch 1 [13.19s]:  training loss=0.769106388092041                                        \n",
      "epoch 2 [13.56s]:  training loss=0.6391317844390869                                       \n",
      "epoch 3 [13.88s]:  training loss=0.5768724679946899                                       \n",
      "epoch 4 [13.27s]:  training loss=0.52371746301651                                         \n",
      "epoch 5 [13.1s]: training loss=0.47625282406806946  validation ndcg@10=0.01734546830317508 [0.43s]\n",
      "epoch 6 [13.31s]:  training loss=0.4364214241504669                                       \n",
      "epoch 7 [13.2s]:  training loss=0.39469075202941895                                       \n",
      "epoch 8 [13.09s]:  training loss=0.35431107878685                                         \n",
      "epoch 9 [13.42s]:  training loss=0.32267364859580994                                      \n",
      "epoch 10 [13.41s]: training loss=0.29689493775367737  validation ndcg@10=0.016861071870493977 [0.44s]\n",
      "epoch 11 [13.26s]:  training loss=0.2749345004558563                                      \n",
      "epoch 12 [13.1s]:  training loss=0.25940170884132385                                      \n",
      "epoch 13 [13.48s]:  training loss=0.23784677684307098                                     \n",
      "epoch 14 [13.08s]:  training loss=0.22227828204631805                                     \n",
      "epoch 15 [13.33s]: training loss=0.2057180106639862  validation ndcg@10=0.020259044978676446 [0.4s]\n",
      "epoch 16 [13.2s]:  training loss=0.19166427850723267                                      \n",
      "epoch 17 [13.19s]:  training loss=0.185140922665596                                       \n",
      "epoch 18 [13.12s]:  training loss=0.17569197714328766                                     \n",
      "epoch 19 [13.35s]:  training loss=0.16118521988391876                                     \n",
      "epoch 20 [13.29s]: training loss=0.1542903482913971  validation ndcg@10=0.019660635855320217 [0.41s]\n",
      "epoch 21 [13.22s]:  training loss=0.14538981020450592                                     \n",
      "epoch 22 [13.24s]:  training loss=0.1398380696773529                                      \n",
      "epoch 23 [13.54s]:  training loss=0.1364259570837021                                      \n",
      "epoch 24 [13.29s]:  training loss=0.13023366034030914                                     \n",
      "epoch 25 [13.1s]: training loss=0.12574969232082367  validation ndcg@10=0.0196641385888354 [0.42s]\n",
      "epoch 26 [13.37s]:  training loss=0.11964870989322662                                     \n",
      "epoch 27 [13.21s]:  training loss=0.1141543909907341                                      \n",
      "epoch 28 [13.37s]:  training loss=0.1107640489935875                                      \n",
      "epoch 29 [13.32s]:  training loss=0.10864241421222687                                     \n",
      "epoch 30 [13.37s]: training loss=0.10155539959669113  validation ndcg@10=0.018230123856526666 [0.43s]\n",
      "epoch 31 [13.15s]:  training loss=0.10253575444221497                                     \n",
      "epoch 32 [15.06s]:  training loss=0.09557449817657471                                     \n",
      "epoch 33 [13.44s]:  training loss=0.09636631608009338                                     \n",
      "epoch 34 [13.36s]:  training loss=0.09280648082494736                                     \n",
      "epoch 35 [13.19s]: training loss=0.08975112438201904  validation ndcg@10=0.02002179808801359 [0.39s]\n",
      "epoch 36 [13.51s]:  training loss=0.09027882665395737                                     \n",
      "epoch 37 [13.17s]:  training loss=0.08522680401802063                                     \n",
      "epoch 38 [13.44s]:  training loss=0.08225889503955841                                     \n",
      "epoch 39 [13.37s]:  training loss=0.08163554221391678                                     \n",
      "epoch 40 [13.36s]: training loss=0.0792338103055954  validation ndcg@10=0.020611122188738486 [0.43s]\n",
      "epoch 41 [13.46s]:  training loss=0.07561133056879044                                     \n",
      "epoch 42 [14.06s]:  training loss=0.07627775520086288                                     \n",
      "epoch 43 [14.06s]:  training loss=0.07382405549287796                                     \n",
      "epoch 44 [13.81s]:  training loss=0.07240525633096695                                     \n",
      "epoch 45 [13.28s]: training loss=0.07041481882333755  validation ndcg@10=0.02169167705679458 [0.44s]\n",
      "epoch 46 [13.59s]:  training loss=0.07043904811143875                                     \n",
      "epoch 47 [13.65s]:  training loss=0.06885761022567749                                     \n",
      "epoch 48 [13.42s]:  training loss=0.06630407273769379                                     \n",
      "epoch 49 [13.56s]:  training loss=0.06459838896989822                                     \n",
      "epoch 50 [13.7s]: training loss=0.06552527844905853  validation ndcg@10=0.020572507122794965 [0.39s]\n",
      "epoch 51 [13.38s]:  training loss=0.06149275600910187                                     \n",
      "epoch 52 [13.32s]:  training loss=0.06251023709774017                                     \n",
      "epoch 53 [13.78s]:  training loss=0.0609537735581398                                      \n",
      "epoch 54 [13.63s]:  training loss=0.06331595033407211                                     \n",
      "epoch 55 [13.61s]: training loss=0.059764862060546875  validation ndcg@10=0.02000852685489182 [0.42s]\n",
      "epoch 56 [13.43s]:  training loss=0.05871289223432541                                     \n",
      "epoch 57 [13.59s]:  training loss=0.056697096675634384                                    \n",
      "epoch 58 [13.44s]:  training loss=0.05603416636586189                                     \n",
      "epoch 59 [13.65s]:  training loss=0.055998049676418304                                    \n",
      "epoch 60 [13.53s]: training loss=0.05349905416369438  validation ndcg@10=0.02166683663267413 [0.44s]\n",
      "epoch 61 [13.29s]:  training loss=0.055611032992601395                                    \n",
      "epoch 62 [13.58s]:  training loss=0.05307492986321449                                     \n",
      "epoch 63 [13.47s]:  training loss=0.051831699907779694                                    \n",
      "epoch 64 [13.39s]:  training loss=0.05458361282944679                                     \n",
      "epoch 65 [13.6s]: training loss=0.05109071359038353  validation ndcg@10=0.020783531488255735 [0.43s]\n",
      "epoch 66 [13.17s]:  training loss=0.051820654422044754                                    \n",
      "epoch 67 [13.47s]:  training loss=0.050842177122831345                                    \n",
      "epoch 68 [13.41s]:  training loss=0.04967928305268288                                     \n",
      "epoch 69 [13.75s]:  training loss=0.0510491319000721                                      \n",
      "epoch 70 [13.16s]: training loss=0.04836411774158478  validation ndcg@10=0.019527928479249668 [0.43s]\n",
      "epoch 1 [50.5s]:  training loss=0.35116225481033325                                       \n",
      "epoch 2 [50.25s]:  training loss=0.27102431654930115                                      \n",
      "epoch 3 [51.07s]:  training loss=0.26643994450569153                                      \n",
      "epoch 4 [51.81s]:  training loss=0.28026002645492554                                      \n",
      "epoch 5 [51.0s]: training loss=0.29465970396995544  validation ndcg@10=0.012727437140354247 [1.11s]\n",
      "epoch 6 [50.75s]:  training loss=0.31403106451034546                                      \n",
      "epoch 7 [51.06s]:  training loss=0.29579389095306396                                      \n",
      "epoch 8 [51.08s]:  training loss=0.3079927861690521                                       \n",
      "epoch 9 [51.09s]:  training loss=0.3033291697502136                                       \n",
      "epoch 10 [52.57s]: training loss=0.33068591356277466  validation ndcg@10=0.012289641588838149 [1.09s]\n",
      "epoch 11 [50.85s]:  training loss=0.31165000796318054                                     \n",
      "epoch 12 [51.08s]:  training loss=0.33597078919410706                                     \n",
      "epoch 13 [50.81s]:  training loss=0.3216696083545685                                      \n",
      "epoch 14 [51.09s]:  training loss=0.35269027948379517                                     \n",
      "epoch 15 [51.51s]: training loss=0.3321986794471741  validation ndcg@10=0.011137440999017756 [1.09s]\n",
      "epoch 16 [51.14s]:  training loss=0.35603004693984985                                     \n",
      "epoch 17 [51.11s]:  training loss=0.3644913136959076                                      \n",
      "epoch 18 [50.96s]:  training loss=0.35823777318000793                                     \n",
      "epoch 19 [50.74s]:  training loss=0.3531167805194855                                      \n",
      "epoch 20 [50.83s]: training loss=0.37460842728614807  validation ndcg@10=0.013544685276265511 [1.11s]\n",
      "epoch 21 [50.71s]:  training loss=0.3972533643245697                                      \n",
      "epoch 22 [51.26s]:  training loss=0.3750571608543396                                      \n",
      "epoch 23 [50.42s]:  training loss=0.3475077152252197                                      \n",
      "epoch 24 [50.99s]:  training loss=0.3813621997833252                                      \n",
      "epoch 25 [51.86s]: training loss=0.3783102333545685  validation ndcg@10=0.014831306430655142 [1.16s]\n",
      "epoch 26 [51.28s]:  training loss=0.3838653266429901                                      \n",
      "epoch 27 [50.98s]:  training loss=0.3806041479110718                                      \n",
      "epoch 28 [50.96s]:  training loss=0.377514511346817                                       \n",
      "epoch 29 [51.1s]:  training loss=0.40544939041137695                                      \n",
      "epoch 30 [52.42s]: training loss=0.4113835394382477  validation ndcg@10=0.012183616679723328 [1.12s]\n",
      "epoch 31 [50.97s]:  training loss=0.39514946937561035                                     \n",
      "epoch 32 [51.45s]:  training loss=0.41796085238456726                                     \n",
      "epoch 33 [51.58s]:  training loss=0.4228847324848175                                      \n",
      "epoch 34 [50.8s]:  training loss=0.39916208386421204                                      \n",
      "epoch 35 [51.08s]: training loss=0.41399720311164856  validation ndcg@10=0.01321246858879323 [1.07s]\n",
      "epoch 36 [51.3s]:  training loss=0.39556869864463806                                      \n",
      "epoch 37 [50.79s]:  training loss=0.40317660570144653                                     \n",
      "epoch 38 [51.06s]:  training loss=0.4021032452583313                                      \n",
      "epoch 39 [50.2s]:  training loss=0.4584394693374634                                       \n",
      "epoch 40 [50.47s]: training loss=0.451796293258667  validation ndcg@10=0.014448985000504063 [1.08s]\n",
      "epoch 41 [50.55s]:  training loss=0.442773699760437                                       \n",
      "epoch 42 [50.46s]:  training loss=0.41889363527297974                                     \n",
      "epoch 43 [50.75s]:  training loss=0.4795719385147095                                      \n",
      "epoch 44 [50.76s]:  training loss=0.4489355683326721                                      \n",
      "epoch 45 [50.5s]: training loss=0.4405478537082672  validation ndcg@10=0.012755660794903175 [1.1s]\n",
      "epoch 46 [50.92s]:  training loss=0.4519531726837158                                      \n",
      "epoch 47 [51.3s]:  training loss=0.43242326378822327                                      \n",
      "epoch 48 [50.55s]:  training loss=0.46140533685684204                                     \n",
      "epoch 49 [52.96s]:  training loss=0.4901425242424011                                      \n",
      "epoch 50 [50.71s]: training loss=0.4778497517108917  validation ndcg@10=0.014226234438899274 [1.11s]\n",
      "epoch 1 [40.96s]:  training loss=0.8232656717300415                                       \n",
      "epoch 2 [41.84s]:  training loss=0.7985301613807678                                       \n",
      "epoch 3 [41.53s]:  training loss=0.788330078125                                           \n",
      "epoch 4 [41.85s]:  training loss=0.7810190320014954                                       \n",
      "epoch 5 [41.02s]: training loss=0.7764319777488708  validation ndcg@10=0.0013928469612082852 [0.93s]\n",
      "epoch 6 [41.51s]:  training loss=0.7716419100761414                                       \n",
      "epoch 7 [41.7s]:  training loss=0.7648507356643677                                        \n",
      "epoch 8 [41.62s]:  training loss=0.7569153904914856                                       \n",
      "epoch 9 [41.94s]:  training loss=0.7509845495223999                                       \n",
      "epoch 10 [41.45s]: training loss=0.7439571619033813  validation ndcg@10=0.004593471447921644 [0.93s]\n",
      "epoch 11 [41.12s]:  training loss=0.7337687611579895                                      \n",
      "epoch 12 [41.06s]:  training loss=0.7288821339607239                                      \n",
      "epoch 13 [41.11s]:  training loss=0.7176186442375183                                      \n",
      "epoch 14 [40.98s]:  training loss=0.7107475399971008                                      \n",
      "epoch 15 [41.32s]: training loss=0.6907168626785278  validation ndcg@10=0.02115117983653814 [0.92s]\n",
      "epoch 16 [41.39s]:  training loss=0.6585231423377991                                      \n",
      "epoch 17 [41.83s]:  training loss=0.6320990324020386                                      \n",
      "epoch 18 [42.38s]:  training loss=0.61533522605896                                        \n",
      "epoch 19 [41.62s]:  training loss=0.6080573201179504                                      \n",
      "epoch 20 [41.94s]: training loss=0.6038976907730103  validation ndcg@10=0.021382449263861673 [0.94s]\n",
      "epoch 21 [43.55s]:  training loss=0.5914925336837769                                      \n",
      "epoch 22 [41.37s]:  training loss=0.582871675491333                                       \n",
      "epoch 23 [40.9s]:  training loss=0.5765319466590881                                       \n",
      "epoch 24 [41.38s]:  training loss=0.5747511386871338                                      \n",
      "epoch 25 [42.17s]: training loss=0.5663151144981384  validation ndcg@10=0.021341104362698263 [0.93s]\n",
      "epoch 26 [41.59s]:  training loss=0.5597472190856934                                      \n",
      "epoch 27 [41.28s]:  training loss=0.5535981059074402                                      \n",
      "epoch 28 [42.05s]:  training loss=0.5450987815856934                                      \n",
      "epoch 29 [41.51s]:  training loss=0.5425780415534973                                      \n",
      "epoch 30 [41.98s]: training loss=0.5376297235488892  validation ndcg@10=0.02163700722034802 [0.92s]\n",
      "epoch 31 [41.87s]:  training loss=0.5312681198120117                                      \n",
      "epoch 32 [43.23s]:  training loss=0.526910126209259                                       \n",
      "epoch 33 [41.75s]:  training loss=0.5247558355331421                                      \n",
      "epoch 34 [43.14s]:  training loss=0.5176225900650024                                      \n",
      "epoch 35 [42.47s]: training loss=0.5092855095863342  validation ndcg@10=0.0211738868967707 [0.95s]\n",
      "epoch 36 [42.56s]:  training loss=0.5094178915023804                                      \n",
      "epoch 37 [42.06s]:  training loss=0.5047361850738525                                      \n",
      "epoch 38 [43.06s]:  training loss=0.4983919560909271                                      \n",
      "epoch 39 [41.46s]:  training loss=0.49496299028396606                                     \n",
      "epoch 40 [42.15s]: training loss=0.488233357667923  validation ndcg@10=0.021572736356669238 [0.93s]\n",
      "epoch 41 [41.93s]:  training loss=0.4857812225818634                                      \n",
      "epoch 42 [41.98s]:  training loss=0.48002302646636963                                     \n",
      "epoch 43 [42.22s]:  training loss=0.47495630383491516                                     \n",
      "epoch 44 [42.07s]:  training loss=0.472027063369751                                       \n",
      "epoch 45 [43.62s]: training loss=0.4662734270095825  validation ndcg@10=0.022048637589513198 [0.91s]\n",
      "epoch 46 [41.88s]:  training loss=0.4597536325454712                                      \n",
      "epoch 47 [41.64s]:  training loss=0.45855262875556946                                     \n",
      "epoch 48 [42.57s]:  training loss=0.45381438732147217                                     \n",
      "epoch 49 [42.17s]:  training loss=0.45223432779312134                                     \n",
      "epoch 50 [41.69s]: training loss=0.44393178820610046  validation ndcg@10=0.021705804252211077 [0.92s]\n",
      "epoch 51 [41.07s]:  training loss=0.43779700994491577                                     \n",
      "epoch 52 [41.18s]:  training loss=0.433538556098938                                       \n",
      "epoch 53 [41.18s]:  training loss=0.4294397532939911                                      \n",
      "epoch 54 [40.8s]:  training loss=0.42624157667160034                                      \n",
      "epoch 55 [41.38s]: training loss=0.42052242159843445  validation ndcg@10=0.021341996983235433 [0.92s]\n",
      "epoch 56 [40.96s]:  training loss=0.4177905321121216                                      \n",
      "epoch 57 [41.39s]:  training loss=0.41009750962257385                                     \n",
      "epoch 58 [40.68s]:  training loss=0.40718594193458557                                     \n",
      "epoch 59 [41.13s]:  training loss=0.40463462471961975                                     \n",
      "epoch 60 [41.06s]: training loss=0.400958389043808  validation ndcg@10=0.021621596255195746 [1.0s]\n",
      "epoch 61 [41.87s]:  training loss=0.39937394857406616                                     \n",
      "epoch 62 [40.97s]:  training loss=0.38830164074897766                                     \n",
      "epoch 63 [40.94s]:  training loss=0.3908301889896393                                      \n",
      "epoch 64 [41.79s]:  training loss=0.3818342983722687                                      \n",
      "epoch 65 [41.49s]: training loss=0.38000133633613586  validation ndcg@10=0.022572730464271544 [0.89s]\n",
      "epoch 66 [40.94s]:  training loss=0.37757056951522827                                     \n",
      "epoch 67 [42.83s]:  training loss=0.37376439571380615                                     \n",
      "epoch 68 [41.19s]:  training loss=0.3724483847618103                                      \n",
      "epoch 69 [40.71s]:  training loss=0.3701446056365967                                      \n",
      "epoch 70 [40.92s]: training loss=0.3656727969646454  validation ndcg@10=0.023189071216705257 [0.94s]\n",
      "epoch 71 [41.1s]:  training loss=0.3603745996952057                                       \n",
      "epoch 72 [40.64s]:  training loss=0.3611503541469574                                      \n",
      "epoch 73 [41.34s]:  training loss=0.3556784689426422                                      \n",
      "epoch 74 [41.39s]:  training loss=0.3561815917491913                                      \n",
      "epoch 75 [41.05s]: training loss=0.3471842110157013  validation ndcg@10=0.022745171025811788 [0.9s]\n",
      "epoch 76 [41.57s]:  training loss=0.3473180830478668                                      \n",
      "epoch 77 [41.26s]:  training loss=0.34906432032585144                                     \n",
      "epoch 78 [41.74s]:  training loss=0.3432340621948242                                      \n",
      "epoch 79 [41.57s]:  training loss=0.33919161558151245                                     \n",
      "epoch 80 [41.49s]: training loss=0.3314589560031891  validation ndcg@10=0.02365075109733938 [0.95s]\n",
      "epoch 81 [40.91s]:  training loss=0.3303152918815613                                      \n",
      "epoch 82 [41.11s]:  training loss=0.32960033416748047                                     \n",
      "epoch 83 [41.57s]:  training loss=0.3257155120372772                                      \n",
      "epoch 84 [40.96s]:  training loss=0.31947389245033264                                     \n",
      "epoch 85 [41.37s]: training loss=0.31858354806900024  validation ndcg@10=0.024485237809060884 [0.92s]\n",
      "epoch 86 [41.75s]:  training loss=0.3155849575996399                                      \n",
      "epoch 87 [42.31s]:  training loss=0.31660693883895874                                     \n",
      "epoch 88 [41.88s]:  training loss=0.3084346055984497                                      \n",
      "epoch 89 [41.03s]:  training loss=0.30649858713150024                                     \n",
      "epoch 90 [41.66s]: training loss=0.30111175775527954  validation ndcg@10=0.0242377690761489 [0.94s]\n",
      "epoch 91 [42.98s]:  training loss=0.3027209937572479                                      \n",
      "epoch 92 [41.49s]:  training loss=0.29297101497650146                                     \n",
      "epoch 93 [41.21s]:  training loss=0.2940920293331146                                      \n",
      "epoch 94 [41.95s]:  training loss=0.2930383086204529                                      \n",
      "epoch 95 [41.45s]: training loss=0.28772038221359253  validation ndcg@10=0.02386550693517936 [0.96s]\n",
      "epoch 96 [41.8s]:  training loss=0.28200438618659973                                      \n",
      "epoch 97 [41.91s]:  training loss=0.28222501277923584                                     \n",
      "epoch 98 [41.23s]:  training loss=0.2769301235675812                                      \n",
      "epoch 99 [41.79s]:  training loss=0.2791600525379181                                      \n",
      "epoch 100 [42.35s]: training loss=0.2768166661262512  validation ndcg@10=0.024114017687922647 [0.97s]\n",
      "epoch 101 [41.91s]:  training loss=0.2703588306903839                                     \n",
      "epoch 102 [41.95s]:  training loss=0.27123481035232544                                    \n",
      "epoch 103 [41.98s]:  training loss=0.2644611597061157                                     \n",
      "epoch 104 [41.69s]:  training loss=0.26393693685531616                                    \n",
      "epoch 105 [41.03s]: training loss=0.26018235087394714  validation ndcg@10=0.023761703499626894 [0.95s]\n",
      "epoch 106 [41.78s]:  training loss=0.25957807898521423                                    \n",
      "epoch 107 [41.84s]:  training loss=0.25692471861839294                                    \n",
      "epoch 108 [41.68s]:  training loss=0.2532111704349518                                     \n",
      "epoch 109 [41.52s]:  training loss=0.24949726462364197                                    \n",
      "epoch 110 [40.42s]: training loss=0.2497646063566208  validation ndcg@10=0.024454637359011794 [0.94s]\n",
      "epoch 1 [15.49s]:  training loss=0.8284986615180969                                       \n",
      "epoch 2 [15.9s]:  training loss=0.7971415519714355                                      \n",
      "epoch 3 [15.67s]:  training loss=0.7857858538627625                                     \n",
      "epoch 4 [15.74s]:  training loss=0.781113862991333                                      \n",
      "epoch 5 [15.78s]: training loss=0.7741063237190247  validation ndcg@10=0.001057546017544197 [0.47s]\n",
      "epoch 6 [15.82s]:  training loss=0.7627896070480347                                     \n",
      "epoch 7 [16.12s]:  training loss=0.7574107646942139                                     \n",
      "epoch 8 [16.27s]:  training loss=0.7503975033760071                                     \n",
      "epoch 9 [17.08s]:  training loss=0.7406343817710876                                     \n",
      "epoch 10 [16.25s]: training loss=0.7238922715187073  validation ndcg@10=0.01284608018976414 [0.45s]\n",
      "epoch 11 [16.3s]:  training loss=0.6955726146697998                                     \n",
      "epoch 12 [16.01s]:  training loss=0.6613187193870544                                    \n",
      "epoch 13 [16.49s]:  training loss=0.6372860074043274                                    \n",
      "epoch 14 [16.02s]:  training loss=0.6297144293785095                                    \n",
      "epoch 15 [15.84s]: training loss=0.6175500750541687  validation ndcg@10=0.018322185905089113 [0.51s]\n",
      "epoch 16 [18.26s]:  training loss=0.6106916069984436                                    \n",
      "epoch 17 [15.22s]:  training loss=0.6008650660514832                                    \n",
      "epoch 18 [15.74s]:  training loss=0.5944582223892212                                    \n",
      "epoch 19 [15.92s]:  training loss=0.5913960933685303                                    \n",
      "epoch 20 [16.07s]: training loss=0.5817713141441345  validation ndcg@10=0.01967224003458312 [0.47s]\n",
      "epoch 21 [16.26s]:  training loss=0.5760207176208496                                    \n",
      "epoch 22 [16.01s]:  training loss=0.5674211978912354                                    \n",
      "epoch 23 [16.42s]:  training loss=0.561345100402832                                     \n",
      "epoch 24 [16.41s]:  training loss=0.5569639205932617                                    \n",
      "epoch 25 [15.83s]: training loss=0.5547410845756531  validation ndcg@10=0.019885339807160986 [0.5s]\n",
      "epoch 26 [16.53s]:  training loss=0.5448301434516907                                    \n",
      "epoch 27 [16.13s]:  training loss=0.5413953065872192                                    \n",
      "epoch 28 [16.49s]:  training loss=0.540669322013855                                     \n",
      "epoch 29 [16.43s]:  training loss=0.5313994884490967                                    \n",
      "epoch 30 [16.4s]: training loss=0.5247615575790405  validation ndcg@10=0.020242622289432178 [0.45s]\n",
      "epoch 31 [16.1s]:  training loss=0.5174543261528015                                     \n",
      "epoch 32 [15.81s]:  training loss=0.5109754800796509                                    \n",
      "epoch 33 [16.9s]:  training loss=0.5056880116462708                                     \n",
      "epoch 34 [16.39s]:  training loss=0.502648115158081                                     \n",
      "epoch 35 [16.71s]: training loss=0.49148663878440857  validation ndcg@10=0.020575218234468173 [0.44s]\n",
      "epoch 36 [16.65s]:  training loss=0.4902624785900116                                    \n",
      "epoch 37 [16.37s]:  training loss=0.4818350672721863                                    \n",
      "epoch 38 [15.77s]:  training loss=0.47802042961120605                                   \n",
      "epoch 39 [16.45s]:  training loss=0.47173598408699036                                   \n",
      "epoch 40 [16.27s]: training loss=0.4695659875869751  validation ndcg@10=0.021161884174335864 [0.48s]\n",
      "epoch 41 [15.98s]:  training loss=0.45872774720191956                                   \n",
      "epoch 42 [16.51s]:  training loss=0.4502251148223877                                    \n",
      "epoch 43 [16.64s]:  training loss=0.44744399189949036                                   \n",
      "epoch 44 [15.98s]:  training loss=0.442947655916214                                     \n",
      "epoch 45 [16.11s]: training loss=0.43833163380622864  validation ndcg@10=0.02084748269116411 [0.47s]\n",
      "epoch 46 [16.31s]:  training loss=0.43600502610206604                                   \n",
      "epoch 47 [16.25s]:  training loss=0.4259599447250366                                    \n",
      "epoch 48 [16.37s]:  training loss=0.422620952129364                                     \n",
      "epoch 49 [16.0s]:  training loss=0.41846412420272827                                    \n",
      "epoch 50 [16.52s]: training loss=0.41513219475746155  validation ndcg@10=0.021312787130780428 [0.45s]\n",
      "epoch 51 [16.13s]:  training loss=0.41151028871536255                                   \n",
      "epoch 52 [15.91s]:  training loss=0.4093421995639801                                    \n",
      "epoch 53 [16.45s]:  training loss=0.40373602509498596                                   \n",
      "epoch 54 [16.26s]:  training loss=0.3996048867702484                                    \n",
      "epoch 55 [16.39s]: training loss=0.39677536487579346  validation ndcg@10=0.022644438523001846 [0.45s]\n",
      "epoch 56 [16.37s]:  training loss=0.3922816812992096                                    \n",
      "epoch 57 [16.29s]:  training loss=0.38891345262527466                                   \n",
      "epoch 58 [16.54s]:  training loss=0.3843139708042145                                    \n",
      "epoch 59 [16.04s]:  training loss=0.380209356546402                                     \n",
      "epoch 60 [16.24s]: training loss=0.38039886951446533  validation ndcg@10=0.022975476452586352 [0.44s]\n",
      "epoch 61 [16.25s]:  training loss=0.37371131777763367                                   \n",
      "epoch 62 [16.15s]:  training loss=0.3671227991580963                                    \n",
      "epoch 63 [16.32s]:  training loss=0.365451455116272                                     \n",
      "epoch 64 [15.94s]:  training loss=0.3597147464752197                                    \n",
      "epoch 65 [16.03s]: training loss=0.3607417047023773  validation ndcg@10=0.023372477377113003 [0.47s]\n",
      "epoch 66 [16.0s]:  training loss=0.35818079113960266                                    \n",
      "epoch 67 [16.18s]:  training loss=0.3513889014720917                                    \n",
      "epoch 68 [16.47s]:  training loss=0.34833192825317383                                   \n",
      "epoch 69 [16.48s]:  training loss=0.3443234860897064                                    \n",
      "epoch 70 [15.94s]: training loss=0.3376617729663849  validation ndcg@10=0.022698013946504378 [0.46s]\n",
      "epoch 71 [16.38s]:  training loss=0.33602792024612427                                   \n",
      "epoch 72 [16.24s]:  training loss=0.33214524388313293                                   \n",
      "epoch 73 [16.24s]:  training loss=0.3258845806121826                                    \n",
      "epoch 74 [16.12s]:  training loss=0.3270764648914337                                    \n",
      "epoch 75 [15.88s]: training loss=0.32023823261260986  validation ndcg@10=0.022872209752912653 [0.48s]\n",
      "epoch 76 [17.01s]:  training loss=0.3153035342693329                                    \n",
      "epoch 77 [16.46s]:  training loss=0.314856618642807                                     \n",
      "epoch 78 [18.44s]:  training loss=0.3122656047344208                                    \n",
      "epoch 79 [15.92s]:  training loss=0.3075651526451111                                    \n",
      "epoch 80 [16.46s]: training loss=0.30662107467651367  validation ndcg@10=0.022159319466441722 [0.44s]\n",
      "epoch 81 [16.5s]:  training loss=0.3032751679420471                                     \n",
      "epoch 82 [16.09s]:  training loss=0.29977357387542725                                   \n",
      "epoch 83 [16.25s]:  training loss=0.29247501492500305                                   \n",
      "epoch 84 [16.45s]:  training loss=0.2936035096645355                                    \n",
      "epoch 85 [16.02s]: training loss=0.2908210754394531  validation ndcg@10=0.02307701009185032 [0.48s]\n",
      "epoch 86 [16.41s]:  training loss=0.28673914074897766                                   \n",
      "epoch 87 [16.31s]:  training loss=0.286793977022171                                     \n",
      "epoch 88 [16.31s]:  training loss=0.2742779850959778                                    \n",
      "epoch 89 [16.54s]:  training loss=0.2732577323913574                                    \n",
      "epoch 90 [16.11s]: training loss=0.2761978507041931  validation ndcg@10=0.021777332114991492 [0.47s]\n",
      "100%|██████████| 50/50 [29:29:44<00:00, 2123.69s/trial, best loss: -0.02889881617403063]\n",
      "Tuning hyperparameters of PinSage Recommender on dataset movielens_100k...\n",
      "use_text_feature=False, use_no_feature=False\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengxuan_yan/opt/miniconda3/envs/torch/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 [7.55s]:  training loss=0.7324574589729309    \n",
      "epoch 2 [6.56s]:  training loss=0.481343150138855     \n",
      "epoch 3 [6.44s]:  training loss=0.4459838271141052    \n",
      "epoch 4 [5.9s]:  training loss=0.42334625124931335    \n",
      "epoch 5 [5.49s]: training loss=0.4106220602989197  validation ndcg@10=0.0682104118087298 [0.08s]\n",
      "epoch 6 [5.3s]:  training loss=0.4085623025894165     \n",
      "epoch 7 [5.2s]:  training loss=0.4007148742675781     \n",
      "epoch 8 [5.12s]:  training loss=0.3933311998844147    \n",
      "epoch 9 [5.26s]:  training loss=0.39009302854537964   \n",
      "epoch 10 [5.12s]: training loss=0.378045916557312  validation ndcg@10=0.0641736374414028 [0.06s]\n",
      "epoch 11 [5.14s]:  training loss=0.3722127676010132   \n",
      "epoch 12 [5.11s]:  training loss=0.36945798993110657  \n",
      "epoch 13 [5.18s]:  training loss=0.3600902557373047   \n",
      "epoch 14 [5.12s]:  training loss=0.3576946258544922   \n",
      "epoch 15 [5.15s]: training loss=0.35976409912109375  validation ndcg@10=0.05883538586857692 [0.09s]\n",
      "epoch 16 [5.26s]:  training loss=0.36174365878105164  \n",
      "epoch 17 [5.29s]:  training loss=0.3511260747909546   \n",
      "epoch 18 [5.33s]:  training loss=0.3506757915019989   \n",
      "epoch 19 [5.29s]:  training loss=0.344274640083313    \n",
      "epoch 20 [5.11s]: training loss=0.3432278037071228  validation ndcg@10=0.05817798659605798 [0.06s]\n",
      "epoch 21 [5.1s]:  training loss=0.33959704637527466   \n",
      "epoch 22 [5.12s]:  training loss=0.3379612863063812   \n",
      "epoch 23 [5.1s]:  training loss=0.3388519287109375    \n",
      "epoch 24 [5.15s]:  training loss=0.3362468481063843   \n",
      "epoch 25 [5.2s]: training loss=0.3373250365257263  validation ndcg@10=0.07089630105997934 [0.07s]\n",
      "epoch 26 [5.1s]:  training loss=0.32938387989997864   \n",
      "epoch 27 [5.18s]:  training loss=0.33152198791503906  \n",
      "epoch 28 [5.27s]:  training loss=0.3330344259738922   \n",
      "epoch 29 [5.14s]:  training loss=0.3261166214942932   \n",
      "epoch 30 [5.14s]: training loss=0.3317921757698059  validation ndcg@10=0.06163546768132258 [0.06s]\n",
      "epoch 31 [7.03s]:  training loss=0.33152490854263306  \n",
      "epoch 32 [5.1s]:  training loss=0.3298787772655487    \n",
      "epoch 33 [5.22s]:  training loss=0.3229900002479553   \n",
      "epoch 34 [5.25s]:  training loss=0.3213786780834198   \n",
      "epoch 35 [5.15s]: training loss=0.3249712586402893  validation ndcg@10=0.06231800991379542 [0.07s]\n",
      "epoch 36 [5.1s]:  training loss=0.32143673300743103   \n",
      "epoch 37 [5.26s]:  training loss=0.31922343373298645  \n",
      "epoch 38 [5.12s]:  training loss=0.32369688153266907  \n",
      "epoch 39 [5.19s]:  training loss=0.3159681260585785   \n",
      "epoch 40 [5.11s]: training loss=0.31402403116226196  validation ndcg@10=0.05304333105563468 [0.07s]\n",
      "epoch 41 [5.12s]:  training loss=0.31415948271751404  \n",
      "epoch 42 [5.11s]:  training loss=0.3201828598976135   \n",
      "epoch 43 [5.16s]:  training loss=0.3158981502056122   \n",
      "epoch 44 [5.15s]:  training loss=0.3164716064929962   \n",
      "epoch 45 [5.18s]: training loss=0.3044129014015198  validation ndcg@10=0.06518689604600297 [0.07s]\n",
      "epoch 46 [5.11s]:  training loss=0.30903196334838867  \n",
      "epoch 47 [5.19s]:  training loss=0.30965912342071533  \n",
      "epoch 48 [5.11s]:  training loss=0.3069608211517334   \n",
      "epoch 49 [5.13s]:  training loss=0.30936020612716675  \n",
      "epoch 50 [5.08s]: training loss=0.31017810106277466  validation ndcg@10=0.060019287177912314 [0.07s]\n",
      "epoch 1 [3.67s]:  training loss=0.9495418071746826                                   \n",
      "epoch 2 [3.73s]:  training loss=0.8171187043190002                                   \n",
      "epoch 3 [3.74s]:  training loss=0.7193474173545837                                   \n",
      "epoch 4 [3.78s]:  training loss=0.6530133485794067                                   \n",
      "epoch 5 [3.71s]: training loss=0.5861100554466248  validation ndcg@10=0.04738999854456348 [0.07s]\n",
      "epoch 6 [3.71s]:  training loss=0.5357570648193359                                   \n",
      "epoch 7 [3.89s]:  training loss=0.5069568753242493                                   \n",
      "epoch 8 [3.79s]:  training loss=0.47315236926078796                                  \n",
      "epoch 9 [3.74s]:  training loss=0.4573189318180084                                   \n",
      "epoch 10 [3.98s]: training loss=0.4487129747867584  validation ndcg@10=0.05320188604376472 [0.08s]\n",
      "epoch 11 [4.03s]:  training loss=0.4335208237171173                                  \n",
      "epoch 12 [3.95s]:  training loss=0.43545156717300415                                 \n",
      "epoch 13 [4.06s]:  training loss=0.4384279251098633                                  \n",
      "epoch 14 [3.93s]:  training loss=0.4299590587615967                                  \n",
      "epoch 15 [4.05s]: training loss=0.4249216318130493  validation ndcg@10=0.055512023392168736 [0.07s]\n",
      "epoch 16 [3.89s]:  training loss=0.42451438307762146                                 \n",
      "epoch 17 [4.0s]:  training loss=0.4232082664966583                                   \n",
      "epoch 18 [3.93s]:  training loss=0.4146171808242798                                  \n",
      "epoch 19 [3.98s]:  training loss=0.41685405373573303                                 \n",
      "epoch 20 [3.87s]: training loss=0.414134681224823  validation ndcg@10=0.05832883797443175 [0.08s]\n",
      "epoch 21 [3.9s]:  training loss=0.4088130295276642                                   \n",
      "epoch 22 [3.85s]:  training loss=0.4115443527698517                                  \n",
      "epoch 23 [3.92s]:  training loss=0.4091528654098511                                  \n",
      "epoch 24 [3.91s]:  training loss=0.4097544848918915                                  \n",
      "epoch 25 [4.01s]: training loss=0.4076403081417084  validation ndcg@10=0.06017426620607337 [0.09s]\n",
      "epoch 26 [3.95s]:  training loss=0.4104006290435791                                  \n",
      "epoch 27 [3.93s]:  training loss=0.3923046588897705                                  \n",
      "epoch 28 [4.0s]:  training loss=0.39943253993988037                                  \n",
      "epoch 29 [4.03s]:  training loss=0.39612138271331787                                 \n",
      "epoch 30 [3.93s]: training loss=0.3951881527900696  validation ndcg@10=0.05997431159149909 [0.08s]\n",
      "epoch 31 [3.81s]:  training loss=0.3919553756713867                                  \n",
      "epoch 32 [3.89s]:  training loss=0.3964216411113739                                  \n",
      "epoch 33 [3.96s]:  training loss=0.39047905802726746                                 \n",
      "epoch 34 [3.94s]:  training loss=0.38423794507980347                                 \n",
      "epoch 35 [3.88s]: training loss=0.3899490237236023  validation ndcg@10=0.06512937214001442 [0.07s]\n",
      "epoch 36 [3.95s]:  training loss=0.394479364156723                                   \n",
      "epoch 37 [3.93s]:  training loss=0.39250364899635315                                 \n",
      "epoch 38 [3.98s]:  training loss=0.3899049758911133                                  \n",
      "epoch 39 [3.78s]:  training loss=0.3786415755748749                                  \n",
      "epoch 40 [4.07s]: training loss=0.38547301292419434  validation ndcg@10=0.06172951256902052 [0.07s]\n",
      "epoch 41 [4.05s]:  training loss=0.38102951645851135                                 \n",
      "epoch 42 [3.99s]:  training loss=0.3837501108646393                                  \n",
      "epoch 43 [3.94s]:  training loss=0.3771602511405945                                  \n",
      "epoch 44 [4.06s]:  training loss=0.3758009076118469                                  \n",
      "epoch 45 [3.97s]: training loss=0.37799912691116333  validation ndcg@10=0.06822024205282944 [0.08s]\n",
      "epoch 46 [3.92s]:  training loss=0.38630640506744385                                 \n",
      "epoch 47 [3.85s]:  training loss=0.37319043278694153                                 \n",
      "epoch 48 [3.9s]:  training loss=0.37340614199638367                                  \n",
      "epoch 49 [3.86s]:  training loss=0.3743785619735718                                  \n",
      "epoch 50 [3.85s]: training loss=0.3675784766674042  validation ndcg@10=0.06457171372150758 [0.07s]\n",
      "epoch 51 [3.88s]:  training loss=0.3694632053375244                                  \n",
      "epoch 52 [3.81s]:  training loss=0.3703312277793884                                  \n",
      "epoch 53 [3.78s]:  training loss=0.36701738834381104                                 \n",
      "epoch 54 [3.8s]:  training loss=0.3641691207885742                                   \n",
      "epoch 55 [3.93s]: training loss=0.36870792508125305  validation ndcg@10=0.06891394639756176 [0.08s]\n",
      "epoch 56 [4.05s]:  training loss=0.36152413487434387                                 \n",
      "epoch 57 [3.93s]:  training loss=0.36951300501823425                                 \n",
      "epoch 58 [3.85s]:  training loss=0.3671346604824066                                  \n",
      "epoch 59 [3.84s]:  training loss=0.36527061462402344                                 \n",
      "epoch 60 [3.85s]: training loss=0.3638460636138916  validation ndcg@10=0.06570363145228648 [0.07s]\n",
      "epoch 61 [3.78s]:  training loss=0.35833796858787537                                 \n",
      "epoch 62 [3.93s]:  training loss=0.35706591606140137                                 \n",
      "epoch 63 [3.96s]:  training loss=0.3541002869606018                                  \n",
      "epoch 64 [3.88s]:  training loss=0.3537003695964813                                  \n",
      "epoch 65 [3.92s]: training loss=0.35753893852233887  validation ndcg@10=0.06751228839806313 [0.07s]\n",
      "epoch 66 [3.86s]:  training loss=0.35834676027297974                                 \n",
      "epoch 67 [3.94s]:  training loss=0.3517892062664032                                  \n",
      "epoch 68 [3.9s]:  training loss=0.35038819909095764                                  \n",
      "epoch 69 [4.05s]:  training loss=0.35301321744918823                                 \n",
      "epoch 70 [3.91s]: training loss=0.34994378685951233  validation ndcg@10=0.07214041379443764 [0.07s]\n",
      "epoch 71 [4.01s]:  training loss=0.3486739695072174                                  \n",
      "epoch 72 [4.04s]:  training loss=0.3576511740684509                                  \n",
      "epoch 73 [4.03s]:  training loss=0.34652864933013916                                 \n",
      "epoch 74 [4.01s]:  training loss=0.3514896631240845                                  \n",
      "epoch 75 [3.92s]: training loss=0.3496812582015991  validation ndcg@10=0.06421814380526854 [0.08s]\n",
      "epoch 76 [3.87s]:  training loss=0.3478231430053711                                  \n",
      "epoch 77 [4.0s]:  training loss=0.35144588351249695                                  \n",
      "epoch 78 [3.91s]:  training loss=0.3482128977775574                                  \n",
      "epoch 79 [3.95s]:  training loss=0.3461977243423462                                  \n",
      "epoch 80 [3.87s]: training loss=0.3400486409664154  validation ndcg@10=0.059975975985456814 [0.07s]\n",
      "epoch 81 [4.0s]:  training loss=0.34826555848121643                                  \n",
      "epoch 82 [3.91s]:  training loss=0.3442058265209198                                  \n",
      "epoch 83 [3.9s]:  training loss=0.3334059715270996                                   \n",
      "epoch 84 [3.93s]:  training loss=0.3400246798992157                                  \n",
      "epoch 85 [3.86s]: training loss=0.34517714381217957  validation ndcg@10=0.07274131026489823 [0.07s]\n",
      "epoch 86 [3.86s]:  training loss=0.33914074301719666                                 \n",
      "epoch 87 [3.97s]:  training loss=0.33881986141204834                                 \n",
      "epoch 88 [3.8s]:  training loss=0.34395092725753784                                  \n",
      "epoch 89 [3.73s]:  training loss=0.34027475118637085                                 \n",
      "epoch 90 [4.1s]: training loss=0.3459011912345886  validation ndcg@10=0.06539620883243785 [0.07s]\n",
      "epoch 91 [3.99s]:  training loss=0.34361645579338074                                 \n",
      "epoch 92 [4.14s]:  training loss=0.3393411934375763                                  \n",
      "epoch 93 [4.0s]:  training loss=0.33902493119239807                                  \n",
      "epoch 94 [4.24s]:  training loss=0.3423428535461426                                  \n",
      "epoch 95 [4.11s]: training loss=0.3406047821044922  validation ndcg@10=0.06914051702351795 [0.08s]\n",
      "epoch 96 [4.34s]:  training loss=0.3351602554321289                                  \n",
      "epoch 97 [4.24s]:  training loss=0.3360241949558258                                  \n",
      "epoch 98 [3.93s]:  training loss=0.33785760402679443                                 \n",
      "epoch 99 [4.02s]:  training loss=0.3386746346950531                                  \n",
      "epoch 100 [3.83s]: training loss=0.3322644531726837  validation ndcg@10=0.06723241826620864 [0.08s]\n",
      "epoch 101 [3.91s]:  training loss=0.3335011601448059                                 \n",
      "epoch 102 [4.09s]:  training loss=0.338823139667511                                  \n",
      "epoch 103 [3.8s]:  training loss=0.33745017647743225                                 \n",
      "epoch 104 [3.81s]:  training loss=0.33286231756210327                                \n",
      "epoch 105 [3.76s]: training loss=0.3304630219936371  validation ndcg@10=0.07402499899185434 [0.08s]\n",
      "epoch 106 [3.81s]:  training loss=0.3325553238391876                                 \n",
      "epoch 107 [3.88s]:  training loss=0.33038294315338135                                \n",
      "epoch 108 [3.81s]:  training loss=0.3308219015598297                                 \n",
      "epoch 109 [3.89s]:  training loss=0.3287118673324585                                 \n",
      "epoch 110 [3.87s]: training loss=0.3275843858718872  validation ndcg@10=0.0762034357616191 [0.07s]\n",
      "epoch 111 [3.87s]:  training loss=0.3340139389038086                                 \n",
      "epoch 112 [3.91s]:  training loss=0.3321995437145233                                 \n",
      "epoch 113 [3.89s]:  training loss=0.33417657017707825                                \n",
      "epoch 114 [3.84s]:  training loss=0.3259049654006958                                 \n",
      "epoch 115 [3.77s]: training loss=0.3277382254600525  validation ndcg@10=0.06639745115539734 [0.07s]\n",
      "epoch 116 [3.86s]:  training loss=0.32754018902778625                                \n",
      "epoch 117 [3.93s]:  training loss=0.324057012796402                                  \n",
      "epoch 118 [3.74s]:  training loss=0.3261803686618805                                 \n",
      "epoch 119 [3.82s]:  training loss=0.32540878653526306                                \n",
      "epoch 120 [3.91s]: training loss=0.3322525918483734  validation ndcg@10=0.07378299781783464 [0.07s]\n",
      "epoch 121 [3.71s]:  training loss=0.3249723017215729                                 \n",
      "epoch 122 [3.76s]:  training loss=0.32720330357551575                                \n",
      "epoch 123 [3.86s]:  training loss=0.32883965969085693                                \n",
      "epoch 124 [3.91s]:  training loss=0.32349157333374023                                \n",
      "epoch 125 [3.89s]: training loss=0.32133588194847107  validation ndcg@10=0.06995715696224508 [0.07s]\n",
      "epoch 126 [3.83s]:  training loss=0.32929202914237976                                \n",
      "epoch 127 [3.86s]:  training loss=0.32572099566459656                                \n",
      "epoch 128 [3.84s]:  training loss=0.32579895853996277                                \n",
      "epoch 129 [3.91s]:  training loss=0.33152443170547485                                \n",
      "epoch 130 [4.0s]: training loss=0.3257806897163391  validation ndcg@10=0.07229070691818945 [0.07s]\n",
      "epoch 131 [3.87s]:  training loss=0.31943002343177795                                \n",
      "epoch 132 [3.82s]:  training loss=0.3256215453147888                                 \n",
      "epoch 133 [3.86s]:  training loss=0.3197677731513977                                 \n",
      "epoch 134 [3.75s]:  training loss=0.3319529592990875                                 \n",
      "epoch 135 [3.92s]: training loss=0.3205987513065338  validation ndcg@10=0.07736756245709936 [0.09s]\n",
      "epoch 136 [3.85s]:  training loss=0.3180854320526123                                 \n",
      "epoch 137 [4.01s]:  training loss=0.32481005787849426                                \n",
      "epoch 138 [3.93s]:  training loss=0.32222411036491394                                \n",
      "epoch 139 [3.83s]:  training loss=0.31996554136276245                                \n",
      "epoch 140 [3.99s]: training loss=0.3227579891681671  validation ndcg@10=0.06990292156027356 [0.08s]\n",
      "epoch 141 [4.05s]:  training loss=0.3260534405708313                                 \n",
      "epoch 142 [3.98s]:  training loss=0.31958216428756714                                \n",
      "epoch 143 [3.97s]:  training loss=0.3193160891532898                                 \n",
      "epoch 144 [3.86s]:  training loss=0.3270266354084015                                 \n",
      "epoch 145 [4.15s]: training loss=0.32213059067726135  validation ndcg@10=0.07513089785879301 [0.08s]\n",
      "epoch 146 [3.99s]:  training loss=0.32612380385398865                                \n",
      "epoch 147 [4.08s]:  training loss=0.31789979338645935                                \n",
      "epoch 148 [3.94s]:  training loss=0.32068175077438354                                \n",
      "epoch 149 [3.8s]:  training loss=0.32313480973243713                                 \n",
      "epoch 150 [3.94s]: training loss=0.31730347871780396  validation ndcg@10=0.07081149934936415 [0.08s]\n",
      "epoch 151 [3.77s]:  training loss=0.32256290316581726                                \n",
      "epoch 152 [3.76s]:  training loss=0.3196835219860077                                 \n",
      "epoch 153 [3.78s]:  training loss=0.327094703912735                                  \n",
      "epoch 154 [3.8s]:  training loss=0.31978070735931396                                 \n",
      "epoch 155 [3.86s]: training loss=0.31780293583869934  validation ndcg@10=0.07097248899504408 [0.09s]\n",
      "epoch 156 [3.9s]:  training loss=0.32229742407798767                                 \n",
      "epoch 157 [3.76s]:  training loss=0.3182680010795593                                 \n",
      "epoch 158 [3.79s]:  training loss=0.31635379791259766                                \n",
      "epoch 159 [3.8s]:  training loss=0.31084153056144714                                 \n",
      "epoch 160 [3.89s]: training loss=0.3214154541492462  validation ndcg@10=0.06718968900881214 [0.08s]\n",
      "epoch 1 [18.56s]:  training loss=5.4350762367248535                                  \n",
      "epoch 2 [18.84s]:  training loss=9.79753303527832                                    \n",
      "epoch 3 [18.87s]:  training loss=11.601080894470215                                  \n",
      "epoch 4 [18.88s]:  training loss=13.175009727478027                                  \n",
      "epoch 5 [18.63s]: training loss=13.946734428405762  validation ndcg@10=0.039710962989907575 [0.21s]\n",
      "epoch 6 [20.36s]:  training loss=14.36378002166748                                   \n",
      "epoch 7 [18.65s]:  training loss=15.373671531677246                                  \n",
      "epoch 8 [19.12s]:  training loss=15.825411796569824                                  \n",
      "epoch 9 [19.08s]:  training loss=16.06342887878418                                   \n",
      "epoch 10 [18.67s]: training loss=16.754077911376953  validation ndcg@10=0.03494995405105369 [0.22s]\n",
      "epoch 11 [18.87s]:  training loss=17.48373794555664                                  \n",
      "epoch 12 [18.42s]:  training loss=17.573312759399414                                 \n",
      "epoch 13 [19.57s]:  training loss=17.198034286499023                                 \n",
      "epoch 14 [19.15s]:  training loss=17.51226043701172                                  \n",
      "epoch 15 [19.36s]: training loss=18.25050163269043  validation ndcg@10=0.061467657331047594 [0.21s]\n",
      "epoch 16 [19.16s]:  training loss=18.357791900634766                                 \n",
      "epoch 17 [18.91s]:  training loss=18.571813583374023                                 \n",
      "epoch 18 [18.53s]:  training loss=18.83809471130371                                  \n",
      "epoch 19 [18.73s]:  training loss=19.603431701660156                                 \n",
      "epoch 20 [18.72s]: training loss=19.86536979675293  validation ndcg@10=0.04203491572369873 [0.22s]\n",
      "epoch 21 [19.36s]:  training loss=20.290002822875977                                 \n",
      "epoch 22 [19.01s]:  training loss=20.330474853515625                                 \n",
      "epoch 23 [18.91s]:  training loss=19.6488094329834                                   \n",
      "epoch 24 [18.95s]:  training loss=20.264310836791992                                 \n",
      "epoch 25 [19.57s]: training loss=20.25269317626953  validation ndcg@10=0.05011788374744143 [0.25s]\n",
      "epoch 26 [19.06s]:  training loss=20.61748695373535                                  \n",
      "epoch 27 [18.93s]:  training loss=20.8956298828125                                   \n",
      "epoch 28 [19.39s]:  training loss=20.908573150634766                                 \n",
      "epoch 29 [19.33s]:  training loss=20.765947341918945                                 \n",
      "epoch 30 [19.06s]: training loss=20.722576141357422  validation ndcg@10=0.0388536715711095 [0.19s]\n",
      "epoch 31 [18.65s]:  training loss=21.506080627441406                                 \n",
      "epoch 32 [19.06s]:  training loss=21.835765838623047                                 \n",
      "epoch 33 [18.94s]:  training loss=21.246191024780273                                 \n",
      "epoch 34 [19.5s]:  training loss=22.0635929107666                                    \n",
      "epoch 35 [19.28s]: training loss=22.219039916992188  validation ndcg@10=0.03844166480088065 [0.23s]\n",
      "epoch 36 [19.02s]:  training loss=22.03078269958496                                  \n",
      "epoch 37 [18.89s]:  training loss=22.08368492126465                                  \n",
      "epoch 38 [18.32s]:  training loss=21.702049255371094                                 \n",
      "epoch 39 [19.48s]:  training loss=22.80450439453125                                  \n",
      "epoch 40 [19.07s]: training loss=22.420377731323242  validation ndcg@10=0.03347939005593442 [0.2s]\n",
      "epoch 1 [5.52s]:  training loss=1.0386103391647339                                   \n",
      "epoch 2 [5.46s]:  training loss=1.3822935819625854                                   \n",
      "epoch 3 [5.51s]:  training loss=1.5569093227386475                                   \n",
      "epoch 4 [5.37s]:  training loss=1.635485053062439                                    \n",
      "epoch 5 [5.33s]: training loss=1.6323150396347046  validation ndcg@10=0.03274513371131444 [0.13s]\n",
      "epoch 6 [5.34s]:  training loss=1.6024603843688965                                   \n",
      "epoch 7 [5.49s]:  training loss=1.5825963020324707                                   \n",
      "epoch 8 [5.28s]:  training loss=1.5673891305923462                                   \n",
      "epoch 9 [5.33s]:  training loss=1.5262449979782104                                   \n",
      "epoch 10 [5.28s]: training loss=1.4432836771011353  validation ndcg@10=0.015662822732085278 [0.15s]\n",
      "epoch 11 [5.47s]:  training loss=1.441771149635315                                   \n",
      "epoch 12 [5.65s]:  training loss=1.4113787412643433                                  \n",
      "epoch 13 [5.73s]:  training loss=1.3584811687469482                                  \n",
      "epoch 14 [5.7s]:  training loss=1.358641505241394                                    \n",
      "epoch 15 [5.33s]: training loss=1.3984017372131348  validation ndcg@10=0.011662164958364613 [0.11s]\n",
      "epoch 16 [5.28s]:  training loss=1.3552196025848389                                  \n",
      "epoch 17 [5.75s]:  training loss=1.3953094482421875                                  \n",
      "epoch 18 [5.64s]:  training loss=1.3889025449752808                                  \n",
      "epoch 19 [5.51s]:  training loss=1.4127904176712036                                  \n",
      "epoch 20 [5.77s]: training loss=1.402306318283081  validation ndcg@10=0.006427897459554357 [0.16s]\n",
      "epoch 21 [5.61s]:  training loss=1.414628028869629                                   \n",
      "epoch 22 [5.88s]:  training loss=1.4587860107421875                                  \n",
      "epoch 23 [5.82s]:  training loss=1.441084861755371                                   \n",
      "epoch 24 [5.72s]:  training loss=1.4432276487350464                                  \n",
      "epoch 25 [5.63s]: training loss=1.4242256879806519  validation ndcg@10=0.010787863611560283 [0.14s]\n",
      "epoch 26 [5.43s]:  training loss=1.4563987255096436                                  \n",
      "epoch 27 [5.71s]:  training loss=1.4631694555282593                                  \n",
      "epoch 28 [6.84s]:  training loss=1.4561023712158203                                  \n",
      "epoch 29 [5.28s]:  training loss=1.5165313482284546                                  \n",
      "epoch 30 [5.43s]: training loss=1.4720946550369263  validation ndcg@10=0.008643127505738816 [0.13s]\n",
      "epoch 1 [10.1s]:  training loss=0.6906054019927979                                   \n",
      "epoch 2 [10.2s]:  training loss=0.48108458518981934                                  \n",
      "epoch 3 [10.1s]:  training loss=0.44290006160736084                                  \n",
      "epoch 4 [10.2s]:  training loss=0.43072494864463806                                  \n",
      "epoch 5 [9.89s]: training loss=0.4157711863517761  validation ndcg@10=0.05752902916152431 [0.15s]\n",
      "epoch 6 [9.77s]:  training loss=0.4028572142124176                                   \n",
      "epoch 7 [9.96s]:  training loss=0.3957796096801758                                   \n",
      "epoch 8 [9.81s]:  training loss=0.39493611454963684                                  \n",
      "epoch 9 [9.78s]:  training loss=0.38030579686164856                                  \n",
      "epoch 10 [9.74s]: training loss=0.3740580081939697  validation ndcg@10=0.0670066305399157 [0.19s]\n",
      "epoch 11 [9.61s]:  training loss=0.3780777156352997                                  \n",
      "epoch 12 [9.64s]:  training loss=0.3665899932384491                                  \n",
      "epoch 13 [9.67s]:  training loss=0.3658735454082489                                  \n",
      "epoch 14 [9.71s]:  training loss=0.3634345233440399                                  \n",
      "epoch 15 [9.74s]: training loss=0.35771000385284424  validation ndcg@10=0.061370199541202354 [0.18s]\n",
      "epoch 16 [9.76s]:  training loss=0.35857176780700684                                 \n",
      "epoch 17 [9.77s]:  training loss=0.3570004105567932                                  \n",
      "epoch 18 [9.74s]:  training loss=0.34893086552619934                                 \n",
      "epoch 19 [9.74s]:  training loss=0.34877848625183105                                 \n",
      "epoch 20 [9.6s]: training loss=0.3455445170402527  validation ndcg@10=0.05922256636486358 [0.17s]\n",
      "epoch 21 [9.78s]:  training loss=0.35265976190567017                                 \n",
      "epoch 22 [9.73s]:  training loss=0.3456454575061798                                  \n",
      "epoch 23 [9.74s]:  training loss=0.34092408418655396                                 \n",
      "epoch 24 [9.83s]:  training loss=0.3357749879360199                                  \n",
      "epoch 25 [9.52s]: training loss=0.3441806137561798  validation ndcg@10=0.0589026987185672 [0.16s]\n",
      "epoch 26 [10.01s]:  training loss=0.3357662856578827                                 \n",
      "epoch 27 [9.71s]:  training loss=0.3404768109321594                                  \n",
      "epoch 28 [10.06s]:  training loss=0.3336782157421112                                 \n",
      "epoch 29 [10.45s]:  training loss=0.33216941356658936                                \n",
      "epoch 30 [10.27s]: training loss=0.3323565423488617  validation ndcg@10=0.06443260018337366 [0.17s]\n",
      "epoch 31 [9.88s]:  training loss=0.33346831798553467                                 \n",
      "epoch 32 [9.83s]:  training loss=0.33407965302467346                                 \n",
      "epoch 33 [9.98s]:  training loss=0.33198851346969604                                 \n",
      "epoch 34 [10.13s]:  training loss=0.3285781145095825                                 \n",
      "epoch 35 [9.84s]: training loss=0.3363281190395355  validation ndcg@10=0.058563362018934306 [0.14s]\n",
      "epoch 1 [18.44s]:  training loss=0.6129040122032166                                  \n",
      "epoch 2 [17.67s]:  training loss=0.43565475940704346                                 \n",
      "epoch 3 [17.83s]:  training loss=0.427592933177948                                   \n",
      "epoch 4 [17.45s]:  training loss=0.4134294092655182                                  \n",
      "epoch 5 [17.82s]: training loss=0.3901420831680298  validation ndcg@10=0.07120981250405935 [0.23s]\n",
      "epoch 6 [17.99s]:  training loss=0.39324912428855896                                 \n",
      "epoch 7 [18.29s]:  training loss=0.3759315013885498                                  \n",
      "epoch 8 [17.49s]:  training loss=0.3712121248245239                                  \n",
      "epoch 9 [17.82s]:  training loss=0.3615281581878662                                  \n",
      "epoch 10 [17.73s]: training loss=0.3549126386642456  validation ndcg@10=0.06297240979818237 [0.2s]\n",
      "epoch 11 [17.67s]:  training loss=0.3480478525161743                                 \n",
      "epoch 12 [17.62s]:  training loss=0.3562137186527252                                 \n",
      "epoch 13 [19.08s]:  training loss=0.3470078706741333                                 \n",
      "epoch 14 [18.03s]:  training loss=0.34405916929244995                                \n",
      "epoch 15 [18.79s]: training loss=0.34522750973701477  validation ndcg@10=0.05144767964004486 [0.2s]\n",
      "epoch 16 [18.47s]:  training loss=0.33630943298339844                                \n",
      "epoch 17 [17.93s]:  training loss=0.341993510723114                                  \n",
      "epoch 18 [17.39s]:  training loss=0.33753713965415955                                \n",
      "epoch 19 [17.8s]:  training loss=0.3325856328010559                                  \n",
      "epoch 20 [17.84s]: training loss=0.32923704385757446  validation ndcg@10=0.0622990758699977 [0.22s]\n",
      "epoch 21 [17.73s]:  training loss=0.334124892950058                                  \n",
      "epoch 22 [17.65s]:  training loss=0.3286479711532593                                 \n",
      "epoch 23 [17.73s]:  training loss=0.3270336091518402                                 \n",
      "epoch 24 [17.81s]:  training loss=0.31859567761421204                                \n",
      "epoch 25 [19.62s]: training loss=0.32068321108818054  validation ndcg@10=0.06078585917197082 [0.19s]\n",
      "epoch 26 [17.36s]:  training loss=0.3184967339038849                                 \n",
      "epoch 27 [17.87s]:  training loss=0.31761983036994934                                \n",
      "epoch 28 [17.52s]:  training loss=0.31842517852783203                                \n",
      "epoch 29 [17.98s]:  training loss=0.32088208198547363                                \n",
      "epoch 30 [17.36s]: training loss=0.31437283754348755  validation ndcg@10=0.06631593473550691 [0.21s]\n",
      "epoch 1 [10.98s]:  training loss=9.761621475219727                                   \n",
      "epoch 2 [10.37s]:  training loss=18.841264724731445                                  \n",
      "epoch 3 [10.17s]:  training loss=23.836536407470703                                  \n",
      "epoch 4 [9.84s]:  training loss=26.1437931060791                                     \n",
      "epoch 5 [9.86s]: training loss=28.319765090942383  validation ndcg@10=0.051896678394405274 [0.12s]\n",
      "epoch 6 [9.82s]:  training loss=29.42890167236328                                    \n",
      "epoch 7 [9.86s]:  training loss=30.25745391845703                                    \n",
      "epoch 8 [9.93s]:  training loss=32.214820861816406                                   \n",
      "epoch 9 [9.86s]:  training loss=33.5268669128418                                     \n",
      "epoch 10 [9.86s]: training loss=34.26191329956055  validation ndcg@10=0.042438448320173024 [0.12s]\n",
      "epoch 11 [10.35s]:  training loss=35.19916534423828                                  \n",
      "epoch 12 [10.02s]:  training loss=35.55189514160156                                  \n",
      "epoch 13 [9.82s]:  training loss=36.10744857788086                                   \n",
      "epoch 14 [9.92s]:  training loss=36.0775260925293                                    \n",
      "epoch 15 [9.82s]: training loss=35.52548599243164  validation ndcg@10=0.04707911025803098 [0.12s]\n",
      "epoch 16 [9.95s]:  training loss=37.1198616027832                                    \n",
      "epoch 17 [9.83s]:  training loss=37.6906623840332                                    \n",
      "epoch 18 [9.75s]:  training loss=38.04388427734375                                   \n",
      "epoch 19 [9.85s]:  training loss=37.56673049926758                                   \n",
      "epoch 20 [9.97s]: training loss=37.39373016357422  validation ndcg@10=0.03989381518561363 [0.11s]\n",
      "epoch 21 [9.89s]:  training loss=38.528133392333984                                  \n",
      "epoch 22 [10.1s]:  training loss=38.79039764404297                                   \n",
      "epoch 23 [10.0s]:  training loss=38.38198471069336                                   \n",
      "epoch 24 [10.0s]:  training loss=39.49109649658203                                   \n",
      "epoch 25 [10.14s]: training loss=38.4692268371582  validation ndcg@10=0.0485963579596984 [0.14s]\n",
      "epoch 26 [9.9s]:  training loss=39.996822357177734                                   \n",
      "epoch 27 [9.94s]:  training loss=40.37459182739258                                   \n",
      "epoch 28 [9.97s]:  training loss=39.927764892578125                                  \n",
      "epoch 29 [9.94s]:  training loss=40.932212829589844                                  \n",
      "epoch 30 [9.91s]: training loss=40.077476501464844  validation ndcg@10=0.04409991291562803 [0.11s]\n",
      "epoch 1 [36.77s]:  training loss=1.2708793878555298                                  \n",
      "epoch 2 [39.16s]:  training loss=2.032485008239746                                   \n",
      "epoch 3 [39.27s]:  training loss=2.35784912109375                                    \n",
      "epoch 4 [38.2s]:  training loss=2.621333360671997                                    \n",
      "epoch 5 [39.22s]: training loss=2.7415313720703125  validation ndcg@10=0.045179339854507096 [0.53s]\n",
      "epoch 6 [38.73s]:  training loss=2.902193307876587                                   \n",
      "epoch 7 [38.27s]:  training loss=2.9972264766693115                                  \n",
      "epoch 8 [39.66s]:  training loss=3.131187677383423                                   \n",
      "epoch 9 [39.07s]:  training loss=3.1363563537597656                                  \n",
      "epoch 10 [39.55s]: training loss=3.358309745788574  validation ndcg@10=0.031906535574748617 [0.49s]\n",
      "epoch 11 [38.41s]:  training loss=3.3787996768951416                                 \n",
      "epoch 12 [38.05s]:  training loss=3.3473145961761475                                 \n",
      "epoch 13 [40.4s]:  training loss=3.555697202682495                                   \n",
      "epoch 14 [38.02s]:  training loss=3.409648895263672                                  \n",
      "epoch 15 [38.5s]: training loss=3.5494720935821533  validation ndcg@10=0.037194641331680926 [0.52s]\n",
      "epoch 16 [37.6s]:  training loss=3.5519838333129883                                  \n",
      "epoch 17 [38.8s]:  training loss=3.583146572113037                                     \n",
      "epoch 18 [38.27s]:  training loss=3.713123083114624                                    \n",
      "epoch 19 [38.99s]:  training loss=3.8430256843566895                                   \n",
      "epoch 20 [38.85s]: training loss=3.6544582843780518  validation ndcg@10=0.04025329639981651 [0.5s]\n",
      "epoch 21 [38.26s]:  training loss=3.782022714614868                                    \n",
      "epoch 22 [39.08s]:  training loss=3.922064781188965                                    \n",
      "epoch 23 [38.85s]:  training loss=3.988898992538452                                    \n",
      "epoch 24 [39.59s]:  training loss=3.91853928565979                                     \n",
      "epoch 25 [39.28s]: training loss=3.8264002799987793  validation ndcg@10=0.03780489541982394 [0.49s]\n",
      "epoch 26 [37.94s]:  training loss=4.033296585083008                                    \n",
      "epoch 27 [38.11s]:  training loss=4.078707695007324                                    \n",
      "epoch 28 [38.68s]:  training loss=3.9580655097961426                                   \n",
      "epoch 29 [38.87s]:  training loss=4.076015949249268                                    \n",
      "epoch 30 [39.11s]: training loss=4.064552307128906  validation ndcg@10=0.03471856760312807 [0.47s]\n",
      "epoch 1 [14.32s]:  training loss=0.8791268467903137                                    \n",
      "epoch 2 [14.95s]:  training loss=0.674714982509613                                     \n",
      "epoch 3 [14.88s]:  training loss=0.5618631839752197                                    \n",
      "epoch 4 [13.89s]:  training loss=0.4842349886894226                                    \n",
      "epoch 5 [13.88s]: training loss=0.45439577102661133  validation ndcg@10=0.05714526982895536 [0.22s]\n",
      "epoch 6 [15.0s]:  training loss=0.43635356426239014                                    \n",
      "epoch 7 [14.71s]:  training loss=0.4303441345691681                                    \n",
      "epoch 8 [14.41s]:  training loss=0.43169859051704407                                   \n",
      "epoch 9 [14.62s]:  training loss=0.4253259003162384                                    \n",
      "epoch 10 [14.32s]: training loss=0.41517361998558044  validation ndcg@10=0.06289109135597934 [0.21s]\n",
      "epoch 11 [14.21s]:  training loss=0.4184632897377014                                   \n",
      "epoch 12 [13.89s]:  training loss=0.41108566522598267                                  \n",
      "epoch 13 [14.21s]:  training loss=0.41081345081329346                                  \n",
      "epoch 14 [16.29s]:  training loss=0.40636762976646423                                  \n",
      "epoch 15 [14.12s]: training loss=0.4035801887512207  validation ndcg@10=0.06048441807026037 [0.2s]\n",
      "epoch 16 [14.23s]:  training loss=0.399848073720932                                    \n",
      "epoch 17 [14.26s]:  training loss=0.40363311767578125                                  \n",
      "epoch 18 [13.96s]:  training loss=0.39279428124427795                                  \n",
      "epoch 19 [14.82s]:  training loss=0.3972884714603424                                   \n",
      "epoch 20 [14.82s]: training loss=0.3917081952095032  validation ndcg@10=0.07025472845122036 [0.21s]\n",
      "epoch 21 [13.96s]:  training loss=0.3888721168041229                                   \n",
      "epoch 22 [14.28s]:  training loss=0.38225558400154114                                  \n",
      "epoch 23 [15.12s]:  training loss=0.38359519839286804                                  \n",
      "epoch 24 [14.61s]:  training loss=0.3854871094226837                                   \n",
      "epoch 25 [14.21s]: training loss=0.37639957666397095  validation ndcg@10=0.07288401950949544 [0.2s]\n",
      "epoch 26 [14.38s]:  training loss=0.3702022135257721                                   \n",
      "epoch 27 [14.61s]:  training loss=0.3703044056892395                                   \n",
      "epoch 28 [15.13s]:  training loss=0.3714871406555176                                   \n",
      "epoch 29 [14.54s]:  training loss=0.36932897567749023                                  \n",
      "epoch 30 [14.41s]: training loss=0.36271393299102783  validation ndcg@10=0.07681636221621348 [0.22s]\n",
      "epoch 31 [14.27s]:  training loss=0.3638205826282501                                   \n",
      "epoch 32 [14.3s]:  training loss=0.35780271887779236                                   \n",
      "epoch 33 [14.47s]:  training loss=0.36385825276374817                                  \n",
      "epoch 34 [14.6s]:  training loss=0.35970038175582886                                   \n",
      "epoch 35 [14.07s]: training loss=0.35328933596611023  validation ndcg@10=0.07778635253106055 [0.24s]\n",
      "epoch 36 [14.46s]:  training loss=0.3542509078979492                                   \n",
      "epoch 37 [15.08s]:  training loss=0.35417449474334717                                  \n",
      "epoch 38 [14.37s]:  training loss=0.35366299748420715                                  \n",
      "epoch 39 [14.05s]:  training loss=0.35032105445861816                                  \n",
      "epoch 40 [14.58s]: training loss=0.3471774458885193  validation ndcg@10=0.06766972749002169 [0.22s]\n",
      "epoch 41 [14.18s]:  training loss=0.3508508503437042                                   \n",
      "epoch 42 [13.98s]:  training loss=0.3547721207141876                                   \n",
      "epoch 43 [13.99s]:  training loss=0.34533846378326416                                  \n",
      "epoch 44 [14.11s]:  training loss=0.3460630774497986                                   \n",
      "epoch 45 [14.78s]: training loss=0.3441137969493866  validation ndcg@10=0.07591026440999646 [0.22s]\n",
      "epoch 46 [14.67s]:  training loss=0.3446774482727051                                   \n",
      "epoch 47 [14.42s]:  training loss=0.33805131912231445                                  \n",
      "epoch 48 [14.78s]:  training loss=0.33843615651130676                                  \n",
      "epoch 49 [14.29s]:  training loss=0.34431877732276917                                  \n",
      "epoch 50 [14.45s]: training loss=0.34266966581344604  validation ndcg@10=0.07780522798569837 [0.22s]\n",
      "epoch 51 [14.3s]:  training loss=0.34104886651039124                                   \n",
      "epoch 52 [13.81s]:  training loss=0.3379937410354614                                   \n",
      "epoch 53 [14.35s]:  training loss=0.33596092462539673                                  \n",
      "epoch 54 [14.4s]:  training loss=0.3434769809246063                                    \n",
      "epoch 55 [14.2s]: training loss=0.3374745547771454  validation ndcg@10=0.0677868016723408 [0.22s]\n",
      "epoch 56 [14.18s]:  training loss=0.3385758697986603                                   \n",
      "epoch 57 [14.94s]:  training loss=0.3336069583892822                                   \n",
      "epoch 58 [15.69s]:  training loss=0.32907941937446594                                  \n",
      "epoch 59 [14.39s]:  training loss=0.32841476798057556                                  \n",
      "epoch 60 [15.3s]: training loss=0.32660582661628723  validation ndcg@10=0.07037199426909709 [0.2s]\n",
      "epoch 61 [14.47s]:  training loss=0.3337264657020569                                   \n",
      "epoch 62 [14.16s]:  training loss=0.325959175825119                                    \n",
      "epoch 63 [15.06s]:  training loss=0.3269491195678711                                   \n",
      "epoch 64 [15.28s]:  training loss=0.3277003765106201                                   \n",
      "epoch 65 [14.29s]: training loss=0.3314937353134155  validation ndcg@10=0.07607731103610905 [0.22s]\n",
      "epoch 66 [14.75s]:  training loss=0.3284232020378113                                   \n",
      "epoch 67 [14.32s]:  training loss=0.32601606845855713                                  \n",
      "epoch 68 [14.75s]:  training loss=0.32875287532806396                                  \n",
      "epoch 69 [14.57s]:  training loss=0.3312579095363617                                   \n",
      "epoch 70 [14.1s]: training loss=0.32229048013687134  validation ndcg@10=0.07471862757276873 [0.25s]\n",
      "epoch 71 [14.99s]:  training loss=0.3263980746269226                                   \n",
      "epoch 72 [15.78s]:  training loss=0.3311261832714081                                   \n",
      "epoch 73 [14.08s]:  training loss=0.32039541006088257                                  \n",
      "epoch 74 [14.45s]:  training loss=0.3224496841430664                                   \n",
      "epoch 75 [14.83s]: training loss=0.3222200870513916  validation ndcg@10=0.06968199560231642 [0.24s]\n",
      "epoch 1 [10.33s]:  training loss=1.0081958770751953                                    \n",
      "epoch 2 [10.06s]:  training loss=0.8840462565422058                                    \n",
      "epoch 3 [8.97s]:  training loss=0.8207868337631226                                     \n",
      "epoch 4 [8.75s]:  training loss=0.7711784243583679                                     \n",
      "epoch 5 [8.96s]: training loss=0.7378057241439819  validation ndcg@10=0.033251576259836005 [0.1s]\n",
      "epoch 6 [8.75s]:  training loss=0.699627697467804                                      \n",
      "epoch 7 [8.73s]:  training loss=0.6575753688812256                                     \n",
      "epoch 8 [8.81s]:  training loss=0.6347830295562744                                     \n",
      "epoch 9 [9.18s]:  training loss=0.6113707423210144                                     \n",
      "epoch 10 [9.05s]: training loss=0.5863028168678284  validation ndcg@10=0.04011474069367921 [0.12s]\n",
      "epoch 11 [8.92s]:  training loss=0.5597362518310547                                    \n",
      "epoch 12 [8.88s]:  training loss=0.5385482907295227                                    \n",
      "epoch 13 [8.86s]:  training loss=0.5253172516822815                                    \n",
      "epoch 14 [8.92s]:  training loss=0.5099608302116394                                    \n",
      "epoch 15 [8.91s]: training loss=0.5019358992576599  validation ndcg@10=0.050964530757511124 [0.12s]\n",
      "epoch 16 [8.79s]:  training loss=0.4848932921886444                                    \n",
      "epoch 17 [8.97s]:  training loss=0.47440388798713684                                   \n",
      "epoch 18 [8.85s]:  training loss=0.4607745409011841                                    \n",
      "epoch 19 [8.92s]:  training loss=0.45917853713035583                                   \n",
      "epoch 20 [8.68s]: training loss=0.4532597064971924  validation ndcg@10=0.04289454656032177 [0.13s]\n",
      "epoch 21 [8.9s]:  training loss=0.45002123713493347                                    \n",
      "epoch 22 [9.12s]:  training loss=0.45053258538246155                                   \n",
      "epoch 23 [9.22s]:  training loss=0.43927037715911865                                   \n",
      "epoch 24 [9.01s]:  training loss=0.4372892677783966                                    \n",
      "epoch 25 [8.85s]: training loss=0.4319295883178711  validation ndcg@10=0.0515322970262205 [0.1s]\n",
      "epoch 26 [8.97s]:  training loss=0.4337448179721832                                    \n",
      "epoch 27 [8.82s]:  training loss=0.4291735887527466                                    \n",
      "epoch 28 [8.89s]:  training loss=0.43072909116744995                                   \n",
      "epoch 29 [8.98s]:  training loss=0.4292462170124054                                    \n",
      "epoch 30 [8.93s]: training loss=0.4274473786354065  validation ndcg@10=0.0492459543148596 [0.13s]\n",
      "epoch 31 [9.1s]:  training loss=0.4239598512649536                                     \n",
      "epoch 32 [9.44s]:  training loss=0.42137575149536133                                   \n",
      "epoch 33 [9.24s]:  training loss=0.42600300908088684                                   \n",
      "epoch 34 [8.94s]:  training loss=0.4172731637954712                                    \n",
      "epoch 35 [9.22s]: training loss=0.4164554476737976  validation ndcg@10=0.05081984767146257 [0.11s]\n",
      "epoch 36 [9.04s]:  training loss=0.4131872057914734                                    \n",
      "epoch 37 [8.94s]:  training loss=0.4175296425819397                                    \n",
      "epoch 38 [9.07s]:  training loss=0.41065624356269836                                   \n",
      "epoch 39 [8.94s]:  training loss=0.41250258684158325                                   \n",
      "epoch 40 [9.02s]: training loss=0.41342949867248535  validation ndcg@10=0.05412414714864828 [0.12s]\n",
      "epoch 41 [9.03s]:  training loss=0.40996095538139343                                   \n",
      "epoch 42 [9.08s]:  training loss=0.4139610230922699                                    \n",
      "epoch 43 [9.02s]:  training loss=0.406261682510376                                     \n",
      "epoch 44 [8.85s]:  training loss=0.4093819260597229                                    \n",
      "epoch 45 [9.0s]: training loss=0.4048020541667938  validation ndcg@10=0.0526910021372306 [0.11s]\n",
      "epoch 46 [8.92s]:  training loss=0.40690869092941284                                   \n",
      "epoch 47 [9.02s]:  training loss=0.40962693095207214                                   \n",
      "epoch 48 [8.91s]:  training loss=0.4126567244529724                                    \n",
      "epoch 49 [9.07s]:  training loss=0.40471941232681274                                   \n",
      "epoch 50 [9.01s]: training loss=0.4082918167114258  validation ndcg@10=0.052144406520558656 [0.11s]\n",
      "epoch 51 [9.16s]:  training loss=0.4051492214202881                                    \n",
      "epoch 52 [8.98s]:  training loss=0.4033299684524536                                    \n",
      "epoch 53 [8.89s]:  training loss=0.4043941795825958                                    \n",
      "epoch 54 [8.9s]:  training loss=0.40201473236083984                                    \n",
      "epoch 55 [8.95s]: training loss=0.4033176004886627  validation ndcg@10=0.06146666794296139 [0.11s]\n",
      "epoch 56 [8.96s]:  training loss=0.4051152765750885                                    \n",
      "epoch 57 [9.0s]:  training loss=0.39745086431503296                                    \n",
      "epoch 58 [8.96s]:  training loss=0.40031200647354126                                   \n",
      "epoch 59 [8.88s]:  training loss=0.3987044394016266                                    \n",
      "epoch 60 [8.81s]: training loss=0.39509081840515137  validation ndcg@10=0.06240833477619776 [0.11s]\n",
      "epoch 61 [9.16s]:  training loss=0.40001699328422546                                   \n",
      "epoch 62 [8.99s]:  training loss=0.40066471695899963                                   \n",
      "epoch 63 [8.94s]:  training loss=0.39675429463386536                                   \n",
      "epoch 64 [8.88s]:  training loss=0.3981650769710541                                    \n",
      "epoch 65 [8.89s]: training loss=0.3933057487010956  validation ndcg@10=0.07076238761326431 [0.14s]\n",
      "epoch 66 [9.0s]:  training loss=0.39737334847450256                                    \n",
      "epoch 67 [8.86s]:  training loss=0.39426860213279724                                   \n",
      "epoch 68 [8.91s]:  training loss=0.3969283699989319                                    \n",
      "epoch 69 [8.89s]:  training loss=0.396097332239151                                     \n",
      "epoch 70 [9.2s]: training loss=0.3866526484489441  validation ndcg@10=0.06030691465845523 [0.11s]\n",
      "epoch 71 [9.17s]:  training loss=0.39643436670303345                                   \n",
      "epoch 72 [9.01s]:  training loss=0.3847048580646515                                    \n",
      "epoch 73 [9.17s]:  training loss=0.38421544432640076                                   \n",
      "epoch 74 [8.84s]:  training loss=0.3859289288520813                                    \n",
      "epoch 75 [9.02s]: training loss=0.3882156312465668  validation ndcg@10=0.06804133750681236 [0.11s]\n",
      "epoch 76 [9.2s]:  training loss=0.38476356863975525                                    \n",
      "epoch 77 [9.14s]:  training loss=0.3922039270401001                                    \n",
      "epoch 78 [9.14s]:  training loss=0.38264039158821106                                   \n",
      "epoch 79 [9.0s]:  training loss=0.38356417417526245                                    \n",
      "epoch 80 [9.12s]: training loss=0.3901282250881195  validation ndcg@10=0.07163996689616565 [0.13s]\n",
      "epoch 81 [8.83s]:  training loss=0.3845275044441223                                    \n",
      "epoch 82 [8.94s]:  training loss=0.3832499384880066                                    \n",
      "epoch 83 [8.91s]:  training loss=0.37927061319351196                                   \n",
      "epoch 84 [8.94s]:  training loss=0.3775470554828644                                    \n",
      "epoch 85 [8.94s]: training loss=0.3770303428173065  validation ndcg@10=0.07314235143348669 [0.14s]\n",
      "epoch 86 [8.93s]:  training loss=0.38992202281951904                                   \n",
      "epoch 87 [8.89s]:  training loss=0.38811707496643066                                   \n",
      "epoch 88 [8.84s]:  training loss=0.3814815282821655                                    \n",
      "epoch 89 [8.84s]:  training loss=0.37748005986213684                                   \n",
      "epoch 90 [9.02s]: training loss=0.3781662583351135  validation ndcg@10=0.059963486415669655 [0.11s]\n",
      "epoch 91 [8.99s]:  training loss=0.37488308548927307                                   \n",
      "epoch 92 [9.23s]:  training loss=0.37419793009757996                                   \n",
      "epoch 93 [9.22s]:  training loss=0.3760372996330261                                    \n",
      "epoch 94 [9.18s]:  training loss=0.3782428801059723                                    \n",
      "epoch 95 [10.45s]: training loss=0.3847845792770386  validation ndcg@10=0.07684243293254701 [0.11s]\n",
      "epoch 96 [9.03s]:  training loss=0.37683022022247314                                   \n",
      "epoch 97 [8.88s]:  training loss=0.3696304261684418                                    \n",
      "epoch 98 [8.93s]:  training loss=0.3690383732318878                                    \n",
      "epoch 99 [8.98s]:  training loss=0.3759450316429138                                    \n",
      "epoch 100 [8.79s]: training loss=0.3659100830554962  validation ndcg@10=0.07000176603042256 [0.12s]\n",
      "epoch 101 [8.83s]:  training loss=0.3733949363231659                                   \n",
      "epoch 102 [8.92s]:  training loss=0.37304943799972534                                  \n",
      "epoch 103 [8.74s]:  training loss=0.36883237957954407                                  \n",
      "epoch 104 [9.09s]:  training loss=0.3748798370361328                                   \n",
      "epoch 105 [8.92s]: training loss=0.3740151524543762  validation ndcg@10=0.07670752018470216 [0.12s]\n",
      "epoch 106 [8.88s]:  training loss=0.36517810821533203                                  \n",
      "epoch 107 [8.94s]:  training loss=0.3728067874908447                                   \n",
      "epoch 108 [9.08s]:  training loss=0.370726078748703                                    \n",
      "epoch 109 [9.0s]:  training loss=0.3636537492275238                                    \n",
      "epoch 110 [8.96s]: training loss=0.3680223822593689  validation ndcg@10=0.08105353215049194 [0.13s]\n",
      "epoch 111 [8.85s]:  training loss=0.3651576340198517                                   \n",
      "epoch 112 [8.84s]:  training loss=0.36719420552253723                                  \n",
      "epoch 113 [8.93s]:  training loss=0.36060523986816406                                  \n",
      "epoch 114 [8.84s]:  training loss=0.358272522687912                                    \n",
      "epoch 115 [9.08s]: training loss=0.35906732082366943  validation ndcg@10=0.07032587653651665 [0.11s]\n",
      "epoch 116 [9.04s]:  training loss=0.35522177815437317                                  \n",
      "epoch 117 [9.0s]:  training loss=0.3585738241672516                                    \n",
      "epoch 118 [9.0s]:  training loss=0.3640134930610657                                    \n",
      "epoch 119 [8.9s]:  training loss=0.3579099178314209                                    \n",
      "epoch 120 [8.82s]: training loss=0.35886305570602417  validation ndcg@10=0.07814643728757502 [0.11s]\n",
      "epoch 121 [8.93s]:  training loss=0.3619594871997833                                   \n",
      "epoch 122 [8.94s]:  training loss=0.3621256351470947                                   \n",
      "epoch 123 [8.89s]:  training loss=0.35971972346305847                                  \n",
      "epoch 124 [8.78s]:  training loss=0.35343506932258606                                  \n",
      "epoch 125 [9.21s]: training loss=0.35992443561553955  validation ndcg@10=0.07965581794692098 [0.12s]\n",
      "epoch 126 [9.02s]:  training loss=0.35687071084976196                                  \n",
      "epoch 127 [9.15s]:  training loss=0.3572331666946411                                   \n",
      "epoch 128 [8.98s]:  training loss=0.35970187187194824                                  \n",
      "epoch 129 [9.04s]:  training loss=0.35759079456329346                                  \n",
      "epoch 130 [8.91s]: training loss=0.35685494542121887  validation ndcg@10=0.07597176746614534 [0.13s]\n",
      "epoch 131 [8.93s]:  training loss=0.35737040638923645                                  \n",
      "epoch 132 [8.96s]:  training loss=0.3563767373561859                                   \n",
      "epoch 133 [8.99s]:  training loss=0.3540775775909424                                   \n",
      "epoch 134 [9.06s]:  training loss=0.3536742329597473                                   \n",
      "epoch 135 [8.95s]: training loss=0.3573462665081024  validation ndcg@10=0.07802081385590391 [0.12s]\n",
      "epoch 1 [9.58s]:  training loss=1.0480198860168457                                       \n",
      "epoch 2 [9.59s]:  training loss=1.0150066614151                                          \n",
      "epoch 3 [9.31s]:  training loss=0.9827114939689636                                       \n",
      "epoch 4 [9.1s]:  training loss=0.9568715691566467                                        \n",
      "epoch 5 [9.3s]: training loss=0.9324296116828918  validation ndcg@10=0.02891171549800431 [0.17s]\n",
      "epoch 6 [9.19s]:  training loss=0.9149526357650757                                       \n",
      "epoch 7 [9.17s]:  training loss=0.8947163820266724                                       \n",
      "epoch 8 [9.13s]:  training loss=0.8818823099136353                                       \n",
      "epoch 9 [9.33s]:  training loss=0.8674646615982056                                       \n",
      "epoch 10 [9.22s]: training loss=0.8459272384643555  validation ndcg@10=0.031078422735864145 [0.15s]\n",
      "epoch 11 [9.02s]:  training loss=0.8330449461936951                                      \n",
      "epoch 12 [9.39s]:  training loss=0.8262355327606201                                      \n",
      "epoch 13 [9.18s]:  training loss=0.8068377375602722                                      \n",
      "epoch 14 [9.3s]:  training loss=0.8022539019584656                                       \n",
      "epoch 15 [9.2s]: training loss=0.7862105965614319  validation ndcg@10=0.03574708056992843 [0.15s]\n",
      "epoch 16 [9.16s]:  training loss=0.7682336568832397                                      \n",
      "epoch 17 [9.51s]:  training loss=0.7726832628250122                                      \n",
      "epoch 18 [9.58s]:  training loss=0.7603780031204224                                      \n",
      "epoch 19 [9.37s]:  training loss=0.7449811100959778                                      \n",
      "epoch 20 [9.31s]: training loss=0.7472913861274719  validation ndcg@10=0.036559938766479845 [0.13s]\n",
      "epoch 21 [9.37s]:  training loss=0.7287977933883667                                      \n",
      "epoch 22 [9.12s]:  training loss=0.7209386825561523                                      \n",
      "epoch 23 [9.34s]:  training loss=0.7112517952919006                                      \n",
      "epoch 24 [9.23s]:  training loss=0.7021124362945557                                      \n",
      "epoch 25 [9.13s]: training loss=0.6984308362007141  validation ndcg@10=0.03798264504382479 [0.15s]\n",
      "epoch 26 [9.45s]:  training loss=0.6836762428283691                                      \n",
      "epoch 27 [9.17s]:  training loss=0.6841839551925659                                      \n",
      "epoch 28 [9.31s]:  training loss=0.67427659034729                                        \n",
      "epoch 29 [9.29s]:  training loss=0.6622673273086548                                      \n",
      "epoch 30 [9.26s]: training loss=0.6601285934448242  validation ndcg@10=0.036993030745252904 [0.13s]\n",
      "epoch 31 [9.1s]:  training loss=0.6470879316329956                                       \n",
      "epoch 32 [9.33s]:  training loss=0.6520769596099854                                      \n",
      "epoch 33 [9.2s]:  training loss=0.6403590440750122                                       \n",
      "epoch 34 [9.36s]:  training loss=0.632144570350647                                       \n",
      "epoch 35 [9.18s]: training loss=0.628828227519989  validation ndcg@10=0.04122712739948253 [0.14s]\n",
      "epoch 36 [9.13s]:  training loss=0.6145707368850708                                      \n",
      "epoch 37 [9.18s]:  training loss=0.6063708662986755                                      \n",
      "epoch 38 [9.24s]:  training loss=0.6059085130691528                                      \n",
      "epoch 39 [9.34s]:  training loss=0.5978831052780151                                      \n",
      "epoch 40 [9.23s]: training loss=0.5921310186386108  validation ndcg@10=0.042470889920123774 [0.13s]\n",
      "epoch 41 [9.2s]:  training loss=0.5777708888053894                                       \n",
      "epoch 42 [9.2s]:  training loss=0.5866090655326843                                       \n",
      "epoch 43 [9.02s]:  training loss=0.5766216516494751                                      \n",
      "epoch 44 [9.1s]:  training loss=0.5702847838401794                                       \n",
      "epoch 45 [9.25s]: training loss=0.5620339512825012  validation ndcg@10=0.04154001173325372 [0.13s]\n",
      "epoch 46 [9.2s]:  training loss=0.5661308169364929                                       \n",
      "epoch 47 [9.24s]:  training loss=0.5589343905448914                                      \n",
      "epoch 48 [9.31s]:  training loss=0.5510773062705994                                      \n",
      "epoch 49 [9.28s]:  training loss=0.5504530668258667                                      \n",
      "epoch 50 [9.27s]: training loss=0.5454580783843994  validation ndcg@10=0.047958826153219614 [0.15s]\n",
      "epoch 51 [9.16s]:  training loss=0.5401875376701355                                      \n",
      "epoch 52 [9.29s]:  training loss=0.5423180460929871                                      \n",
      "epoch 53 [9.2s]:  training loss=0.5303924679756165                                       \n",
      "epoch 54 [9.13s]:  training loss=0.5203500390052795                                      \n",
      "epoch 55 [9.28s]: training loss=0.5213402509689331  validation ndcg@10=0.047491040194625 [0.15s]\n",
      "epoch 56 [9.35s]:  training loss=0.5152310729026794                                      \n",
      "epoch 57 [9.04s]:  training loss=0.5101243257522583                                      \n",
      "epoch 58 [9.14s]:  training loss=0.5052539110183716                                      \n",
      "epoch 59 [11.06s]:  training loss=0.5100749731063843                                     \n",
      "epoch 60 [8.94s]: training loss=0.5001751780509949  validation ndcg@10=0.04734046113235213 [0.14s]\n",
      "epoch 61 [9.3s]:  training loss=0.5020749568939209                                       \n",
      "epoch 62 [9.35s]:  training loss=0.49351152777671814                                     \n",
      "epoch 63 [9.18s]:  training loss=0.48369118571281433                                     \n",
      "epoch 64 [9.15s]:  training loss=0.4942013621330261                                      \n",
      "epoch 65 [9.21s]: training loss=0.4861748218536377  validation ndcg@10=0.05058702541950289 [0.14s]\n",
      "epoch 66 [9.39s]:  training loss=0.4834013283252716                                      \n",
      "epoch 67 [9.23s]:  training loss=0.48989033699035645                                     \n",
      "epoch 68 [9.24s]:  training loss=0.4808935821056366                                      \n",
      "epoch 69 [9.2s]:  training loss=0.48722556233406067                                      \n",
      "epoch 70 [9.18s]: training loss=0.47662055492401123  validation ndcg@10=0.053650975048384626 [0.14s]\n",
      "epoch 71 [9.18s]:  training loss=0.47536560893058777                                     \n",
      "epoch 72 [9.17s]:  training loss=0.46847715973854065                                     \n",
      "epoch 73 [9.36s]:  training loss=0.46598824858665466                                     \n",
      "epoch 74 [9.18s]:  training loss=0.47486403584480286                                     \n",
      "epoch 75 [9.56s]: training loss=0.4683992564678192  validation ndcg@10=0.05304379064128805 [0.18s]\n",
      "epoch 76 [9.45s]:  training loss=0.4646497666835785                                      \n",
      "epoch 77 [9.32s]:  training loss=0.46267640590667725                                     \n",
      "epoch 78 [9.12s]:  training loss=0.4639835059642792                                      \n",
      "epoch 79 [9.14s]:  training loss=0.456596314907074                                       \n",
      "epoch 80 [9.35s]: training loss=0.4564203917980194  validation ndcg@10=0.05261735128501196 [0.17s]\n",
      "epoch 81 [9.24s]:  training loss=0.4605579078197479                                      \n",
      "epoch 82 [9.24s]:  training loss=0.4589575529098511                                      \n",
      "epoch 83 [9.13s]:  training loss=0.45925772190093994                                     \n",
      "epoch 84 [9.36s]:  training loss=0.4571791887283325                                      \n",
      "epoch 85 [9.14s]: training loss=0.4495394825935364  validation ndcg@10=0.05061579678301247 [0.14s]\n",
      "epoch 86 [9.06s]:  training loss=0.45331189036369324                                     \n",
      "epoch 87 [9.16s]:  training loss=0.45017141103744507                                     \n",
      "epoch 88 [9.39s]:  training loss=0.44742467999458313                                     \n",
      "epoch 89 [9.22s]:  training loss=0.4478738009929657                                      \n",
      "epoch 90 [9.14s]: training loss=0.45271825790405273  validation ndcg@10=0.05124677704404648 [0.15s]\n",
      "epoch 91 [9.32s]:  training loss=0.4410099387168884                                      \n",
      "epoch 92 [9.18s]:  training loss=0.45007026195526123                                     \n",
      "epoch 93 [9.08s]:  training loss=0.4423932135105133                                      \n",
      "epoch 94 [9.11s]:  training loss=0.4437220096588135                                      \n",
      "epoch 95 [9.19s]: training loss=0.4404214918613434  validation ndcg@10=0.05268800464466789 [0.16s]\n",
      "epoch 1 [11.15s]:  training loss=0.5471136569976807                                      \n",
      "epoch 2 [11.56s]:  training loss=0.44512757658958435                                    \n",
      "epoch 3 [12.1s]:  training loss=0.422160267829895                                       \n",
      "epoch 4 [11.83s]:  training loss=0.4073215126991272                                     \n",
      "epoch 5 [11.97s]: training loss=0.4022722542285919  validation ndcg@10=0.05485639809728149 [0.17s]\n",
      "epoch 6 [11.74s]:  training loss=0.3956649601459503                                     \n",
      "epoch 7 [11.75s]:  training loss=0.39773133397102356                                    \n",
      "epoch 8 [11.47s]:  training loss=0.3961142301559448                                     \n",
      "epoch 9 [12.06s]:  training loss=0.39666518568992615                                    \n",
      "epoch 10 [12.36s]: training loss=0.3942968547344208  validation ndcg@10=0.053701127385712207 [0.18s]\n",
      "epoch 11 [12.4s]:  training loss=0.3888799846172333                                     \n",
      "epoch 12 [12.12s]:  training loss=0.3865499794483185                                    \n",
      "epoch 13 [12.03s]:  training loss=0.38341841101646423                                   \n",
      "epoch 14 [11.56s]:  training loss=0.39651232957839966                                   \n",
      "epoch 15 [11.72s]: training loss=0.39771559834480286  validation ndcg@10=0.04923175208407299 [0.17s]\n",
      "epoch 16 [11.87s]:  training loss=0.3869032561779022                                    \n",
      "epoch 17 [11.79s]:  training loss=0.38429662585258484                                   \n",
      "epoch 18 [11.66s]:  training loss=0.38545119762420654                                   \n",
      "epoch 19 [11.6s]:  training loss=0.3840245008468628                                     \n",
      "epoch 20 [11.55s]: training loss=0.3847660422325134  validation ndcg@10=0.048429722555789215 [0.15s]\n",
      "epoch 21 [12.09s]:  training loss=0.3900560140609741                                    \n",
      "epoch 22 [12.16s]:  training loss=0.38254109025001526                                   \n",
      "epoch 23 [11.74s]:  training loss=0.3785402476787567                                    \n",
      "epoch 24 [11.61s]:  training loss=0.38545459508895874                                   \n",
      "epoch 25 [11.86s]: training loss=0.3862479031085968  validation ndcg@10=0.048854602395843924 [0.16s]\n",
      "epoch 26 [11.48s]:  training loss=0.3889518082141876                                    \n",
      "epoch 27 [12.54s]:  training loss=0.39181315898895264                                   \n",
      "epoch 28 [11.77s]:  training loss=0.3916264474391937                                    \n",
      "epoch 29 [11.78s]:  training loss=0.38623175024986267                                   \n",
      "epoch 30 [11.58s]: training loss=0.389397531747818  validation ndcg@10=0.05059298149546516 [0.16s]\n",
      "epoch 1 [19.83s]:  training loss=0.9042631983757019                                     \n",
      "epoch 2 [22.94s]:  training loss=0.7504609227180481                                     \n",
      "epoch 3 [21.66s]:  training loss=0.6460744142532349                                     \n",
      "epoch 4 [21.88s]:  training loss=0.5765038728713989                                     \n",
      "epoch 5 [21.24s]: training loss=0.517754852771759  validation ndcg@10=0.05096010365956474 [0.31s]\n",
      "epoch 6 [21.81s]:  training loss=0.4824923574924469                                     \n",
      "epoch 7 [22.11s]:  training loss=0.4567427635192871                                     \n",
      "epoch 8 [21.76s]:  training loss=0.45313796401023865                                    \n",
      "epoch 9 [21.38s]:  training loss=0.44827592372894287                                    \n",
      "epoch 10 [21.56s]: training loss=0.42982369661331177  validation ndcg@10=0.057925642484146236 [0.29s]\n",
      "epoch 11 [21.45s]:  training loss=0.4227618873119354                                    \n",
      "epoch 12 [21.61s]:  training loss=0.4248286485671997                                    \n",
      "epoch 13 [21.44s]:  training loss=0.4260006844997406                                    \n",
      "epoch 14 [22.99s]:  training loss=0.41727808117866516                                   \n",
      "epoch 15 [20.01s]: training loss=0.4210304915904999  validation ndcg@10=0.05266378236659877 [0.31s]\n",
      "epoch 16 [21.79s]:  training loss=0.41819465160369873                                   \n",
      "epoch 17 [22.04s]:  training loss=0.4149671196937561                                    \n",
      "epoch 18 [21.28s]:  training loss=0.4055914878845215                                    \n",
      "epoch 19 [21.05s]:  training loss=0.40185999870300293                                   \n",
      "epoch 20 [20.7s]: training loss=0.4063694477081299  validation ndcg@10=0.0554750664200694 [0.32s]\n",
      "epoch 21 [21.42s]:  training loss=0.4064050018787384                                    \n",
      "epoch 22 [22.21s]:  training loss=0.404392808675766                                     \n",
      "epoch 23 [20.53s]:  training loss=0.4057398736476898                                    \n",
      "epoch 24 [21.78s]:  training loss=0.39792776107788086                                   \n",
      "epoch 25 [21.67s]: training loss=0.39928290247917175  validation ndcg@10=0.07063028688938834 [0.29s]\n",
      "epoch 26 [22.05s]:  training loss=0.4036411643028259                                    \n",
      "epoch 27 [22.64s]:  training loss=0.390492707490921                                     \n",
      "epoch 28 [21.0s]:  training loss=0.3929280638694763                                     \n",
      "epoch 29 [22.43s]:  training loss=0.3912881016731262                                    \n",
      "epoch 30 [21.21s]: training loss=0.391200989484787  validation ndcg@10=0.07124210341408019 [0.3s]\n",
      "epoch 31 [21.91s]:  training loss=0.39088869094848633                                   \n",
      "epoch 32 [21.03s]:  training loss=0.3860838711261749                                    \n",
      "epoch 33 [21.53s]:  training loss=0.38645118474960327                                   \n",
      "epoch 34 [21.29s]:  training loss=0.38070642948150635                                   \n",
      "epoch 35 [21.86s]: training loss=0.38019874691963196  validation ndcg@10=0.07433061178447314 [0.29s]\n",
      "epoch 36 [20.75s]:  training loss=0.3745403587818146                                    \n",
      "epoch 37 [22.1s]:  training loss=0.37805938720703125                                    \n",
      "epoch 38 [20.92s]:  training loss=0.37152570486068726                                   \n",
      "epoch 39 [22.09s]:  training loss=0.3849254250526428                                    \n",
      "epoch 40 [21.0s]: training loss=0.37294483184814453  validation ndcg@10=0.07911238583688918 [0.3s]\n",
      "epoch 41 [21.35s]:  training loss=0.3743460476398468                                    \n",
      "epoch 42 [22.03s]:  training loss=0.37139326333999634                                   \n",
      "epoch 43 [20.84s]:  training loss=0.3768969178199768                                    \n",
      "epoch 44 [21.62s]:  training loss=0.36643630266189575                                   \n",
      "epoch 45 [21.84s]: training loss=0.36892056465148926  validation ndcg@10=0.07175387021382196 [0.33s]\n",
      "epoch 46 [20.6s]:  training loss=0.36210712790489197                                    \n",
      "epoch 47 [22.18s]:  training loss=0.3690668046474457                                    \n",
      "epoch 48 [21.21s]:  training loss=0.36420997977256775                                   \n",
      "epoch 49 [21.53s]:  training loss=0.36737060546875                                      \n",
      "epoch 50 [20.95s]: training loss=0.36606475710868835  validation ndcg@10=0.07422462696060879 [0.3s]\n",
      "epoch 51 [23.84s]:  training loss=0.3591073155403137                                    \n",
      "epoch 52 [21.31s]:  training loss=0.35823574662208557                                   \n",
      "epoch 53 [22.15s]:  training loss=0.36142033338546753                                   \n",
      "epoch 54 [21.99s]:  training loss=0.35114023089408875                                   \n",
      "epoch 55 [22.24s]: training loss=0.35017868876457214  validation ndcg@10=0.07670060723844184 [0.33s]\n",
      "epoch 56 [22.48s]:  training loss=0.3548487722873688                                    \n",
      "epoch 57 [20.3s]:  training loss=0.3566713035106659                                     \n",
      "epoch 58 [22.32s]:  training loss=0.3533901870250702                                    \n",
      "epoch 59 [20.9s]:  training loss=0.349788099527359                                      \n",
      "epoch 60 [22.02s]: training loss=0.35178443789482117  validation ndcg@10=0.07138031357392903 [0.29s]\n",
      "epoch 61 [20.37s]:  training loss=0.34370219707489014                                   \n",
      "epoch 62 [22.15s]:  training loss=0.34801357984542847                                   \n",
      "epoch 63 [20.77s]:  training loss=0.35053110122680664                                   \n",
      "epoch 64 [21.95s]:  training loss=0.3455362617969513                                    \n",
      "epoch 65 [20.65s]: training loss=0.3500118553638458  validation ndcg@10=0.07019857960905887 [0.31s]\n",
      "epoch 1 [33.53s]:  training loss=0.987289547920227                                      \n",
      "epoch 2 [35.37s]:  training loss=0.859298825263977                                      \n",
      "epoch 3 [34.27s]:  training loss=0.7803871035575867                                     \n",
      "epoch 4 [33.95s]:  training loss=0.7231016159057617                                     \n",
      "epoch 5 [33.57s]: training loss=0.675105631351471  validation ndcg@10=0.037102454435085246 [0.43s]\n",
      "epoch 6 [33.93s]:  training loss=0.6366775035858154                                     \n",
      "epoch 7 [33.72s]:  training loss=0.6032695770263672                                     \n",
      "epoch 8 [34.83s]:  training loss=0.5596730709075928                                     \n",
      "epoch 9 [33.64s]:  training loss=0.5347657203674316                                     \n",
      "epoch 10 [34.76s]: training loss=0.5184990167617798  validation ndcg@10=0.04872652727805544 [0.42s]\n",
      "epoch 11 [34.1s]:  training loss=0.49066171050071716                                    \n",
      "epoch 12 [34.55s]:  training loss=0.47625768184661865                                   \n",
      "epoch 13 [33.42s]:  training loss=0.4694148898124695                                    \n",
      "epoch 14 [33.93s]:  training loss=0.45566654205322266                                   \n",
      "epoch 15 [35.27s]: training loss=0.4497363865375519  validation ndcg@10=0.050928475881902226 [0.38s]\n",
      "epoch 16 [31.33s]:  training loss=0.4445797801017761                                    \n",
      "epoch 17 [32.44s]:  training loss=0.4385930895805359                                    \n",
      "epoch 18 [32.61s]:  training loss=0.4350370466709137                                    \n",
      "epoch 19 [32.38s]:  training loss=0.434145987033844                                     \n",
      "epoch 20 [33.03s]: training loss=0.4255262613296509  validation ndcg@10=0.05602978273641893 [0.45s]\n",
      "epoch 21 [33.49s]:  training loss=0.4243949353694916                                    \n",
      "epoch 22 [34.23s]:  training loss=0.42860567569732666                                   \n",
      "epoch 23 [33.67s]:  training loss=0.42095860838890076                                   \n",
      "epoch 24 [32.93s]:  training loss=0.4217017889022827                                    \n",
      "epoch 25 [34.92s]: training loss=0.4163641929626465  validation ndcg@10=0.05232510971710304 [0.39s]\n",
      "epoch 26 [33.27s]:  training loss=0.4152415096759796                                    \n",
      "epoch 27 [34.15s]:  training loss=0.4173136055469513                                    \n",
      "epoch 28 [35.37s]:  training loss=0.4101411700248718                                    \n",
      "epoch 29 [33.3s]:  training loss=0.40399813652038574                                    \n",
      "epoch 30 [34.12s]: training loss=0.416705459356308  validation ndcg@10=0.05119299707356087 [0.4s]\n",
      "epoch 31 [32.0s]:  training loss=0.4060763716697693                                     \n",
      "epoch 32 [33.74s]:  training loss=0.4148305654525757                                    \n",
      "epoch 33 [33.88s]:  training loss=0.40391427278518677                                   \n",
      "epoch 34 [34.72s]:  training loss=0.4134591221809387                                    \n",
      "epoch 35 [31.99s]: training loss=0.4058726727962494  validation ndcg@10=0.05440081039889114 [0.39s]\n",
      "epoch 36 [33.55s]:  training loss=0.4052841067314148                                    \n",
      "epoch 37 [32.75s]:  training loss=0.4008040130138397                                    \n",
      "epoch 38 [33.1s]:  training loss=0.4030693769454956                                     \n",
      "epoch 39 [33.17s]:  training loss=0.40126028656959534                                   \n",
      "epoch 40 [35.8s]: training loss=0.399415522813797  validation ndcg@10=0.06541474319647815 [0.4s]\n",
      "epoch 41 [32.76s]:  training loss=0.40204256772994995                                   \n",
      "epoch 42 [32.88s]:  training loss=0.39442023634910583                                   \n",
      "epoch 43 [32.85s]:  training loss=0.39609041810035706                                   \n",
      "epoch 44 [32.58s]:  training loss=0.3957133889198303                                    \n",
      "epoch 45 [33.04s]: training loss=0.39554646611213684  validation ndcg@10=0.061148975270790204 [0.42s]\n",
      "epoch 46 [31.72s]:  training loss=0.3945262134075165                                    \n",
      "epoch 47 [33.36s]:  training loss=0.39829379320144653                                   \n",
      "epoch 48 [33.44s]:  training loss=0.3955611288547516                                    \n",
      "epoch 49 [34.09s]:  training loss=0.39039406180381775                                   \n",
      "epoch 50 [33.34s]: training loss=0.3894786536693573  validation ndcg@10=0.0655221289847957 [0.43s]\n",
      "epoch 51 [33.15s]:  training loss=0.39101293683052063                                   \n",
      "epoch 52 [36.72s]:  training loss=0.388759583234787                                     \n",
      "epoch 53 [32.11s]:  training loss=0.3861342966556549                                    \n",
      "epoch 54 [34.25s]:  training loss=0.38792476058006287                                   \n",
      "epoch 55 [32.44s]: training loss=0.38625651597976685  validation ndcg@10=0.061448617092777484 [0.39s]\n",
      "epoch 56 [33.3s]:  training loss=0.3885353207588196                                     \n",
      "epoch 57 [32.88s]:  training loss=0.39016321301460266                                   \n",
      "epoch 58 [34.2s]:  training loss=0.38832032680511475                                    \n",
      "epoch 59 [32.2s]:  training loss=0.3803064823150635                                     \n",
      "epoch 60 [33.16s]: training loss=0.3833424150943756  validation ndcg@10=0.06362651091067718 [0.39s]\n",
      "epoch 61 [33.28s]:  training loss=0.3882124722003937                                    \n",
      "epoch 62 [33.39s]:  training loss=0.3810509741306305                                    \n",
      "epoch 63 [33.19s]:  training loss=0.38466185331344604                                   \n",
      "epoch 64 [35.74s]:  training loss=0.37363922595977783                                   \n",
      "epoch 65 [32.92s]: training loss=0.37484633922576904  validation ndcg@10=0.06482310313770612 [0.49s]\n",
      "epoch 66 [32.51s]:  training loss=0.3728134036064148                                    \n",
      "epoch 67 [33.16s]:  training loss=0.37460747361183167                                   \n",
      "epoch 68 [33.11s]:  training loss=0.37471169233322144                                   \n",
      "epoch 69 [33.54s]:  training loss=0.37541714310646057                                   \n",
      "epoch 70 [32.04s]: training loss=0.36874306201934814  validation ndcg@10=0.06896021729729776 [0.42s]\n",
      "epoch 71 [33.56s]:  training loss=0.37392935156822205                                   \n",
      "epoch 72 [32.31s]:  training loss=0.37098535895347595                                   \n",
      "epoch 73 [31.21s]:  training loss=0.370695024728775                                     \n",
      "epoch 74 [32.63s]:  training loss=0.3776877522468567                                    \n",
      "epoch 75 [33.06s]: training loss=0.367440789937973  validation ndcg@10=0.07256260519488644 [0.37s]\n",
      "epoch 76 [34.78s]:  training loss=0.36733707785606384                                   \n",
      "epoch 77 [32.22s]:  training loss=0.36309799551963806                                   \n",
      "epoch 78 [33.46s]:  training loss=0.36945071816444397                                   \n",
      "epoch 79 [33.53s]:  training loss=0.36651766300201416                                   \n",
      "epoch 80 [33.97s]: training loss=0.36811918020248413  validation ndcg@10=0.06450548330563054 [0.38s]\n",
      "epoch 81 [32.48s]:  training loss=0.3643224835395813                                    \n",
      "epoch 82 [33.27s]:  training loss=0.3616452217102051                                    \n",
      "epoch 83 [32.17s]:  training loss=0.35990241169929504                                   \n",
      "epoch 84 [34.01s]:  training loss=0.3571043014526367                                    \n",
      "epoch 85 [32.42s]: training loss=0.35835063457489014  validation ndcg@10=0.07288370175366686 [0.44s]\n",
      "epoch 86 [33.37s]:  training loss=0.354496031999588                                     \n",
      "epoch 87 [34.9s]:  training loss=0.36468932032585144                                    \n",
      "epoch 88 [33.22s]:  training loss=0.3590790331363678                                    \n",
      "epoch 89 [33.18s]:  training loss=0.3614208400249481                                    \n",
      "epoch 90 [32.68s]: training loss=0.3642650544643402  validation ndcg@10=0.07246208813240383 [0.41s]\n",
      "epoch 91 [34.54s]:  training loss=0.36275121569633484                                   \n",
      "epoch 92 [31.87s]:  training loss=0.3593224585056305                                    \n",
      "epoch 93 [33.7s]:  training loss=0.35761886835098267                                    \n",
      "epoch 94 [32.96s]:  training loss=0.3549489974975586                                    \n",
      "epoch 95 [33.28s]: training loss=0.35211214423179626  validation ndcg@10=0.06670926078554389 [0.42s]\n",
      "epoch 96 [32.78s]:  training loss=0.3561832904815674                                    \n",
      "epoch 97 [32.64s]:  training loss=0.35510632395744324                                   \n",
      "epoch 98 [33.61s]:  training loss=0.35677286982536316                                   \n",
      "epoch 99 [32.21s]:  training loss=0.35210222005844116                                   \n",
      "epoch 100 [33.65s]: training loss=0.35041308403015137  validation ndcg@10=0.06546040560624107 [0.4s]\n",
      "epoch 101 [32.11s]:  training loss=0.3601403534412384                                   \n",
      "epoch 102 [33.44s]:  training loss=0.3562353253364563                                   \n",
      "epoch 103 [32.97s]:  training loss=0.35759860277175903                                  \n",
      "epoch 104 [33.19s]:  training loss=0.3546639680862427                                   \n",
      "epoch 105 [32.94s]: training loss=0.35125359892845154  validation ndcg@10=0.06813343568511715 [0.38s]\n",
      "epoch 106 [33.18s]:  training loss=0.3519629240036011                                   \n",
      "epoch 107 [34.14s]:  training loss=0.35530850291252136                                  \n",
      "epoch 108 [32.82s]:  training loss=0.35137277841567993                                  \n",
      "epoch 109 [35.51s]:  training loss=0.3528018295764923                                   \n",
      "epoch 110 [33.49s]: training loss=0.34560683369636536  validation ndcg@10=0.07236761019257601 [0.46s]\n",
      "epoch 1 [4.9s]:  training loss=1.0434184074401855                                         \n",
      "epoch 2 [4.62s]:  training loss=1.0104387998580933                                        \n",
      "epoch 3 [4.18s]:  training loss=0.9992838501930237                                        \n",
      "epoch 4 [4.12s]:  training loss=0.985563337802887                                         \n",
      "epoch 5 [4.17s]: training loss=0.9604329466819763  validation ndcg@10=0.024230897127512543 [0.07s]\n",
      "epoch 6 [4.06s]:  training loss=0.9456424117088318                                        \n",
      "epoch 7 [4.2s]:  training loss=0.9420477151870728                                         \n",
      "epoch 8 [3.95s]:  training loss=0.9160258173942566                                        \n",
      "epoch 9 [4.07s]:  training loss=0.920035183429718                                         \n",
      "epoch 10 [3.99s]: training loss=0.9087724685668945  validation ndcg@10=0.0270391968378527 [0.08s]\n",
      "epoch 11 [4.08s]:  training loss=0.8897820115089417                                       \n",
      "epoch 12 [4.16s]:  training loss=0.8756726980209351                                       \n",
      "epoch 13 [4.04s]:  training loss=0.8672845363616943                                       \n",
      "epoch 14 [4.17s]:  training loss=0.8570468425750732                                       \n",
      "epoch 15 [4.3s]: training loss=0.8598331809043884  validation ndcg@10=0.027033196070182928 [0.07s]\n",
      "epoch 16 [4.32s]:  training loss=0.8377158045768738                                       \n",
      "epoch 17 [4.33s]:  training loss=0.8318278789520264                                       \n",
      "epoch 18 [4.23s]:  training loss=0.8201017379760742                                       \n",
      "epoch 19 [4.06s]:  training loss=0.8074692487716675                                       \n",
      "epoch 20 [4.18s]: training loss=0.8065351843833923  validation ndcg@10=0.031465830034186654 [0.08s]\n",
      "epoch 21 [4.32s]:  training loss=0.8077586889266968                                       \n",
      "epoch 22 [4.04s]:  training loss=0.7888140678405762                                       \n",
      "epoch 23 [4.33s]:  training loss=0.7840609550476074                                       \n",
      "epoch 24 [4.32s]:  training loss=0.7733491659164429                                       \n",
      "epoch 25 [4.35s]: training loss=0.765319287776947  validation ndcg@10=0.033129215908001684 [0.11s]\n",
      "epoch 26 [4.34s]:  training loss=0.7498459219932556                                       \n",
      "epoch 27 [4.16s]:  training loss=0.7584513425827026                                       \n",
      "epoch 28 [4.22s]:  training loss=0.7522381544113159                                       \n",
      "epoch 29 [4.16s]:  training loss=0.7423794865608215                                       \n",
      "epoch 30 [4.2s]: training loss=0.7366823554039001  validation ndcg@10=0.03406169414863317 [0.08s]\n",
      "epoch 31 [4.09s]:  training loss=0.7366155385971069                                       \n",
      "epoch 32 [4.25s]:  training loss=0.7237737774848938                                       \n",
      "epoch 33 [4.13s]:  training loss=0.7147421836853027                                       \n",
      "epoch 34 [4.06s]:  training loss=0.7064380049705505                                       \n",
      "epoch 35 [4.46s]: training loss=0.7067742347717285  validation ndcg@10=0.03604546710049427 [0.07s]\n",
      "epoch 36 [4.25s]:  training loss=0.6967881917953491                                       \n",
      "epoch 37 [4.29s]:  training loss=0.6903077960014343                                       \n",
      "epoch 38 [4.14s]:  training loss=0.6851477026939392                                       \n",
      "epoch 39 [4.2s]:  training loss=0.6768427491188049                                        \n",
      "epoch 40 [4.3s]: training loss=0.6762861013412476  validation ndcg@10=0.03640792398646381 [0.07s]\n",
      "epoch 41 [4.11s]:  training loss=0.6650580763816833                                       \n",
      "epoch 42 [4.2s]:  training loss=0.666729748249054                                         \n",
      "epoch 43 [4.28s]:  training loss=0.6640082001686096                                       \n",
      "epoch 44 [4.42s]:  training loss=0.6528365612030029                                       \n",
      "epoch 45 [4.19s]: training loss=0.6445804238319397  validation ndcg@10=0.03655072289233133 [0.08s]\n",
      "epoch 46 [4.07s]:  training loss=0.6534310579299927                                       \n",
      "epoch 47 [4.16s]:  training loss=0.6407910585403442                                       \n",
      "epoch 48 [4.18s]:  training loss=0.6334285736083984                                       \n",
      "epoch 49 [4.06s]:  training loss=0.6264839172363281                                       \n",
      "epoch 50 [4.17s]: training loss=0.6229603290557861  validation ndcg@10=0.038033736005988604 [0.08s]\n",
      "epoch 51 [4.23s]:  training loss=0.6184439063072205                                       \n",
      "epoch 52 [4.26s]:  training loss=0.6188761591911316                                       \n",
      "epoch 53 [4.11s]:  training loss=0.6132821440696716                                       \n",
      "epoch 54 [4.0s]:  training loss=0.6034976840019226                                        \n",
      "epoch 55 [4.21s]: training loss=0.6033471822738647  validation ndcg@10=0.041787305043057596 [0.1s]\n",
      "epoch 56 [4.05s]:  training loss=0.5932132601737976                                       \n",
      "epoch 57 [4.17s]:  training loss=0.5916962027549744                                       \n",
      "epoch 58 [4.07s]:  training loss=0.5923783779144287                                       \n",
      "epoch 59 [4.12s]:  training loss=0.586597204208374                                        \n",
      "epoch 60 [4.11s]: training loss=0.5781694650650024  validation ndcg@10=0.04333399401468163 [0.08s]\n",
      "epoch 61 [4.18s]:  training loss=0.5757718682289124                                       \n",
      "epoch 62 [4.07s]:  training loss=0.5728833079338074                                       \n",
      "epoch 63 [4.1s]:  training loss=0.5633187890052795                                        \n",
      "epoch 64 [4.29s]:  training loss=0.5737873315811157                                       \n",
      "epoch 65 [4.21s]: training loss=0.5594266057014465  validation ndcg@10=0.04554154398657363 [0.07s]\n",
      "epoch 66 [4.19s]:  training loss=0.555135190486908                                        \n",
      "epoch 67 [4.08s]:  training loss=0.5548519492149353                                       \n",
      "epoch 68 [4.12s]:  training loss=0.5545282363891602                                       \n",
      "epoch 69 [4.09s]:  training loss=0.5521876215934753                                       \n",
      "epoch 70 [3.97s]: training loss=0.5448538064956665  validation ndcg@10=0.047859357916831886 [0.07s]\n",
      "epoch 71 [4.16s]:  training loss=0.5337634682655334                                       \n",
      "epoch 72 [4.13s]:  training loss=0.5325884222984314                                       \n",
      "epoch 73 [4.03s]:  training loss=0.5359177589416504                                       \n",
      "epoch 74 [4.25s]:  training loss=0.5348069667816162                                       \n",
      "epoch 75 [4.04s]: training loss=0.5377723574638367  validation ndcg@10=0.04938409058618652 [0.09s]\n",
      "epoch 76 [4.29s]:  training loss=0.5190339088439941                                       \n",
      "epoch 77 [5.73s]:  training loss=0.5187923312187195                                       \n",
      "epoch 78 [4.84s]:  training loss=0.5268017053604126                                       \n",
      "epoch 79 [4.32s]:  training loss=0.5164750218391418                                       \n",
      "epoch 80 [4.24s]: training loss=0.5142418742179871  validation ndcg@10=0.04969995947832077 [0.08s]\n",
      "epoch 81 [4.43s]:  training loss=0.5115091800689697                                       \n",
      "epoch 82 [4.03s]:  training loss=0.504304051399231                                        \n",
      "epoch 83 [4.1s]:  training loss=0.5035229325294495                                        \n",
      "epoch 84 [4.1s]:  training loss=0.5037811398506165                                        \n",
      "epoch 85 [4.03s]: training loss=0.5003439784049988  validation ndcg@10=0.0522143912905036 [0.08s]\n",
      "epoch 86 [4.18s]:  training loss=0.49702757596969604                                      \n",
      "epoch 87 [4.13s]:  training loss=0.49420660734176636                                      \n",
      "epoch 88 [4.26s]:  training loss=0.4921676516532898                                       \n",
      "epoch 89 [4.19s]:  training loss=0.4965965747833252                                       \n",
      "epoch 90 [4.11s]: training loss=0.485676109790802  validation ndcg@10=0.05190224133231741 [0.07s]\n",
      "epoch 91 [4.27s]:  training loss=0.4920039474964142                                       \n",
      "epoch 92 [4.08s]:  training loss=0.4779573678970337                                       \n",
      "epoch 93 [4.06s]:  training loss=0.4777897596359253                                       \n",
      "epoch 94 [4.23s]:  training loss=0.4846939146518707                                       \n",
      "epoch 95 [4.21s]: training loss=0.4778677225112915  validation ndcg@10=0.05300203084957408 [0.08s]\n",
      "epoch 96 [4.42s]:  training loss=0.4815376400947571                                       \n",
      "epoch 97 [4.19s]:  training loss=0.4730536937713623                                       \n",
      "epoch 98 [4.16s]:  training loss=0.4738040566444397                                       \n",
      "epoch 99 [4.25s]:  training loss=0.47257065773010254                                      \n",
      "epoch 100 [4.17s]: training loss=0.46666961908340454  validation ndcg@10=0.05205138947247058 [0.07s]\n",
      "epoch 101 [4.11s]:  training loss=0.46593037247657776                                     \n",
      "epoch 102 [4.19s]:  training loss=0.4680922329425812                                      \n",
      "epoch 103 [4.28s]:  training loss=0.4642898440361023                                      \n",
      "epoch 104 [4.27s]:  training loss=0.46541860699653625                                     \n",
      "epoch 105 [4.16s]: training loss=0.4636247754096985  validation ndcg@10=0.04851761427905947 [0.07s]\n",
      "epoch 106 [4.23s]:  training loss=0.46328413486480713                                     \n",
      "epoch 107 [4.14s]:  training loss=0.46463003754615784                                     \n",
      "epoch 108 [3.97s]:  training loss=0.46164458990097046                                     \n",
      "epoch 109 [4.09s]:  training loss=0.4612257778644562                                      \n",
      "epoch 110 [4.17s]: training loss=0.4571225047111511  validation ndcg@10=0.0510956677294492 [0.08s]\n",
      "epoch 111 [4.17s]:  training loss=0.452457994222641                                       \n",
      "epoch 112 [4.07s]:  training loss=0.45413076877593994                                     \n",
      "epoch 113 [4.1s]:  training loss=0.4535145163536072                                       \n",
      "epoch 114 [4.13s]:  training loss=0.4559634029865265                                      \n",
      "epoch 115 [4.15s]: training loss=0.4492429196834564  validation ndcg@10=0.05055124390953851 [0.07s]\n",
      "epoch 116 [4.1s]:  training loss=0.451433926820755                                        \n",
      "epoch 117 [4.04s]:  training loss=0.45355504751205444                                     \n",
      "epoch 118 [4.24s]:  training loss=0.45312657952308655                                     \n",
      "epoch 119 [4.14s]:  training loss=0.44636887311935425                                     \n",
      "epoch 120 [4.14s]: training loss=0.4454754590988159  validation ndcg@10=0.051993296650796286 [0.07s]\n",
      "epoch 1 [19.99s]:  training loss=1.0201387405395508                                       \n",
      "epoch 2 [21.11s]:  training loss=0.9565059542655945                                       \n",
      "epoch 3 [20.7s]:  training loss=0.9109138250350952                                        \n",
      "epoch 4 [20.82s]:  training loss=0.8718932867050171                                       \n",
      "epoch 5 [20.82s]: training loss=0.8379853963851929  validation ndcg@10=0.028778102035630863 [0.32s]\n",
      "epoch 6 [20.8s]:  training loss=0.8076076507568359                                        \n",
      "epoch 7 [20.66s]:  training loss=0.7848978042602539                                       \n",
      "epoch 8 [20.59s]:  training loss=0.7741256952285767                                       \n",
      "epoch 9 [20.89s]:  training loss=0.7573172450065613                                       \n",
      "epoch 10 [23.31s]: training loss=0.7312185764312744  validation ndcg@10=0.03322972928759899 [0.31s]\n",
      "epoch 11 [20.46s]:  training loss=0.7152620553970337                                      \n",
      "epoch 12 [21.16s]:  training loss=0.7002090811729431                                      \n",
      "epoch 13 [20.58s]:  training loss=0.6756348609924316                                      \n",
      "epoch 14 [21.22s]:  training loss=0.6649383306503296                                      \n",
      "epoch 15 [20.56s]: training loss=0.6502330303192139  validation ndcg@10=0.033885866319748395 [0.28s]\n",
      "epoch 16 [21.0s]:  training loss=0.636364221572876                                        \n",
      "epoch 17 [21.07s]:  training loss=0.6239119172096252                                      \n",
      "epoch 18 [21.6s]:  training loss=0.612442135810852                                        \n",
      "epoch 19 [20.99s]:  training loss=0.5986213088035583                                      \n",
      "epoch 20 [20.54s]: training loss=0.578794538974762  validation ndcg@10=0.04058145706606935 [0.38s]\n",
      "epoch 21 [21.34s]:  training loss=0.5772585272789001                                      \n",
      "epoch 22 [20.93s]:  training loss=0.5668525099754333                                      \n",
      "epoch 23 [21.35s]:  training loss=0.5522303581237793                                      \n",
      "epoch 24 [20.5s]:  training loss=0.5466912984848022                                       \n",
      "epoch 25 [22.1s]: training loss=0.5346431136131287  validation ndcg@10=0.04855102766536814 [0.34s]\n",
      "epoch 26 [21.74s]:  training loss=0.5245029926300049                                      \n",
      "epoch 27 [21.7s]:  training loss=0.5167393684387207                                       \n",
      "epoch 28 [20.8s]:  training loss=0.5085420608520508                                       \n",
      "epoch 29 [21.38s]:  training loss=0.5014464259147644                                      \n",
      "epoch 30 [23.83s]: training loss=0.49388960003852844  validation ndcg@10=0.055252145741726974 [0.34s]\n",
      "epoch 31 [20.62s]:  training loss=0.48881232738494873                                     \n",
      "epoch 32 [21.15s]:  training loss=0.48887351155281067                                     \n",
      "epoch 33 [21.36s]:  training loss=0.47400954365730286                                     \n",
      "epoch 34 [21.42s]:  training loss=0.46901318430900574                                     \n",
      "epoch 35 [21.18s]: training loss=0.46787211298942566  validation ndcg@10=0.05899668740464508 [0.29s]\n",
      "epoch 36 [21.34s]:  training loss=0.45729050040245056                                     \n",
      "epoch 37 [21.06s]:  training loss=0.4633462727069855                                      \n",
      "epoch 38 [21.01s]:  training loss=0.4576705992221832                                      \n",
      "epoch 39 [20.93s]:  training loss=0.4506804049015045                                      \n",
      "epoch 40 [20.35s]: training loss=0.4505383372306824  validation ndcg@10=0.0568887721785837 [0.28s]\n",
      "epoch 41 [21.01s]:  training loss=0.45192626118659973                                     \n",
      "epoch 42 [21.25s]:  training loss=0.45077013969421387                                     \n",
      "epoch 43 [20.98s]:  training loss=0.4429531395435333                                      \n",
      "epoch 44 [20.98s]:  training loss=0.4439602792263031                                      \n",
      "epoch 45 [21.29s]: training loss=0.43974676728248596  validation ndcg@10=0.06215238342763688 [0.29s]\n",
      "epoch 46 [21.16s]:  training loss=0.439338743686676                                       \n",
      "epoch 47 [20.77s]:  training loss=0.4435070753097534                                      \n",
      "epoch 48 [21.03s]:  training loss=0.4388485252857208                                      \n",
      "epoch 49 [21.71s]:  training loss=0.42901748418807983                                     \n",
      "epoch 50 [21.86s]: training loss=0.4332132339477539  validation ndcg@10=0.05926021148887178 [0.29s]\n",
      "epoch 51 [23.27s]:  training loss=0.4328421652317047                                      \n",
      "epoch 52 [22.08s]:  training loss=0.43086767196655273                                     \n",
      "epoch 53 [20.56s]:  training loss=0.43243998289108276                                     \n",
      "epoch 54 [20.88s]:  training loss=0.42772239446640015                                     \n",
      "epoch 55 [21.69s]: training loss=0.42621737718582153  validation ndcg@10=0.05919319418892275 [0.26s]\n",
      "epoch 56 [21.47s]:  training loss=0.428887277841568                                       \n",
      "epoch 57 [20.57s]:  training loss=0.42551445960998535                                     \n",
      "epoch 58 [21.43s]:  training loss=0.41891908645629883                                     \n",
      "epoch 59 [21.48s]:  training loss=0.41672319173812866                                     \n",
      "epoch 60 [21.1s]: training loss=0.4212174713611603  validation ndcg@10=0.055548044913400874 [0.26s]\n",
      "epoch 61 [20.82s]:  training loss=0.42564427852630615                                     \n",
      "epoch 62 [21.19s]:  training loss=0.4191453754901886                                      \n",
      "epoch 63 [21.14s]:  training loss=0.4236963391304016                                      \n",
      "epoch 64 [21.45s]:  training loss=0.4285673499107361                                      \n",
      "epoch 65 [21.12s]: training loss=0.42087969183921814  validation ndcg@10=0.05675696474275591 [0.29s]\n",
      "epoch 66 [21.45s]:  training loss=0.4229303002357483                                      \n",
      "epoch 67 [20.58s]:  training loss=0.42351025342941284                                     \n",
      "epoch 68 [21.06s]:  training loss=0.42010554671287537                                     \n",
      "epoch 69 [20.55s]:  training loss=0.42020365595817566                                     \n",
      "epoch 70 [21.29s]: training loss=0.4155573844909668  validation ndcg@10=0.05707242287483848 [0.29s]\n",
      "epoch 1 [3.11s]:  training loss=0.7778586149215698                                        \n",
      "epoch 2 [3.05s]:  training loss=0.5304568409919739                                        \n",
      "epoch 3 [3.02s]:  training loss=0.46428608894348145                                       \n",
      "epoch 4 [3.02s]:  training loss=0.4404280185699463                                        \n",
      "epoch 5 [3.08s]: training loss=0.43453076481819153  validation ndcg@10=0.054361611638639624 [0.06s]\n",
      "epoch 6 [2.85s]:  training loss=0.42302197217941284                                       \n",
      "epoch 7 [2.89s]:  training loss=0.41535547375679016                                       \n",
      "epoch 8 [3.13s]:  training loss=0.4095527231693268                                        \n",
      "epoch 9 [4.24s]:  training loss=0.4085617959499359                                        \n",
      "epoch 10 [3.63s]: training loss=0.3904365599155426  validation ndcg@10=0.06993388049217548 [0.06s]\n",
      "epoch 11 [3.06s]:  training loss=0.3958704471588135                                       \n",
      "epoch 12 [3.05s]:  training loss=0.39231544733047485                                      \n",
      "epoch 13 [3.04s]:  training loss=0.38770392537117004                                      \n",
      "epoch 14 [3.13s]:  training loss=0.3834229111671448                                       \n",
      "epoch 15 [3.0s]: training loss=0.37677010893821716  validation ndcg@10=0.07175953854426233 [0.05s]\n",
      "epoch 16 [2.92s]:  training loss=0.3742963671684265                                       \n",
      "epoch 17 [3.06s]:  training loss=0.3761884868144989                                       \n",
      "epoch 18 [2.92s]:  training loss=0.36849480867385864                                      \n",
      "epoch 19 [3.07s]:  training loss=0.36247891187667847                                      \n",
      "epoch 20 [2.98s]: training loss=0.35875725746154785  validation ndcg@10=0.07400182381406949 [0.06s]\n",
      "epoch 21 [2.98s]:  training loss=0.36268144845962524                                      \n",
      "epoch 22 [2.81s]:  training loss=0.35214763879776                                         \n",
      "epoch 23 [2.99s]:  training loss=0.3542740046977997                                       \n",
      "epoch 24 [2.86s]:  training loss=0.35010775923728943                                      \n",
      "epoch 25 [3.04s]: training loss=0.3556104898452759  validation ndcg@10=0.07210168643015816 [0.06s]\n",
      "epoch 26 [2.95s]:  training loss=0.35016390681266785                                      \n",
      "epoch 27 [3.01s]:  training loss=0.3438558876514435                                       \n",
      "epoch 28 [2.87s]:  training loss=0.3495410680770874                                       \n",
      "epoch 29 [2.87s]:  training loss=0.3521023094654083                                       \n",
      "epoch 30 [2.91s]: training loss=0.34527549147605896  validation ndcg@10=0.07168378604998465 [0.06s]\n",
      "epoch 31 [2.98s]:  training loss=0.3417333662509918                                       \n",
      "epoch 32 [3.02s]:  training loss=0.3396526575088501                                       \n",
      "epoch 33 [2.81s]:  training loss=0.3365418016910553                                       \n",
      "epoch 34 [2.99s]:  training loss=0.34581610560417175                                      \n",
      "epoch 35 [3.01s]: training loss=0.34356796741485596  validation ndcg@10=0.05876722801422894 [0.06s]\n",
      "epoch 36 [2.94s]:  training loss=0.3385138213634491                                       \n",
      "epoch 37 [2.96s]:  training loss=0.33258673548698425                                      \n",
      "epoch 38 [3.18s]:  training loss=0.328059583902359                                        \n",
      "epoch 39 [3.15s]:  training loss=0.3341338038444519                                       \n",
      "epoch 40 [3.14s]: training loss=0.33668139576911926  validation ndcg@10=0.07052123134277373 [0.06s]\n",
      "epoch 41 [3.12s]:  training loss=0.3286195397377014                                       \n",
      "epoch 42 [3.07s]:  training loss=0.3295882046222687                                       \n",
      "epoch 43 [2.96s]:  training loss=0.33575382828712463                                      \n",
      "epoch 44 [3.02s]:  training loss=0.3360218107700348                                       \n",
      "epoch 45 [2.88s]: training loss=0.3223339021205902  validation ndcg@10=0.05799324826197889 [0.05s]\n",
      "epoch 1 [6.08s]:  training loss=0.88132643699646                                          \n",
      "epoch 2 [6.04s]:  training loss=1.0959323644638062                                       \n",
      "epoch 3 [5.96s]:  training loss=1.224408507347107                                        \n",
      "epoch 4 [6.07s]:  training loss=1.3199042081832886                                       \n",
      "epoch 5 [6.02s]: training loss=1.3100509643554688  validation ndcg@10=0.0362159917801651 [0.14s]\n",
      "epoch 6 [6.18s]:  training loss=1.303189992904663                                        \n",
      "epoch 7 [6.04s]:  training loss=1.3096309900283813                                       \n",
      "epoch 8 [6.01s]:  training loss=1.2860760688781738                                       \n",
      "epoch 9 [6.07s]:  training loss=1.2438616752624512                                       \n",
      "epoch 10 [5.91s]: training loss=1.2099684476852417  validation ndcg@10=0.026170640581092797 [0.15s]\n",
      "epoch 11 [5.9s]:  training loss=1.1722095012664795                                       \n",
      "epoch 12 [5.86s]:  training loss=1.1629952192306519                                      \n",
      "epoch 13 [5.98s]:  training loss=1.1537810564041138                                      \n",
      "epoch 14 [6.23s]:  training loss=1.1450772285461426                                      \n",
      "epoch 15 [6.02s]: training loss=1.1556282043457031  validation ndcg@10=0.006999662676267851 [0.13s]\n",
      "epoch 16 [6.11s]:  training loss=1.1399147510528564                                      \n",
      "epoch 17 [6.01s]:  training loss=1.1401346921920776                                      \n",
      "epoch 18 [6.1s]:  training loss=1.110713243484497                                        \n",
      "epoch 19 [6.1s]:  training loss=1.1034656763076782                                       \n",
      "epoch 20 [6.16s]: training loss=1.133407473564148  validation ndcg@10=0.0025493825303161627 [0.14s]\n",
      "epoch 21 [6.02s]:  training loss=1.1494399309158325                                      \n",
      "epoch 22 [6.1s]:  training loss=1.157503604888916                                        \n",
      "epoch 23 [5.87s]:  training loss=1.1616051197052002                                      \n",
      "epoch 24 [5.82s]:  training loss=1.1682605743408203                                      \n",
      "epoch 25 [5.82s]: training loss=1.166089653968811  validation ndcg@10=0.012107344568569236 [0.16s]\n",
      "epoch 26 [5.91s]:  training loss=1.1927319765090942                                      \n",
      "epoch 27 [5.83s]:  training loss=1.2279585599899292                                      \n",
      "epoch 28 [6.02s]:  training loss=1.1788724660873413                                      \n",
      "epoch 29 [5.79s]:  training loss=1.2237151861190796                                      \n",
      "epoch 30 [5.88s]: training loss=1.2189944982528687  validation ndcg@10=0.010498546309421423 [0.13s]\n",
      "epoch 1 [4.16s]:  training loss=0.549256443977356                                        \n",
      "epoch 2 [4.21s]:  training loss=0.4458036720752716                                      \n",
      "epoch 3 [4.24s]:  training loss=0.4163847267627716                                      \n",
      "epoch 4 [4.15s]:  training loss=0.40726238489151                                        \n",
      "epoch 5 [4.34s]: training loss=0.3950892984867096  validation ndcg@10=0.06652912944195838 [0.08s]\n",
      "epoch 6 [4.22s]:  training loss=0.4051528573036194                                      \n",
      "epoch 7 [4.2s]:  training loss=0.39440205693244934                                      \n",
      "epoch 8 [4.28s]:  training loss=0.3913134038448334                                      \n",
      "epoch 9 [4.37s]:  training loss=0.3873104751110077                                      \n",
      "epoch 10 [4.19s]: training loss=0.3862147927284241  validation ndcg@10=0.04653521875618778 [0.08s]\n",
      "epoch 11 [4.15s]:  training loss=0.3788922429084778                                     \n",
      "epoch 12 [4.38s]:  training loss=0.3769463300704956                                     \n",
      "epoch 13 [4.27s]:  training loss=0.3747018575668335                                     \n",
      "epoch 14 [4.23s]:  training loss=0.38535863161087036                                    \n",
      "epoch 15 [4.13s]: training loss=0.3800385892391205  validation ndcg@10=0.045938991912983985 [0.09s]\n",
      "epoch 16 [4.17s]:  training loss=0.38115203380584717                                    \n",
      "epoch 17 [4.16s]:  training loss=0.3771003186702728                                     \n",
      "epoch 18 [4.19s]:  training loss=0.37702468037605286                                    \n",
      "epoch 19 [4.1s]:  training loss=0.37020015716552734                                     \n",
      "epoch 20 [4.06s]: training loss=0.38262492418289185  validation ndcg@10=0.04200570056755749 [0.08s]\n",
      "epoch 21 [4.42s]:  training loss=0.3668209910392761                                     \n",
      "epoch 22 [4.2s]:  training loss=0.36993423104286194                                     \n",
      "epoch 23 [4.2s]:  training loss=0.3610895276069641                                      \n",
      "epoch 24 [4.43s]:  training loss=0.37359464168548584                                    \n",
      "epoch 25 [5.04s]: training loss=0.37231379747390747  validation ndcg@10=0.03840222503214449 [0.28s]\n",
      "epoch 26 [4.99s]:  training loss=0.3676507771015167                                     \n",
      "epoch 27 [4.29s]:  training loss=0.36663544178009033                                    \n",
      "epoch 28 [4.12s]:  training loss=0.3717358708381653                                     \n",
      "epoch 29 [4.2s]:  training loss=0.37012720108032227                                     \n",
      "epoch 30 [4.24s]: training loss=0.36073583364486694  validation ndcg@10=0.03390951968213176 [0.09s]\n",
      "epoch 1 [4.95s]:  training loss=1.0869940519332886                                      \n",
      "epoch 2 [4.99s]:  training loss=1.042988657951355                                       \n",
      "epoch 3 [5.08s]:  training loss=0.9917253851890564                                      \n",
      "epoch 4 [5.08s]:  training loss=0.9518792629241943                                      \n",
      "epoch 5 [5.11s]: training loss=0.9338183403015137  validation ndcg@10=0.03124609827212689 [0.08s]\n",
      "epoch 6 [5.09s]:  training loss=0.9201082587242126                                      \n",
      "epoch 7 [5.15s]:  training loss=0.89450603723526                                        \n",
      "epoch 8 [5.04s]:  training loss=0.8778781294822693                                      \n",
      "epoch 9 [5.01s]:  training loss=0.8522451519966125                                      \n",
      "epoch 10 [5.01s]: training loss=0.8359786868095398  validation ndcg@10=0.03322053297564836 [0.1s]\n",
      "epoch 11 [4.98s]:  training loss=0.8231202960014343                                     \n",
      "epoch 12 [5.06s]:  training loss=0.8081939816474915                                     \n",
      "epoch 13 [4.96s]:  training loss=0.8088211417198181                                     \n",
      "epoch 14 [4.94s]:  training loss=0.7815093398094177                                     \n",
      "epoch 15 [5.0s]: training loss=0.7652221322059631  validation ndcg@10=0.036310150914620754 [0.09s]\n",
      "epoch 16 [4.99s]:  training loss=0.7532530426979065                                     \n",
      "epoch 17 [4.95s]:  training loss=0.7421104311943054                                     \n",
      "epoch 18 [4.97s]:  training loss=0.7363026142120361                                     \n",
      "epoch 19 [5.07s]:  training loss=0.7343086004257202                                     \n",
      "epoch 20 [5.05s]: training loss=0.7121481895446777  validation ndcg@10=0.03942460644347553 [0.09s]\n",
      "epoch 21 [4.93s]:  training loss=0.703096330165863                                      \n",
      "epoch 22 [4.97s]:  training loss=0.6891672015190125                                     \n",
      "epoch 23 [4.98s]:  training loss=0.6820790767669678                                     \n",
      "epoch 24 [4.97s]:  training loss=0.675324022769928                                      \n",
      "epoch 25 [4.99s]: training loss=0.6608521342277527  validation ndcg@10=0.03892193165450372 [0.08s]\n",
      "epoch 26 [4.92s]:  training loss=0.6564717888832092                                     \n",
      "epoch 27 [5.04s]:  training loss=0.6506819725036621                                     \n",
      "epoch 28 [4.97s]:  training loss=0.6299082040786743                                     \n",
      "epoch 29 [5.03s]:  training loss=0.6328986883163452                                     \n",
      "epoch 30 [5.0s]: training loss=0.6266430616378784  validation ndcg@10=0.03880756233842841 [0.08s]\n",
      "epoch 31 [4.99s]:  training loss=0.623838484287262                                      \n",
      "epoch 32 [5.08s]:  training loss=0.6055448055267334                                     \n",
      "epoch 33 [5.04s]:  training loss=0.5963042974472046                                     \n",
      "epoch 34 [4.9s]:  training loss=0.5874691009521484                                      \n",
      "epoch 35 [4.93s]: training loss=0.5765842199325562  validation ndcg@10=0.039232075105082295 [0.08s]\n",
      "epoch 36 [4.92s]:  training loss=0.5771315097808838                                     \n",
      "epoch 37 [5.06s]:  training loss=0.5693070292472839                                     \n",
      "epoch 38 [4.99s]:  training loss=0.5612818002700806                                     \n",
      "epoch 39 [4.98s]:  training loss=0.556717574596405                                      \n",
      "epoch 40 [5.04s]: training loss=0.5552870631217957  validation ndcg@10=0.04359955751100112 [0.08s]\n",
      "epoch 41 [5.03s]:  training loss=0.544977068901062                                      \n",
      "epoch 42 [5.03s]:  training loss=0.5381396412849426                                     \n",
      "epoch 43 [5.11s]:  training loss=0.5284894108772278                                     \n",
      "epoch 44 [5.11s]:  training loss=0.5240718126296997                                     \n",
      "epoch 45 [5.18s]: training loss=0.5210673213005066  validation ndcg@10=0.0497854051204195 [0.08s]\n",
      "epoch 46 [5.04s]:  training loss=0.5138815641403198                                     \n",
      "epoch 47 [4.91s]:  training loss=0.5125209093093872                                     \n",
      "epoch 48 [5.07s]:  training loss=0.5059168338775635                                     \n",
      "epoch 49 [4.96s]:  training loss=0.5024210810661316                                     \n",
      "epoch 50 [5.02s]: training loss=0.5016458034515381  validation ndcg@10=0.04904091829776427 [0.08s]\n",
      "epoch 51 [5.06s]:  training loss=0.49590933322906494                                    \n",
      "epoch 52 [4.94s]:  training loss=0.4881778061389923                                     \n",
      "epoch 53 [4.95s]:  training loss=0.48824265599250793                                    \n",
      "epoch 54 [5.06s]:  training loss=0.48673155903816223                                    \n",
      "epoch 55 [4.97s]: training loss=0.4841408431529999  validation ndcg@10=0.04716769777981579 [0.08s]\n",
      "epoch 56 [4.93s]:  training loss=0.4795825779438019                                     \n",
      "epoch 57 [4.99s]:  training loss=0.4737972021102905                                     \n",
      "epoch 58 [4.96s]:  training loss=0.47146037220954895                                    \n",
      "epoch 59 [5.06s]:  training loss=0.46371209621429443                                    \n",
      "epoch 60 [4.9s]: training loss=0.46114853024482727  validation ndcg@10=0.049470623779818694 [0.08s]\n",
      "epoch 61 [5.08s]:  training loss=0.46488407254219055                                    \n",
      "epoch 62 [4.93s]:  training loss=0.45895305275917053                                    \n",
      "epoch 63 [4.89s]:  training loss=0.4634193480014801                                     \n",
      "epoch 64 [4.88s]:  training loss=0.4560145139694214                                     \n",
      "epoch 65 [4.96s]: training loss=0.45397573709487915  validation ndcg@10=0.05499814120912343 [0.09s]\n",
      "epoch 66 [4.93s]:  training loss=0.45405900478363037                                    \n",
      "epoch 67 [4.99s]:  training loss=0.4513237476348877                                     \n",
      "epoch 68 [4.9s]:  training loss=0.44899776577949524                                     \n",
      "epoch 69 [5.0s]:  training loss=0.44883885979652405                                     \n",
      "epoch 70 [4.95s]: training loss=0.4479960501194  validation ndcg@10=0.05435646219065949 [0.08s]\n",
      "epoch 71 [5.11s]:  training loss=0.4420210123062134                                     \n",
      "epoch 72 [4.9s]:  training loss=0.44726815819740295                                     \n",
      "epoch 73 [4.91s]:  training loss=0.44469085335731506                                    \n",
      "epoch 74 [6.74s]:  training loss=0.44199952483177185                                    \n",
      "epoch 75 [5.01s]: training loss=0.44669488072395325  validation ndcg@10=0.051917644942164706 [0.08s]\n",
      "epoch 76 [4.99s]:  training loss=0.4426993429660797                                     \n",
      "epoch 77 [5.05s]:  training loss=0.44106385111808777                                    \n",
      "epoch 78 [5.06s]:  training loss=0.437733918428421                                      \n",
      "epoch 79 [4.99s]:  training loss=0.43641549348831177                                    \n",
      "epoch 80 [4.97s]: training loss=0.435921847820282  validation ndcg@10=0.05099790239722519 [0.08s]\n",
      "epoch 81 [4.99s]:  training loss=0.43719837069511414                                    \n",
      "epoch 82 [5.16s]:  training loss=0.430441677570343                                      \n",
      "epoch 83 [4.92s]:  training loss=0.43806299567222595                                    \n",
      "epoch 84 [5.02s]:  training loss=0.4384889304637909                                     \n",
      "epoch 85 [5.18s]: training loss=0.4322793185710907  validation ndcg@10=0.05375098773607263 [0.09s]\n",
      "epoch 86 [5.02s]:  training loss=0.43081241846084595                                    \n",
      "epoch 87 [4.83s]:  training loss=0.4316047132015228                                     \n",
      "epoch 88 [5.05s]:  training loss=0.43207472562789917                                    \n",
      "epoch 89 [5.03s]:  training loss=0.42860543727874756                                    \n",
      "epoch 90 [4.92s]: training loss=0.4383819103240967  validation ndcg@10=0.04947811384353551 [0.09s]\n",
      "epoch 1 [10.03s]:  training loss=0.9748218059539795                                     \n",
      "epoch 2 [10.93s]:  training loss=0.8808256983757019                                     \n",
      "epoch 3 [10.86s]:  training loss=0.7969064712524414                                     \n",
      "epoch 4 [10.49s]:  training loss=0.7399306297302246                                     \n",
      "epoch 5 [10.92s]: training loss=0.6854076981544495  validation ndcg@10=0.032669004666933094 [0.17s]\n",
      "epoch 6 [11.27s]:  training loss=0.6484931111335754                                     \n",
      "epoch 7 [11.14s]:  training loss=0.6098050475120544                                     \n",
      "epoch 8 [10.98s]:  training loss=0.5794623494148254                                     \n",
      "epoch 9 [10.96s]:  training loss=0.5481387972831726                                     \n",
      "epoch 10 [11.81s]: training loss=0.5240349769592285  validation ndcg@10=0.04691415576773076 [0.21s]\n",
      "epoch 11 [11.55s]:  training loss=0.5044876337051392                                    \n",
      "epoch 12 [10.84s]:  training loss=0.48530593514442444                                   \n",
      "epoch 13 [10.8s]:  training loss=0.47951579093933105                                    \n",
      "epoch 14 [11.02s]:  training loss=0.4718088209629059                                    \n",
      "epoch 15 [11.0s]: training loss=0.45852819085121155  validation ndcg@10=0.054736048047016435 [0.17s]\n",
      "epoch 16 [10.88s]:  training loss=0.4545589089393616                                    \n",
      "epoch 17 [11.25s]:  training loss=0.441582590341568                                     \n",
      "epoch 18 [11.05s]:  training loss=0.44349974393844604                                   \n",
      "epoch 19 [10.82s]:  training loss=0.43908342719078064                                   \n",
      "epoch 20 [10.88s]: training loss=0.42975783348083496  validation ndcg@10=0.05591827758637572 [0.17s]\n",
      "epoch 21 [11.08s]:  training loss=0.43102285265922546                                   \n",
      "epoch 22 [11.04s]:  training loss=0.4268508851528168                                    \n",
      "epoch 23 [11.07s]:  training loss=0.4262676239013672                                    \n",
      "epoch 24 [11.18s]:  training loss=0.42611417174339294                                   \n",
      "epoch 25 [10.75s]: training loss=0.4176003038883209  validation ndcg@10=0.0532907082678327 [0.16s]\n",
      "epoch 26 [10.71s]:  training loss=0.425926148891449                                     \n",
      "epoch 27 [10.78s]:  training loss=0.4195123016834259                                    \n",
      "epoch 28 [10.93s]:  training loss=0.42252564430236816                                   \n",
      "epoch 29 [11.05s]:  training loss=0.4273054003715515                                    \n",
      "epoch 30 [11.71s]: training loss=0.4290727972984314  validation ndcg@10=0.04972169590807596 [0.23s]\n",
      "epoch 31 [11.07s]:  training loss=0.42024531960487366                                   \n",
      "epoch 32 [12.72s]:  training loss=0.4169493019580841                                    \n",
      "epoch 33 [11.34s]:  training loss=0.4170784652233124                                    \n",
      "epoch 34 [11.11s]:  training loss=0.4123382866382599                                    \n",
      "epoch 35 [11.21s]: training loss=0.41216152906417847  validation ndcg@10=0.06050922852808456 [0.17s]\n",
      "epoch 36 [10.76s]:  training loss=0.4106321334838867                                    \n",
      "epoch 37 [10.87s]:  training loss=0.4101073741912842                                    \n",
      "epoch 38 [10.84s]:  training loss=0.412910521030426                                     \n",
      "epoch 39 [11.0s]:  training loss=0.4126892685890198                                     \n",
      "epoch 40 [10.91s]: training loss=0.4151570796966553  validation ndcg@10=0.05804702028773023 [0.18s]\n",
      "epoch 41 [11.14s]:  training loss=0.4133618175983429                                    \n",
      "epoch 42 [10.79s]:  training loss=0.4018104374408722                                    \n",
      "epoch 43 [10.79s]:  training loss=0.4050801396369934                                    \n",
      "epoch 44 [10.89s]:  training loss=0.4020535945892334                                    \n",
      "epoch 45 [11.0s]: training loss=0.4121205806732178  validation ndcg@10=0.06373912201735699 [0.19s]\n",
      "epoch 46 [10.94s]:  training loss=0.40399470925331116                                   \n",
      "epoch 47 [10.89s]:  training loss=0.4005568325519562                                    \n",
      "epoch 48 [11.08s]:  training loss=0.40449321269989014                                   \n",
      "epoch 49 [10.75s]:  training loss=0.4016039967536926                                    \n",
      "epoch 50 [11.21s]: training loss=0.3996170163154602  validation ndcg@10=0.0612311246450392 [0.21s]\n",
      "epoch 51 [10.93s]:  training loss=0.39977070689201355                                   \n",
      "epoch 52 [10.82s]:  training loss=0.39612147212028503                                   \n",
      "epoch 53 [11.0s]:  training loss=0.39920106530189514                                    \n",
      "epoch 54 [10.75s]:  training loss=0.3976403474807739                                    \n",
      "epoch 55 [11.24s]: training loss=0.3964794874191284  validation ndcg@10=0.06161186512300622 [0.2s]\n",
      "epoch 56 [11.16s]:  training loss=0.39080554246902466                                   \n",
      "epoch 57 [10.96s]:  training loss=0.39084768295288086                                   \n",
      "epoch 58 [11.37s]:  training loss=0.3886612355709076                                    \n",
      "epoch 59 [11.08s]:  training loss=0.39078208804130554                                   \n",
      "epoch 60 [11.18s]: training loss=0.3834049105644226  validation ndcg@10=0.0627493776141054 [0.17s]\n",
      "epoch 61 [11.07s]:  training loss=0.3894117474555969                                    \n",
      "epoch 62 [11.23s]:  training loss=0.39156654477119446                                   \n",
      "epoch 63 [10.8s]:  training loss=0.3880355656147003                                     \n",
      "epoch 64 [10.98s]:  training loss=0.38599708676338196                                   \n",
      "epoch 65 [10.97s]: training loss=0.38960570096969604  validation ndcg@10=0.0606018505459481 [0.17s]\n",
      "epoch 66 [10.87s]:  training loss=0.38590893149375916                                   \n",
      "epoch 67 [10.9s]:  training loss=0.38696667551994324                                    \n",
      "epoch 68 [11.3s]:  training loss=0.38525381684303284                                    \n",
      "epoch 69 [11.1s]:  training loss=0.38068902492523193                                    \n",
      "epoch 70 [13.23s]: training loss=0.38253554701805115  validation ndcg@10=0.07436800234247684 [0.17s]\n",
      "epoch 71 [10.86s]:  training loss=0.3865067958831787                                    \n",
      "epoch 72 [10.73s]:  training loss=0.3861480951309204                                    \n",
      "epoch 73 [11.17s]:  training loss=0.3780009150505066                                    \n",
      "epoch 74 [11.09s]:  training loss=0.38216713070869446                                   \n",
      "epoch 75 [11.0s]: training loss=0.3829699754714966  validation ndcg@10=0.0657597157390191 [0.21s]\n",
      "epoch 76 [11.19s]:  training loss=0.3750692903995514                                    \n",
      "epoch 77 [10.92s]:  training loss=0.3777164816856384                                    \n",
      "epoch 78 [11.2s]:  training loss=0.3799399733543396                                     \n",
      "epoch 79 [11.47s]:  training loss=0.3778527081012726                                    \n",
      "epoch 80 [10.85s]: training loss=0.3728645145893097  validation ndcg@10=0.06690511287756293 [0.16s]\n",
      "epoch 81 [11.05s]:  training loss=0.37650156021118164                                   \n",
      "epoch 82 [11.2s]:  training loss=0.3733581602573395                                     \n",
      "epoch 83 [10.86s]:  training loss=0.3759576976299286                                    \n",
      "epoch 84 [10.87s]:  training loss=0.3754652738571167                                    \n",
      "epoch 85 [11.16s]: training loss=0.36843788623809814  validation ndcg@10=0.0698226483464537 [0.16s]\n",
      "epoch 86 [11.1s]:  training loss=0.3709765374660492                                     \n",
      "epoch 87 [10.7s]:  training loss=0.37368783354759216                                    \n",
      "epoch 88 [10.77s]:  training loss=0.36891841888427734                                   \n",
      "epoch 89 [11.08s]:  training loss=0.37349915504455566                                   \n",
      "epoch 90 [11.12s]: training loss=0.3629119098186493  validation ndcg@10=0.0721846098405704 [0.19s]\n",
      "epoch 91 [10.84s]:  training loss=0.37040984630584717                                   \n",
      "epoch 92 [11.07s]:  training loss=0.36198070645332336                                   \n",
      "epoch 93 [10.97s]:  training loss=0.37377500534057617                                   \n",
      "epoch 94 [10.75s]:  training loss=0.36324313282966614                                   \n",
      "epoch 95 [11.21s]: training loss=0.3657854199409485  validation ndcg@10=0.06549764280423415 [0.19s]\n",
      "epoch 1 [9.03s]:  training loss=0.9596317410469055                                      \n",
      "epoch 2 [9.03s]:  training loss=0.816484808921814                                       \n",
      "epoch 3 [8.43s]:  training loss=0.7437089085578918                                      \n",
      "epoch 4 [8.58s]:  training loss=0.674752414226532                                       \n",
      "epoch 5 [8.49s]: training loss=0.6326541900634766  validation ndcg@10=0.03814485658617621 [0.1s]\n",
      "epoch 6 [8.17s]:  training loss=0.5821307301521301                                      \n",
      "epoch 7 [8.48s]:  training loss=0.5514874458312988                                      \n",
      "epoch 8 [8.35s]:  training loss=0.5284460783004761                                      \n",
      "epoch 9 [8.14s]:  training loss=0.4940357804298401                                      \n",
      "epoch 10 [8.6s]: training loss=0.47699448466300964  validation ndcg@10=0.05395307256816427 [0.12s]\n",
      "epoch 11 [8.51s]:  training loss=0.46146634221076965                                    \n",
      "epoch 12 [8.55s]:  training loss=0.4506629705429077                                     \n",
      "epoch 13 [8.5s]:  training loss=0.4446122646331787                                      \n",
      "epoch 14 [8.36s]:  training loss=0.4347144067287445                                     \n",
      "epoch 15 [10.42s]: training loss=0.4329342842102051  validation ndcg@10=0.05713821574109075 [0.12s]\n",
      "epoch 16 [8.45s]:  training loss=0.43026870489120483                                    \n",
      "epoch 17 [8.31s]:  training loss=0.42375731468200684                                    \n",
      "epoch 18 [8.17s]:  training loss=0.42845407128334045                                    \n",
      "epoch 19 [8.22s]:  training loss=0.4310970604419708                                     \n",
      "epoch 20 [8.28s]: training loss=0.4215557277202606  validation ndcg@10=0.05201358402052103 [0.1s]\n",
      "epoch 21 [8.5s]:  training loss=0.419135183095932                                       \n",
      "epoch 22 [8.55s]:  training loss=0.4164777398109436                                     \n",
      "epoch 23 [8.48s]:  training loss=0.4143517315387726                                     \n",
      "epoch 24 [8.34s]:  training loss=0.4189170002937317                                     \n",
      "epoch 25 [8.29s]: training loss=0.4150903820991516  validation ndcg@10=0.05787276441390108 [0.12s]\n",
      "epoch 26 [8.56s]:  training loss=0.41233572363853455                                    \n",
      "epoch 27 [8.26s]:  training loss=0.41224125027656555                                    \n",
      "epoch 28 [8.34s]:  training loss=0.40059807896614075                                    \n",
      "epoch 29 [8.2s]:  training loss=0.40407595038414                                        \n",
      "epoch 30 [8.28s]: training loss=0.40771016478538513  validation ndcg@10=0.061815827593571585 [0.1s]\n",
      "epoch 31 [8.29s]:  training loss=0.3990602493286133                                     \n",
      "epoch 32 [8.27s]:  training loss=0.3991146385669708                                     \n",
      "epoch 33 [8.72s]:  training loss=0.41077300906181335                                    \n",
      "epoch 34 [8.7s]:  training loss=0.4002596139907837                                      \n",
      "epoch 35 [8.52s]: training loss=0.4004420340061188  validation ndcg@10=0.055116300019253654 [0.11s]\n",
      "epoch 36 [8.52s]:  training loss=0.4028102159500122                                     \n",
      "epoch 37 [8.63s]:  training loss=0.39917275309562683                                    \n",
      "epoch 38 [8.42s]:  training loss=0.3995296061038971                                     \n",
      "epoch 39 [8.74s]:  training loss=0.39349523186683655                                    \n",
      "epoch 40 [8.67s]: training loss=0.3975223898887634  validation ndcg@10=0.06829891031070025 [0.12s]\n",
      "epoch 41 [9.07s]:  training loss=0.3952038586139679                                     \n",
      "epoch 42 [8.76s]:  training loss=0.3864922523498535                                     \n",
      "epoch 43 [8.45s]:  training loss=0.3899845778942108                                     \n",
      "epoch 44 [8.44s]:  training loss=0.3875406086444855                                     \n",
      "epoch 45 [8.51s]: training loss=0.3861895501613617  validation ndcg@10=0.07116737683962938 [0.11s]\n",
      "epoch 46 [8.51s]:  training loss=0.38804498314857483                                    \n",
      "epoch 47 [8.35s]:  training loss=0.38612812757492065                                    \n",
      "epoch 48 [8.45s]:  training loss=0.3797740340232849                                     \n",
      "epoch 49 [8.5s]:  training loss=0.3832206130027771                                      \n",
      "epoch 50 [8.38s]: training loss=0.3824230432510376  validation ndcg@10=0.06875419766342591 [0.1s]\n",
      "epoch 51 [8.36s]:  training loss=0.3772922158241272                                     \n",
      "epoch 52 [8.3s]:  training loss=0.37930983304977417                                     \n",
      "epoch 53 [8.25s]:  training loss=0.37971097230911255                                    \n",
      "epoch 54 [8.48s]:  training loss=0.3755019009113312                                     \n",
      "epoch 55 [8.34s]: training loss=0.3743695318698883  validation ndcg@10=0.06451836093771152 [0.13s]\n",
      "epoch 56 [8.33s]:  training loss=0.3729430139064789                                     \n",
      "epoch 57 [8.35s]:  training loss=0.3734329342842102                                     \n",
      "epoch 58 [8.41s]:  training loss=0.37410977482795715                                    \n",
      "epoch 59 [8.35s]:  training loss=0.37541401386260986                                    \n",
      "epoch 60 [8.35s]: training loss=0.3726145625114441  validation ndcg@10=0.07297482925723459 [0.11s]\n",
      "epoch 61 [8.34s]:  training loss=0.36498263478279114                                    \n",
      "epoch 62 [8.26s]:  training loss=0.36798909306526184                                    \n",
      "epoch 63 [10.42s]:  training loss=0.36885538697242737                                   \n",
      "epoch 64 [8.43s]:  training loss=0.3644196093082428                                     \n",
      "epoch 65 [8.31s]: training loss=0.3622405230998993  validation ndcg@10=0.0709903765072734 [0.1s]\n",
      "epoch 66 [8.29s]:  training loss=0.3621966540813446                                     \n",
      "epoch 67 [8.28s]:  training loss=0.3635448217391968                                     \n",
      "epoch 68 [8.38s]:  training loss=0.3650813102722168                                     \n",
      "epoch 69 [8.2s]:  training loss=0.3606981635093689                                      \n",
      "epoch 70 [8.28s]: training loss=0.3621583580970764  validation ndcg@10=0.07538345708939945 [0.1s]\n",
      "epoch 71 [8.26s]:  training loss=0.36174851655960083                                    \n",
      "epoch 72 [8.34s]:  training loss=0.3573647141456604                                     \n",
      "epoch 73 [8.33s]:  training loss=0.3580099642276764                                     \n",
      "epoch 74 [8.44s]:  training loss=0.3550752103328705                                     \n",
      "epoch 75 [8.47s]: training loss=0.3589079976081848  validation ndcg@10=0.07739231679959437 [0.1s]\n",
      "epoch 76 [8.24s]:  training loss=0.3531789481639862                                     \n",
      "epoch 77 [8.32s]:  training loss=0.36112403869628906                                    \n",
      "epoch 78 [8.43s]:  training loss=0.35433241724967957                                    \n",
      "epoch 79 [8.27s]:  training loss=0.35359618067741394                                    \n",
      "epoch 80 [8.24s]: training loss=0.35461241006851196  validation ndcg@10=0.07262477860737393 [0.1s]\n",
      "epoch 81 [8.38s]:  training loss=0.3512071967124939                                     \n",
      "epoch 82 [8.4s]:  training loss=0.3544856011867523                                      \n",
      "epoch 83 [8.4s]:  training loss=0.35506027936935425                                     \n",
      "epoch 84 [8.26s]:  training loss=0.3496702015399933                                     \n",
      "epoch 85 [8.36s]: training loss=0.355762243270874  validation ndcg@10=0.06931896829008309 [0.12s]\n",
      "epoch 86 [8.52s]:  training loss=0.34972381591796875                                    \n",
      "epoch 87 [8.35s]:  training loss=0.34659722447395325                                    \n",
      "epoch 88 [8.16s]:  training loss=0.3536127805709839                                     \n",
      "epoch 89 [8.32s]:  training loss=0.3489806652069092                                     \n",
      "epoch 90 [8.43s]: training loss=0.352430522441864  validation ndcg@10=0.06726498694968305 [0.11s]\n",
      "epoch 91 [8.3s]:  training loss=0.35048651695251465                                     \n",
      "epoch 92 [8.26s]:  training loss=0.3519516885280609                                     \n",
      "epoch 93 [8.36s]:  training loss=0.3543199896812439                                     \n",
      "epoch 94 [8.32s]:  training loss=0.347247838973999                                      \n",
      "epoch 95 [8.13s]: training loss=0.3431287705898285  validation ndcg@10=0.07309415603126218 [0.11s]\n",
      "epoch 96 [8.53s]:  training loss=0.3469783365726471                                     \n",
      "epoch 97 [8.66s]:  training loss=0.3504449725151062                                     \n",
      "epoch 98 [8.17s]:  training loss=0.34574320912361145                                    \n",
      "epoch 99 [8.37s]:  training loss=0.3436224162578583                                     \n",
      "epoch 100 [8.4s]: training loss=0.343524694442749  validation ndcg@10=0.06955476169831887 [0.1s]\n",
      "epoch 1 [23.11s]:  training loss=0.8450099229812622                                     \n",
      "epoch 2 [23.62s]:  training loss=0.6349759101867676                                     \n",
      "epoch 3 [25.1s]:  training loss=0.5238490104675293                                      \n",
      "epoch 4 [27.46s]:  training loss=0.47007885575294495                                    \n",
      "epoch 5 [24.0s]: training loss=0.4484563171863556  validation ndcg@10=0.05417531975460806 [0.31s]\n",
      "epoch 6 [24.95s]:  training loss=0.4341229796409607                                     \n",
      "epoch 7 [25.12s]:  training loss=0.4274176061153412                                     \n",
      "epoch 8 [24.8s]:  training loss=0.4246315062046051                                      \n",
      "epoch 9 [24.47s]:  training loss=0.41489139199256897                                    \n",
      "epoch 10 [24.65s]: training loss=0.41107678413391113  validation ndcg@10=0.061112175086239416 [0.36s]\n",
      "epoch 11 [24.9s]:  training loss=0.41318798065185547                                    \n",
      "epoch 12 [25.03s]:  training loss=0.40570101141929626                                   \n",
      "epoch 13 [25.73s]:  training loss=0.4056110084056854                                    \n",
      "epoch 14 [24.89s]:  training loss=0.40150362253189087                                   \n",
      "epoch 15 [25.8s]: training loss=0.39768317341804504  validation ndcg@10=0.06566439036888465 [0.31s]\n",
      "epoch 16 [25.43s]:  training loss=0.3952236473560333                                    \n",
      "epoch 17 [25.4s]:  training loss=0.38852688670158386                                    \n",
      "epoch 18 [24.09s]:  training loss=0.3819449543952942                                    \n",
      "epoch 19 [24.51s]:  training loss=0.3839651644229889                                    \n",
      "epoch 20 [24.51s]: training loss=0.3834834694862366  validation ndcg@10=0.07437165331824362 [0.3s]\n",
      "epoch 21 [25.38s]:  training loss=0.37711793184280396                                   \n",
      "epoch 22 [24.61s]:  training loss=0.3782423436641693                                    \n",
      "epoch 23 [26.41s]:  training loss=0.3706742227077484                                    \n",
      "epoch 24 [25.58s]:  training loss=0.3684525787830353                                    \n",
      "epoch 25 [25.69s]: training loss=0.3672344982624054  validation ndcg@10=0.06545491137453036 [0.35s]\n",
      "epoch 26 [24.96s]:  training loss=0.3614959716796875                                    \n",
      "epoch 27 [25.64s]:  training loss=0.3640115559101105                                    \n",
      "epoch 28 [26.14s]:  training loss=0.3596993684768677                                    \n",
      "epoch 29 [25.53s]:  training loss=0.35808393359184265                                   \n",
      "epoch 30 [26.49s]: training loss=0.3580898940563202  validation ndcg@10=0.0857887634682054 [0.33s]\n",
      "epoch 31 [25.35s]:  training loss=0.36031201481819153                                   \n",
      "epoch 32 [26.31s]:  training loss=0.34974783658981323                                   \n",
      "epoch 33 [25.27s]:  training loss=0.3470539450645447                                    \n",
      "epoch 34 [25.81s]:  training loss=0.3422158658504486                                    \n",
      "epoch 35 [25.83s]: training loss=0.34895727038383484  validation ndcg@10=0.064611509053732 [0.35s]\n",
      "epoch 36 [25.53s]:  training loss=0.35639727115631104                                   \n",
      "epoch 37 [25.99s]:  training loss=0.348010390996933                                     \n",
      "epoch 38 [25.77s]:  training loss=0.34388914704322815                                   \n",
      "epoch 39 [25.86s]:  training loss=0.3427477777004242                                    \n",
      "epoch 40 [25.2s]: training loss=0.34837237000465393  validation ndcg@10=0.07998826330585702 [0.35s]\n",
      "epoch 41 [26.47s]:  training loss=0.3444422483444214                                    \n",
      "epoch 42 [25.95s]:  training loss=0.34142354130744934                                   \n",
      "epoch 43 [25.48s]:  training loss=0.3399953842163086                                    \n",
      "epoch 44 [25.67s]:  training loss=0.33921512961387634                                   \n",
      "epoch 45 [26.48s]: training loss=0.34117504954338074  validation ndcg@10=0.07322288584130927 [0.36s]\n",
      "epoch 46 [25.19s]:  training loss=0.33939510583877563                                   \n",
      "epoch 47 [24.93s]:  training loss=0.33587148785591125                                   \n",
      "epoch 48 [25.59s]:  training loss=0.33283188939094543                                   \n",
      "epoch 49 [26.29s]:  training loss=0.336357444524765                                     \n",
      "epoch 50 [25.62s]: training loss=0.3308969736099243  validation ndcg@10=0.0661609432352979 [0.34s]\n",
      "epoch 51 [25.56s]:  training loss=0.33360210061073303                                   \n",
      "epoch 52 [25.91s]:  training loss=0.3360525965690613                                    \n",
      "epoch 53 [26.76s]:  training loss=0.32984691858291626                                   \n",
      "epoch 54 [25.43s]:  training loss=0.3299131393432617                                    \n",
      "epoch 55 [25.37s]: training loss=0.3317274749279022  validation ndcg@10=0.07675336061270319 [0.33s]\n",
      "epoch 1 [14.77s]:  training loss=0.8439170122146606                                     \n",
      "epoch 2 [14.45s]:  training loss=0.6045099496841431                                    \n",
      "epoch 3 [14.45s]:  training loss=0.49605444073677063                                   \n",
      "epoch 4 [14.56s]:  training loss=0.4541747272014618                                    \n",
      "epoch 5 [14.57s]: training loss=0.43655794858932495  validation ndcg@10=0.055450969049821774 [0.21s]\n",
      "epoch 6 [14.49s]:  training loss=0.4262927770614624                                    \n",
      "epoch 7 [14.48s]:  training loss=0.42320510745048523                                   \n",
      "epoch 8 [14.73s]:  training loss=0.4196985065937042                                    \n",
      "epoch 9 [14.41s]:  training loss=0.40941324830055237                                   \n",
      "epoch 10 [14.41s]: training loss=0.4074178636074066  validation ndcg@10=0.06342894417481157 [0.21s]\n",
      "epoch 11 [14.52s]:  training loss=0.4035263955593109                                   \n",
      "epoch 12 [14.52s]:  training loss=0.39746397733688354                                  \n",
      "epoch 13 [14.74s]:  training loss=0.3899444043636322                                   \n",
      "epoch 14 [14.71s]:  training loss=0.3959793448448181                                   \n",
      "epoch 15 [14.66s]: training loss=0.3882704973220825  validation ndcg@10=0.06723404278435556 [0.21s]\n",
      "epoch 16 [14.66s]:  training loss=0.38650354743003845                                  \n",
      "epoch 17 [14.51s]:  training loss=0.388831228017807                                    \n",
      "epoch 18 [14.67s]:  training loss=0.3766371011734009                                   \n",
      "epoch 19 [14.57s]:  training loss=0.3770159184932709                                   \n",
      "epoch 20 [14.77s]: training loss=0.3709087073802948  validation ndcg@10=0.06659122502359414 [0.22s]\n",
      "epoch 21 [14.99s]:  training loss=0.3704148828983307                                   \n",
      "epoch 22 [14.67s]:  training loss=0.36314159631729126                                  \n",
      "epoch 23 [14.64s]:  training loss=0.35976502299308777                                  \n",
      "epoch 24 [14.9s]:  training loss=0.3643859624862671                                    \n",
      "epoch 25 [14.56s]: training loss=0.3598693609237671  validation ndcg@10=0.06501665095188722 [0.2s]\n",
      "epoch 26 [14.68s]:  training loss=0.3574821650981903                                   \n",
      "epoch 27 [14.66s]:  training loss=0.3536882698535919                                   \n",
      "epoch 28 [15.03s]:  training loss=0.3540239632129669                                   \n",
      "epoch 29 [14.61s]:  training loss=0.3531559705734253                                   \n",
      "epoch 30 [14.39s]: training loss=0.35210204124450684  validation ndcg@10=0.06745163414356124 [0.2s]\n",
      "epoch 31 [14.64s]:  training loss=0.3484349548816681                                   \n",
      "epoch 32 [14.67s]:  training loss=0.3470134735107422                                   \n",
      "epoch 33 [14.64s]:  training loss=0.34790369868278503                                  \n",
      "epoch 34 [14.3s]:  training loss=0.34218335151672363                                   \n",
      "epoch 35 [14.44s]: training loss=0.3435981869697571  validation ndcg@10=0.06579516561709313 [0.2s]\n",
      "epoch 36 [14.52s]:  training loss=0.33847418427467346                                  \n",
      "epoch 37 [14.77s]:  training loss=0.33756837248802185                                  \n",
      "epoch 38 [14.89s]:  training loss=0.34106552600860596                                  \n",
      "epoch 39 [14.43s]:  training loss=0.34077173471450806                                  \n",
      "epoch 40 [14.47s]: training loss=0.33706676959991455  validation ndcg@10=0.060600137416074465 [0.19s]\n",
      "epoch 41 [14.32s]:  training loss=0.334297776222229                                    \n",
      "epoch 42 [14.89s]:  training loss=0.33527234196662903                                  \n",
      "epoch 43 [14.57s]:  training loss=0.33114585280418396                                  \n",
      "epoch 44 [14.57s]:  training loss=0.3335558772087097                                   \n",
      "epoch 45 [16.59s]: training loss=0.33039677143096924  validation ndcg@10=0.06202443929148993 [0.19s]\n",
      "epoch 46 [14.31s]:  training loss=0.32858937978744507                                  \n",
      "epoch 47 [14.18s]:  training loss=0.3289909064769745                                   \n",
      "epoch 48 [14.37s]:  training loss=0.32929137349128723                                  \n",
      "epoch 49 [14.64s]:  training loss=0.32952505350112915                                  \n",
      "epoch 50 [14.63s]: training loss=0.32539281249046326  validation ndcg@10=0.06482110632790958 [0.22s]\n",
      "epoch 51 [14.85s]:  training loss=0.3268483579158783                                   \n",
      "epoch 52 [14.34s]:  training loss=0.3219486474990845                                   \n",
      "epoch 53 [14.68s]:  training loss=0.3337863087654114                                   \n",
      "epoch 54 [14.59s]:  training loss=0.32344454526901245                                  \n",
      "epoch 55 [14.71s]: training loss=0.32343217730522156  validation ndcg@10=0.059376018694997534 [0.2s]\n",
      "epoch 1 [16.73s]:  training loss=0.9697629809379578                                    \n",
      "epoch 2 [16.71s]:  training loss=0.8443405032157898                                    \n",
      "epoch 3 [15.51s]:  training loss=0.7631887197494507                                    \n",
      "epoch 4 [14.56s]:  training loss=0.7108273506164551                                    \n",
      "epoch 5 [14.31s]: training loss=0.6523931622505188  validation ndcg@10=0.03668894835493709 [0.19s]\n",
      "epoch 6 [14.38s]:  training loss=0.6136441230773926                                    \n",
      "epoch 7 [14.32s]:  training loss=0.568697452545166                                     \n",
      "epoch 8 [14.42s]:  training loss=0.5409993529319763                                    \n",
      "epoch 9 [14.28s]:  training loss=0.5064350962638855                                    \n",
      "epoch 10 [14.4s]: training loss=0.4908452332019806  validation ndcg@10=0.05329921939367574 [0.19s]\n",
      "epoch 11 [14.3s]:  training loss=0.4795280992984772                                    \n",
      "epoch 12 [14.24s]:  training loss=0.46151217818260193                                  \n",
      "epoch 13 [14.46s]:  training loss=0.44517645239830017                                  \n",
      "epoch 14 [14.37s]:  training loss=0.4496150612831116                                   \n",
      "epoch 15 [14.46s]: training loss=0.4438183009624481  validation ndcg@10=0.05566969152166496 [0.18s]\n",
      "epoch 16 [14.37s]:  training loss=0.4317048490047455                                   \n",
      "epoch 17 [14.37s]:  training loss=0.4333042502403259                                   \n",
      "epoch 18 [14.45s]:  training loss=0.43298304080963135                                  \n",
      "epoch 19 [14.33s]:  training loss=0.41998034715652466                                  \n",
      "epoch 20 [14.44s]: training loss=0.42626553773880005  validation ndcg@10=0.05691344412926255 [0.18s]\n",
      "epoch 21 [14.43s]:  training loss=0.42303162813186646                                  \n",
      "epoch 22 [14.41s]:  training loss=0.42445501685142517                                  \n",
      "epoch 23 [14.32s]:  training loss=0.4165593981742859                                   \n",
      "epoch 24 [14.3s]:  training loss=0.41624143719673157                                   \n",
      "epoch 25 [14.32s]: training loss=0.4113781154155731  validation ndcg@10=0.05971772492131309 [0.19s]\n",
      "epoch 26 [14.38s]:  training loss=0.4098314642906189                                   \n",
      "epoch 27 [14.27s]:  training loss=0.40201783180236816                                  \n",
      "epoch 28 [14.22s]:  training loss=0.4086973965167999                                   \n",
      "epoch 29 [14.36s]:  training loss=0.4037850797176361                                   \n",
      "epoch 30 [14.39s]: training loss=0.4066392183303833  validation ndcg@10=0.05931986218724776 [0.21s]\n",
      "epoch 31 [14.34s]:  training loss=0.4110489785671234                                   \n",
      "epoch 32 [14.43s]:  training loss=0.4054545760154724                                   \n",
      "epoch 33 [14.39s]:  training loss=0.4014585614204407                                   \n",
      "epoch 34 [14.56s]:  training loss=0.400664746761322                                    \n",
      "epoch 35 [14.61s]: training loss=0.4032249450683594  validation ndcg@10=0.05870612363278086 [0.16s]\n",
      "epoch 36 [14.35s]:  training loss=0.40090668201446533                                  \n",
      "epoch 37 [14.39s]:  training loss=0.4033748507499695                                   \n",
      "epoch 38 [14.21s]:  training loss=0.3944392502307892                                   \n",
      "epoch 39 [14.43s]:  training loss=0.39403268694877625                                  \n",
      "epoch 40 [17.12s]: training loss=0.3959895968437195  validation ndcg@10=0.05657156093425252 [0.23s]\n",
      "epoch 41 [14.89s]:  training loss=0.39357176423072815                                  \n",
      "epoch 42 [14.26s]:  training loss=0.3961591124534607                                   \n",
      "epoch 43 [14.27s]:  training loss=0.3863779604434967                                   \n",
      "epoch 44 [14.29s]:  training loss=0.3957442343235016                                   \n",
      "epoch 45 [14.27s]: training loss=0.3936445713043213  validation ndcg@10=0.06611138807299015 [0.2s]\n",
      "epoch 46 [14.37s]:  training loss=0.39047110080718994                                  \n",
      "epoch 47 [14.37s]:  training loss=0.39148861169815063                                  \n",
      "epoch 48 [14.25s]:  training loss=0.3861074149608612                                   \n",
      "epoch 49 [14.31s]:  training loss=0.3857214152812958                                   \n",
      "epoch 50 [14.32s]: training loss=0.38549119234085083  validation ndcg@10=0.06046058515767029 [0.17s]\n",
      "epoch 51 [14.43s]:  training loss=0.38260504603385925                                  \n",
      "epoch 52 [14.36s]:  training loss=0.38453468680381775                                  \n",
      "epoch 53 [14.47s]:  training loss=0.3829832077026367                                   \n",
      "epoch 54 [14.27s]:  training loss=0.38143500685691833                                  \n",
      "epoch 55 [14.21s]: training loss=0.3846454620361328  validation ndcg@10=0.0697554156082908 [0.19s]\n",
      "epoch 56 [14.68s]:  training loss=0.3828805387020111                                   \n",
      "epoch 57 [14.31s]:  training loss=0.37546637654304504                                  \n",
      "epoch 58 [14.59s]:  training loss=0.38139501214027405                                  \n",
      "epoch 59 [14.37s]:  training loss=0.37461304664611816                                  \n",
      "epoch 60 [14.5s]: training loss=0.3727874159812927  validation ndcg@10=0.07190382348252207 [0.18s]\n",
      "epoch 61 [14.37s]:  training loss=0.3775251507759094                                   \n",
      "epoch 62 [14.35s]:  training loss=0.3723897933959961                                   \n",
      "epoch 63 [14.62s]:  training loss=0.3689303696155548                                   \n",
      "epoch 64 [14.46s]:  training loss=0.3728439211845398                                   \n",
      "epoch 65 [14.55s]: training loss=0.3735388517379761  validation ndcg@10=0.07184511205043828 [0.19s]\n",
      "epoch 66 [14.37s]:  training loss=0.36392760276794434                                  \n",
      "epoch 67 [14.34s]:  training loss=0.3650050461292267                                   \n",
      "epoch 68 [14.37s]:  training loss=0.3625164330005646                                   \n",
      "epoch 69 [14.35s]:  training loss=0.3580697476863861                                   \n",
      "epoch 70 [14.55s]: training loss=0.3685154318809509  validation ndcg@10=0.07666992894052131 [0.19s]\n",
      "epoch 71 [14.39s]:  training loss=0.364838182926178                                    \n",
      "epoch 72 [14.41s]:  training loss=0.36263301968574524                                  \n",
      "epoch 73 [14.33s]:  training loss=0.3677007555961609                                   \n",
      "epoch 74 [14.37s]:  training loss=0.3699095547199249                                   \n",
      "epoch 75 [14.33s]: training loss=0.3623323142528534  validation ndcg@10=0.06619442780503365 [0.19s]\n",
      "epoch 76 [14.4s]:  training loss=0.36079782247543335                                   \n",
      "epoch 77 [14.42s]:  training loss=0.3540000915527344                                   \n",
      "epoch 78 [14.91s]:  training loss=0.3560559153556824                                   \n",
      "epoch 79 [14.49s]:  training loss=0.3552965819835663                                   \n",
      "epoch 80 [14.34s]: training loss=0.3597734868526459  validation ndcg@10=0.06933389402799815 [0.18s]\n",
      "epoch 81 [14.27s]:  training loss=0.357555627822876                                    \n",
      "epoch 82 [14.46s]:  training loss=0.35910388827323914                                  \n",
      "epoch 83 [14.22s]:  training loss=0.35473117232322693                                  \n",
      "epoch 84 [14.38s]:  training loss=0.35090604424476624                                  \n",
      "epoch 85 [14.25s]: training loss=0.3573831021785736  validation ndcg@10=0.07139393037721845 [0.19s]\n",
      "epoch 86 [14.3s]:  training loss=0.3584999144077301                                    \n",
      "epoch 87 [14.3s]:  training loss=0.35473379492759705                                   \n",
      "epoch 88 [14.3s]:  training loss=0.35587552189826965                                   \n",
      "epoch 89 [14.44s]:  training loss=0.35201114416122437                                  \n",
      "epoch 90 [15.82s]: training loss=0.35315048694610596  validation ndcg@10=0.07114277585714503 [0.2s]\n",
      "epoch 91 [14.49s]:  training loss=0.35236307978630066                                  \n",
      "epoch 92 [14.53s]:  training loss=0.3555470407009125                                   \n",
      "epoch 93 [14.56s]:  training loss=0.35915473103523254                                  \n",
      "epoch 94 [14.52s]:  training loss=0.3525639474391937                                   \n",
      "epoch 95 [14.76s]: training loss=0.3559187650680542  validation ndcg@10=0.06831777717687025 [0.24s]\n",
      "epoch 1 [6.01s]:  training loss=1.1038423776626587                                      \n",
      "epoch 2 [5.95s]:  training loss=1.0886882543563843                                      \n",
      "epoch 3 [6.05s]:  training loss=1.0477776527404785                                      \n",
      "epoch 4 [6.05s]:  training loss=1.04338800907135                                        \n",
      "epoch 5 [6.31s]: training loss=1.0142590999603271  validation ndcg@10=0.01800545581390901 [0.13s]\n",
      "epoch 6 [6.14s]:  training loss=1.0011999607086182                                      \n",
      "epoch 7 [6.17s]:  training loss=0.9854881763458252                                      \n",
      "epoch 8 [6.24s]:  training loss=0.9728864431381226                                      \n",
      "epoch 9 [6.19s]:  training loss=0.955800473690033                                       \n",
      "epoch 10 [6.18s]: training loss=0.9449020624160767  validation ndcg@10=0.01821862309632434 [0.1s]\n",
      "epoch 11 [6.11s]:  training loss=0.9364382028579712                                     \n",
      "epoch 12 [6.22s]:  training loss=0.9332252740859985                                     \n",
      "epoch 13 [6.06s]:  training loss=0.9176895022392273                                     \n",
      "epoch 14 [6.0s]:  training loss=0.9095813035964966                                      \n",
      "epoch 15 [6.4s]: training loss=0.9022541046142578  validation ndcg@10=0.021152758394898934 [0.09s]\n",
      "epoch 16 [6.39s]:  training loss=0.8860732913017273                                     \n",
      "epoch 17 [6.06s]:  training loss=0.8811240792274475                                     \n",
      "epoch 18 [6.24s]:  training loss=0.8767088055610657                                     \n",
      "epoch 19 [5.99s]:  training loss=0.8649723529815674                                     \n",
      "epoch 20 [6.07s]: training loss=0.8576222658157349  validation ndcg@10=0.021069447490908805 [0.09s]\n",
      "epoch 21 [6.01s]:  training loss=0.8476366996765137                                     \n",
      "epoch 22 [6.03s]:  training loss=0.8386197090148926                                     \n",
      "epoch 23 [6.01s]:  training loss=0.8353092670440674                                     \n",
      "epoch 24 [5.97s]:  training loss=0.8317037224769592                                     \n",
      "epoch 25 [5.88s]: training loss=0.8130510449409485  validation ndcg@10=0.02560205285704032 [0.1s]\n",
      "epoch 26 [6.13s]:  training loss=0.8203654289245605                                     \n",
      "epoch 27 [5.95s]:  training loss=0.8094462156295776                                     \n",
      "epoch 28 [5.97s]:  training loss=0.8133596777915955                                     \n",
      "epoch 29 [5.91s]:  training loss=0.7929911017417908                                     \n",
      "epoch 30 [5.96s]: training loss=0.7890853881835938  validation ndcg@10=0.027873052292950864 [0.1s]\n",
      "epoch 31 [5.99s]:  training loss=0.7822360992431641                                     \n",
      "epoch 32 [6.06s]:  training loss=0.7795739769935608                                     \n",
      "epoch 33 [6.02s]:  training loss=0.7697082757949829                                     \n",
      "epoch 34 [5.95s]:  training loss=0.7706965208053589                                     \n",
      "epoch 35 [6.16s]: training loss=0.7601833343505859  validation ndcg@10=0.032445222529481996 [0.09s]\n",
      "epoch 36 [5.98s]:  training loss=0.7592789530754089                                     \n",
      "epoch 37 [6.18s]:  training loss=0.7525755167007446                                     \n",
      "epoch 38 [6.0s]:  training loss=0.7532342076301575                                      \n",
      "epoch 39 [6.03s]:  training loss=0.746325671672821                                      \n",
      "epoch 40 [6.12s]: training loss=0.7299131155014038  validation ndcg@10=0.03175186211050104 [0.12s]\n",
      "epoch 41 [6.21s]:  training loss=0.7313879132270813                                     \n",
      "epoch 42 [6.09s]:  training loss=0.7210947275161743                                     \n",
      "epoch 43 [5.98s]:  training loss=0.7222720980644226                                     \n",
      "epoch 44 [6.16s]:  training loss=0.7196287512779236                                     \n",
      "epoch 45 [6.07s]: training loss=0.7198702692985535  validation ndcg@10=0.03276416904739692 [0.11s]\n",
      "epoch 46 [6.38s]:  training loss=0.7109491229057312                                     \n",
      "epoch 47 [6.2s]:  training loss=0.6955258250236511                                      \n",
      "epoch 48 [6.36s]:  training loss=0.7051620483398438                                     \n",
      "epoch 49 [6.44s]:  training loss=0.6939650177955627                                     \n",
      "epoch 50 [6.25s]: training loss=0.6822167634963989  validation ndcg@10=0.034286174644720446 [0.2s]\n",
      "epoch 51 [6.16s]:  training loss=0.6804965734481812                                     \n",
      "epoch 52 [6.06s]:  training loss=0.6850559711456299                                     \n",
      "epoch 53 [6.24s]:  training loss=0.6764686107635498                                     \n",
      "epoch 54 [6.38s]:  training loss=0.6750184893608093                                     \n",
      "epoch 55 [6.2s]: training loss=0.6629597544670105  validation ndcg@10=0.03619793004254203 [0.1s]\n",
      "epoch 56 [5.96s]:  training loss=0.6666043996810913                                     \n",
      "epoch 57 [6.02s]:  training loss=0.6548470258712769                                     \n",
      "epoch 58 [5.86s]:  training loss=0.6593586802482605                                     \n",
      "epoch 59 [6.05s]:  training loss=0.6516266465187073                                     \n",
      "epoch 60 [6.2s]: training loss=0.6432854533195496  validation ndcg@10=0.038085264348018455 [0.09s]\n",
      "epoch 61 [6.01s]:  training loss=0.6387813687324524                                     \n",
      "epoch 62 [6.04s]:  training loss=0.6388739347457886                                     \n",
      "epoch 63 [6.1s]:  training loss=0.6327905654907227                                      \n",
      "epoch 64 [6.01s]:  training loss=0.6372862458229065                                     \n",
      "epoch 65 [6.26s]: training loss=0.6305097341537476  validation ndcg@10=0.038481201434323695 [0.12s]\n",
      "epoch 66 [6.44s]:  training loss=0.6291413307189941                                     \n",
      "epoch 67 [6.32s]:  training loss=0.6195458769798279                                     \n",
      "epoch 68 [6.44s]:  training loss=0.617707371711731                                      \n",
      "epoch 69 [6.0s]:  training loss=0.618442952632904                                       \n",
      "epoch 70 [6.07s]: training loss=0.6163873076438904  validation ndcg@10=0.04027813935403822 [0.1s]\n",
      "epoch 71 [6.11s]:  training loss=0.6058221459388733                                     \n",
      "epoch 72 [6.09s]:  training loss=0.6119851469993591                                     \n",
      "epoch 73 [6.16s]:  training loss=0.6008531451225281                                     \n",
      "epoch 74 [6.1s]:  training loss=0.5972279906272888                                      \n",
      "epoch 75 [5.92s]: training loss=0.5889284014701843  validation ndcg@10=0.039177344590773364 [0.09s]\n",
      "epoch 76 [6.14s]:  training loss=0.5909818410873413                                     \n",
      "epoch 77 [6.17s]:  training loss=0.5853336453437805                                     \n",
      "epoch 78 [6.18s]:  training loss=0.5826356410980225                                     \n",
      "epoch 79 [6.05s]:  training loss=0.5793895721435547                                     \n",
      "epoch 80 [6.18s]: training loss=0.5738463401794434  validation ndcg@10=0.03974652938117728 [0.1s]\n",
      "epoch 81 [6.24s]:  training loss=0.572908341884613                                      \n",
      "epoch 82 [6.13s]:  training loss=0.563340425491333                                      \n",
      "epoch 83 [6.0s]:  training loss=0.5683565139770508                                      \n",
      "epoch 84 [6.03s]:  training loss=0.5630276799201965                                     \n",
      "epoch 85 [5.97s]: training loss=0.5536079406738281  validation ndcg@10=0.041472051268894645 [0.1s]\n",
      "epoch 86 [6.07s]:  training loss=0.5610908269882202                                     \n",
      "epoch 87 [6.18s]:  training loss=0.551702082157135                                      \n",
      "epoch 88 [6.07s]:  training loss=0.5537343621253967                                     \n",
      "epoch 89 [5.98s]:  training loss=0.5504119396209717                                     \n",
      "epoch 90 [6.18s]: training loss=0.5478541254997253  validation ndcg@10=0.04281491990018428 [0.1s]\n",
      "epoch 91 [6.18s]:  training loss=0.5420953631401062                                     \n",
      "epoch 92 [6.03s]:  training loss=0.5445548892021179                                     \n",
      "epoch 93 [6.11s]:  training loss=0.5368978977203369                                     \n",
      "epoch 94 [6.2s]:  training loss=0.5401989817619324                                      \n",
      "epoch 95 [6.02s]: training loss=0.534301221370697  validation ndcg@10=0.045980352713161046 [0.1s]\n",
      "epoch 96 [6.21s]:  training loss=0.526250422000885                                      \n",
      "epoch 97 [6.23s]:  training loss=0.5206612944602966                                     \n",
      "epoch 98 [6.12s]:  training loss=0.5280418395996094                                     \n",
      "epoch 99 [6.05s]:  training loss=0.5255064964294434                                     \n",
      "epoch 100 [6.0s]: training loss=0.523841381072998  validation ndcg@10=0.046715073545622016 [0.1s]\n",
      "epoch 101 [6.06s]:  training loss=0.5135877728462219                                    \n",
      "epoch 102 [6.09s]:  training loss=0.5199423432350159                                    \n",
      "epoch 103 [6.04s]:  training loss=0.5101908445358276                                    \n",
      "epoch 104 [6.01s]:  training loss=0.5117467045783997                                    \n",
      "epoch 105 [5.9s]: training loss=0.5080664753913879  validation ndcg@10=0.04960559359840566 [0.1s]\n",
      "epoch 106 [5.91s]:  training loss=0.508466362953186                                     \n",
      "epoch 107 [5.99s]:  training loss=0.508931577205658                                     \n",
      "epoch 108 [6.04s]:  training loss=0.5040361881256104                                    \n",
      "epoch 109 [6.46s]:  training loss=0.5053983926773071                                    \n",
      "epoch 110 [6.31s]: training loss=0.49668431282043457  validation ndcg@10=0.050048327682603824 [0.12s]\n",
      "epoch 111 [6.34s]:  training loss=0.4927315413951874                                    \n",
      "epoch 112 [6.22s]:  training loss=0.49812448024749756                                   \n",
      "epoch 113 [5.99s]:  training loss=0.4951202869415283                                    \n",
      "epoch 114 [5.97s]:  training loss=0.4919450879096985                                    \n",
      "epoch 115 [5.93s]: training loss=0.4883415699005127  validation ndcg@10=0.04986165189044596 [0.1s]\n",
      "epoch 116 [5.96s]:  training loss=0.48564666509628296                                   \n",
      "epoch 117 [6.04s]:  training loss=0.4895744323730469                                    \n",
      "epoch 118 [5.93s]:  training loss=0.4899367690086365                                    \n",
      "epoch 119 [6.14s]:  training loss=0.48430562019348145                                   \n",
      "epoch 120 [6.32s]: training loss=0.4838724136352539  validation ndcg@10=0.04744473376820497 [0.1s]\n",
      "epoch 121 [6.36s]:  training loss=0.47841790318489075                                   \n",
      "epoch 122 [6.24s]:  training loss=0.47779449820518494                                   \n",
      "epoch 123 [7.59s]:  training loss=0.4781738221645355                                    \n",
      "epoch 124 [5.83s]:  training loss=0.475676566362381                                     \n",
      "epoch 125 [6.36s]: training loss=0.47895634174346924  validation ndcg@10=0.048380936125104995 [0.1s]\n",
      "epoch 126 [6.11s]:  training loss=0.4677538573741913                                    \n",
      "epoch 127 [6.18s]:  training loss=0.47757217288017273                                   \n",
      "epoch 128 [5.97s]:  training loss=0.4741249680519104                                    \n",
      "epoch 129 [6.13s]:  training loss=0.469323992729187                                     \n",
      "epoch 130 [6.17s]: training loss=0.4678080677986145  validation ndcg@10=0.047203723180183184 [0.11s]\n",
      "epoch 131 [6.09s]:  training loss=0.46685194969177246                                   \n",
      "epoch 132 [5.97s]:  training loss=0.4615268409252167                                    \n",
      "epoch 133 [6.26s]:  training loss=0.46546122431755066                                   \n",
      "epoch 134 [6.04s]:  training loss=0.4668938219547272                                    \n",
      "epoch 135 [6.15s]: training loss=0.46339336037635803  validation ndcg@10=0.04789545899472631 [0.1s]\n",
      "epoch 1 [20.57s]:  training loss=0.731772780418396                                      \n",
      "epoch 2 [21.2s]:  training loss=0.4992952048778534                                     \n",
      "epoch 3 [20.99s]:  training loss=0.4455054998397827                                    \n",
      "epoch 4 [21.27s]:  training loss=0.43430474400520325                                   \n",
      "epoch 5 [21.67s]: training loss=0.42328423261642456  validation ndcg@10=0.06337782844673881 [0.28s]\n",
      "epoch 6 [21.24s]:  training loss=0.4138049781322479                                    \n",
      "epoch 7 [21.37s]:  training loss=0.4113266170024872                                    \n",
      "epoch 8 [21.23s]:  training loss=0.4017811119556427                                    \n",
      "epoch 9 [20.87s]:  training loss=0.3980213403701782                                    \n",
      "epoch 10 [21.55s]: training loss=0.389935702085495  validation ndcg@10=0.07405409405607084 [0.27s]\n",
      "epoch 11 [22.02s]:  training loss=0.3842422664165497                                   \n",
      "epoch 12 [20.28s]:  training loss=0.38262563943862915                                  \n",
      "epoch 13 [21.23s]:  training loss=0.3744800090789795                                   \n",
      "epoch 14 [20.82s]:  training loss=0.3692944645881653                                   \n",
      "epoch 15 [21.07s]: training loss=0.3672386407852173  validation ndcg@10=0.06843056630518307 [0.31s]\n",
      "epoch 16 [21.18s]:  training loss=0.3590892553329468                                   \n",
      "epoch 17 [20.81s]:  training loss=0.358662486076355                                    \n",
      "epoch 18 [21.26s]:  training loss=0.3569236695766449                                   \n",
      "epoch 19 [21.66s]:  training loss=0.3596792221069336                                   \n",
      "epoch 20 [21.4s]: training loss=0.34818536043167114  validation ndcg@10=0.06607480104666447 [0.28s]\n",
      "epoch 21 [20.68s]:  training loss=0.3471733629703522                                   \n",
      "epoch 22 [21.24s]:  training loss=0.3461705148220062                                   \n",
      "epoch 23 [21.33s]:  training loss=0.34366267919540405                                  \n",
      "epoch 24 [21.35s]:  training loss=0.339324027299881                                    \n",
      "epoch 25 [21.29s]: training loss=0.3484151363372803  validation ndcg@10=0.06842309919527373 [0.31s]\n",
      "epoch 26 [20.5s]:  training loss=0.3375609815120697                                    \n",
      "epoch 27 [21.2s]:  training loss=0.33794477581977844                                   \n",
      "epoch 28 [21.3s]:  training loss=0.3317892551422119                                    \n",
      "epoch 29 [21.11s]:  training loss=0.33432623744010925                                  \n",
      "epoch 30 [21.31s]: training loss=0.33658987283706665  validation ndcg@10=0.07376714715757646 [0.31s]\n",
      "epoch 31 [21.21s]:  training loss=0.331735759973526                                    \n",
      "epoch 32 [21.46s]:  training loss=0.32917729020118713                                  \n",
      "epoch 33 [21.12s]:  training loss=0.3258562386035919                                   \n",
      "epoch 34 [20.83s]:  training loss=0.3281882703304291                                   \n",
      "epoch 35 [21.05s]: training loss=0.32034751772880554  validation ndcg@10=0.0579118666774913 [0.4s]\n",
      "epoch 1 [16.22s]:  training loss=0.5421507954597473                                    \n",
      "epoch 2 [15.99s]:  training loss=0.4415489435195923                                    \n",
      "epoch 3 [15.85s]:  training loss=0.4247341752052307                                    \n",
      "epoch 4 [17.76s]:  training loss=0.41289713978767395                                   \n",
      "epoch 5 [15.61s]: training loss=0.3985721170902252  validation ndcg@10=0.057445274551193366 [0.23s]\n",
      "epoch 6 [15.94s]:  training loss=0.4061768651008606                                    \n",
      "epoch 7 [15.42s]:  training loss=0.3936586380004883                                    \n",
      "epoch 8 [15.89s]:  training loss=0.39023083448410034                                   \n",
      "epoch 9 [15.49s]:  training loss=0.3918803930282593                                    \n",
      "epoch 10 [15.91s]: training loss=0.3863280117511749  validation ndcg@10=0.056399243248526576 [0.18s]\n",
      "epoch 11 [15.73s]:  training loss=0.380617618560791                                    \n",
      "epoch 12 [16.08s]:  training loss=0.38782283663749695                                  \n",
      "epoch 13 [15.87s]:  training loss=0.3916853070259094                                   \n",
      "epoch 14 [15.48s]:  training loss=0.3839492201805115                                   \n",
      "epoch 15 [16.06s]: training loss=0.3854670524597168  validation ndcg@10=0.03978981071169421 [0.21s]\n",
      "epoch 16 [16.83s]:  training loss=0.38825723528862                                     \n",
      "epoch 17 [16.14s]:  training loss=0.38577067852020264                                  \n",
      "epoch 18 [15.52s]:  training loss=0.38662394881248474                                  \n",
      "epoch 19 [16.04s]:  training loss=0.3774632215499878                                   \n",
      "epoch 20 [15.71s]: training loss=0.3912836015224457  validation ndcg@10=0.044920684208418334 [0.21s]\n",
      "epoch 21 [15.88s]:  training loss=0.3826972246170044                                   \n",
      "epoch 22 [15.83s]:  training loss=0.3821849822998047                                   \n",
      "epoch 23 [16.16s]:  training loss=0.3878753185272217                                   \n",
      "epoch 24 [15.65s]:  training loss=0.37646088004112244                                  \n",
      "epoch 25 [16.02s]: training loss=0.37452977895736694  validation ndcg@10=0.0507477264830723 [0.18s]\n",
      "epoch 26 [15.54s]:  training loss=0.38400527834892273                                  \n",
      "epoch 27 [15.86s]:  training loss=0.3877382278442383                                   \n",
      "epoch 28 [15.65s]:  training loss=0.3823156952857971                                   \n",
      "epoch 29 [15.62s]:  training loss=0.38268208503723145                                  \n",
      "epoch 30 [15.49s]: training loss=0.380973219871521  validation ndcg@10=0.04811070603293252 [0.19s]\n",
      "epoch 1 [8.88s]:  training loss=0.9265878200531006                                     \n",
      "epoch 2 [8.89s]:  training loss=0.763678789138794                                      \n",
      "epoch 3 [8.73s]:  training loss=0.6553045511245728                                     \n",
      "epoch 4 [8.54s]:  training loss=0.5807446837425232                                     \n",
      "epoch 5 [8.52s]: training loss=0.5222319960594177  validation ndcg@10=0.03680526143529265 [0.12s]\n",
      "epoch 6 [8.53s]:  training loss=0.48436200618743896                                    \n",
      "epoch 7 [8.5s]:  training loss=0.468451589345932                                       \n",
      "epoch 8 [8.47s]:  training loss=0.4404235780239105                                     \n",
      "epoch 9 [8.49s]:  training loss=0.43704214692115784                                    \n",
      "epoch 10 [8.56s]: training loss=0.42780280113220215  validation ndcg@10=0.0515342169878445 [0.1s]\n",
      "epoch 11 [8.43s]:  training loss=0.43105942010879517                                   \n",
      "epoch 12 [8.71s]:  training loss=0.4230654835700989                                    \n",
      "epoch 13 [8.6s]:  training loss=0.42521223425865173                                    \n",
      "epoch 14 [8.49s]:  training loss=0.4151003360748291                                    \n",
      "epoch 15 [8.63s]: training loss=0.4158605933189392  validation ndcg@10=0.05002859513825739 [0.12s]\n",
      "epoch 16 [8.64s]:  training loss=0.41312986612319946                                   \n",
      "epoch 17 [8.47s]:  training loss=0.41305479407310486                                   \n",
      "epoch 18 [8.5s]:  training loss=0.41161760687828064                                    \n",
      "epoch 19 [8.54s]:  training loss=0.40437087416648865                                   \n",
      "epoch 20 [8.77s]: training loss=0.407196968793869  validation ndcg@10=0.05583822081582637 [0.12s]\n",
      "epoch 21 [8.66s]:  training loss=0.3997487425804138                                    \n",
      "epoch 22 [8.63s]:  training loss=0.4019189178943634                                    \n",
      "epoch 23 [8.43s]:  training loss=0.39644303917884827                                   \n",
      "epoch 24 [8.73s]:  training loss=0.3962639272212982                                    \n",
      "epoch 25 [8.71s]: training loss=0.395320326089859  validation ndcg@10=0.06574606577760289 [0.11s]\n",
      "epoch 26 [8.51s]:  training loss=0.38701412081718445                                   \n",
      "epoch 27 [8.5s]:  training loss=0.3924638330936432                                     \n",
      "epoch 28 [8.6s]:  training loss=0.39177507162094116                                    \n",
      "epoch 29 [8.42s]:  training loss=0.38481947779655457                                   \n",
      "epoch 30 [8.84s]: training loss=0.38308271765708923  validation ndcg@10=0.0617866681891588 [0.11s]\n",
      "epoch 31 [8.84s]:  training loss=0.38275206089019775                                   \n",
      "epoch 32 [8.74s]:  training loss=0.3838997781276703                                    \n",
      "epoch 33 [8.46s]:  training loss=0.37930864095687866                                   \n",
      "epoch 34 [8.55s]:  training loss=0.378510981798172                                     \n",
      "epoch 35 [8.57s]: training loss=0.3704243004322052  validation ndcg@10=0.0610051747166262 [0.12s]\n",
      "epoch 36 [8.46s]:  training loss=0.37355318665504456                                   \n",
      "epoch 37 [8.86s]:  training loss=0.3757777810096741                                    \n",
      "epoch 38 [9.39s]:  training loss=0.3722434341907501                                    \n",
      "epoch 39 [9.63s]:  training loss=0.3672788739204407                                    \n",
      "epoch 40 [8.55s]: training loss=0.3688980042934418  validation ndcg@10=0.06359876729709169 [0.11s]\n",
      "epoch 41 [8.76s]:  training loss=0.3653773367404938                                    \n",
      "epoch 42 [8.72s]:  training loss=0.3615354299545288                                    \n",
      "epoch 43 [8.84s]:  training loss=0.3618455231189728                                    \n",
      "epoch 44 [8.69s]:  training loss=0.35678327083587646                                   \n",
      "epoch 45 [8.55s]: training loss=0.3658783435821533  validation ndcg@10=0.07135749677580369 [0.12s]\n",
      "epoch 46 [8.56s]:  training loss=0.36176440119743347                                   \n",
      "epoch 47 [8.36s]:  training loss=0.3614571690559387                                    \n",
      "epoch 48 [8.92s]:  training loss=0.35421064496040344                                   \n",
      "epoch 49 [9.0s]:  training loss=0.3591548204421997                                     \n",
      "epoch 50 [8.61s]: training loss=0.35304954648017883  validation ndcg@10=0.06574820064465317 [0.11s]\n",
      "epoch 51 [8.68s]:  training loss=0.35287851095199585                                   \n",
      "epoch 52 [8.57s]:  training loss=0.35799306631088257                                   \n",
      "epoch 53 [8.65s]:  training loss=0.3545837700366974                                    \n",
      "epoch 54 [8.66s]:  training loss=0.3480733036994934                                    \n",
      "epoch 55 [8.47s]: training loss=0.3479468524456024  validation ndcg@10=0.06771460896059096 [0.12s]\n",
      "epoch 56 [8.5s]:  training loss=0.34790855646133423                                    \n",
      "epoch 57 [8.51s]:  training loss=0.34750330448150635                                   \n",
      "epoch 58 [8.62s]:  training loss=0.350831538438797                                     \n",
      "epoch 59 [8.49s]:  training loss=0.3512575924396515                                    \n",
      "epoch 60 [8.53s]: training loss=0.3422742486000061  validation ndcg@10=0.06440598235761269 [0.13s]\n",
      "epoch 61 [8.6s]:  training loss=0.3505620062351227                                     \n",
      "epoch 62 [8.6s]:  training loss=0.34362566471099854                                    \n",
      "epoch 63 [8.64s]:  training loss=0.3421374559402466                                    \n",
      "epoch 64 [8.56s]:  training loss=0.3432897627353668                                    \n",
      "epoch 65 [8.45s]: training loss=0.3436128497123718  validation ndcg@10=0.07169111795806739 [0.12s]\n",
      "epoch 66 [8.56s]:  training loss=0.3485719561576843                                    \n",
      "epoch 67 [8.52s]:  training loss=0.34061306715011597                                   \n",
      "epoch 68 [8.61s]:  training loss=0.3415936827659607                                    \n",
      "epoch 69 [8.77s]:  training loss=0.34529122710227966                                   \n",
      "epoch 70 [8.84s]: training loss=0.34560877084732056  validation ndcg@10=0.06316892358631906 [0.12s]\n",
      "epoch 71 [8.77s]:  training loss=0.335022509098053                                     \n",
      "epoch 72 [8.93s]:  training loss=0.34109240770339966                                   \n",
      "epoch 73 [8.57s]:  training loss=0.3418744206428528                                    \n",
      "epoch 74 [8.56s]:  training loss=0.3431088626384735                                    \n",
      "epoch 75 [8.71s]: training loss=0.33907416462898254  validation ndcg@10=0.06688232226963368 [0.12s]\n",
      "epoch 76 [8.59s]:  training loss=0.33236515522003174                                   \n",
      "epoch 77 [8.7s]:  training loss=0.33826273679733276                                    \n",
      "epoch 78 [8.5s]:  training loss=0.3364637792110443                                     \n",
      "epoch 79 [8.69s]:  training loss=0.3345378041267395                                    \n",
      "epoch 80 [8.82s]: training loss=0.33036044239997864  validation ndcg@10=0.07289316688928019 [0.12s]\n",
      "epoch 81 [8.83s]:  training loss=0.33298540115356445                                   \n",
      "epoch 82 [9.05s]:  training loss=0.33401405811309814                                   \n",
      "epoch 83 [8.65s]:  training loss=0.3349289298057556                                    \n",
      "epoch 84 [8.62s]:  training loss=0.3320155739784241                                    \n",
      "epoch 85 [8.66s]: training loss=0.33214113116264343  validation ndcg@10=0.06827740509079001 [0.11s]\n",
      "epoch 86 [8.69s]:  training loss=0.3373565971851349                                    \n",
      "epoch 87 [8.72s]:  training loss=0.3239912688732147                                    \n",
      "epoch 88 [8.57s]:  training loss=0.33002588152885437                                   \n",
      "epoch 89 [8.62s]:  training loss=0.33034583926200867                                   \n",
      "epoch 90 [8.64s]: training loss=0.3346922993659973  validation ndcg@10=0.06633013896713633 [0.13s]\n",
      "epoch 91 [8.57s]:  training loss=0.3295656144618988                                    \n",
      "epoch 92 [8.62s]:  training loss=0.3305186629295349                                    \n",
      "epoch 93 [8.53s]:  training loss=0.32858070731163025                                   \n",
      "epoch 94 [8.44s]:  training loss=0.3308277726173401                                    \n",
      "epoch 95 [8.68s]: training loss=0.3312874436378479  validation ndcg@10=0.0663613725634231 [0.11s]\n",
      "epoch 96 [8.62s]:  training loss=0.3309835195541382                                    \n",
      "epoch 97 [8.68s]:  training loss=0.3271843492984772                                    \n",
      "epoch 98 [8.61s]:  training loss=0.32782286405563354                                   \n",
      "epoch 99 [8.69s]:  training loss=0.3270801901817322                                    \n",
      "epoch 100 [8.57s]: training loss=0.32626521587371826  validation ndcg@10=0.0744681407005442 [0.12s]\n",
      "epoch 101 [8.63s]:  training loss=0.32811635732650757                                  \n",
      "epoch 102 [8.54s]:  training loss=0.32458385825157166                                  \n",
      "epoch 103 [8.62s]:  training loss=0.3287643492221832                                   \n",
      "epoch 104 [8.6s]:  training loss=0.328275591135025                                     \n",
      "epoch 105 [8.75s]: training loss=0.32576966285705566  validation ndcg@10=0.06997656288443987 [0.13s]\n",
      "epoch 106 [8.62s]:  training loss=0.32128316164016724                                  \n",
      "epoch 107 [8.56s]:  training loss=0.325855016708374                                    \n",
      "epoch 108 [8.61s]:  training loss=0.32706543803215027                                  \n",
      "epoch 109 [8.46s]:  training loss=0.32449376583099365                                  \n",
      "epoch 110 [8.7s]: training loss=0.3214131295681  validation ndcg@10=0.06944188008142169 [0.13s]\n",
      "epoch 111 [8.94s]:  training loss=0.32788217067718506                                  \n",
      "epoch 112 [8.94s]:  training loss=0.3241909146308899                                   \n",
      "epoch 113 [8.5s]:  training loss=0.3144558072090149                                    \n",
      "epoch 114 [8.53s]:  training loss=0.3259415030479431                                   \n",
      "epoch 115 [8.53s]: training loss=0.32148468494415283  validation ndcg@10=0.06981859938420666 [0.12s]\n",
      "epoch 116 [8.51s]:  training loss=0.32102715969085693                                  \n",
      "epoch 117 [8.44s]:  training loss=0.3244273364543915                                   \n",
      "epoch 118 [8.5s]:  training loss=0.33577680587768555                                   \n",
      "epoch 119 [8.51s]:  training loss=0.31801244616508484                                  \n",
      "epoch 120 [8.59s]: training loss=0.3224192261695862  validation ndcg@10=0.06738513980620915 [0.12s]\n",
      "epoch 121 [8.63s]:  training loss=0.3268614411354065                                   \n",
      "epoch 122 [10.59s]:  training loss=0.3147968649864197                                  \n",
      "epoch 123 [8.43s]:  training loss=0.3213738203048706                                   \n",
      "epoch 124 [8.56s]:  training loss=0.3207811713218689                                   \n",
      "epoch 125 [8.49s]: training loss=0.31717151403427124  validation ndcg@10=0.07230891483265986 [0.12s]\n",
      "epoch 1 [16.08s]:  training loss=1.0021604299545288                                    \n",
      "epoch 2 [17.82s]:  training loss=0.8974683880805969                                    \n",
      "epoch 3 [17.55s]:  training loss=0.8396929502487183                                    \n",
      "epoch 4 [17.52s]:  training loss=0.7895627617835999                                    \n",
      "epoch 5 [18.04s]: training loss=0.7495937347412109  validation ndcg@10=0.042416962963984124 [0.26s]\n",
      "epoch 6 [17.34s]:  training loss=0.7164873480796814                                    \n",
      "epoch 7 [17.87s]:  training loss=0.685977578163147                                     \n",
      "epoch 8 [17.38s]:  training loss=0.6665234565734863                                    \n",
      "epoch 9 [17.55s]:  training loss=0.6291106939315796                                    \n",
      "epoch 10 [17.48s]: training loss=0.609505832195282  validation ndcg@10=0.0429290520401457 [0.25s]\n",
      "epoch 11 [18.1s]:  training loss=0.593779981136322                                     \n",
      "epoch 12 [16.85s]:  training loss=0.5664043426513672                                   \n",
      "epoch 13 [17.63s]:  training loss=0.5479446649551392                                   \n",
      "epoch 14 [17.08s]:  training loss=0.5321033596992493                                   \n",
      "epoch 15 [17.69s]: training loss=0.5171535015106201  validation ndcg@10=0.05403895682057419 [0.31s]\n",
      "epoch 16 [17.47s]:  training loss=0.504483699798584                                    \n",
      "epoch 17 [17.9s]:  training loss=0.4949253499507904                                    \n",
      "epoch 18 [17.63s]:  training loss=0.48489633202552795                                  \n",
      "epoch 19 [17.57s]:  training loss=0.4742797911167145                                   \n",
      "epoch 20 [17.87s]: training loss=0.4680631160736084  validation ndcg@10=0.057479668719124744 [0.25s]\n",
      "epoch 21 [17.18s]:  training loss=0.46032875776290894                                  \n",
      "epoch 22 [17.77s]:  training loss=0.4559441804885864                                   \n",
      "epoch 23 [17.32s]:  training loss=0.44778120517730713                                  \n",
      "epoch 24 [18.26s]:  training loss=0.4474247694015503                                   \n",
      "epoch 25 [18.2s]: training loss=0.4505000412464142  validation ndcg@10=0.055023358433460054 [0.3s]\n",
      "epoch 26 [17.49s]:  training loss=0.43772977590560913                                  \n",
      "epoch 27 [16.91s]:  training loss=0.4343717694282532                                   \n",
      "epoch 28 [17.67s]:  training loss=0.43698206543922424                                  \n",
      "epoch 29 [18.25s]:  training loss=0.43592190742492676                                  \n",
      "epoch 30 [17.16s]: training loss=0.4345570504665375  validation ndcg@10=0.054909018231303126 [0.26s]\n",
      "epoch 31 [18.26s]:  training loss=0.4306422770023346                                   \n",
      "epoch 32 [16.78s]:  training loss=0.4357309937477112                                   \n",
      "epoch 33 [17.98s]:  training loss=0.4241051971912384                                   \n",
      "epoch 34 [17.94s]:  training loss=0.4332192540168762                                   \n",
      "epoch 35 [18.39s]: training loss=0.4316740036010742  validation ndcg@10=0.057326608155560475 [0.3s]\n",
      "epoch 36 [16.86s]:  training loss=0.41866427659988403                                  \n",
      "epoch 37 [18.15s]:  training loss=0.4239840805530548                                   \n",
      "epoch 38 [18.42s]:  training loss=0.430684894323349                                    \n",
      "epoch 39 [16.95s]:  training loss=0.4225277006626129                                   \n",
      "epoch 40 [18.34s]: training loss=0.4237576425075531  validation ndcg@10=0.06041040835595721 [0.29s]\n",
      "epoch 41 [17.43s]:  training loss=0.42543238401412964                                  \n",
      "epoch 42 [18.31s]:  training loss=0.41917115449905396                                  \n",
      "epoch 43 [17.53s]:  training loss=0.42407503724098206                                  \n",
      "epoch 44 [18.41s]:  training loss=0.4159200191497803                                   \n",
      "epoch 45 [17.93s]: training loss=0.41642093658447266  validation ndcg@10=0.058484348218169946 [0.26s]\n",
      "epoch 46 [17.86s]:  training loss=0.4140286445617676                                   \n",
      "epoch 47 [17.74s]:  training loss=0.4188823401927948                                   \n",
      "epoch 48 [17.99s]:  training loss=0.41291990876197815                                  \n",
      "epoch 49 [17.72s]:  training loss=0.41086113452911377                                  \n",
      "epoch 50 [17.86s]: training loss=0.41515761613845825  validation ndcg@10=0.05986473787346608 [0.26s]\n",
      "epoch 51 [19.35s]:  training loss=0.4116259217262268                                   \n",
      "epoch 52 [17.83s]:  training loss=0.4140230417251587                                   \n",
      "epoch 53 [18.5s]:  training loss=0.4087175726890564                                    \n",
      "epoch 54 [18.27s]:  training loss=0.4057767391204834                                   \n",
      "epoch 55 [19.18s]: training loss=0.4141272008419037  validation ndcg@10=0.06348263977501574 [0.3s]\n",
      "epoch 56 [17.37s]:  training loss=0.41531386971473694                                  \n",
      "epoch 57 [18.22s]:  training loss=0.41312605142593384                                  \n",
      "epoch 58 [17.58s]:  training loss=0.40449780225753784                                  \n",
      "epoch 59 [18.25s]:  training loss=0.4014592170715332                                   \n",
      "epoch 60 [17.46s]: training loss=0.4119889736175537  validation ndcg@10=0.05768788497774402 [0.3s]\n",
      "epoch 61 [18.16s]:  training loss=0.40807580947875977                                  \n",
      "epoch 62 [18.45s]:  training loss=0.39859676361083984                                  \n",
      "epoch 63 [17.3s]:  training loss=0.3978796899318695                                    \n",
      "epoch 64 [18.38s]:  training loss=0.3977501690387726                                   \n",
      "epoch 65 [17.63s]: training loss=0.39992380142211914  validation ndcg@10=0.06620768601343098 [0.25s]\n",
      "epoch 66 [18.42s]:  training loss=0.40292778611183167                                  \n",
      "epoch 67 [17.29s]:  training loss=0.4087717831134796                                   \n",
      "epoch 68 [18.14s]:  training loss=0.40268394351005554                                  \n",
      "epoch 69 [17.67s]:  training loss=0.4046871066093445                                   \n",
      "epoch 70 [18.29s]: training loss=0.40075159072875977  validation ndcg@10=0.0656211213311114 [0.3s]\n",
      "epoch 71 [17.4s]:  training loss=0.40502411127090454                                   \n",
      "epoch 72 [18.16s]:  training loss=0.39327558875083923                                  \n",
      "epoch 73 [17.51s]:  training loss=0.40089818835258484                                  \n",
      "epoch 74 [18.03s]:  training loss=0.40095216035842896                                  \n",
      "epoch 75 [17.87s]: training loss=0.4024372696876526  validation ndcg@10=0.06577482552463575 [0.27s]\n",
      "epoch 76 [18.07s]:  training loss=0.3942257761955261                                   \n",
      "epoch 77 [17.74s]:  training loss=0.3923890292644501                                   \n",
      "epoch 78 [18.13s]:  training loss=0.40142154693603516                                  \n",
      "epoch 79 [18.11s]:  training loss=0.39725151658058167                                  \n",
      "epoch 80 [17.78s]: training loss=0.3945293724536896  validation ndcg@10=0.06064693441396739 [0.27s]\n",
      "epoch 81 [18.13s]:  training loss=0.3936297595500946                                   \n",
      "epoch 82 [17.48s]:  training loss=0.3931471109390259                                   \n",
      "epoch 83 [18.47s]:  training loss=0.3848299980163574                                   \n",
      "epoch 84 [17.92s]:  training loss=0.39433082938194275                                  \n",
      "epoch 85 [18.76s]: training loss=0.39385122060775757  validation ndcg@10=0.06126685036258848 [0.3s]\n",
      "epoch 86 [18.23s]:  training loss=0.39490145444869995                                  \n",
      "epoch 87 [17.77s]:  training loss=0.3916095495223999                                   \n",
      "epoch 88 [18.01s]:  training loss=0.39047232270240784                                  \n",
      "epoch 89 [17.81s]:  training loss=0.39402157068252563                                  \n",
      "epoch 90 [18.06s]: training loss=0.3896642029285431  validation ndcg@10=0.06528610021254752 [0.25s]\n",
      "epoch 1 [3.17s]:  training loss=0.9394232034683228                                      \n",
      "epoch 2 [2.87s]:  training loss=0.7342588901519775                                      \n",
      "epoch 3 [3.08s]:  training loss=0.6183512806892395                                      \n",
      "epoch 4 [2.87s]:  training loss=0.5387168526649475                                      \n",
      "epoch 5 [2.91s]: training loss=0.487339586019516  validation ndcg@10=0.053385454563435936 [0.05s]\n",
      "epoch 6 [2.84s]:  training loss=0.4561443626880646                                      \n",
      "epoch 7 [2.72s]:  training loss=0.4472030699253082                                      \n",
      "epoch 8 [2.71s]:  training loss=0.4297413229942322                                      \n",
      "epoch 9 [2.73s]:  training loss=0.4347900450229645                                      \n",
      "epoch 10 [2.66s]: training loss=0.4288763999938965  validation ndcg@10=0.059193211277289895 [0.06s]\n",
      "epoch 11 [2.61s]:  training loss=0.422722727060318                                      \n",
      "epoch 12 [2.58s]:  training loss=0.41349518299102783                                    \n",
      "epoch 13 [2.53s]:  training loss=0.4152261018753052                                     \n",
      "epoch 14 [2.62s]:  training loss=0.4109968841075897                                     \n",
      "epoch 15 [2.65s]: training loss=0.41142889857292175  validation ndcg@10=0.058101903473829285 [0.05s]\n",
      "epoch 16 [2.62s]:  training loss=0.4100179672241211                                     \n",
      "epoch 17 [2.62s]:  training loss=0.40721240639686584                                    \n",
      "epoch 18 [2.62s]:  training loss=0.4065588116645813                                     \n",
      "epoch 19 [2.62s]:  training loss=0.40767258405685425                                    \n",
      "epoch 20 [2.56s]: training loss=0.3961642384529114  validation ndcg@10=0.0636207166732343 [0.05s]\n",
      "epoch 21 [2.61s]:  training loss=0.39809122681617737                                    \n",
      "epoch 22 [2.58s]:  training loss=0.38738423585891724                                    \n",
      "epoch 23 [2.51s]:  training loss=0.39457979798316956                                    \n",
      "epoch 24 [2.59s]:  training loss=0.39396020770072937                                    \n",
      "epoch 25 [2.58s]: training loss=0.38528621196746826  validation ndcg@10=0.06826993882972944 [0.05s]\n",
      "epoch 26 [2.62s]:  training loss=0.38358187675476074                                    \n",
      "epoch 27 [2.61s]:  training loss=0.38207417726516724                                    \n",
      "epoch 28 [2.6s]:  training loss=0.37621766328811646                                     \n",
      "epoch 29 [2.66s]:  training loss=0.37764474749565125                                    \n",
      "epoch 30 [2.52s]: training loss=0.38241147994995117  validation ndcg@10=0.06352931233297447 [0.05s]\n",
      "epoch 31 [2.6s]:  training loss=0.3727371394634247                                      \n",
      "epoch 32 [2.58s]:  training loss=0.3726368248462677                                     \n",
      "epoch 33 [2.55s]:  training loss=0.37195640802383423                                    \n",
      "epoch 34 [2.57s]:  training loss=0.3682492971420288                                     \n",
      "epoch 35 [2.5s]: training loss=0.36571234464645386  validation ndcg@10=0.0780423131095452 [0.05s]\n",
      "epoch 36 [2.66s]:  training loss=0.3640006184577942                                     \n",
      "epoch 37 [2.56s]:  training loss=0.3653085231781006                                     \n",
      "epoch 38 [2.62s]:  training loss=0.35861146450042725                                    \n",
      "epoch 39 [2.54s]:  training loss=0.35833939909935                                       \n",
      "epoch 40 [2.52s]: training loss=0.3659308850765228  validation ndcg@10=0.07854997235788005 [0.05s]\n",
      "epoch 41 [2.57s]:  training loss=0.353657603263855                                      \n",
      "epoch 42 [2.59s]:  training loss=0.35128188133239746                                    \n",
      "epoch 43 [2.51s]:  training loss=0.3539382815361023                                     \n",
      "epoch 44 [2.57s]:  training loss=0.35308918356895447                                    \n",
      "epoch 45 [2.54s]: training loss=0.35518887639045715  validation ndcg@10=0.07430006238217775 [0.05s]\n",
      "epoch 46 [2.5s]:  training loss=0.34948137402534485                                     \n",
      "epoch 47 [2.57s]:  training loss=0.3505716323852539                                     \n",
      "epoch 48 [2.53s]:  training loss=0.34298983216285706                                    \n",
      "epoch 49 [2.53s]:  training loss=0.3477293848991394                                     \n",
      "epoch 50 [2.56s]: training loss=0.34940850734710693  validation ndcg@10=0.0690991036934633 [0.05s]\n",
      "epoch 51 [2.58s]:  training loss=0.34508436918258667                                    \n",
      "epoch 52 [2.58s]:  training loss=0.34364086389541626                                    \n",
      "epoch 53 [2.64s]:  training loss=0.34888455271720886                                    \n",
      "epoch 54 [2.55s]:  training loss=0.34176453948020935                                    \n",
      "epoch 55 [2.58s]: training loss=0.34313225746154785  validation ndcg@10=0.06772431540621236 [0.05s]\n",
      "epoch 56 [2.63s]:  training loss=0.34069475531578064                                    \n",
      "epoch 57 [2.54s]:  training loss=0.337342768907547                                      \n",
      "epoch 58 [2.63s]:  training loss=0.3466150164604187                                     \n",
      "epoch 59 [2.59s]:  training loss=0.3362518548965454                                     \n",
      "epoch 60 [2.64s]: training loss=0.33875057101249695  validation ndcg@10=0.06841804858473022 [0.05s]\n",
      "epoch 61 [2.56s]:  training loss=0.3373141884803772                                     \n",
      "epoch 62 [2.58s]:  training loss=0.33732080459594727                                    \n",
      "epoch 63 [2.57s]:  training loss=0.33506646752357483                                    \n",
      "epoch 64 [2.52s]:  training loss=0.33728837966918945                                    \n",
      "epoch 65 [2.5s]: training loss=0.3391861617565155  validation ndcg@10=0.06796010407862432 [0.05s]\n",
      "epoch 1 [30.76s]:  training loss=0.789801836013794                                      \n",
      "epoch 2 [29.33s]:  training loss=0.5733602046966553                                    \n",
      "epoch 3 [30.08s]:  training loss=0.46945345401763916                                   \n",
      "epoch 4 [30.49s]:  training loss=0.4398070275783539                                    \n",
      "epoch 5 [31.0s]: training loss=0.42988744378089905  validation ndcg@10=0.054452869654974666 [0.32s]\n",
      "epoch 6 [30.26s]:  training loss=0.42540889978408813                                   \n",
      "epoch 7 [30.43s]:  training loss=0.4230419993400574                                    \n",
      "epoch 8 [30.69s]:  training loss=0.41224735975265503                                   \n",
      "epoch 9 [31.17s]:  training loss=0.4052630066871643                                    \n",
      "epoch 10 [30.83s]: training loss=0.403072327375412  validation ndcg@10=0.0557663979218954 [0.32s]\n",
      "epoch 11 [30.77s]:  training loss=0.3983789086341858                                   \n",
      "epoch 12 [30.57s]:  training loss=0.389553427696228                                    \n",
      "epoch 13 [30.18s]:  training loss=0.3851443827152252                                   \n",
      "epoch 14 [30.55s]:  training loss=0.39047297835350037                                  \n",
      "epoch 15 [30.81s]: training loss=0.37805676460266113  validation ndcg@10=0.06444872964523846 [0.36s]\n",
      "epoch 16 [30.74s]:  training loss=0.38032376766204834                                  \n",
      "epoch 17 [30.17s]:  training loss=0.3661908805370331                                   \n",
      "epoch 18 [29.84s]:  training loss=0.36490964889526367                                  \n",
      "epoch 19 [29.5s]:  training loss=0.36250802874565125                                   \n",
      "epoch 20 [29.57s]: training loss=0.35226285457611084  validation ndcg@10=0.06873252265014468 [0.34s]\n",
      "epoch 21 [29.8s]:  training loss=0.35651925206184387                                   \n",
      "epoch 22 [29.62s]:  training loss=0.3636910915374756                                   \n",
      "epoch 23 [29.71s]:  training loss=0.3574168384075165                                   \n",
      "epoch 24 [29.33s]:  training loss=0.3524817228317261                                   \n",
      "epoch 25 [29.7s]: training loss=0.3503033518791199  validation ndcg@10=0.0726331428491665 [0.33s]\n",
      "epoch 26 [29.66s]:  training loss=0.34824347496032715                                  \n",
      "epoch 27 [30.36s]:  training loss=0.3488249182701111                                   \n",
      "epoch 28 [31.71s]:  training loss=0.35204511880874634                                  \n",
      "epoch 29 [27.72s]:  training loss=0.34917497634887695                                  \n",
      "epoch 30 [27.73s]: training loss=0.34558531641960144  validation ndcg@10=0.06638152406959197 [0.3s]\n",
      "epoch 31 [28.55s]:  training loss=0.3436853587627411                                   \n",
      "epoch 32 [28.49s]:  training loss=0.3402925729751587                                   \n",
      "epoch 33 [28.29s]:  training loss=0.3354327976703644                                   \n",
      "epoch 34 [28.34s]:  training loss=0.33543625473976135                                  \n",
      "epoch 35 [27.86s]: training loss=0.34226420521736145  validation ndcg@10=0.07009805197852585 [0.29s]\n",
      "epoch 36 [28.32s]:  training loss=0.3407020568847656                                   \n",
      "epoch 37 [28.27s]:  training loss=0.3329540491104126                                   \n",
      "epoch 38 [27.72s]:  training loss=0.33543893694877625                                  \n",
      "epoch 39 [28.16s]:  training loss=0.335328072309494                                    \n",
      "epoch 40 [28.06s]: training loss=0.33265596628189087  validation ndcg@10=0.07408096945445462 [0.3s]\n",
      "epoch 41 [28.44s]:  training loss=0.33048728108406067                                  \n",
      "epoch 42 [28.47s]:  training loss=0.32547539472579956                                  \n",
      "epoch 43 [27.79s]:  training loss=0.3307029902935028                                   \n",
      "epoch 44 [30.03s]:  training loss=0.3286733329296112                                   \n",
      "epoch 45 [28.02s]: training loss=0.32934701442718506  validation ndcg@10=0.07540121185335963 [0.35s]\n",
      "epoch 46 [28.85s]:  training loss=0.3267993927001953                                   \n",
      "epoch 47 [28.26s]:  training loss=0.32807984948158264                                  \n",
      "epoch 48 [28.29s]:  training loss=0.32725006341934204                                  \n",
      "epoch 49 [28.64s]:  training loss=0.3311428427696228                                   \n",
      "epoch 50 [28.41s]: training loss=0.3196347951889038  validation ndcg@10=0.07075504494328919 [0.31s]\n",
      "epoch 51 [28.5s]:  training loss=0.32214611768722534                                   \n",
      "epoch 52 [28.61s]:  training loss=0.3200628459453583                                   \n",
      "epoch 53 [27.91s]:  training loss=0.32727789878845215                                  \n",
      "epoch 54 [28.05s]:  training loss=0.31741684675216675                                  \n",
      "epoch 55 [27.76s]: training loss=0.3154035210609436  validation ndcg@10=0.061279816304872683 [0.31s]\n",
      "epoch 56 [28.35s]:  training loss=0.3250817060470581                                   \n",
      "epoch 57 [28.24s]:  training loss=0.32097551226615906                                  \n",
      "epoch 58 [28.01s]:  training loss=0.31761208176612854                                  \n",
      "epoch 59 [30.58s]:  training loss=0.316408634185791                                    \n",
      "epoch 60 [28.57s]: training loss=0.3160330355167389  validation ndcg@10=0.059158360254572405 [0.33s]\n",
      "epoch 61 [28.32s]:  training loss=0.3173716366291046                                   \n",
      "epoch 62 [27.98s]:  training loss=0.3170962631702423                                   \n",
      "epoch 63 [28.73s]:  training loss=0.31938493251800537                                  \n",
      "epoch 64 [28.29s]:  training loss=0.3207622170448303                                   \n",
      "epoch 65 [28.94s]: training loss=0.3148239850997925  validation ndcg@10=0.05790564548214224 [0.36s]\n",
      "epoch 66 [28.51s]:  training loss=0.31345152854919434                                  \n",
      "epoch 67 [28.25s]:  training loss=0.3091829717159271                                   \n",
      "epoch 68 [28.2s]:  training loss=0.30684345960617065                                   \n",
      "epoch 69 [28.58s]:  training loss=0.3092615604400635                                   \n",
      "epoch 70 [28.4s]: training loss=0.31051450967788696  validation ndcg@10=0.07064965605105905 [0.3s]\n",
      "epoch 1 [19.99s]:  training loss=0.9741190671920776                                     \n",
      "epoch 2 [20.77s]:  training loss=0.8343191146850586                                     \n",
      "epoch 3 [20.74s]:  training loss=0.7587699890136719                                     \n",
      "epoch 4 [20.7s]:  training loss=0.6990700364112854                                      \n",
      "epoch 5 [20.59s]: training loss=0.6524332165718079  validation ndcg@10=0.04419592332350193 [0.25s]\n",
      "epoch 6 [23.05s]:  training loss=0.6060108542442322                                     \n",
      "epoch 7 [20.61s]:  training loss=0.5639094114303589                                     \n",
      "epoch 8 [20.54s]:  training loss=0.5374827980995178                                     \n",
      "epoch 9 [20.77s]:  training loss=0.5057209134101868                                     \n",
      "epoch 10 [20.4s]: training loss=0.48608556389808655  validation ndcg@10=0.05442855175048359 [0.25s]\n",
      "epoch 11 [21.12s]:  training loss=0.47442901134490967                                   \n",
      "epoch 12 [20.73s]:  training loss=0.464464008808136                                     \n",
      "epoch 13 [20.97s]:  training loss=0.44652462005615234                                   \n",
      "epoch 14 [21.12s]:  training loss=0.4462640881538391                                    \n",
      "epoch 15 [20.83s]: training loss=0.44061437249183655  validation ndcg@10=0.06089132751429906 [0.25s]\n",
      "epoch 16 [21.12s]:  training loss=0.43513908982276917                                   \n",
      "epoch 17 [21.04s]:  training loss=0.4314323961734772                                    \n",
      "epoch 18 [20.76s]:  training loss=0.42252224683761597                                   \n",
      "epoch 19 [21.21s]:  training loss=0.4254363179206848                                    \n",
      "epoch 20 [21.03s]: training loss=0.42633259296417236  validation ndcg@10=0.05851732925856493 [0.25s]\n",
      "epoch 21 [21.07s]:  training loss=0.4165894389152527                                    \n",
      "epoch 22 [20.53s]:  training loss=0.4217078983783722                                    \n",
      "epoch 23 [20.98s]:  training loss=0.4199002981185913                                    \n",
      "epoch 24 [21.03s]:  training loss=0.42066946625709534                                   \n",
      "epoch 25 [21.05s]: training loss=0.41342878341674805  validation ndcg@10=0.06474298635370973 [0.29s]\n",
      "epoch 26 [20.43s]:  training loss=0.4112880229949951                                    \n",
      "epoch 27 [21.15s]:  training loss=0.4101395010948181                                    \n",
      "epoch 28 [22.23s]:  training loss=0.4147507846355438                                    \n",
      "epoch 29 [20.91s]:  training loss=0.412525475025177                                     \n",
      "epoch 30 [21.26s]: training loss=0.41034793853759766  validation ndcg@10=0.06520094521830241 [0.27s]\n",
      "epoch 31 [21.25s]:  training loss=0.4099348187446594                                    \n",
      "epoch 32 [21.03s]:  training loss=0.4041334390640259                                    \n",
      "epoch 33 [20.78s]:  training loss=0.40545889735221863                                   \n",
      "epoch 34 [20.47s]:  training loss=0.41183820366859436                                   \n",
      "epoch 35 [20.61s]: training loss=0.40250006318092346  validation ndcg@10=0.061250849834757884 [0.29s]\n",
      "epoch 36 [20.86s]:  training loss=0.4007855951786041                                    \n",
      "epoch 37 [21.52s]:  training loss=0.4029797315597534                                    \n",
      "epoch 38 [21.32s]:  training loss=0.39587923884391785                                   \n",
      "epoch 39 [21.85s]:  training loss=0.4021042585372925                                    \n",
      "epoch 40 [20.85s]: training loss=0.3987652659416199  validation ndcg@10=0.06527906519250225 [0.26s]\n",
      "epoch 41 [20.81s]:  training loss=0.3922674059867859                                    \n",
      "epoch 42 [20.74s]:  training loss=0.38868290185928345                                   \n",
      "epoch 43 [20.59s]:  training loss=0.3926231861114502                                    \n",
      "epoch 44 [20.94s]:  training loss=0.3944612741470337                                    \n",
      "epoch 45 [21.7s]: training loss=0.39801090955734253  validation ndcg@10=0.06624015672730013 [0.32s]\n",
      "epoch 46 [21.57s]:  training loss=0.38567066192626953                                   \n",
      "epoch 47 [20.49s]:  training loss=0.3875570595264435                                    \n",
      "epoch 48 [21.56s]:  training loss=0.3929041028022766                                    \n",
      "epoch 49 [20.96s]:  training loss=0.386242151260376                                     \n",
      "epoch 50 [20.8s]: training loss=0.389194518327713  validation ndcg@10=0.06415351617303329 [0.25s]\n",
      "epoch 51 [23.1s]:  training loss=0.38739949464797974                                    \n",
      "epoch 52 [20.59s]:  training loss=0.38451477885246277                                   \n",
      "epoch 53 [21.33s]:  training loss=0.38642382621765137                                   \n",
      "epoch 54 [21.05s]:  training loss=0.3814934492111206                                    \n",
      "epoch 55 [20.59s]: training loss=0.3859080374240875  validation ndcg@10=0.06662531296588277 [0.25s]\n",
      "epoch 56 [21.37s]:  training loss=0.3807002604007721                                    \n",
      "epoch 57 [20.38s]:  training loss=0.3777000904083252                                    \n",
      "epoch 58 [21.63s]:  training loss=0.37758150696754456                                   \n",
      "epoch 59 [21.15s]:  training loss=0.3806394934654236                                    \n",
      "epoch 60 [21.09s]: training loss=0.37897244095802307  validation ndcg@10=0.07007340514723927 [0.26s]\n",
      "epoch 61 [21.03s]:  training loss=0.38279613852500916                                   \n",
      "epoch 62 [20.89s]:  training loss=0.3789724111557007                                    \n",
      "epoch 63 [20.86s]:  training loss=0.3763662278652191                                    \n",
      "epoch 64 [20.7s]:  training loss=0.37555068731307983                                    \n",
      "epoch 65 [20.74s]: training loss=0.378978967666626  validation ndcg@10=0.06759173762050154 [0.32s]\n",
      "epoch 66 [21.87s]:  training loss=0.3661859631538391                                    \n",
      "epoch 67 [21.14s]:  training loss=0.37129685282707214                                   \n",
      "epoch 68 [20.61s]:  training loss=0.37518981099128723                                   \n",
      "epoch 69 [21.13s]:  training loss=0.37491506338119507                                   \n",
      "epoch 70 [21.04s]: training loss=0.3679499924182892  validation ndcg@10=0.07018000121504032 [0.35s]\n",
      "epoch 71 [22.25s]:  training loss=0.3685101568698883                                    \n",
      "epoch 72 [21.19s]:  training loss=0.36785414814949036                                   \n",
      "epoch 73 [23.0s]:  training loss=0.3689662516117096                                     \n",
      "epoch 74 [21.15s]:  training loss=0.36822620034217834                                   \n",
      "epoch 75 [21.26s]: training loss=0.36827850341796875  validation ndcg@10=0.07445717625501158 [0.25s]\n",
      "epoch 76 [20.39s]:  training loss=0.36245280504226685                                   \n",
      "epoch 77 [21.39s]:  training loss=0.3652576804161072                                    \n",
      "epoch 78 [21.08s]:  training loss=0.36569249629974365                                   \n",
      "epoch 79 [20.63s]:  training loss=0.3661530315876007                                    \n",
      "epoch 80 [20.83s]: training loss=0.364561527967453  validation ndcg@10=0.06617276027643916 [0.27s]\n",
      "epoch 81 [20.97s]:  training loss=0.35620349645614624                                   \n",
      "epoch 82 [20.97s]:  training loss=0.36107006669044495                                   \n",
      "epoch 83 [21.2s]:  training loss=0.3598043620586395                                     \n",
      "epoch 84 [20.77s]:  training loss=0.35791242122650146                                   \n",
      "epoch 85 [21.25s]: training loss=0.3567638397216797  validation ndcg@10=0.07255466961272734 [0.25s]\n",
      "epoch 86 [21.09s]:  training loss=0.36172163486480713                                   \n",
      "epoch 87 [20.75s]:  training loss=0.36057108640670776                                   \n",
      "epoch 88 [21.67s]:  training loss=0.35407039523124695                                   \n",
      "epoch 89 [20.65s]:  training loss=0.35784581303596497                                   \n",
      "epoch 90 [21.11s]: training loss=0.3491438627243042  validation ndcg@10=0.0676075402663727 [0.26s]\n",
      "epoch 91 [20.73s]:  training loss=0.3517649471759796                                    \n",
      "epoch 92 [21.63s]:  training loss=0.3527970612049103                                    \n",
      "epoch 93 [21.36s]:  training loss=0.3509067893028259                                    \n",
      "epoch 94 [20.59s]:  training loss=0.3557833433151245                                    \n",
      "epoch 95 [21.17s]: training loss=0.35302701592445374  validation ndcg@10=0.07497666810088623 [0.26s]\n",
      "epoch 96 [23.34s]:  training loss=0.3606164753437042                                    \n",
      "epoch 97 [21.91s]:  training loss=0.34995099902153015                                   \n",
      "epoch 98 [22.52s]:  training loss=0.34752601385116577                                   \n",
      "epoch 99 [20.93s]:  training loss=0.34652552008628845                                   \n",
      "epoch 100 [21.29s]: training loss=0.3514627516269684  validation ndcg@10=0.07809690168746514 [0.26s]\n",
      "epoch 101 [20.73s]:  training loss=0.34378308057785034                                  \n",
      "epoch 102 [20.49s]:  training loss=0.3469501733779907                                   \n",
      "epoch 103 [21.65s]:  training loss=0.34857869148254395                                  \n",
      "epoch 104 [21.09s]:  training loss=0.3455130159854889                                   \n",
      "epoch 105 [20.91s]: training loss=0.34489011764526367  validation ndcg@10=0.06758114552641818 [0.27s]\n",
      "epoch 106 [21.02s]:  training loss=0.3458475172519684                                   \n",
      "epoch 107 [21.27s]:  training loss=0.34542539715766907                                  \n",
      "epoch 108 [21.01s]:  training loss=0.3451192378997803                                   \n",
      "epoch 109 [21.48s]:  training loss=0.3460351526737213                                   \n",
      "epoch 110 [20.5s]: training loss=0.34701108932495117  validation ndcg@10=0.07027062881064716 [0.25s]\n",
      "epoch 111 [20.68s]:  training loss=0.34422045946121216                                  \n",
      "epoch 112 [20.71s]:  training loss=0.3450582027435303                                   \n",
      "epoch 113 [20.42s]:  training loss=0.3466639220714569                                   \n",
      "epoch 114 [20.79s]:  training loss=0.3434722423553467                                   \n",
      "epoch 115 [20.97s]: training loss=0.33895277976989746  validation ndcg@10=0.07177762996451971 [0.25s]\n",
      "epoch 116 [20.98s]:  training loss=0.344646692276001                                    \n",
      "epoch 117 [20.99s]:  training loss=0.3394835889339447                                   \n",
      "epoch 118 [23.23s]:  training loss=0.34126120805740356                                  \n",
      "epoch 119 [20.46s]:  training loss=0.3420209586620331                                   \n",
      "epoch 120 [21.05s]: training loss=0.3458295464515686  validation ndcg@10=0.06785047753030758 [0.29s]\n",
      "epoch 121 [20.64s]:  training loss=0.34167489409446716                                  \n",
      "epoch 122 [21.57s]:  training loss=0.3359919786453247                                   \n",
      "epoch 123 [21.58s]:  training loss=0.3354470729827881                                   \n",
      "epoch 124 [20.75s]:  training loss=0.3379572629928589                                   \n",
      "epoch 125 [20.53s]: training loss=0.3307739794254303  validation ndcg@10=0.06659522917530933 [0.25s]\n",
      "epoch 1 [22.51s]:  training loss=0.6869426369667053                                     \n",
      "epoch 2 [22.63s]:  training loss=0.46081796288490295                                    \n",
      "epoch 3 [22.98s]:  training loss=0.44741418957710266                                    \n",
      "epoch 4 [22.07s]:  training loss=0.4274783730506897                                     \n",
      "epoch 5 [23.27s]: training loss=0.4168701767921448  validation ndcg@10=0.05728540828525438 [0.29s]\n",
      "epoch 6 [21.91s]:  training loss=0.4107232689857483                                     \n",
      "epoch 7 [23.09s]:  training loss=0.39732155203819275                                    \n",
      "epoch 8 [22.28s]:  training loss=0.3893183469772339                                     \n",
      "epoch 9 [23.08s]:  training loss=0.37877658009529114                                    \n",
      "epoch 10 [23.13s]: training loss=0.3727186322212219  validation ndcg@10=0.07950129338699309 [0.3s]\n",
      "epoch 11 [22.7s]:  training loss=0.37551942467689514                                    \n",
      "epoch 12 [22.93s]:  training loss=0.3629958927631378                                    \n",
      "epoch 13 [24.52s]:  training loss=0.3663412928581238                                    \n",
      "epoch 14 [22.73s]:  training loss=0.3640674352645874                                    \n",
      "epoch 15 [22.75s]: training loss=0.3516336679458618  validation ndcg@10=0.061932773843518286 [0.31s]\n",
      "epoch 16 [22.66s]:  training loss=0.3519953489303589                                    \n",
      "epoch 17 [22.67s]:  training loss=0.34814736247062683                                   \n",
      "epoch 18 [23.2s]:  training loss=0.33804255723953247                                    \n",
      "epoch 19 [21.98s]:  training loss=0.34806153178215027                                   \n",
      "epoch 20 [23.3s]: training loss=0.33996230363845825  validation ndcg@10=0.07238488513241796 [0.28s]\n",
      "epoch 21 [22.94s]:  training loss=0.33657127618789673                                   \n",
      "epoch 22 [23.35s]:  training loss=0.33768221735954285                                   \n",
      "epoch 23 [22.92s]:  training loss=0.3315574824810028                                    \n",
      "epoch 24 [22.96s]:  training loss=0.32976779341697693                                   \n",
      "epoch 25 [22.82s]: training loss=0.3356034755706787  validation ndcg@10=0.07444102776472339 [0.29s]\n",
      "epoch 26 [23.05s]:  training loss=0.32825011014938354                                   \n",
      "epoch 27 [22.85s]:  training loss=0.3288465142250061                                    \n",
      "epoch 28 [22.73s]:  training loss=0.3235284686088562                                    \n",
      "epoch 29 [23.38s]:  training loss=0.32983478903770447                                   \n",
      "epoch 30 [22.5s]: training loss=0.32231274247169495  validation ndcg@10=0.06230188997998299 [0.3s]\n",
      "epoch 31 [23.04s]:  training loss=0.3349704146385193                                    \n",
      "epoch 32 [24.69s]:  training loss=0.3268079459667206                                    \n",
      "epoch 33 [22.3s]:  training loss=0.32511624693870544                                    \n",
      "epoch 34 [23.09s]:  training loss=0.3304404318332672                                    \n",
      "epoch 35 [23.05s]: training loss=0.3242413401603699  validation ndcg@10=0.053234768429728845 [0.31s]\n",
      "epoch 1 [3.61s]:  training loss=0.6208085417747498                                      \n",
      "epoch 2 [3.63s]:  training loss=0.4480089247226715                                      \n",
      "epoch 3 [3.47s]:  training loss=0.42677903175354004                                     \n",
      "epoch 4 [3.69s]:  training loss=0.40282490849494934                                     \n",
      "epoch 5 [3.57s]: training loss=0.39122340083122253  validation ndcg@10=0.06176210059946025 [0.06s]\n",
      "epoch 6 [3.45s]:  training loss=0.3800729811191559                                      \n",
      "epoch 7 [3.35s]:  training loss=0.3675106167793274                                      \n",
      "epoch 8 [3.39s]:  training loss=0.3762497305870056                                      \n",
      "epoch 9 [3.17s]:  training loss=0.3672926723957062                                      \n",
      "epoch 10 [3.17s]: training loss=0.3609178960323334  validation ndcg@10=0.062514890968188 [0.05s]\n",
      "epoch 11 [3.11s]:  training loss=0.3531795144081116                                     \n",
      "epoch 12 [3.1s]:  training loss=0.3576008975505829                                      \n",
      "epoch 13 [3.1s]:  training loss=0.35281607508659363                                     \n",
      "epoch 14 [3.16s]:  training loss=0.3452703356742859                                     \n",
      "epoch 15 [3.07s]: training loss=0.3457358479499817  validation ndcg@10=0.05470804204491139 [0.07s]\n",
      "epoch 16 [3.11s]:  training loss=0.3444766700267792                                     \n",
      "epoch 17 [3.09s]:  training loss=0.3372529447078705                                     \n",
      "epoch 18 [3.09s]:  training loss=0.3362193703651428                                     \n",
      "epoch 19 [3.14s]:  training loss=0.34344688057899475                                    \n",
      "epoch 20 [3.11s]: training loss=0.33418241143226624  validation ndcg@10=0.06807349734890077 [0.08s]\n",
      "epoch 21 [3.09s]:  training loss=0.3270685076713562                                     \n",
      "epoch 22 [3.07s]:  training loss=0.330384761095047                                      \n",
      "epoch 23 [3.1s]:  training loss=0.3284931480884552                                      \n",
      "epoch 24 [3.04s]:  training loss=0.32626110315322876                                    \n",
      "epoch 25 [3.11s]: training loss=0.3333265483379364  validation ndcg@10=0.06085669156847232 [0.06s]\n",
      "epoch 26 [3.1s]:  training loss=0.3193124234676361                                      \n",
      "epoch 27 [3.07s]:  training loss=0.32211169600486755                                    \n",
      "epoch 28 [3.04s]:  training loss=0.3213711977005005                                     \n",
      "epoch 29 [3.13s]:  training loss=0.3200131952762604                                     \n",
      "epoch 30 [3.21s]: training loss=0.32146313786506653  validation ndcg@10=0.06313353482659043 [0.06s]\n",
      "epoch 31 [3.17s]:  training loss=0.31943657994270325                                    \n",
      "epoch 32 [3.17s]:  training loss=0.31916138529777527                                    \n",
      "epoch 33 [3.16s]:  training loss=0.32049354910850525                                    \n",
      "epoch 34 [3.15s]:  training loss=0.3199264407157898                                     \n",
      "epoch 35 [3.11s]: training loss=0.31950241327285767  validation ndcg@10=0.04695900878611089 [0.05s]\n",
      "epoch 36 [3.15s]:  training loss=0.31206709146499634                                    \n",
      "epoch 37 [3.19s]:  training loss=0.31386125087738037                                    \n",
      "epoch 38 [3.19s]:  training loss=0.3179846405982971                                     \n",
      "epoch 39 [3.18s]:  training loss=0.30814146995544434                                    \n",
      "epoch 40 [3.11s]: training loss=0.3190344274044037  validation ndcg@10=0.06288596042065348 [0.06s]\n",
      "epoch 41 [3.12s]:  training loss=0.3137929439544678                                     \n",
      "epoch 42 [3.18s]:  training loss=0.30814340710639954                                    \n",
      "epoch 43 [3.07s]:  training loss=0.3134484887123108                                     \n",
      "epoch 44 [3.12s]:  training loss=0.3115517497062683                                     \n",
      "epoch 45 [3.07s]: training loss=0.3102048933506012  validation ndcg@10=0.05201358402052103 [0.07s]\n",
      "epoch 1 [11.81s]:  training loss=0.5590288043022156                                     \n",
      "epoch 2 [12.45s]:  training loss=0.4511500895023346                                     \n",
      "epoch 3 [12.54s]:  training loss=0.4237156808376312                                     \n",
      "epoch 4 [12.24s]:  training loss=0.4027968645095825                                     \n",
      "epoch 5 [12.34s]: training loss=0.39031490683555603  validation ndcg@10=0.06458096501577594 [0.15s]\n",
      "epoch 6 [12.62s]:  training loss=0.3822227418422699                                     \n",
      "epoch 7 [12.89s]:  training loss=0.375164270401001                                      \n",
      "epoch 8 [12.53s]:  training loss=0.36829692125320435                                    \n",
      "epoch 9 [12.29s]:  training loss=0.3628387749195099                                     \n",
      "epoch 10 [12.56s]: training loss=0.35838210582733154  validation ndcg@10=0.06674888349609635 [0.18s]\n",
      "epoch 11 [12.41s]:  training loss=0.360150009393692                                     \n",
      "epoch 12 [12.3s]:  training loss=0.35740309953689575                                    \n",
      "epoch 13 [12.42s]:  training loss=0.35167908668518066                                   \n",
      "epoch 14 [12.09s]:  training loss=0.34768909215927124                                   \n",
      "epoch 15 [12.43s]: training loss=0.3515774607658386  validation ndcg@10=0.06426791438779446 [0.15s]\n",
      "epoch 16 [14.63s]:  training loss=0.34360381960868835                                   \n",
      "epoch 17 [12.2s]:  training loss=0.33899375796318054                                    \n",
      "epoch 18 [12.34s]:  training loss=0.33589720726013184                                   \n",
      "epoch 19 [12.63s]:  training loss=0.3501206934452057                                    \n",
      "epoch 20 [12.27s]: training loss=0.33989959955215454  validation ndcg@10=0.058073565055885436 [0.16s]\n",
      "epoch 21 [13.36s]:  training loss=0.3379131555557251                                    \n",
      "epoch 22 [12.68s]:  training loss=0.3349016606807709                                    \n",
      "epoch 23 [12.32s]:  training loss=0.33454081416130066                                   \n",
      "epoch 24 [12.74s]:  training loss=0.3346489667892456                                    \n",
      "epoch 25 [12.85s]: training loss=0.3315396010875702  validation ndcg@10=0.04000500048759392 [0.18s]\n",
      "epoch 26 [12.49s]:  training loss=0.33054712414741516                                   \n",
      "epoch 27 [12.68s]:  training loss=0.33172932267189026                                   \n",
      "epoch 28 [12.34s]:  training loss=0.32673949003219604                                   \n",
      "epoch 29 [12.57s]:  training loss=0.32630935311317444                                   \n",
      "epoch 30 [13.19s]: training loss=0.3294721245765686  validation ndcg@10=0.05165699856009941 [0.17s]\n",
      "epoch 31 [12.79s]:  training loss=0.3328845202922821                                    \n",
      "epoch 32 [12.55s]:  training loss=0.3341173529624939                                    \n",
      "epoch 33 [12.13s]:  training loss=0.32499706745147705                                   \n",
      "epoch 34 [12.4s]:  training loss=0.325610488653183                                      \n",
      "epoch 35 [12.62s]: training loss=0.3339056372642517  validation ndcg@10=0.05432263230440559 [0.15s]\n",
      "epoch 1 [16.91s]:  training loss=1.0616875886917114                                     \n",
      "epoch 2 [17.91s]:  training loss=1.0315377712249756                                    \n",
      "epoch 3 [16.88s]:  training loss=1.0183894634246826                                    \n",
      "epoch 4 [17.85s]:  training loss=0.9788617491722107                                    \n",
      "epoch 5 [17.27s]: training loss=0.968967854976654  validation ndcg@10=0.02275290135631291 [0.27s]\n",
      "epoch 6 [17.93s]:  training loss=0.9515470862388611                                    \n",
      "epoch 7 [17.58s]:  training loss=0.9335418343544006                                    \n",
      "epoch 8 [17.83s]:  training loss=0.9126816391944885                                    \n",
      "epoch 9 [17.81s]:  training loss=0.8992453813552856                                    \n",
      "epoch 10 [17.67s]: training loss=0.8911077380180359  validation ndcg@10=0.02534343774849159 [0.27s]\n",
      "epoch 11 [20.06s]:  training loss=0.8819122314453125                                   \n",
      "epoch 12 [18.2s]:  training loss=0.8628548383712769                                    \n",
      "epoch 13 [17.67s]:  training loss=0.8494607210159302                                   \n",
      "epoch 14 [17.13s]:  training loss=0.8324978351593018                                   \n",
      "epoch 15 [18.41s]: training loss=0.8254000544548035  validation ndcg@10=0.028879729028016 [0.26s]\n",
      "epoch 16 [17.54s]:  training loss=0.8245265483856201                                   \n",
      "epoch 17 [17.29s]:  training loss=0.8057345747947693                                   \n",
      "epoch 18 [16.93s]:  training loss=0.798308253288269                                    \n",
      "epoch 19 [17.39s]:  training loss=0.7884330749511719                                   \n",
      "epoch 20 [17.48s]: training loss=0.7853095531463623  validation ndcg@10=0.03464359839521068 [0.28s]\n",
      "epoch 21 [17.49s]:  training loss=0.7686142325401306                                   \n",
      "epoch 22 [17.44s]:  training loss=0.770760715007782                                    \n",
      "epoch 23 [17.86s]:  training loss=0.7537750601768494                                   \n",
      "epoch 24 [17.29s]:  training loss=0.7399498224258423                                   \n",
      "epoch 25 [17.44s]: training loss=0.741306722164154  validation ndcg@10=0.03720604912334853 [0.28s]\n",
      "epoch 26 [17.82s]:  training loss=0.7304461002349854                                   \n",
      "epoch 27 [17.08s]:  training loss=0.7192342877388                                      \n",
      "epoch 28 [17.52s]:  training loss=0.7238044738769531                                   \n",
      "epoch 29 [17.72s]:  training loss=0.7081106305122375                                   \n",
      "epoch 30 [17.56s]: training loss=0.7062707543373108  validation ndcg@10=0.03889043520082323 [0.26s]\n",
      "epoch 31 [17.44s]:  training loss=0.7053773403167725                                   \n",
      "epoch 32 [17.37s]:  training loss=0.6879921555519104                                   \n",
      "epoch 33 [17.64s]:  training loss=0.6863610744476318                                   \n",
      "epoch 34 [17.54s]:  training loss=0.6844515800476074                                   \n",
      "epoch 35 [19.76s]: training loss=0.6724560856819153  validation ndcg@10=0.03723441230720134 [0.3s]\n",
      "epoch 36 [17.7s]:  training loss=0.6688622236251831                                    \n",
      "epoch 37 [17.6s]:  training loss=0.6663672924041748                                    \n",
      "epoch 38 [17.41s]:  training loss=0.6538182497024536                                   \n",
      "epoch 39 [17.26s]:  training loss=0.6565609574317932                                   \n",
      "epoch 40 [17.79s]: training loss=0.6550462245941162  validation ndcg@10=0.036660461032673536 [0.29s]\n",
      "epoch 41 [18.15s]:  training loss=0.639579176902771                                    \n",
      "epoch 42 [17.24s]:  training loss=0.629155695438385                                    \n",
      "epoch 43 [17.87s]:  training loss=0.6396497488021851                                   \n",
      "epoch 44 [17.22s]:  training loss=0.6187105774879456                                   \n",
      "epoch 45 [17.58s]: training loss=0.6183612942695618  validation ndcg@10=0.03796818950397008 [0.25s]\n",
      "epoch 46 [16.89s]:  training loss=0.6111183762550354                                   \n",
      "epoch 47 [17.78s]:  training loss=0.6120561957359314                                   \n",
      "epoch 48 [17.43s]:  training loss=0.5984654426574707                                   \n",
      "epoch 49 [17.51s]:  training loss=0.5975867509841919                                   \n",
      "epoch 50 [17.21s]: training loss=0.5966284871101379  validation ndcg@10=0.04147061045512189 [0.26s]\n",
      "epoch 51 [17.33s]:  training loss=0.5847490429878235                                   \n",
      "epoch 52 [18.23s]:  training loss=0.5880649089813232                                   \n",
      "epoch 53 [17.1s]:  training loss=0.5791444778442383                                    \n",
      "epoch 54 [17.9s]:  training loss=0.5767654776573181                                    \n",
      "epoch 55 [17.22s]: training loss=0.5778597593307495  validation ndcg@10=0.042821230684063565 [0.25s]\n",
      "epoch 56 [17.14s]:  training loss=0.5690056681632996                                   \n",
      "epoch 57 [18.05s]:  training loss=0.5638790130615234                                   \n",
      "epoch 58 [17.22s]:  training loss=0.5557164549827576                                   \n",
      "epoch 59 [18.25s]:  training loss=0.5536556839942932                                   \n",
      "epoch 60 [19.35s]: training loss=0.5530477166175842  validation ndcg@10=0.04471289437393072 [0.25s]\n",
      "epoch 61 [17.57s]:  training loss=0.5453782081604004                                   \n",
      "epoch 62 [17.68s]:  training loss=0.5440919995307922                                   \n",
      "epoch 63 [17.34s]:  training loss=0.5439881086349487                                   \n",
      "epoch 64 [17.31s]:  training loss=0.5355302691459656                                   \n",
      "epoch 65 [17.29s]: training loss=0.5342116951942444  validation ndcg@10=0.05106020838822311 [0.24s]\n",
      "epoch 66 [17.57s]:  training loss=0.5312718152999878                                   \n",
      "epoch 67 [17.57s]:  training loss=0.523991584777832                                    \n",
      "epoch 68 [17.97s]:  training loss=0.5249247550964355                                   \n",
      "epoch 69 [17.49s]:  training loss=0.5208746790885925                                   \n",
      "epoch 70 [16.96s]: training loss=0.5096259117126465  validation ndcg@10=0.05163022040639711 [0.24s]\n",
      "epoch 71 [17.01s]:  training loss=0.5087896585464478                                   \n",
      "epoch 72 [18.64s]:  training loss=0.5133059024810791                                   \n",
      "epoch 73 [17.69s]:  training loss=0.5069580078125                                      \n",
      "epoch 74 [18.1s]:  training loss=0.5121997594833374                                    \n",
      "epoch 75 [17.56s]: training loss=0.4968612790107727  validation ndcg@10=0.053066358925968726 [0.24s]\n",
      "epoch 76 [16.93s]:  training loss=0.49330583214759827                                  \n",
      "epoch 77 [17.63s]:  training loss=0.4981550872325897                                   \n",
      "epoch 78 [17.18s]:  training loss=0.4883166253566742                                   \n",
      "epoch 79 [17.47s]:  training loss=0.49464336037635803                                  \n",
      "epoch 80 [16.68s]: training loss=0.486958384513855  validation ndcg@10=0.0550416197333793 [0.25s]\n",
      "epoch 81 [17.55s]:  training loss=0.4872244894504547                                   \n",
      "epoch 82 [17.4s]:  training loss=0.4821474254131317                                    \n",
      "epoch 83 [17.54s]:  training loss=0.47782832384109497                                  \n",
      "epoch 84 [17.91s]:  training loss=0.4764527678489685                                   \n",
      "epoch 85 [19.39s]: training loss=0.4798814356327057  validation ndcg@10=0.053096856761639256 [0.24s]\n",
      "epoch 86 [17.53s]:  training loss=0.47339287400245667                                  \n",
      "epoch 87 [17.41s]:  training loss=0.47566694021224976                                  \n",
      "epoch 88 [17.83s]:  training loss=0.4730819761753082                                   \n",
      "epoch 89 [17.76s]:  training loss=0.4673551321029663                                   \n",
      "epoch 90 [17.63s]: training loss=0.47285163402557373  validation ndcg@10=0.05227878837462313 [0.25s]\n",
      "epoch 91 [17.6s]:  training loss=0.46660301089286804                                   \n",
      "epoch 92 [18.27s]:  training loss=0.4678341746330261                                   \n",
      "epoch 93 [18.62s]:  training loss=0.4642987847328186                                   \n",
      "epoch 94 [17.45s]:  training loss=0.46820148825645447                                  \n",
      "epoch 95 [18.22s]: training loss=0.46201926469802856  validation ndcg@10=0.05398189022025026 [0.26s]\n",
      "epoch 96 [18.33s]:  training loss=0.4604964554309845                                   \n",
      "epoch 97 [18.4s]:  training loss=0.47029104828834534                                   \n",
      "epoch 98 [17.97s]:  training loss=0.45278501510620117                                  \n",
      "epoch 99 [17.29s]:  training loss=0.459233820438385                                    \n",
      "epoch 100 [18.31s]: training loss=0.4502273499965668  validation ndcg@10=0.05526768269949545 [0.25s]\n",
      "epoch 101 [18.24s]:  training loss=0.4577678143978119                                  \n",
      "epoch 102 [17.8s]:  training loss=0.4482731223106384                                   \n",
      "epoch 103 [18.8s]:  training loss=0.45017707347869873                                  \n",
      "epoch 104 [17.81s]:  training loss=0.45037907361984253                                 \n",
      "epoch 105 [17.82s]: training loss=0.4476512372493744  validation ndcg@10=0.05274932759544496 [0.26s]\n",
      "epoch 106 [17.45s]:  training loss=0.44651830196380615                                 \n",
      "epoch 107 [18.17s]:  training loss=0.4450453817844391                                  \n",
      "epoch 108 [17.57s]:  training loss=0.44667938351631165                                 \n",
      "epoch 109 [17.43s]:  training loss=0.44488269090652466                                 \n",
      "epoch 110 [17.96s]: training loss=0.44658908247947693  validation ndcg@10=0.05376828140380143 [0.28s]\n",
      "epoch 111 [17.8s]:  training loss=0.44431114196777344                                  \n",
      "epoch 112 [19.0s]:  training loss=0.44971224665641785                                  \n",
      "epoch 113 [17.7s]:  training loss=0.4430215060710907                                   \n",
      "epoch 114 [17.37s]:  training loss=0.4424894154071808                                  \n",
      "epoch 115 [17.52s]: training loss=0.440305233001709  validation ndcg@10=0.054717782506638835 [0.26s]\n",
      "epoch 116 [17.73s]:  training loss=0.43560072779655457                                 \n",
      "epoch 117 [18.11s]:  training loss=0.44065335392951965                                 \n",
      "epoch 118 [17.63s]:  training loss=0.4399559497833252                                  \n",
      "epoch 119 [17.57s]:  training loss=0.4419477880001068                                  \n",
      "epoch 120 [17.54s]: training loss=0.4392164647579193  validation ndcg@10=0.054060240742565796 [0.28s]\n",
      "epoch 121 [17.66s]:  training loss=0.4365132749080658                                  \n",
      "epoch 122 [17.76s]:  training loss=0.43826889991760254                                 \n",
      "epoch 123 [17.92s]:  training loss=0.437197208404541                                   \n",
      "epoch 124 [17.51s]:  training loss=0.4361187815666199                                  \n",
      "epoch 125 [18.11s]: training loss=0.43857887387275696  validation ndcg@10=0.054372929754499186 [0.26s]\n",
      "epoch 1 [9.58s]:  training loss=1.0599489212036133                                      \n",
      "epoch 2 [9.36s]:  training loss=0.9830556511878967                                      \n",
      "epoch 3 [9.22s]:  training loss=0.9359611868858337                                      \n",
      "epoch 4 [9.45s]:  training loss=0.8946499824523926                                      \n",
      "epoch 5 [9.56s]: training loss=0.8560448288917542  validation ndcg@10=0.028925028697849846 [0.15s]\n",
      "epoch 6 [9.38s]:  training loss=0.8365283012390137                                      \n",
      "epoch 7 [9.23s]:  training loss=0.8055318593978882                                      \n",
      "epoch 8 [9.22s]:  training loss=0.7836800217628479                                      \n",
      "epoch 9 [9.22s]:  training loss=0.757681667804718                                       \n",
      "epoch 10 [9.23s]: training loss=0.7395960092544556  validation ndcg@10=0.03114006580973633 [0.13s]\n",
      "epoch 11 [9.09s]:  training loss=0.7198739647865295                                     \n",
      "epoch 12 [9.39s]:  training loss=0.7005045413970947                                     \n",
      "epoch 13 [9.17s]:  training loss=0.6829720735549927                                     \n",
      "epoch 14 [9.55s]:  training loss=0.6681180000305176                                     \n",
      "epoch 15 [9.47s]: training loss=0.6528443694114685  validation ndcg@10=0.03704810782890265 [0.14s]\n",
      "epoch 16 [9.77s]:  training loss=0.6383568644523621                                     \n",
      "epoch 17 [9.52s]:  training loss=0.624642014503479                                      \n",
      "epoch 18 [9.37s]:  training loss=0.6035953164100647                                     \n",
      "epoch 19 [9.2s]:  training loss=0.5985909104347229                                      \n",
      "epoch 20 [9.53s]: training loss=0.5909629464149475  validation ndcg@10=0.038578384154104424 [0.16s]\n",
      "epoch 21 [9.46s]:  training loss=0.5742209553718567                                     \n",
      "epoch 22 [9.34s]:  training loss=0.5595751404762268                                     \n",
      "epoch 23 [9.35s]:  training loss=0.5466623306274414                                     \n",
      "epoch 24 [9.52s]:  training loss=0.5411369800567627                                     \n",
      "epoch 25 [9.62s]: training loss=0.5317869186401367  validation ndcg@10=0.04048213698598117 [0.14s]\n",
      "epoch 26 [9.27s]:  training loss=0.52105712890625                                       \n",
      "epoch 27 [9.53s]:  training loss=0.5156317949295044                                     \n",
      "epoch 28 [9.65s]:  training loss=0.508272111415863                                      \n",
      "epoch 29 [9.66s]:  training loss=0.5032284259796143                                     \n",
      "epoch 30 [11.12s]: training loss=0.4902258813381195  validation ndcg@10=0.04218136487758616 [0.13s]\n",
      "epoch 31 [9.44s]:  training loss=0.4829978048801422                                     \n",
      "epoch 32 [9.39s]:  training loss=0.47658994793891907                                    \n",
      "epoch 33 [9.52s]:  training loss=0.48266878724098206                                    \n",
      "epoch 34 [9.63s]:  training loss=0.4721553921699524                                     \n",
      "epoch 35 [9.81s]: training loss=0.47359806299209595  validation ndcg@10=0.04522348119845499 [0.19s]\n",
      "epoch 36 [9.81s]:  training loss=0.4647890329360962                                     \n",
      "epoch 37 [9.46s]:  training loss=0.4620315432548523                                     \n",
      "epoch 38 [9.32s]:  training loss=0.46223515272140503                                    \n",
      "epoch 39 [9.56s]:  training loss=0.45302799344062805                                    \n",
      "epoch 40 [9.25s]: training loss=0.4505990743637085  validation ndcg@10=0.04413649542136247 [0.13s]\n",
      "epoch 41 [9.28s]:  training loss=0.44577234983444214                                    \n",
      "epoch 42 [9.51s]:  training loss=0.45029202103614807                                    \n",
      "epoch 43 [9.95s]:  training loss=0.4434078335762024                                     \n",
      "epoch 44 [9.56s]:  training loss=0.44285446405410767                                    \n",
      "epoch 45 [9.68s]: training loss=0.442966490983963  validation ndcg@10=0.04698577116818212 [0.14s]\n",
      "epoch 46 [9.69s]:  training loss=0.4424940049648285                                     \n",
      "epoch 47 [9.76s]:  training loss=0.4394054710865021                                     \n",
      "epoch 48 [9.47s]:  training loss=0.4374109208583832                                     \n",
      "epoch 49 [9.59s]:  training loss=0.4346545934677124                                     \n",
      "epoch 50 [9.36s]: training loss=0.4266415536403656  validation ndcg@10=0.04483077283594802 [0.14s]\n",
      "epoch 51 [9.14s]:  training loss=0.43370217084884644                                    \n",
      "epoch 52 [9.48s]:  training loss=0.4329985976219177                                     \n",
      "epoch 53 [9.29s]:  training loss=0.4343281388282776                                     \n",
      "epoch 54 [9.47s]:  training loss=0.42948076128959656                                    \n",
      "epoch 55 [9.4s]: training loss=0.4303472638130188  validation ndcg@10=0.046997653189858324 [0.13s]\n",
      "epoch 56 [9.39s]:  training loss=0.4234418272972107                                     \n",
      "epoch 57 [9.3s]:  training loss=0.4199654459953308                                      \n",
      "epoch 58 [9.22s]:  training loss=0.4198547899723053                                     \n",
      "epoch 59 [9.55s]:  training loss=0.4160096347332001                                     \n",
      "epoch 60 [9.34s]: training loss=0.4250381588935852  validation ndcg@10=0.04891053807978031 [0.13s]\n",
      "epoch 61 [9.5s]:  training loss=0.4218734800815582                                      \n",
      "epoch 62 [9.36s]:  training loss=0.4180740714073181                                     \n",
      "epoch 63 [9.35s]:  training loss=0.42150160670280457                                    \n",
      "epoch 64 [9.49s]:  training loss=0.4221467971801758                                     \n",
      "epoch 65 [9.68s]: training loss=0.4087558388710022  validation ndcg@10=0.049148481717622906 [0.14s]\n",
      "epoch 66 [9.12s]:  training loss=0.41240036487579346                                    \n",
      "epoch 67 [9.34s]:  training loss=0.41222459077835083                                    \n",
      "epoch 68 [9.35s]:  training loss=0.41512200236320496                                    \n",
      "epoch 69 [9.41s]:  training loss=0.41450098156929016                                    \n",
      "epoch 70 [9.55s]: training loss=0.40727537870407104  validation ndcg@10=0.04742191697007962 [0.18s]\n",
      "epoch 71 [9.43s]:  training loss=0.41344374418258667                                    \n",
      "epoch 72 [9.6s]:  training loss=0.4212399423122406                                      \n",
      "epoch 73 [9.43s]:  training loss=0.41777145862579346                                    \n",
      "epoch 74 [9.14s]:  training loss=0.4181601107120514                                     \n",
      "epoch 75 [9.14s]: training loss=0.41319769620895386  validation ndcg@10=0.04718453478561353 [0.14s]\n",
      "epoch 76 [9.41s]:  training loss=0.4179435074329376                                     \n",
      "epoch 77 [9.31s]:  training loss=0.4109855890274048                                     \n",
      "epoch 78 [9.34s]:  training loss=0.40489667654037476                                    \n",
      "epoch 79 [9.19s]:  training loss=0.41192686557769775                                    \n",
      "epoch 80 [8.98s]: training loss=0.4097365438938141  validation ndcg@10=0.054877759228768745 [0.13s]\n",
      "epoch 81 [9.17s]:  training loss=0.41101399064064026                                    \n",
      "epoch 82 [9.46s]:  training loss=0.4111737608909607                                     \n",
      "epoch 83 [9.25s]:  training loss=0.41252005100250244                                    \n",
      "epoch 84 [9.23s]:  training loss=0.409334272146225                                      \n",
      "epoch 85 [9.7s]: training loss=0.408952534198761  validation ndcg@10=0.05244724265638166 [0.13s]\n",
      "epoch 86 [11.28s]:  training loss=0.4120972156524658                                    \n",
      "epoch 87 [9.68s]:  training loss=0.401153564453125                                      \n",
      "epoch 88 [9.26s]:  training loss=0.40831393003463745                                    \n",
      "epoch 89 [9.06s]:  training loss=0.40625181794166565                                    \n",
      "epoch 90 [8.98s]: training loss=0.40879759192466736  validation ndcg@10=0.056341543690449185 [0.16s]\n",
      "epoch 91 [9.43s]:  training loss=0.4067656099796295                                     \n",
      "epoch 92 [9.22s]:  training loss=0.4076976776123047                                     \n",
      "epoch 93 [9.68s]:  training loss=0.4048480987548828                                     \n",
      "epoch 94 [9.49s]:  training loss=0.40534815192222595                                    \n",
      "epoch 95 [9.73s]: training loss=0.40210697054862976  validation ndcg@10=0.06062619216792184 [0.13s]\n",
      "epoch 96 [9.62s]:  training loss=0.4065382778644562                                     \n",
      "epoch 97 [9.65s]:  training loss=0.4077799916267395                                     \n",
      "epoch 98 [9.24s]:  training loss=0.39629045128822327                                    \n",
      "epoch 99 [9.62s]:  training loss=0.4055337607860565                                     \n",
      "epoch 100 [9.6s]: training loss=0.4044129550457001  validation ndcg@10=0.05486615347506878 [0.13s]\n",
      "epoch 101 [9.21s]:  training loss=0.41060104966163635                                   \n",
      "epoch 102 [9.31s]:  training loss=0.41175663471221924                                   \n",
      "epoch 103 [9.56s]:  training loss=0.4031102657318115                                    \n",
      "epoch 104 [9.58s]:  training loss=0.400818794965744                                     \n",
      "epoch 105 [9.51s]: training loss=0.3981172740459442  validation ndcg@10=0.060311551231741416 [0.13s]\n",
      "epoch 106 [9.5s]:  training loss=0.4001353979110718                                     \n",
      "epoch 107 [9.45s]:  training loss=0.4058186709880829                                    \n",
      "epoch 108 [9.31s]:  training loss=0.3954755663871765                                    \n",
      "epoch 109 [9.35s]:  training loss=0.3905557096004486                                    \n",
      "epoch 110 [9.14s]: training loss=0.39051592350006104  validation ndcg@10=0.05756600179529565 [0.13s]\n",
      "epoch 111 [9.59s]:  training loss=0.4028147757053375                                    \n",
      "epoch 112 [9.6s]:  training loss=0.3975287973880768                                     \n",
      "epoch 113 [9.21s]:  training loss=0.4007280468940735                                    \n",
      "epoch 114 [9.34s]:  training loss=0.3925153613090515                                    \n",
      "epoch 115 [9.52s]: training loss=0.3952838182449341  validation ndcg@10=0.058775132610568506 [0.15s]\n",
      "epoch 116 [9.28s]:  training loss=0.3946051597595215                                    \n",
      "epoch 117 [9.34s]:  training loss=0.3969040513038635                                    \n",
      "epoch 118 [9.4s]:  training loss=0.3948647081851959                                     \n",
      "epoch 119 [9.45s]:  training loss=0.39410707354545593                                   \n",
      "epoch 120 [9.51s]: training loss=0.3958820402622223  validation ndcg@10=0.053058927451411816 [0.14s]\n",
      "epoch 1 [13.45s]:  training loss=0.8286482095718384                                     \n",
      "epoch 2 [13.43s]:  training loss=0.5924563407897949                                     \n",
      "epoch 3 [13.05s]:  training loss=0.48760244250297546                                    \n",
      "epoch 4 [12.86s]:  training loss=0.444979727268219                                      \n",
      "epoch 5 [12.71s]: training loss=0.4375401437282562  validation ndcg@10=0.058894392850701964 [0.21s]\n",
      "epoch 6 [13.61s]:  training loss=0.427339106798172                                      \n",
      "epoch 7 [13.53s]:  training loss=0.4282698333263397                                     \n",
      "epoch 8 [13.34s]:  training loss=0.4176986515522003                                     \n",
      "epoch 9 [12.98s]:  training loss=0.41700106859207153                                    \n",
      "epoch 10 [12.74s]: training loss=0.4044208526611328  validation ndcg@10=0.05879025730885012 [0.25s]\n",
      "epoch 11 [13.33s]:  training loss=0.4034441411495209                                    \n",
      "epoch 12 [13.04s]:  training loss=0.39670878648757935                                   \n",
      "epoch 13 [13.01s]:  training loss=0.38716229796409607                                   \n",
      "epoch 14 [12.86s]:  training loss=0.38679054379463196                                   \n",
      "epoch 15 [12.9s]: training loss=0.39275097846984863  validation ndcg@10=0.06697654270173897 [0.17s]\n",
      "epoch 16 [13.17s]:  training loss=0.379316508769989                                     \n",
      "epoch 17 [14.94s]:  training loss=0.378173828125                                        \n",
      "epoch 18 [13.12s]:  training loss=0.3793509900569916                                    \n",
      "epoch 19 [12.7s]:  training loss=0.37595343589782715                                    \n",
      "epoch 20 [12.71s]: training loss=0.3670156002044678  validation ndcg@10=0.07964684348934471 [0.18s]\n",
      "epoch 21 [12.51s]:  training loss=0.36445462703704834                                   \n",
      "epoch 22 [12.79s]:  training loss=0.3644600510597229                                    \n",
      "epoch 23 [12.99s]:  training loss=0.3625068962574005                                    \n",
      "epoch 24 [13.16s]:  training loss=0.3607373833656311                                    \n",
      "epoch 25 [12.82s]: training loss=0.35216817259788513  validation ndcg@10=0.07544361749522281 [0.17s]\n",
      "epoch 26 [12.86s]:  training loss=0.36258450150489807                                   \n",
      "epoch 27 [13.7s]:  training loss=0.3496640920639038                                     \n",
      "epoch 28 [12.75s]:  training loss=0.35381683707237244                                   \n",
      "epoch 29 [13.1s]:  training loss=0.35174560546875                                       \n",
      "epoch 30 [13.48s]: training loss=0.3503522276878357  validation ndcg@10=0.07453033898649171 [0.21s]\n",
      "epoch 31 [12.87s]:  training loss=0.3430839478969574                                    \n",
      "epoch 32 [12.61s]:  training loss=0.3506331741809845                                    \n",
      "epoch 33 [13.23s]:  training loss=0.35008689761161804                                   \n",
      "epoch 34 [13.66s]:  training loss=0.34757888317108154                                   \n",
      "epoch 35 [13.54s]: training loss=0.3425540328025818  validation ndcg@10=0.06765232479038909 [0.17s]\n",
      "epoch 36 [13.08s]:  training loss=0.34703773260116577                                    \n",
      "epoch 37 [12.87s]:  training loss=0.34740906953811646                                    \n",
      "epoch 38 [12.52s]:  training loss=0.3403337597846985                                     \n",
      "epoch 39 [12.9s]:  training loss=0.33794692158699036                                     \n",
      "epoch 40 [12.78s]: training loss=0.33843761682510376  validation ndcg@10=0.07533292702544707 [0.2s]\n",
      "epoch 41 [12.92s]:  training loss=0.33874720335006714                                    \n",
      "epoch 42 [13.18s]:  training loss=0.3359776735305786                                     \n",
      "epoch 43 [12.5s]:  training loss=0.33509108424186707                                     \n",
      "epoch 44 [13.0s]:  training loss=0.336269348859787                                       \n",
      "epoch 45 [12.64s]: training loss=0.3303893208503723  validation ndcg@10=0.07411097661208807 [0.17s]\n",
      "epoch 1 [17.04s]:  training loss=0.5898034572601318                                      \n",
      "epoch 2 [17.52s]:  training loss=0.5522249937057495                                      \n",
      "epoch 3 [17.91s]:  training loss=0.6007883548736572                                      \n",
      "epoch 4 [16.96s]:  training loss=0.636664867401123                                       \n",
      "epoch 5 [17.33s]: training loss=0.6577200293540955  validation ndcg@10=0.04269951547616223 [0.21s]\n",
      "epoch 6 [17.03s]:  training loss=0.6784644722938538                                      \n",
      "epoch 7 [18.51s]:  training loss=0.7021991610527039                                      \n",
      "epoch 8 [18.19s]:  training loss=0.7161151170730591                                      \n",
      "epoch 9 [17.12s]:  training loss=0.7112784385681152                                      \n",
      "epoch 10 [17.44s]: training loss=0.7476672530174255  validation ndcg@10=0.06282373417788224 [0.23s]\n",
      "epoch 11 [17.42s]:  training loss=0.7357955574989319                                     \n",
      "epoch 12 [17.26s]:  training loss=0.7563700675964355                                     \n",
      "epoch 13 [17.26s]:  training loss=0.7690936326980591                                     \n",
      "epoch 14 [17.0s]:  training loss=0.7547275424003601                                      \n",
      "epoch 15 [17.17s]: training loss=0.7638417482376099  validation ndcg@10=0.05371887267198262 [0.2s]\n",
      "epoch 16 [17.27s]:  training loss=0.7719884514808655                                     \n",
      "epoch 17 [17.38s]:  training loss=0.8008115887641907                                     \n",
      "epoch 18 [17.2s]:  training loss=0.7944049835205078                                      \n",
      "epoch 19 [17.27s]:  training loss=0.8165513873100281                                     \n",
      "epoch 20 [17.24s]: training loss=0.8107747435569763  validation ndcg@10=0.0414650034009406 [0.2s]\n",
      "epoch 21 [17.27s]:  training loss=0.7883551120758057                                     \n",
      "epoch 22 [17.04s]:  training loss=0.8134158849716187                                     \n",
      "epoch 23 [17.21s]:  training loss=0.8132317066192627                                     \n",
      "epoch 24 [17.01s]:  training loss=0.8072242736816406                                     \n",
      "epoch 25 [16.98s]: training loss=0.836625337600708  validation ndcg@10=0.03727141383965852 [0.21s]\n",
      "epoch 26 [17.12s]:  training loss=0.8235692977905273                                     \n",
      "epoch 27 [17.48s]:  training loss=0.8312848210334778                                     \n",
      "epoch 28 [17.15s]:  training loss=0.834126889705658                                      \n",
      "epoch 29 [17.28s]:  training loss=0.812843382358551                                      \n",
      "epoch 30 [17.04s]: training loss=0.8193104267120361  validation ndcg@10=0.03710329347317401 [0.24s]\n",
      "epoch 31 [17.45s]:  training loss=0.8200177550315857                                     \n",
      "epoch 32 [17.72s]:  training loss=0.8393787741661072                                     \n",
      "epoch 33 [19.66s]:  training loss=0.8508821129798889                                     \n",
      "epoch 34 [17.71s]:  training loss=0.8386154770851135                                     \n",
      "epoch 35 [17.45s]: training loss=0.8375928401947021  validation ndcg@10=0.03963467615121525 [0.24s]\n",
      "epoch 1 [12.06s]:  training loss=1.036732792854309                                       \n",
      "epoch 2 [12.76s]:  training loss=0.9640147089958191                                     \n",
      "epoch 3 [12.45s]:  training loss=0.9220072627067566                                     \n",
      "epoch 4 [11.76s]:  training loss=0.8851787447929382                                     \n",
      "epoch 5 [11.37s]: training loss=0.8600501418113708  validation ndcg@10=0.027442590164623814 [0.18s]\n",
      "epoch 6 [11.05s]:  training loss=0.8252020478248596                                     \n",
      "epoch 7 [11.12s]:  training loss=0.8111853003501892                                     \n",
      "epoch 8 [11.3s]:  training loss=0.7879632115364075                                      \n",
      "epoch 9 [11.16s]:  training loss=0.7668565511703491                                     \n",
      "epoch 10 [11.12s]: training loss=0.7476010322570801  validation ndcg@10=0.03352600791170828 [0.19s]\n",
      "epoch 11 [11.12s]:  training loss=0.7422264814376831                                    \n",
      "epoch 12 [11.12s]:  training loss=0.7149598002433777                                    \n",
      "epoch 13 [11.21s]:  training loss=0.6896281838417053                                    \n",
      "epoch 14 [11.13s]:  training loss=0.6936302781105042                                    \n",
      "epoch 15 [11.18s]: training loss=0.6706826686859131  validation ndcg@10=0.0323125629285107 [0.14s]\n",
      "epoch 16 [11.11s]:  training loss=0.65725177526474                                      \n",
      "epoch 17 [11.05s]:  training loss=0.6506264805793762                                    \n",
      "epoch 18 [11.13s]:  training loss=0.638255774974823                                     \n",
      "epoch 19 [11.09s]:  training loss=0.6257598400115967                                    \n",
      "epoch 20 [11.16s]: training loss=0.6133322715759277  validation ndcg@10=0.03492537271705808 [0.16s]\n",
      "epoch 21 [11.1s]:  training loss=0.6051933765411377                                     \n",
      "epoch 22 [10.98s]:  training loss=0.5926613807678223                                    \n",
      "epoch 23 [11.12s]:  training loss=0.5806735754013062                                    \n",
      "epoch 24 [11.05s]:  training loss=0.5775619745254517                                    \n",
      "epoch 25 [11.12s]: training loss=0.5595159530639648  validation ndcg@10=0.04094560462248361 [0.14s]\n",
      "epoch 26 [11.31s]:  training loss=0.5571574568748474                                    \n",
      "epoch 27 [11.1s]:  training loss=0.5453314185142517                                     \n",
      "epoch 28 [11.11s]:  training loss=0.5391466617584229                                    \n",
      "epoch 29 [11.01s]:  training loss=0.52535480260849                                      \n",
      "epoch 30 [11.09s]: training loss=0.5226609110832214  validation ndcg@10=0.04633311042296336 [0.18s]\n",
      "epoch 31 [11.17s]:  training loss=0.5086871981620789                                    \n",
      "epoch 32 [11.03s]:  training loss=0.5042498111724854                                    \n",
      "epoch 33 [11.06s]:  training loss=0.49570518732070923                                   \n",
      "epoch 34 [11.2s]:  training loss=0.48913297057151794                                    \n",
      "epoch 35 [12.92s]: training loss=0.4901072084903717  validation ndcg@10=0.04589048941487738 [0.14s]\n",
      "epoch 36 [11.29s]:  training loss=0.4801074266433716                                    \n",
      "epoch 37 [11.23s]:  training loss=0.47415685653686523                                   \n",
      "epoch 38 [11.18s]:  training loss=0.46856769919395447                                   \n",
      "epoch 39 [11.06s]:  training loss=0.47060471773147583                                   \n",
      "epoch 40 [11.21s]: training loss=0.4653678834438324  validation ndcg@10=0.04633068654757916 [0.17s]\n",
      "epoch 41 [11.1s]:  training loss=0.4634320139884949                                     \n",
      "epoch 42 [11.21s]:  training loss=0.45706918835639954                                   \n",
      "epoch 43 [11.09s]:  training loss=0.46040472388267517                                   \n",
      "epoch 44 [11.13s]:  training loss=0.46020275354385376                                   \n",
      "epoch 45 [11.1s]: training loss=0.4546110928058624  validation ndcg@10=0.05263836480956833 [0.19s]\n",
      "epoch 46 [11.47s]:  training loss=0.4464160203933716                                    \n",
      "epoch 47 [11.59s]:  training loss=0.4525373578071594                                    \n",
      "epoch 48 [11.17s]:  training loss=0.4480469226837158                                    \n",
      "epoch 49 [11.08s]:  training loss=0.44444021582603455                                   \n",
      "epoch 50 [11.16s]: training loss=0.4399993121623993  validation ndcg@10=0.04928489803611869 [0.15s]\n",
      "epoch 51 [11.11s]:  training loss=0.4426592290401459                                    \n",
      "epoch 52 [11.22s]:  training loss=0.43924763798713684                                   \n",
      "epoch 53 [11.07s]:  training loss=0.43401262164115906                                   \n",
      "epoch 54 [11.13s]:  training loss=0.43237102031707764                                   \n",
      "epoch 55 [11.03s]: training loss=0.438115656375885  validation ndcg@10=0.05060493900082643 [0.19s]\n",
      "epoch 56 [11.06s]:  training loss=0.4303712546825409                                    \n",
      "epoch 57 [11.17s]:  training loss=0.4363424479961395                                    \n",
      "epoch 58 [11.19s]:  training loss=0.4295189678668976                                    \n",
      "epoch 59 [11.37s]:  training loss=0.4296218752861023                                    \n",
      "epoch 60 [11.75s]: training loss=0.43375223875045776  validation ndcg@10=0.052048285570202905 [0.16s]\n",
      "epoch 61 [11.13s]:  training loss=0.4273446202278137                                    \n",
      "epoch 62 [11.13s]:  training loss=0.4261482059955597                                    \n",
      "epoch 63 [11.06s]:  training loss=0.42984628677368164                                   \n",
      "epoch 64 [11.15s]:  training loss=0.42812806367874146                                   \n",
      "epoch 65 [11.08s]: training loss=0.4345620274543762  validation ndcg@10=0.05044731803686845 [0.14s]\n",
      "epoch 66 [11.15s]:  training loss=0.42661404609680176                                   \n",
      "epoch 67 [11.04s]:  training loss=0.4240935444831848                                    \n",
      "epoch 68 [11.09s]:  training loss=0.42120105028152466                                   \n",
      "epoch 69 [11.1s]:  training loss=0.41874048113822937                                    \n",
      "epoch 70 [11.04s]: training loss=0.42701002955436707  validation ndcg@10=0.051277527913752896 [0.15s]\n",
      "epoch 1 [20.68s]:  training loss=0.9073076248168945                                     \n",
      "epoch 2 [22.45s]:  training loss=0.7458153963088989                                     \n",
      "epoch 3 [20.66s]:  training loss=0.6341981887817383                                     \n",
      "epoch 4 [20.58s]:  training loss=0.5609519481658936                                     \n",
      "epoch 5 [20.69s]: training loss=0.5008721947669983  validation ndcg@10=0.052270876788962264 [0.27s]\n",
      "epoch 6 [20.77s]:  training loss=0.47620317339897156                                    \n",
      "epoch 7 [20.79s]:  training loss=0.4541643261909485                                     \n",
      "epoch 8 [20.62s]:  training loss=0.4381619393825531                                     \n",
      "epoch 9 [20.65s]:  training loss=0.44096824526786804                                    \n",
      "epoch 10 [20.53s]: training loss=0.43104082345962524  validation ndcg@10=0.05327863965127371 [0.27s]\n",
      "epoch 11 [20.64s]:  training loss=0.42245757579803467                                   \n",
      "epoch 12 [20.63s]:  training loss=0.420226514339447                                     \n",
      "epoch 13 [20.86s]:  training loss=0.4195306897163391                                    \n",
      "epoch 14 [21.41s]:  training loss=0.41775405406951904                                   \n",
      "epoch 15 [20.64s]: training loss=0.4099383056163788  validation ndcg@10=0.05175914534444004 [0.27s]\n",
      "epoch 16 [20.67s]:  training loss=0.40666744112968445                                   \n",
      "epoch 17 [20.71s]:  training loss=0.4082324504852295                                    \n",
      "epoch 18 [21.32s]:  training loss=0.4035826027393341                                    \n",
      "epoch 19 [21.01s]:  training loss=0.4074667990207672                                    \n",
      "epoch 20 [21.19s]: training loss=0.40100815892219543  validation ndcg@10=0.05934794484127539 [0.38s]\n",
      "epoch 21 [21.18s]:  training loss=0.4066314697265625                                    \n",
      "epoch 22 [22.94s]:  training loss=0.4018659293651581                                    \n",
      "epoch 23 [21.33s]:  training loss=0.3913605511188507                                    \n",
      "epoch 24 [21.25s]:  training loss=0.3960837423801422                                    \n",
      "epoch 25 [21.14s]: training loss=0.39269664883613586  validation ndcg@10=0.057511374855537825 [0.28s]\n",
      "epoch 26 [21.1s]:  training loss=0.3843832314014435                                     \n",
      "epoch 27 [21.25s]:  training loss=0.39522531628608704                                   \n",
      "epoch 28 [21.19s]:  training loss=0.38853657245635986                                   \n",
      "epoch 29 [21.14s]:  training loss=0.38728731870651245                                   \n",
      "epoch 30 [21.38s]: training loss=0.3835966885089874  validation ndcg@10=0.06553544548082949 [0.28s]\n",
      "epoch 31 [21.13s]:  training loss=0.3808303773403168                                    \n",
      "epoch 32 [21.07s]:  training loss=0.3764580190181732                                    \n",
      "epoch 33 [21.39s]:  training loss=0.3786103427410126                                    \n",
      "epoch 34 [21.08s]:  training loss=0.37979769706726074                                   \n",
      "epoch 35 [21.22s]: training loss=0.37670615315437317  validation ndcg@10=0.07217386780457725 [0.28s]\n",
      "epoch 36 [21.25s]:  training loss=0.3731168806552887                                    \n",
      "epoch 37 [21.21s]:  training loss=0.3682613670825958                                    \n",
      "epoch 38 [21.2s]:  training loss=0.3760577142238617                                     \n",
      "epoch 39 [21.72s]:  training loss=0.3698151707649231                                    \n",
      "epoch 40 [21.22s]: training loss=0.36738914251327515  validation ndcg@10=0.06194845311353656 [0.28s]\n",
      "epoch 41 [23.04s]:  training loss=0.3660884499549866                                    \n",
      "epoch 42 [21.5s]:  training loss=0.36336550116539                                       \n",
      "epoch 43 [21.21s]:  training loss=0.36132708191871643                                   \n",
      "epoch 44 [21.12s]:  training loss=0.3629172146320343                                    \n",
      "epoch 45 [21.67s]: training loss=0.3642040491104126  validation ndcg@10=0.06363634168756442 [0.36s]\n",
      "epoch 46 [21.33s]:  training loss=0.3605060577392578                                    \n",
      "epoch 47 [21.27s]:  training loss=0.3584287166595459                                    \n",
      "epoch 48 [21.3s]:  training loss=0.3533450663089752                                     \n",
      "epoch 49 [21.28s]:  training loss=0.3557074964046478                                    \n",
      "epoch 50 [21.55s]: training loss=0.3532005846500397  validation ndcg@10=0.06553581892858688 [0.33s]\n",
      "epoch 51 [21.2s]:  training loss=0.3521043658256531                                     \n",
      "epoch 52 [21.28s]:  training loss=0.34877753257751465                                   \n",
      "epoch 53 [21.2s]:  training loss=0.346009224653244                                      \n",
      "epoch 54 [21.33s]:  training loss=0.3525531589984894                                    \n",
      "epoch 55 [21.22s]: training loss=0.354610800743103  validation ndcg@10=0.07843729852814614 [0.33s]\n",
      "epoch 56 [21.32s]:  training loss=0.3480938673019409                                    \n",
      "epoch 57 [21.24s]:  training loss=0.34557798504829407                                   \n",
      "epoch 58 [21.16s]:  training loss=0.34109240770339966                                   \n",
      "epoch 59 [21.32s]:  training loss=0.34288477897644043                                   \n",
      "epoch 60 [22.87s]: training loss=0.3441559970378876  validation ndcg@10=0.06894777024351595 [0.34s]\n",
      "epoch 61 [21.12s]:  training loss=0.3447297513484955                                    \n",
      "epoch 62 [21.46s]:  training loss=0.338305801153183                                     \n",
      "epoch 63 [21.36s]:  training loss=0.34387630224227905                                   \n",
      "epoch 64 [21.78s]:  training loss=0.3436645567417145                                    \n",
      "epoch 65 [21.71s]: training loss=0.33867406845092773  validation ndcg@10=0.07190315432409518 [0.28s]\n",
      "epoch 66 [21.23s]:  training loss=0.34079229831695557                                   \n",
      "epoch 67 [21.36s]:  training loss=0.33955177664756775                                   \n",
      "epoch 68 [21.42s]:  training loss=0.33613330125808716                                   \n",
      "epoch 69 [21.31s]:  training loss=0.33343908190727234                                   \n",
      "epoch 70 [21.22s]: training loss=0.33472368121147156  validation ndcg@10=0.07858514314236957 [0.31s]\n",
      "epoch 71 [21.42s]:  training loss=0.3336929976940155                                    \n",
      "epoch 72 [21.24s]:  training loss=0.33508846163749695                                   \n",
      "epoch 73 [21.29s]:  training loss=0.3421497344970703                                    \n",
      "epoch 74 [21.34s]:  training loss=0.33641529083251953                                   \n",
      "epoch 75 [21.3s]: training loss=0.33140113949775696  validation ndcg@10=0.07172123913608314 [0.3s]\n",
      "epoch 76 [21.43s]:  training loss=0.33236008882522583                                   \n",
      "epoch 77 [21.34s]:  training loss=0.3293047249317169                                    \n",
      "epoch 78 [23.03s]:  training loss=0.3355554938316345                                    \n",
      "epoch 79 [21.28s]:  training loss=0.3266577422618866                                    \n",
      "epoch 80 [21.38s]: training loss=0.3249727189540863  validation ndcg@10=0.06497322534420598 [0.28s]\n",
      "epoch 81 [21.29s]:  training loss=0.3286688029766083                                    \n",
      "epoch 82 [21.63s]:  training loss=0.32851141691207886                                   \n",
      "epoch 83 [21.32s]:  training loss=0.32745274901390076                                   \n",
      "epoch 84 [21.2s]:  training loss=0.3329266607761383                                     \n",
      "epoch 85 [21.23s]: training loss=0.332340806722641  validation ndcg@10=0.06946767489620274 [0.3s]\n",
      "epoch 86 [21.32s]:  training loss=0.3236759901046753                                    \n",
      "epoch 87 [21.26s]:  training loss=0.32076960802078247                                   \n",
      "epoch 88 [21.15s]:  training loss=0.3275865614414215                                    \n",
      "epoch 89 [21.26s]:  training loss=0.3274673819541931                                    \n",
      "epoch 90 [21.85s]: training loss=0.331196129322052  validation ndcg@10=0.0687603696274936 [0.32s]\n",
      "epoch 91 [21.33s]:  training loss=0.3256833851337433                                    \n",
      "epoch 92 [21.66s]:  training loss=0.3236550986766815                                    \n",
      "epoch 93 [21.41s]:  training loss=0.3194519579410553                                    \n",
      "epoch 94 [21.74s]:  training loss=0.3244437277317047                                    \n",
      "epoch 95 [21.44s]: training loss=0.3242170214653015  validation ndcg@10=0.06796783140474595 [0.31s]\n",
      "epoch 1 [8.32s]:  training loss=0.7633704543113708                                       \n",
      "epoch 2 [8.74s]:  training loss=0.5082536339759827                                       \n",
      "epoch 3 [8.39s]:  training loss=0.4434319734573364                                       \n",
      "epoch 4 [8.31s]:  training loss=0.4432080090045929                                       \n",
      "epoch 5 [8.34s]: training loss=0.42828571796417236  validation ndcg@10=0.06757975166295113 [0.13s]\n",
      "epoch 6 [10.68s]:  training loss=0.4165838658809662                                      \n",
      "epoch 7 [8.51s]:  training loss=0.40611180663108826                                      \n",
      "epoch 8 [8.74s]:  training loss=0.40278759598731995                                      \n",
      "epoch 9 [8.61s]:  training loss=0.3948960602283478                                       \n",
      "epoch 10 [8.53s]: training loss=0.3940035402774811  validation ndcg@10=0.06693749806405595 [0.15s]\n",
      "epoch 11 [8.58s]:  training loss=0.3821623921394348                                      \n",
      "epoch 12 [8.82s]:  training loss=0.3798653781414032                                      \n",
      "epoch 13 [8.54s]:  training loss=0.3769496977329254                                      \n",
      "epoch 14 [8.44s]:  training loss=0.37202635407447815                                     \n",
      "epoch 15 [8.46s]: training loss=0.3678866922855377  validation ndcg@10=0.07674682711887827 [0.11s]\n",
      "epoch 16 [8.62s]:  training loss=0.35640767216682434                                     \n",
      "epoch 17 [8.27s]:  training loss=0.36768797039985657                                     \n",
      "epoch 18 [8.23s]:  training loss=0.3619527816772461                                      \n",
      "epoch 19 [8.34s]:  training loss=0.3522500693798065                                      \n",
      "epoch 20 [8.33s]: training loss=0.34638720750808716  validation ndcg@10=0.06735665487148544 [0.12s]\n",
      "epoch 21 [8.59s]:  training loss=0.3561329245567322                                      \n",
      "epoch 22 [8.48s]:  training loss=0.34550777077674866                                     \n",
      "epoch 23 [8.52s]:  training loss=0.3466023802757263                                      \n",
      "epoch 24 [8.54s]:  training loss=0.35137835144996643                                     \n",
      "epoch 25 [8.61s]: training loss=0.3429524302482605  validation ndcg@10=0.05867552659195925 [0.12s]\n",
      "epoch 26 [8.78s]:  training loss=0.34178993105888367                                     \n",
      "epoch 27 [8.7s]:  training loss=0.33773788809776306                                      \n",
      "epoch 28 [8.36s]:  training loss=0.34366026520729065                                     \n",
      "epoch 29 [8.29s]:  training loss=0.3345729410648346                                      \n",
      "epoch 30 [8.45s]: training loss=0.33496561646461487  validation ndcg@10=0.06845728585518776 [0.12s]\n",
      "epoch 31 [8.37s]:  training loss=0.32884180545806885                                     \n",
      "epoch 32 [8.46s]:  training loss=0.3341866135597229                                      \n",
      "epoch 33 [8.34s]:  training loss=0.3287414014339447                                      \n",
      "epoch 34 [8.34s]:  training loss=0.33043110370635986                                     \n",
      "epoch 35 [8.28s]: training loss=0.3261488676071167  validation ndcg@10=0.0660034055187273 [0.12s]\n",
      "epoch 36 [8.67s]:  training loss=0.32978111505508423                                     \n",
      "epoch 37 [9.21s]:  training loss=0.32114937901496887                                     \n",
      "epoch 38 [8.67s]:  training loss=0.3257853090763092                                      \n",
      "epoch 39 [8.62s]:  training loss=0.32652926445007324                                     \n",
      "epoch 40 [8.55s]: training loss=0.31991851329803467  validation ndcg@10=0.06005569013344004 [0.15s]\n",
      "epoch 1 [23.94s]:  training loss=0.9943959712982178                                      \n",
      "epoch 2 [26.63s]:  training loss=0.8849313855171204                                     \n",
      "epoch 3 [26.89s]:  training loss=0.8016757369041443                                     \n",
      "epoch 4 [26.9s]:  training loss=0.7507655024528503                                      \n",
      "epoch 5 [26.06s]: training loss=0.7116512656211853  validation ndcg@10=0.03470729645896478 [0.37s]\n",
      "epoch 6 [26.33s]:  training loss=0.6688528060913086                                     \n",
      "epoch 7 [26.57s]:  training loss=0.6339762806892395                                     \n",
      "epoch 8 [29.02s]:  training loss=0.6047909259796143                                     \n",
      "epoch 9 [25.86s]:  training loss=0.5718389749526978                                     \n",
      "epoch 10 [26.21s]: training loss=0.5472516417503357  validation ndcg@10=0.046851988768008904 [0.36s]\n",
      "epoch 11 [26.22s]:  training loss=0.5291467308998108                                    \n",
      "epoch 12 [25.87s]:  training loss=0.5032640695571899                                    \n",
      "epoch 13 [26.33s]:  training loss=0.49457740783691406                                   \n",
      "epoch 14 [26.05s]:  training loss=0.47871795296669006                                   \n",
      "epoch 15 [26.42s]: training loss=0.4657852053642273  validation ndcg@10=0.05851869637667083 [0.31s]\n",
      "epoch 16 [26.28s]:  training loss=0.4630044102668762                                    \n",
      "epoch 17 [25.75s]:  training loss=0.4526982605457306                                    \n",
      "epoch 18 [26.39s]:  training loss=0.44594985246658325                                   \n",
      "epoch 19 [25.88s]:  training loss=0.4479723572731018                                    \n",
      "epoch 20 [25.68s]: training loss=0.4344760775566101  validation ndcg@10=0.05480392884943973 [0.29s]\n",
      "epoch 21 [26.19s]:  training loss=0.4339969754219055                                    \n",
      "epoch 22 [26.19s]:  training loss=0.4257669150829315                                    \n",
      "epoch 23 [25.45s]:  training loss=0.4279310405254364                                    \n",
      "epoch 24 [26.54s]:  training loss=0.4318751394748688                                    \n",
      "epoch 25 [25.88s]: training loss=0.42957553267478943  validation ndcg@10=0.05852557283299439 [0.29s]\n",
      "epoch 26 [27.98s]:  training loss=0.425371915102005                                     \n",
      "epoch 27 [25.73s]:  training loss=0.42366236448287964                                   \n",
      "epoch 28 [25.37s]:  training loss=0.42249351739883423                                   \n",
      "epoch 29 [25.79s]:  training loss=0.42000216245651245                                   \n",
      "epoch 30 [25.12s]: training loss=0.4159657657146454  validation ndcg@10=0.05704938366717367 [0.34s]\n",
      "epoch 31 [26.06s]:  training loss=0.42019122838974                                      \n",
      "epoch 32 [25.35s]:  training loss=0.4232402741909027                                    \n",
      "epoch 33 [25.15s]:  training loss=0.4119997024536133                                    \n",
      "epoch 34 [25.66s]:  training loss=0.41934385895729065                                   \n",
      "epoch 35 [25.37s]: training loss=0.41381701827049255  validation ndcg@10=0.059003577951276094 [0.37s]\n",
      "epoch 36 [25.47s]:  training loss=0.41591677069664                                      \n",
      "epoch 37 [25.46s]:  training loss=0.409929096698761                                     \n",
      "epoch 38 [25.19s]:  training loss=0.41001132130622864                                   \n",
      "epoch 39 [25.18s]:  training loss=0.4142305850982666                                    \n",
      "epoch 40 [24.89s]: training loss=0.4086137115955353  validation ndcg@10=0.06076840517630066 [0.28s]\n",
      "epoch 41 [25.43s]:  training loss=0.41060248017311096                                   \n",
      "epoch 42 [25.77s]:  training loss=0.40659746527671814                                   \n",
      "epoch 43 [28.26s]:  training loss=0.40812280774116516                                   \n",
      "epoch 44 [25.61s]:  training loss=0.3970872461795807                                    \n",
      "epoch 45 [25.91s]: training loss=0.40158385038375854  validation ndcg@10=0.051428237442028575 [0.31s]\n",
      "epoch 46 [25.3s]:  training loss=0.40708354115486145                                    \n",
      "epoch 47 [25.39s]:  training loss=0.4002891778945923                                    \n",
      "epoch 48 [25.19s]:  training loss=0.4047755300998688                                    \n",
      "epoch 49 [25.8s]:  training loss=0.4031986892223358                                     \n",
      "epoch 50 [25.69s]: training loss=0.3951503336429596  validation ndcg@10=0.06253547105391745 [0.3s]\n",
      "epoch 51 [25.74s]:  training loss=0.400866836309433                                     \n",
      "epoch 52 [25.72s]:  training loss=0.4014551043510437                                    \n",
      "epoch 53 [25.17s]:  training loss=0.40180128812789917                                   \n",
      "epoch 54 [25.61s]:  training loss=0.3981795310974121                                    \n",
      "epoch 55 [25.63s]: training loss=0.3981499671936035  validation ndcg@10=0.06525662558821414 [0.29s]\n",
      "epoch 56 [25.6s]:  training loss=0.3947789967060089                                     \n",
      "epoch 57 [25.49s]:  training loss=0.3942512571811676                                    \n",
      "epoch 58 [25.74s]:  training loss=0.39050644636154175                                   \n",
      "epoch 59 [25.64s]:  training loss=0.39224973320961                                      \n",
      "epoch 60 [25.81s]: training loss=0.3913063704967499  validation ndcg@10=0.06306740975633703 [0.29s]\n",
      "epoch 61 [27.83s]:  training loss=0.38953694701194763                                   \n",
      "epoch 62 [24.08s]:  training loss=0.38853636384010315                                   \n",
      "epoch 63 [25.27s]:  training loss=0.391730397939682                                     \n",
      "epoch 64 [25.44s]:  training loss=0.38905853033065796                                   \n",
      "epoch 65 [24.47s]: training loss=0.3884478211402893  validation ndcg@10=0.06521177898259967 [0.36s]\n",
      "epoch 66 [25.65s]:  training loss=0.38662347197532654                                   \n",
      "epoch 67 [25.75s]:  training loss=0.3872758746147156                                    \n",
      "epoch 68 [25.49s]:  training loss=0.382114976644516                                     \n",
      "epoch 69 [24.43s]:  training loss=0.3880622684955597                                    \n",
      "epoch 70 [25.05s]: training loss=0.38467955589294434  validation ndcg@10=0.06638523514509666 [0.29s]\n",
      "epoch 71 [25.35s]:  training loss=0.38612642884254456                                   \n",
      "epoch 72 [24.93s]:  training loss=0.3888893127441406                                    \n",
      "epoch 73 [25.71s]:  training loss=0.3768322765827179                                    \n",
      "epoch 74 [24.9s]:  training loss=0.3889690935611725                                     \n",
      "epoch 75 [25.94s]: training loss=0.3839925527572632  validation ndcg@10=0.06453837730807138 [0.3s]\n",
      "epoch 76 [25.23s]:  training loss=0.38716748356819153                                   \n",
      "epoch 77 [25.1s]:  training loss=0.3852913975715637                                     \n",
      "epoch 78 [25.65s]:  training loss=0.37591835856437683                                   \n",
      "epoch 79 [25.73s]:  training loss=0.3764205574989319                                    \n",
      "epoch 80 [26.69s]: training loss=0.3760770857334137  validation ndcg@10=0.07049117682674695 [0.37s]\n",
      "epoch 81 [25.01s]:  training loss=0.37938642501831055                                   \n",
      "epoch 82 [25.62s]:  training loss=0.37420201301574707                                   \n",
      "epoch 83 [25.91s]:  training loss=0.3781832158565521                                    \n",
      "epoch 84 [25.8s]:  training loss=0.38270848989486694                                    \n",
      "epoch 85 [26.31s]: training loss=0.37421083450317383  validation ndcg@10=0.07147245427163407 [0.29s]\n",
      "epoch 86 [26.02s]:  training loss=0.3756319284439087                                    \n",
      "epoch 87 [24.26s]:  training loss=0.37094342708587646                                   \n",
      "epoch 88 [26.6s]:  training loss=0.3765181303024292                                     \n",
      "epoch 89 [25.21s]:  training loss=0.37359073758125305                                   \n",
      "epoch 90 [25.71s]: training loss=0.3774609863758087  validation ndcg@10=0.06635100471125398 [0.33s]\n",
      "epoch 91 [25.57s]:  training loss=0.3746197819709778                                    \n",
      "epoch 92 [26.18s]:  training loss=0.3710707426071167                                    \n",
      "epoch 93 [25.34s]:  training loss=0.3716934025287628                                    \n",
      "epoch 94 [25.46s]:  training loss=0.3696362376213074                                    \n",
      "epoch 95 [25.8s]: training loss=0.3730791211128235  validation ndcg@10=0.06268138822966988 [0.33s]\n",
      "epoch 96 [25.15s]:  training loss=0.36946722865104675                                   \n",
      "epoch 97 [25.77s]:  training loss=0.37272998690605164                                   \n",
      "epoch 98 [25.24s]:  training loss=0.36465904116630554                                   \n",
      "epoch 99 [27.35s]:  training loss=0.36889955401420593                                   \n",
      "epoch 100 [24.94s]: training loss=0.3676076829433441  validation ndcg@10=0.07132580663980508 [0.37s]\n",
      "epoch 101 [25.57s]:  training loss=0.367982417345047                                    \n",
      "epoch 102 [25.61s]:  training loss=0.3654453456401825                                   \n",
      "epoch 103 [24.95s]:  training loss=0.3568026125431061                                   \n",
      "epoch 104 [25.57s]:  training loss=0.3646097779273987                                   \n",
      "epoch 105 [25.31s]: training loss=0.3661404252052307  validation ndcg@10=0.0651849479123227 [0.35s]\n",
      "epoch 106 [25.91s]:  training loss=0.36463677883148193                                  \n",
      "epoch 107 [25.08s]:  training loss=0.366140753030777                                    \n",
      "epoch 108 [25.56s]:  training loss=0.35778680443763733                                  \n",
      "epoch 109 [26.06s]:  training loss=0.35771268606185913                                  \n",
      "epoch 110 [25.83s]: training loss=0.36765429377555847  validation ndcg@10=0.07613363346560319 [0.33s]\n",
      "epoch 111 [25.83s]:  training loss=0.35708898305892944                                  \n",
      "epoch 112 [25.48s]:  training loss=0.36272749304771423                                  \n",
      "epoch 113 [24.89s]:  training loss=0.3582349717617035                                   \n",
      "epoch 114 [25.61s]:  training loss=0.3631512224674225                                   \n",
      "epoch 115 [25.36s]: training loss=0.35938817262649536  validation ndcg@10=0.06636587514505456 [0.36s]\n",
      "epoch 116 [25.81s]:  training loss=0.3595045506954193                                   \n",
      "epoch 117 [25.88s]:  training loss=0.3635312616825104                                   \n",
      "epoch 118 [25.49s]:  training loss=0.3576306998729706                                   \n",
      "epoch 119 [28.07s]:  training loss=0.3592071235179901                                   \n",
      "epoch 120 [24.62s]: training loss=0.35428744554519653  validation ndcg@10=0.0684270468449627 [0.29s]\n",
      "epoch 121 [25.67s]:  training loss=0.35638144612312317                                  \n",
      "epoch 122 [25.27s]:  training loss=0.34955117106437683                                  \n",
      "epoch 123 [25.36s]:  training loss=0.35597777366638184                                  \n",
      "epoch 124 [25.22s]:  training loss=0.3544471859931946                                   \n",
      "epoch 125 [26.08s]: training loss=0.35834190249443054  validation ndcg@10=0.0692763966555331 [0.3s]\n",
      "epoch 126 [24.82s]:  training loss=0.3471066951751709                                   \n",
      "epoch 127 [25.76s]:  training loss=0.353386253118515                                    \n",
      "epoch 128 [25.1s]:  training loss=0.35371750593185425                                   \n",
      "epoch 129 [25.71s]:  training loss=0.3576051592826843                                   \n",
      "epoch 130 [26.23s]: training loss=0.3529675304889679  validation ndcg@10=0.06926856519969529 [0.34s]\n",
      "epoch 131 [25.74s]:  training loss=0.3530094623565674                                   \n",
      "epoch 132 [25.35s]:  training loss=0.35802969336509705                                  \n",
      "epoch 133 [25.36s]:  training loss=0.35548457503318787                                  \n",
      "epoch 134 [25.63s]:  training loss=0.3522443175315857                                   \n",
      "epoch 135 [26.25s]: training loss=0.3516930043697357  validation ndcg@10=0.06936757415195312 [0.29s]\n",
      "epoch 1 [7.81s]:  training loss=0.6294394135475159                                       \n",
      "epoch 2 [7.26s]:  training loss=0.6129847168922424                                       \n",
      "epoch 3 [6.95s]:  training loss=0.6926136016845703                                       \n",
      "epoch 4 [6.77s]:  training loss=0.724133312702179                                        \n",
      "epoch 5 [6.85s]: training loss=0.7947176694869995  validation ndcg@10=0.031854947326822806 [0.11s]\n",
      "epoch 6 [6.8s]:  training loss=0.8220074772834778                                        \n",
      "epoch 7 [6.83s]:  training loss=0.8388543725013733                                       \n",
      "epoch 8 [6.71s]:  training loss=0.8712270855903625                                       \n",
      "epoch 9 [6.72s]:  training loss=0.890755832195282                                        \n",
      "epoch 10 [6.88s]: training loss=0.9194816946983337  validation ndcg@10=0.050343187677337094 [0.11s]\n",
      "epoch 11 [6.86s]:  training loss=0.9522112011909485                                      \n",
      "epoch 12 [6.73s]:  training loss=0.9442792534828186                                      \n",
      "epoch 13 [6.73s]:  training loss=0.962044358253479                                       \n",
      "epoch 14 [8.56s]:  training loss=0.9802558422088623                                      \n",
      "epoch 15 [7.08s]: training loss=0.9621613621711731  validation ndcg@10=0.05074487430937772 [0.12s]\n",
      "epoch 16 [6.94s]:  training loss=0.9799578785896301                                      \n",
      "epoch 17 [7.11s]:  training loss=1.0260605812072754                                      \n",
      "epoch 18 [6.8s]:  training loss=1.014042615890503                                        \n",
      "epoch 19 [6.73s]:  training loss=1.0460124015808105                                      \n",
      "epoch 20 [6.76s]: training loss=1.0323328971862793  validation ndcg@10=0.04715212395624973 [0.16s]\n",
      "epoch 21 [6.91s]:  training loss=1.0523817539215088                                      \n",
      "epoch 22 [6.75s]:  training loss=1.0389074087142944                                      \n",
      "epoch 23 [6.97s]:  training loss=1.0480613708496094                                      \n",
      "epoch 24 [6.92s]:  training loss=1.068424105644226                                       \n",
      "epoch 25 [7.06s]: training loss=1.0323740243911743  validation ndcg@10=0.05158081830257651 [0.16s]\n",
      "epoch 26 [7.0s]:  training loss=1.072057843208313                                        \n",
      "epoch 27 [7.0s]:  training loss=1.054815411567688                                        \n",
      "epoch 28 [7.24s]:  training loss=1.0713553428649902                                      \n",
      "epoch 29 [6.9s]:  training loss=1.0916203260421753                                       \n",
      "epoch 30 [6.83s]: training loss=1.0527387857437134  validation ndcg@10=0.0422890515802843 [0.11s]\n",
      "epoch 31 [7.01s]:  training loss=1.074774146080017                                       \n",
      "epoch 32 [6.95s]:  training loss=1.0932492017745972                                      \n",
      "epoch 33 [6.84s]:  training loss=1.0782530307769775                                      \n",
      "epoch 34 [7.0s]:  training loss=1.0901557207107544                                       \n",
      "epoch 35 [7.0s]: training loss=1.0924850702285767  validation ndcg@10=0.030873497454105718 [0.12s]\n",
      "epoch 36 [7.07s]:  training loss=1.089731216430664                                       \n",
      "epoch 37 [6.93s]:  training loss=1.132064938545227                                       \n",
      "epoch 38 [7.01s]:  training loss=1.0610482692718506                                      \n",
      "epoch 39 [6.78s]:  training loss=1.105536699295044                                       \n",
      "epoch 40 [6.97s]: training loss=1.1142131090164185  validation ndcg@10=0.05153509930449648 [0.11s]\n",
      "epoch 41 [6.75s]:  training loss=1.044790267944336                                       \n",
      "epoch 42 [6.8s]:  training loss=1.1105326414108276                                       \n",
      "epoch 43 [7.15s]:  training loss=1.1227325201034546                                      \n",
      "epoch 44 [6.97s]:  training loss=1.087862253189087                                       \n",
      "epoch 45 [6.84s]: training loss=1.0990734100341797  validation ndcg@10=0.03907742535720006 [0.11s]\n",
      "epoch 46 [6.92s]:  training loss=1.0847667455673218                                      \n",
      "epoch 47 [7.01s]:  training loss=1.1119506359100342                                      \n",
      "epoch 48 [6.86s]:  training loss=1.0937738418579102                                      \n",
      "epoch 49 [6.93s]:  training loss=1.0882114171981812                                      \n",
      "epoch 50 [6.85s]: training loss=1.103896975517273  validation ndcg@10=0.030888296579779276 [0.13s]\n",
      "epoch 1 [7.17s]:  training loss=0.6756572127342224                                       \n",
      "epoch 2 [7.34s]:  training loss=0.46267345547676086                                      \n",
      "epoch 3 [7.04s]:  training loss=0.4397463798522949                                       \n",
      "epoch 4 [7.24s]:  training loss=0.42637842893600464                                      \n",
      "epoch 5 [7.1s]: training loss=0.4092755615711212  validation ndcg@10=0.06654683800363302 [0.11s]\n",
      "epoch 6 [7.15s]:  training loss=0.40727606415748596                                      \n",
      "epoch 7 [7.0s]:  training loss=0.3909386098384857                                        \n",
      "epoch 8 [7.02s]:  training loss=0.387959361076355                                        \n",
      "epoch 9 [6.99s]:  training loss=0.37876957654953003                                      \n",
      "epoch 10 [6.97s]: training loss=0.38043680787086487  validation ndcg@10=0.08498988174994834 [0.1s]\n",
      "epoch 11 [7.06s]:  training loss=0.3707893192768097                                      \n",
      "epoch 12 [7.31s]:  training loss=0.3620903491973877                                      \n",
      "epoch 13 [7.25s]:  training loss=0.35836127400398254                                     \n",
      "epoch 14 [7.21s]:  training loss=0.3547193706035614                                      \n",
      "epoch 15 [7.14s]: training loss=0.3507627546787262  validation ndcg@10=0.060122809387403074 [0.1s]\n",
      "epoch 16 [7.13s]:  training loss=0.3538225293159485                                      \n",
      "epoch 17 [7.0s]:  training loss=0.345989853143692                                        \n",
      "epoch 18 [7.17s]:  training loss=0.3492750823497772                                      \n",
      "epoch 19 [7.05s]:  training loss=0.34948787093162537                                     \n",
      "epoch 20 [7.22s]: training loss=0.3457697927951813  validation ndcg@10=0.06462248445401207 [0.1s]\n",
      "epoch 21 [7.02s]:  training loss=0.3416614532470703                                      \n",
      "epoch 22 [6.97s]:  training loss=0.3367934226989746                                      \n",
      "epoch 23 [6.93s]:  training loss=0.34113243222236633                                     \n",
      "epoch 24 [7.02s]:  training loss=0.3387262523174286                                      \n",
      "epoch 25 [7.01s]: training loss=0.3333591818809509  validation ndcg@10=0.05709995549911126 [0.11s]\n",
      "epoch 26 [6.94s]:  training loss=0.33623987436294556                                     \n",
      "epoch 27 [7.04s]:  training loss=0.33634650707244873                                     \n",
      "epoch 28 [7.07s]:  training loss=0.3330368995666504                                      \n",
      "epoch 29 [7.0s]:  training loss=0.3315783143043518                                       \n",
      "epoch 30 [7.16s]: training loss=0.32593339681625366  validation ndcg@10=0.056269237214366635 [0.1s]\n",
      "epoch 31 [7.0s]:  training loss=0.33199480175971985                                      \n",
      "epoch 32 [7.03s]:  training loss=0.3236241638660431                                      \n",
      "epoch 33 [6.81s]:  training loss=0.3244098722934723                                      \n",
      "epoch 34 [7.05s]:  training loss=0.3288619816303253                                      \n",
      "epoch 35 [6.87s]: training loss=0.32808566093444824  validation ndcg@10=0.06063673775819621 [0.11s]\n",
      "epoch 1 [16.48s]:  training loss=0.5259273052215576                                      \n",
      "epoch 2 [18.35s]:  training loss=0.45574092864990234                                    \n",
      "epoch 3 [19.6s]:  training loss=0.4437045156955719                                      \n",
      "epoch 4 [16.66s]:  training loss=0.4405449628829956                                     \n",
      "epoch 5 [17.81s]: training loss=0.440984308719635  validation ndcg@10=0.04576168827531268 [0.37s]\n",
      "epoch 6 [17.46s]:  training loss=0.4284432828426361                                     \n",
      "epoch 7 [18.07s]:  training loss=0.42360275983810425                                    \n",
      "epoch 8 [17.93s]:  training loss=0.42568153142929077                                    \n",
      "epoch 9 [18.19s]:  training loss=0.4242751896381378                                     \n",
      "epoch 10 [18.13s]: training loss=0.4268013536930084  validation ndcg@10=0.0556169643162091 [0.3s]\n",
      "epoch 11 [18.07s]:  training loss=0.43163982033729553                                   \n",
      "epoch 12 [18.27s]:  training loss=0.42307350039482117                                   \n",
      "epoch 13 [18.14s]:  training loss=0.4355131685733795                                    \n",
      "epoch 14 [17.98s]:  training loss=0.43010154366493225                                   \n",
      "epoch 15 [17.99s]: training loss=0.4284713864326477  validation ndcg@10=0.05224651928879325 [0.3s]\n",
      "epoch 16 [18.3s]:  training loss=0.4229296147823334                                     \n",
      "epoch 17 [18.11s]:  training loss=0.4271854758262634                                    \n",
      "epoch 18 [18.53s]:  training loss=0.4257992208003998                                    \n",
      "epoch 19 [17.86s]:  training loss=0.41930586099624634                                   \n",
      "epoch 20 [17.75s]: training loss=0.4129098057746887  validation ndcg@10=0.030676294123293337 [0.27s]\n",
      "epoch 21 [18.5s]:  training loss=0.42207837104797363                                    \n",
      "epoch 22 [18.9s]:  training loss=0.42196956276893616                                    \n",
      "epoch 23 [18.05s]:  training loss=0.4306122064590454                                    \n",
      "epoch 24 [18.27s]:  training loss=0.4222339391708374                                    \n",
      "epoch 25 [17.3s]: training loss=0.42036521434783936  validation ndcg@10=0.05577992436625692 [0.27s]\n",
      "epoch 26 [18.24s]:  training loss=0.4240163266658783                                    \n",
      "epoch 27 [17.58s]:  training loss=0.42173802852630615                                   \n",
      "epoch 28 [17.56s]:  training loss=0.42264074087142944                                   \n",
      "epoch 29 [17.87s]:  training loss=0.421959787607193                                     \n",
      "epoch 30 [17.6s]: training loss=0.42538875341415405  validation ndcg@10=0.044807069280017466 [0.32s]\n",
      "epoch 31 [17.8s]:  training loss=0.4180164337158203                                     \n",
      "epoch 32 [17.77s]:  training loss=0.4245574176311493                                    \n",
      "epoch 33 [17.69s]:  training loss=0.4189499318599701                                    \n",
      "epoch 34 [17.5s]:  training loss=0.4206467270851135                                     \n",
      "epoch 35 [17.79s]: training loss=0.4216434955596924  validation ndcg@10=0.028816887773837663 [0.32s]\n",
      "epoch 36 [18.68s]:  training loss=0.42535555362701416                                   \n",
      "epoch 37 [18.14s]:  training loss=0.4200734496116638                                    \n",
      "epoch 38 [19.53s]:  training loss=0.4240108132362366                                    \n",
      "epoch 39 [17.66s]:  training loss=0.4168313145637512                                    \n",
      "epoch 40 [17.79s]: training loss=0.4092426002025604  validation ndcg@10=0.02123674988093761 [0.28s]\n",
      "epoch 41 [18.14s]:  training loss=0.42843112349510193                                   \n",
      "epoch 42 [18.12s]:  training loss=0.4133397340774536                                    \n",
      "epoch 43 [18.63s]:  training loss=0.41618141531944275                                   \n",
      "epoch 44 [18.19s]:  training loss=0.4116716980934143                                    \n",
      "epoch 45 [18.31s]: training loss=0.42411181330680847  validation ndcg@10=0.0055731350610692485 [0.26s]\n",
      "epoch 46 [18.39s]:  training loss=0.4131740629673004                                    \n",
      "epoch 47 [18.17s]:  training loss=0.40989580750465393                                   \n",
      "epoch 48 [18.03s]:  training loss=0.41551393270492554                                   \n",
      "epoch 49 [18.29s]:  training loss=0.4264298379421234                                    \n",
      "epoch 50 [18.34s]: training loss=0.418902188539505  validation ndcg@10=0.009831806722819744 [0.27s]\n",
      "epoch 1 [5.42s]:  training loss=0.5412191152572632                                      \n",
      "epoch 2 [5.25s]:  training loss=0.4783455431461334                                    \n",
      "epoch 3 [5.12s]:  training loss=0.46801328659057617                                   \n",
      "epoch 4 [5.17s]:  training loss=0.46093741059303284                                   \n",
      "epoch 5 [5.04s]: training loss=0.46747711300849915  validation ndcg@10=0.048300665233077814 [0.09s]\n",
      "epoch 6 [5.19s]:  training loss=0.4497150182723999                                    \n",
      "epoch 7 [4.73s]:  training loss=0.4568372666835785                                    \n",
      "epoch 8 [4.85s]:  training loss=0.4496401250362396                                    \n",
      "epoch 9 [4.82s]:  training loss=0.44387924671173096                                   \n",
      "epoch 10 [4.84s]: training loss=0.44310644268989563  validation ndcg@10=0.024998471618716284 [0.09s]\n",
      "epoch 11 [4.77s]:  training loss=0.4497565031051636                                   \n",
      "epoch 12 [4.82s]:  training loss=0.44843247532844543                                  \n",
      "epoch 13 [4.79s]:  training loss=0.43438640236854553                                  \n",
      "epoch 14 [4.82s]:  training loss=0.4364650845527649                                   \n",
      "epoch 15 [4.78s]: training loss=0.4457961916923523  validation ndcg@10=0.0048042886725019225 [0.09s]\n",
      "epoch 16 [4.72s]:  training loss=0.43956488370895386                                  \n",
      "epoch 17 [4.84s]:  training loss=0.43553218245506287                                  \n",
      "epoch 18 [4.91s]:  training loss=0.4370027184486389                                   \n",
      "epoch 19 [4.79s]:  training loss=0.44184935092926025                                  \n",
      "epoch 20 [4.91s]: training loss=0.4366894066333771  validation ndcg@10=0.007841369811130941 [0.09s]\n",
      "epoch 21 [4.79s]:  training loss=0.43939000368118286                                  \n",
      "epoch 22 [4.88s]:  training loss=0.43877092003822327                                  \n",
      "epoch 23 [5.0s]:  training loss=0.4491594731807709                                    \n",
      "epoch 24 [4.76s]:  training loss=0.4383217394351959                                   \n",
      "epoch 25 [4.77s]: training loss=0.4434843361377716  validation ndcg@10=0.007283835552425419 [0.09s]\n",
      "epoch 26 [4.79s]:  training loss=0.4386967420578003                                   \n",
      "epoch 27 [4.82s]:  training loss=0.436277836561203                                    \n",
      "epoch 28 [4.89s]:  training loss=0.4327816069126129                                   \n",
      "epoch 29 [4.75s]:  training loss=0.4428168833255768                                   \n",
      "epoch 30 [4.78s]: training loss=0.4427354335784912  validation ndcg@10=0.003548969349309196 [0.09s]\n",
      "epoch 1 [5.47s]:  training loss=0.6560819745063782                                    \n",
      "epoch 2 [5.48s]:  training loss=0.46466484665870667                                   \n",
      "epoch 3 [5.67s]:  training loss=0.4380093514919281                                    \n",
      "epoch 4 [5.72s]:  training loss=0.42529767751693726                                   \n",
      "epoch 5 [5.57s]: training loss=0.40920960903167725  validation ndcg@10=0.06934837725748413 [0.12s]\n",
      "epoch 6 [5.65s]:  training loss=0.39294150471687317                                   \n",
      "epoch 7 [5.81s]:  training loss=0.38772153854370117                                   \n",
      "epoch 8 [5.67s]:  training loss=0.376993328332901                                     \n",
      "epoch 9 [5.72s]:  training loss=0.3778330683708191                                    \n",
      "epoch 10 [5.66s]: training loss=0.3669590950012207  validation ndcg@10=0.05820668219574962 [0.11s]\n",
      "epoch 11 [5.66s]:  training loss=0.35885074734687805                                  \n",
      "epoch 12 [5.59s]:  training loss=0.3597755432128906                                   \n",
      "epoch 13 [5.55s]:  training loss=0.36070194840431213                                  \n",
      "epoch 14 [5.77s]:  training loss=0.3569990396499634                                   \n",
      "epoch 15 [5.73s]: training loss=0.34946051239967346  validation ndcg@10=0.06424521098757806 [0.1s]\n",
      "epoch 16 [5.8s]:  training loss=0.3470415472984314                                    \n",
      "epoch 17 [5.95s]:  training loss=0.3476095199584961                                   \n",
      "epoch 18 [5.86s]:  training loss=0.3421044647693634                                   \n",
      "epoch 19 [5.58s]:  training loss=0.34484103322029114                                  \n",
      "epoch 20 [5.6s]: training loss=0.3410268723964691  validation ndcg@10=0.06733294438504633 [0.09s]\n",
      "epoch 21 [5.57s]:  training loss=0.332363486289978                                    \n",
      "epoch 22 [5.49s]:  training loss=0.33456048369407654                                  \n",
      "epoch 23 [5.69s]:  training loss=0.3330428898334503                                   \n",
      "epoch 24 [7.57s]:  training loss=0.3324969410896301                                   \n",
      "epoch 25 [5.84s]: training loss=0.3346698582172394  validation ndcg@10=0.04892714131561147 [0.21s]\n",
      "epoch 26 [6.11s]:  training loss=0.3278537690639496                                   \n",
      "epoch 27 [5.8s]:  training loss=0.32482773065567017                                   \n",
      "epoch 28 [5.93s]:  training loss=0.33170798420906067                                  \n",
      "epoch 29 [5.64s]:  training loss=0.33079105615615845                                  \n",
      "epoch 30 [5.6s]: training loss=0.32900258898735046  validation ndcg@10=0.05972658165324422 [0.1s]\n",
      "epoch 1 [16.71s]:  training loss=0.6621817350387573                                   \n",
      "epoch 2 [17.67s]:  training loss=0.45174625515937805                                  \n",
      "epoch 3 [17.8s]:  training loss=0.43356484174728394                                   \n",
      "epoch 4 [18.19s]:  training loss=0.4248971939086914                                   \n",
      "epoch 5 [17.93s]: training loss=0.411573588848114  validation ndcg@10=0.06652031107186512 [0.24s]\n",
      "epoch 6 [17.94s]:  training loss=0.40415725111961365                                  \n",
      "epoch 7 [17.71s]:  training loss=0.3910984396934509                                   \n",
      "epoch 8 [18.12s]:  training loss=0.3911202549934387                                   \n",
      "epoch 9 [17.7s]:  training loss=0.380280077457428                                     \n",
      "epoch 10 [18.06s]: training loss=0.37490081787109375  validation ndcg@10=0.07009659347445636 [0.28s]\n",
      "epoch 11 [17.96s]:  training loss=0.36988365650177                                    \n",
      "epoch 12 [17.9s]:  training loss=0.3630087673664093                                   \n",
      "epoch 13 [18.06s]:  training loss=0.35545650124549866                                 \n",
      "epoch 14 [18.64s]:  training loss=0.3601670563220978                                  \n",
      "epoch 15 [18.28s]: training loss=0.35478371381759644  validation ndcg@10=0.0644395664075317 [0.25s]\n",
      "epoch 16 [18.24s]:  training loss=0.35029640793800354                                 \n",
      "epoch 17 [18.33s]:  training loss=0.34794390201568604                                 \n",
      "epoch 18 [17.99s]:  training loss=0.34816429018974304                                 \n",
      "epoch 19 [18.2s]:  training loss=0.3391255736351013                                   \n",
      "epoch 20 [18.19s]: training loss=0.34279584884643555  validation ndcg@10=0.06511818270454743 [0.29s]\n",
      "epoch 21 [18.34s]:  training loss=0.33892226219177246                                 \n",
      "epoch 22 [20.13s]:  training loss=0.34049853682518005                                 \n",
      "epoch 23 [17.32s]:  training loss=0.3413868248462677                                  \n",
      "epoch 24 [18.26s]:  training loss=0.3377126157283783                                  \n",
      "epoch 25 [18.23s]: training loss=0.33490732312202454  validation ndcg@10=0.0630935798067633 [0.25s]\n",
      "epoch 26 [18.13s]:  training loss=0.33458584547042847                                 \n",
      "epoch 27 [18.1s]:  training loss=0.3297590911388397                                   \n",
      "epoch 28 [17.94s]:  training loss=0.3283516764640808                                  \n",
      "epoch 29 [17.8s]:  training loss=0.33215340971946716                                  \n",
      "epoch 30 [18.06s]: training loss=0.3274218738079071  validation ndcg@10=0.055216673079604185 [0.26s]\n",
      "epoch 31 [17.13s]:  training loss=0.3304712176322937                                  \n",
      "epoch 32 [17.62s]:  training loss=0.32808637619018555                                 \n",
      "epoch 33 [17.71s]:  training loss=0.32610902190208435                                 \n",
      "epoch 34 [18.07s]:  training loss=0.32682564854621887                                 \n",
      "epoch 35 [17.98s]: training loss=0.3244776725769043  validation ndcg@10=0.05915780542919412 [0.29s]\n",
      "100%|██████████| 50/50 [12:44:12<00:00, 917.06s/trial, best loss: -0.0857887634682054]\n"
     ]
    }
   ],
   "source": [
    "tune_pinsage('adobe_core5', use_text_feature=False, use_no_feature=False, use_only_text=False)\n",
    "# no text\n",
    "tune_pinsage('adobe_core5', use_text_feature=False, use_no_feature=True, use_only_text=False)\n",
    "# no feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters of PinSage Recommender on dataset movielens_100k...\n",
      "use_text_feature=False, use_no_feature=True, use_only_text=False\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengxuan_yan/opt/miniconda3/envs/torch/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 [33.72s]:  training loss=0.6936063766479492   \n",
      "epoch 2 [34.12s]:  training loss=0.5093884468078613   \n",
      "epoch 3 [33.39s]:  training loss=0.47025322914123535  \n",
      "epoch 4 [32.77s]:  training loss=0.4237317740917206   \n",
      "epoch 5 [32.42s]: training loss=0.3964024782180786  validation ndcg@10=0.05698711055927033 [0.5s]\n",
      "epoch 6 [33.14s]:  training loss=0.37654680013656616  \n",
      "epoch 7 [33.18s]:  training loss=0.3620908260345459   \n",
      "epoch 8 [33.4s]:  training loss=0.34853512048721313   \n",
      "epoch 9 [33.34s]:  training loss=0.35513243079185486  \n",
      "epoch 10 [33.24s]: training loss=0.33690837025642395  validation ndcg@10=0.05058978595118428 [0.53s]\n",
      "epoch 11 [33.35s]:  training loss=0.33975034952163696 \n",
      "epoch 12 [33.23s]:  training loss=0.333355188369751   \n",
      "epoch 13 [35.14s]:  training loss=0.3320879340171814  \n",
      "epoch 14 [32.72s]:  training loss=0.33134105801582336 \n",
      "epoch 15 [33.29s]: training loss=0.3234368860721588  validation ndcg@10=0.04733395675244561 [0.49s]\n",
      "epoch 16 [33.13s]:  training loss=0.3193741738796234  \n",
      "epoch 17 [33.31s]:  training loss=0.3173943758010864  \n",
      "epoch 18 [33.26s]:  training loss=0.32235637307167053 \n",
      "epoch 19 [33.48s]:  training loss=0.31848856806755066 \n",
      "epoch 20 [33.08s]: training loss=0.31342291831970215  validation ndcg@10=0.04244140907587983 [0.57s]\n",
      "epoch 21 [33.2s]:  training loss=0.32156291604042053  \n",
      "epoch 22 [32.73s]:  training loss=0.3162204325199127  \n",
      "epoch 23 [32.66s]:  training loss=0.3110911548137665  \n",
      "epoch 24 [33.13s]:  training loss=0.31763288378715515 \n",
      "epoch 25 [32.64s]: training loss=0.3096586763858795  validation ndcg@10=0.061076329214688155 [0.52s]\n",
      "epoch 26 [33.09s]:  training loss=0.3112308979034424  \n",
      "epoch 27 [36.43s]:  training loss=0.303947776556015   \n",
      "epoch 28 [32.99s]:  training loss=0.3086931109428406  \n",
      "epoch 29 [33.31s]:  training loss=0.30691438913345337 \n",
      "epoch 30 [33.44s]: training loss=0.30192458629608154  validation ndcg@10=0.04860513163808414 [0.69s]\n",
      "epoch 31 [33.17s]:  training loss=0.3078434467315674  \n",
      "epoch 32 [33.66s]:  training loss=0.301470011472702   \n",
      "epoch 33 [34.09s]:  training loss=0.3070848286151886  \n",
      "epoch 34 [32.89s]:  training loss=0.30264535546302795 \n",
      "epoch 35 [32.32s]: training loss=0.3015763759613037  validation ndcg@10=0.05875190800864724 [0.52s]\n",
      "epoch 36 [33.59s]:  training loss=0.2987881600856781  \n",
      "epoch 37 [34.15s]:  training loss=0.30367928743362427 \n",
      "epoch 38 [33.82s]:  training loss=0.2952899634838104  \n",
      "epoch 39 [34.76s]:  training loss=0.3018808364868164  \n",
      "epoch 40 [34.39s]: training loss=0.3051471412181854  validation ndcg@10=0.05023563938877092 [0.51s]\n",
      "epoch 41 [35.88s]:  training loss=0.3053784966468811  \n",
      "epoch 42 [33.18s]:  training loss=0.3033522665500641  \n",
      "epoch 43 [33.54s]:  training loss=0.306672066450119   \n",
      "epoch 44 [33.7s]:  training loss=0.2929512858390808   \n",
      "epoch 45 [34.35s]: training loss=0.29484811425209045  validation ndcg@10=0.06342948094857001 [0.51s]\n",
      "epoch 46 [33.25s]:  training loss=0.29388177394866943 \n",
      "epoch 47 [34.34s]:  training loss=0.29846808314323425 \n",
      "epoch 48 [34.93s]:  training loss=0.29880255460739136 \n",
      "epoch 49 [33.22s]:  training loss=0.2971362769603729  \n",
      "epoch 50 [34.23s]: training loss=0.29380297660827637  validation ndcg@10=0.05077046022765224 [0.49s]\n",
      "epoch 51 [33.26s]:  training loss=0.30154475569725037 \n",
      "epoch 52 [33.93s]:  training loss=0.301326185464859   \n",
      "epoch 53 [33.56s]:  training loss=0.30354073643684387 \n",
      "epoch 54 [33.57s]:  training loss=0.2949879467487335  \n",
      "epoch 55 [36.08s]: training loss=0.29615721106529236  validation ndcg@10=0.04265522444394617 [0.57s]\n",
      "epoch 56 [33.7s]:  training loss=0.29974132776260376  \n",
      "epoch 57 [32.86s]:  training loss=0.3046526312828064  \n",
      "epoch 58 [33.72s]:  training loss=0.2927361726760864  \n",
      "epoch 59 [33.61s]:  training loss=0.291355699300766   \n",
      "epoch 60 [33.55s]: training loss=0.2984646260738373  validation ndcg@10=0.043435563294868695 [0.56s]\n",
      "epoch 61 [33.91s]:  training loss=0.2942139208316803  \n",
      "epoch 62 [32.87s]:  training loss=0.29351797699928284 \n",
      "epoch 63 [34.05s]:  training loss=0.30104711651802063 \n",
      "epoch 64 [33.91s]:  training loss=0.2972021996974945  \n",
      "epoch 65 [34.34s]: training loss=0.2952842116355896  validation ndcg@10=0.0426696102725428 [0.63s]\n",
      "epoch 66 [34.97s]:  training loss=0.2962004840373993  \n",
      "epoch 67 [32.86s]:  training loss=0.2928837239742279  \n",
      "epoch 68 [33.96s]:  training loss=0.2920113801956177  \n",
      "epoch 69 [33.77s]:  training loss=0.2887740433216095  \n",
      "epoch 70 [35.69s]: training loss=0.2954419255256653  validation ndcg@10=0.046043899229005994 [0.48s]\n",
      "epoch 1 [9.66s]:  training loss=0.9799802899360657                                     \n",
      "epoch 2 [8.15s]:  training loss=0.9697794318199158                                     \n",
      "epoch 3 [8.11s]:  training loss=0.9596877694129944                                     \n",
      "epoch 4 [7.94s]:  training loss=0.9489671587944031                                     \n",
      "epoch 5 [7.91s]: training loss=0.9390642642974854  validation ndcg@10=0.05584056579183264 [0.11s]\n",
      "epoch 6 [7.89s]:  training loss=0.9267491102218628                                     \n",
      "epoch 7 [8.08s]:  training loss=0.9146522283554077                                     \n",
      "epoch 8 [8.07s]:  training loss=0.9002463221549988                                     \n",
      "epoch 9 [8.14s]:  training loss=0.8862423896789551                                     \n",
      "epoch 10 [7.97s]: training loss=0.8723986148834229  validation ndcg@10=0.054434074880880125 [0.11s]\n",
      "epoch 11 [8.06s]:  training loss=0.858178973197937                                     \n",
      "epoch 12 [7.95s]:  training loss=0.8400912284851074                                    \n",
      "epoch 13 [7.96s]:  training loss=0.826566755771637                                     \n",
      "epoch 14 [8.36s]:  training loss=0.8067605495452881                                    \n",
      "epoch 15 [8.39s]: training loss=0.7818297147750854  validation ndcg@10=0.059827822865601905 [0.11s]\n",
      "epoch 16 [8.09s]:  training loss=0.7532581686973572                                    \n",
      "epoch 17 [7.92s]:  training loss=0.7317725419998169                                    \n",
      "epoch 18 [7.92s]:  training loss=0.7160252332687378                                    \n",
      "epoch 19 [7.9s]:  training loss=0.7109917402267456                                     \n",
      "epoch 20 [7.94s]: training loss=0.7008530497550964  validation ndcg@10=0.062069831813508836 [0.11s]\n",
      "epoch 21 [8.01s]:  training loss=0.6905103921890259                                    \n",
      "epoch 22 [7.88s]:  training loss=0.6838083863258362                                    \n",
      "epoch 23 [7.91s]:  training loss=0.6738355755805969                                    \n",
      "epoch 24 [8.15s]:  training loss=0.6680220365524292                                    \n",
      "epoch 25 [8.0s]: training loss=0.6615702509880066  validation ndcg@10=0.06492300856011791 [0.11s]\n",
      "epoch 26 [7.91s]:  training loss=0.6591845750808716                                    \n",
      "epoch 27 [7.95s]:  training loss=0.6559678316116333                                    \n",
      "epoch 28 [7.88s]:  training loss=0.6534886360168457                                    \n",
      "epoch 29 [7.9s]:  training loss=0.6408324837684631                                     \n",
      "epoch 30 [7.89s]: training loss=0.639310359954834  validation ndcg@10=0.06553681754659686 [0.11s]\n",
      "epoch 31 [7.92s]:  training loss=0.6345776319503784                                    \n",
      "epoch 32 [8.03s]:  training loss=0.6329199075698853                                    \n",
      "epoch 33 [7.8s]:  training loss=0.6232737302780151                                     \n",
      "epoch 34 [7.95s]:  training loss=0.6252643465995789                                    \n",
      "epoch 35 [7.88s]: training loss=0.6192376613616943  validation ndcg@10=0.06851065329899407 [0.11s]\n",
      "epoch 36 [7.9s]:  training loss=0.61423259973526                                       \n",
      "epoch 37 [7.91s]:  training loss=0.6174685955047607                                    \n",
      "epoch 38 [7.88s]:  training loss=0.6057816743850708                                    \n",
      "epoch 39 [8.01s]:  training loss=0.6011044979095459                                    \n",
      "epoch 40 [7.92s]: training loss=0.5978719592094421  validation ndcg@10=0.07187311977835391 [0.11s]\n",
      "epoch 41 [7.94s]:  training loss=0.5946036577224731                                    \n",
      "epoch 42 [8.0s]:  training loss=0.5877737402915955                                     \n",
      "epoch 43 [7.92s]:  training loss=0.5842429399490356                                    \n",
      "epoch 44 [7.96s]:  training loss=0.5817005634307861                                    \n",
      "epoch 45 [7.88s]: training loss=0.5833001136779785  validation ndcg@10=0.07219135660030189 [0.12s]\n",
      "epoch 46 [8.12s]:  training loss=0.5765699148178101                                    \n",
      "epoch 47 [8.12s]:  training loss=0.5727310180664062                                    \n",
      "epoch 48 [8.17s]:  training loss=0.5708868503570557                                    \n",
      "epoch 49 [8.13s]:  training loss=0.5710124969482422                                    \n",
      "epoch 50 [7.95s]: training loss=0.565610945224762  validation ndcg@10=0.06973962014459602 [0.11s]\n",
      "epoch 51 [7.96s]:  training loss=0.56239253282547                                      \n",
      "epoch 52 [7.87s]:  training loss=0.5574619174003601                                    \n",
      "epoch 53 [9.7s]:  training loss=0.558067262172699                                      \n",
      "epoch 54 [8.04s]:  training loss=0.5576326251029968                                    \n",
      "epoch 55 [8.15s]: training loss=0.5534219741821289  validation ndcg@10=0.06830741896271257 [0.13s]\n",
      "epoch 56 [7.97s]:  training loss=0.5535077452659607                                    \n",
      "epoch 57 [7.94s]:  training loss=0.5496093034744263                                    \n",
      "epoch 58 [7.94s]:  training loss=0.546191930770874                                     \n",
      "epoch 59 [7.93s]:  training loss=0.5432569980621338                                    \n",
      "epoch 60 [7.92s]: training loss=0.5415831208229065  validation ndcg@10=0.06987856001250127 [0.11s]\n",
      "epoch 61 [7.92s]:  training loss=0.5511050820350647                                    \n",
      "epoch 62 [7.97s]:  training loss=0.5428032279014587                                    \n",
      "epoch 63 [8.09s]:  training loss=0.5349836349487305                                    \n",
      "epoch 64 [7.98s]:  training loss=0.5362141728401184                                    \n",
      "epoch 65 [7.93s]: training loss=0.5409474968910217  validation ndcg@10=0.07082061574332939 [0.12s]\n",
      "epoch 66 [7.95s]:  training loss=0.5346171855926514                                    \n",
      "epoch 67 [8.01s]:  training loss=0.5310461521148682                                    \n",
      "epoch 68 [7.63s]:  training loss=0.5330981016159058                                    \n",
      "epoch 69 [7.7s]:  training loss=0.5314857363700867                                     \n",
      "epoch 70 [7.77s]: training loss=0.5218767523765564  validation ndcg@10=0.07121291999177859 [0.11s]\n",
      "epoch 1 [19.47s]:  training loss=0.9820389747619629                                    \n",
      "epoch 2 [20.63s]:  training loss=0.9738517999649048                                    \n",
      "epoch 3 [20.93s]:  training loss=0.965928316116333                                     \n",
      "epoch 4 [21.1s]:  training loss=0.9560949206352234                                     \n",
      "epoch 5 [20.49s]: training loss=0.9481760859489441  validation ndcg@10=0.048615051989950794 [0.23s]\n",
      "epoch 6 [20.55s]:  training loss=0.9369338154792786                                    \n",
      "epoch 7 [20.62s]:  training loss=0.9269967079162598                                    \n",
      "epoch 8 [20.65s]:  training loss=0.9178789258003235                                    \n",
      "epoch 9 [20.47s]:  training loss=0.9065225124359131                                    \n",
      "epoch 10 [20.72s]: training loss=0.8942942023277283  validation ndcg@10=0.06408629647563424 [0.25s]\n",
      "epoch 11 [20.72s]:  training loss=0.8845450282096863                                   \n",
      "epoch 12 [20.38s]:  training loss=0.8713780045509338                                   \n",
      "epoch 13 [20.93s]:  training loss=0.8623668551445007                                   \n",
      "epoch 14 [22.37s]:  training loss=0.8486564755439758                                   \n",
      "epoch 15 [20.37s]: training loss=0.8379664421081543  validation ndcg@10=0.06870829099965929 [0.28s]\n",
      "epoch 16 [21.33s]:  training loss=0.8253021836280823                                   \n",
      "epoch 17 [20.92s]:  training loss=0.8118131160736084                                   \n",
      "epoch 18 [21.05s]:  training loss=0.7990604639053345                                   \n",
      "epoch 19 [20.48s]:  training loss=0.7851481437683105                                   \n",
      "epoch 20 [21.12s]: training loss=0.7719908356666565  validation ndcg@10=0.06748609396842492 [0.24s]\n",
      "epoch 21 [21.08s]:  training loss=0.760604977607727                                    \n",
      "epoch 22 [20.96s]:  training loss=0.744276225566864                                    \n",
      "epoch 23 [20.72s]:  training loss=0.7342237830162048                                   \n",
      "epoch 24 [20.85s]:  training loss=0.7184642553329468                                   \n",
      "epoch 25 [20.93s]: training loss=0.7088046669960022  validation ndcg@10=0.06840496233528869 [0.24s]\n",
      "epoch 26 [21.04s]:  training loss=0.6961356401443481                                   \n",
      "epoch 27 [20.96s]:  training loss=0.6842933297157288                                   \n",
      "epoch 28 [20.78s]:  training loss=0.6738440990447998                                   \n",
      "epoch 29 [21.2s]:  training loss=0.6623861789703369                                    \n",
      "epoch 30 [21.47s]: training loss=0.656629741191864  validation ndcg@10=0.0644946465123375 [0.25s]\n",
      "epoch 31 [21.25s]:  training loss=0.6417145133018494                                   \n",
      "epoch 32 [20.93s]:  training loss=0.6399679183959961                                   \n",
      "epoch 33 [21.43s]:  training loss=0.6311619877815247                                   \n",
      "epoch 34 [21.77s]:  training loss=0.622527539730072                                      \n",
      "epoch 35 [21.37s]: training loss=0.6143720149993896  validation ndcg@10=0.06349340485110726 [0.3s]\n",
      "epoch 36 [23.43s]:  training loss=0.6088804602622986                                     \n",
      "epoch 37 [21.45s]:  training loss=0.6090293526649475                                     \n",
      "epoch 38 [21.71s]:  training loss=0.5976641774177551                                     \n",
      "epoch 39 [21.62s]:  training loss=0.5951729416847229                                     \n",
      "epoch 40 [21.3s]: training loss=0.5901692509651184  validation ndcg@10=0.06529023666264029 [0.24s]\n",
      "epoch 1 [10.7s]:  training loss=0.9851781725883484                                       \n",
      "epoch 2 [10.47s]:  training loss=0.9827761650085449                                      \n",
      "epoch 3 [9.73s]:  training loss=0.9779456853866577                                       \n",
      "epoch 4 [9.73s]:  training loss=0.9754704236984253                                       \n",
      "epoch 5 [9.77s]: training loss=0.9721674919128418  validation ndcg@10=0.013422462660146602 [0.15s]\n",
      "epoch 6 [9.71s]:  training loss=0.9684622883796692                                       \n",
      "epoch 7 [9.99s]:  training loss=0.9648143649101257                                       \n",
      "epoch 8 [10.07s]:  training loss=0.9611365795135498                                      \n",
      "epoch 9 [9.68s]:  training loss=0.9586659669876099                                       \n",
      "epoch 10 [9.47s]: training loss=0.9546251893043518  validation ndcg@10=0.026526349389607903 [0.18s]\n",
      "epoch 11 [9.89s]:  training loss=0.9508927464485168                                      \n",
      "epoch 12 [9.74s]:  training loss=0.9469184875488281                                      \n",
      "epoch 13 [9.71s]:  training loss=0.9433512091636658                                      \n",
      "epoch 14 [9.47s]:  training loss=0.9410523772239685                                      \n",
      "epoch 15 [9.43s]: training loss=0.9349244236946106  validation ndcg@10=0.03477841581703973 [0.15s]\n",
      "epoch 16 [9.31s]:  training loss=0.9289499521255493                                      \n",
      "epoch 17 [9.47s]:  training loss=0.9267993569374084                                      \n",
      "epoch 18 [9.31s]:  training loss=0.9213880300521851                                      \n",
      "epoch 19 [9.38s]:  training loss=0.917982816696167                                       \n",
      "epoch 20 [9.3s]: training loss=0.9128895998001099  validation ndcg@10=0.031549862057765374 [0.14s]\n",
      "epoch 21 [9.4s]:  training loss=0.909449577331543                                        \n",
      "epoch 22 [9.42s]:  training loss=0.9047172665596008                                      \n",
      "epoch 23 [9.34s]:  training loss=0.8986060619354248                                      \n",
      "epoch 24 [9.45s]:  training loss=0.8951922059059143                                      \n",
      "epoch 25 [9.44s]: training loss=0.8891430497169495  validation ndcg@10=0.04111402053900936 [0.14s]\n",
      "epoch 26 [9.47s]:  training loss=0.8826995491981506                                      \n",
      "epoch 27 [9.36s]:  training loss=0.8789563775062561                                      \n",
      "epoch 28 [9.43s]:  training loss=0.8720685839653015                                      \n",
      "epoch 29 [9.49s]:  training loss=0.8667063117027283                                      \n",
      "epoch 30 [9.43s]: training loss=0.8603006601333618  validation ndcg@10=0.04615350847372303 [0.15s]\n",
      "epoch 31 [9.46s]:  training loss=0.8541313409805298                                      \n",
      "epoch 32 [9.43s]:  training loss=0.8478190302848816                                      \n",
      "epoch 33 [9.52s]:  training loss=0.8428454995155334                                      \n",
      "epoch 34 [9.46s]:  training loss=0.8355761170387268                                      \n",
      "epoch 35 [9.41s]: training loss=0.8301099538803101  validation ndcg@10=0.050412602107309176 [0.15s]\n",
      "epoch 36 [9.45s]:  training loss=0.8218086957931519                                      \n",
      "epoch 37 [11.29s]:  training loss=0.8118728399276733                                     \n",
      "epoch 38 [9.53s]:  training loss=0.8033183217048645                                      \n",
      "epoch 39 [9.73s]:  training loss=0.7973209619522095                                      \n",
      "epoch 40 [9.59s]: training loss=0.7888238430023193  validation ndcg@10=0.05901041700901447 [0.19s]\n",
      "epoch 41 [9.57s]:  training loss=0.7820452451705933                                      \n",
      "epoch 42 [9.45s]:  training loss=0.7743905186653137                                      \n",
      "epoch 43 [9.37s]:  training loss=0.7618314027786255                                      \n",
      "epoch 44 [9.46s]:  training loss=0.7613735795021057                                      \n",
      "epoch 45 [9.55s]: training loss=0.7489284873008728  validation ndcg@10=0.0552295775433372 [0.15s]\n",
      "epoch 46 [9.39s]:  training loss=0.7465015649795532                                      \n",
      "epoch 47 [9.4s]:  training loss=0.7393096089363098                                       \n",
      "epoch 48 [9.52s]:  training loss=0.732117772102356                                       \n",
      "epoch 49 [9.54s]:  training loss=0.7316060066223145                                      \n",
      "epoch 50 [9.52s]: training loss=0.7219881415367126  validation ndcg@10=0.06068437304615776 [0.15s]\n",
      "epoch 51 [9.48s]:  training loss=0.7155385613441467                                      \n",
      "epoch 52 [9.57s]:  training loss=0.709092915058136                                       \n",
      "epoch 53 [9.46s]:  training loss=0.7138199210166931                                      \n",
      "epoch 54 [9.57s]:  training loss=0.7070276141166687                                      \n",
      "epoch 55 [9.48s]: training loss=0.7021538019180298  validation ndcg@10=0.06081591849815875 [0.14s]\n",
      "epoch 56 [9.68s]:  training loss=0.7017704844474792                                      \n",
      "epoch 57 [9.66s]:  training loss=0.6941679120063782                                      \n",
      "epoch 58 [9.49s]:  training loss=0.6920468211174011                                      \n",
      "epoch 59 [9.58s]:  training loss=0.6829585433006287                                      \n",
      "epoch 60 [9.75s]: training loss=0.6883419156074524  validation ndcg@10=0.06252656519597845 [0.19s]\n",
      "epoch 61 [9.55s]:  training loss=0.6809225082397461                                      \n",
      "epoch 62 [9.48s]:  training loss=0.6815067529678345                                      \n",
      "epoch 63 [9.62s]:  training loss=0.6785138249397278                                      \n",
      "epoch 64 [10.01s]:  training loss=0.6755251884460449                                     \n",
      "epoch 65 [9.84s]: training loss=0.6733777523040771  validation ndcg@10=0.06350792645696884 [0.14s]\n",
      "epoch 66 [9.67s]:  training loss=0.672276496887207                                       \n",
      "epoch 67 [9.75s]:  training loss=0.6689559817314148                                      \n",
      "epoch 68 [9.77s]:  training loss=0.6615234613418579                                      \n",
      "epoch 69 [9.65s]:  training loss=0.6647495627403259                                      \n",
      "epoch 70 [9.8s]: training loss=0.657910943031311  validation ndcg@10=0.06684019858188282 [0.15s]\n",
      "epoch 71 [9.87s]:  training loss=0.6565971970558167                                      \n",
      "epoch 72 [9.78s]:  training loss=0.6587544679641724                                      \n",
      "epoch 73 [9.8s]:  training loss=0.6512876152992249                                       \n",
      "epoch 74 [9.42s]:  training loss=0.6532658934593201                                      \n",
      "epoch 75 [9.49s]: training loss=0.6502375602722168  validation ndcg@10=0.06715171093617753 [0.15s]\n",
      "epoch 76 [9.53s]:  training loss=0.6451562643051147                                      \n",
      "epoch 77 [9.5s]:  training loss=0.6499884128570557                                       \n",
      "epoch 78 [9.32s]:  training loss=0.6439674496650696                                      \n",
      "epoch 79 [9.73s]:  training loss=0.6395006775856018                                      \n",
      "epoch 80 [9.55s]: training loss=0.6412625908851624  validation ndcg@10=0.06855894971337048 [0.15s]\n",
      "epoch 81 [9.36s]:  training loss=0.6365143060684204                                      \n",
      "epoch 82 [9.6s]:  training loss=0.635578989982605                                        \n",
      "epoch 83 [9.52s]:  training loss=0.6348171234130859                                      \n",
      "epoch 84 [9.47s]:  training loss=0.6331098675727844                                      \n",
      "epoch 85 [9.52s]: training loss=0.6309033632278442  validation ndcg@10=0.06633365963752856 [0.15s]\n",
      "epoch 86 [11.2s]:  training loss=0.6292542815208435                                      \n",
      "epoch 87 [9.8s]:  training loss=0.6234187483787537                                       \n",
      "epoch 88 [9.63s]:  training loss=0.6305374503135681                                      \n",
      "epoch 89 [9.47s]:  training loss=0.6213218569755554                                      \n",
      "epoch 90 [9.56s]: training loss=0.6194220781326294  validation ndcg@10=0.0676175017244953 [0.14s]\n",
      "epoch 91 [9.28s]:  training loss=0.6154327392578125                                      \n",
      "epoch 92 [9.34s]:  training loss=0.6155803203582764                                      \n",
      "epoch 93 [9.42s]:  training loss=0.6121652722358704                                      \n",
      "epoch 94 [9.43s]:  training loss=0.6137903332710266                                      \n",
      "epoch 95 [9.44s]: training loss=0.6168345212936401  validation ndcg@10=0.06725663382998864 [0.15s]\n",
      "epoch 96 [9.74s]:  training loss=0.6116819381713867                                      \n",
      "epoch 97 [9.56s]:  training loss=0.6085047125816345                                      \n",
      "epoch 98 [9.45s]:  training loss=0.6055103540420532                                      \n",
      "epoch 99 [9.23s]:  training loss=0.6076173186302185                                      \n",
      "epoch 100 [9.3s]: training loss=0.6096237301826477  validation ndcg@10=0.0691148173195201 [0.14s]\n",
      "epoch 101 [9.12s]:  training loss=0.6070948839187622                                     \n",
      "epoch 102 [9.4s]:  training loss=0.5968246459960938                                      \n",
      "epoch 103 [9.5s]:  training loss=0.6039376258850098                                      \n",
      "epoch 104 [9.21s]:  training loss=0.60122150182724                                       \n",
      "epoch 105 [9.28s]: training loss=0.6004809737205505  validation ndcg@10=0.07069521478099738 [0.15s]\n",
      "epoch 106 [9.35s]:  training loss=0.5911422371864319                                     \n",
      "epoch 107 [9.24s]:  training loss=0.5905278325080872                                     \n",
      "epoch 108 [9.18s]:  training loss=0.5912308096885681                                     \n",
      "epoch 109 [9.35s]:  training loss=0.5901046395301819                                     \n",
      "epoch 110 [9.38s]: training loss=0.5897365808486938  validation ndcg@10=0.07142084984398954 [0.16s]\n",
      "epoch 111 [9.33s]:  training loss=0.5856877565383911                                     \n",
      "epoch 112 [9.2s]:  training loss=0.5816704630851746                                      \n",
      "epoch 113 [9.33s]:  training loss=0.5875333547592163                                     \n",
      "epoch 114 [9.27s]:  training loss=0.5841457843780518                                     \n",
      "epoch 115 [9.29s]: training loss=0.5841159224510193  validation ndcg@10=0.0712255082937289 [0.15s]\n",
      "epoch 116 [9.25s]:  training loss=0.5808057188987732                                     \n",
      "epoch 117 [9.3s]:  training loss=0.5845991969108582                                      \n",
      "epoch 118 [9.28s]:  training loss=0.5811523199081421                                     \n",
      "epoch 119 [9.36s]:  training loss=0.572560727596283                                      \n",
      "epoch 120 [9.41s]: training loss=0.5786140561103821  validation ndcg@10=0.07103320304776041 [0.15s]\n",
      "epoch 121 [9.43s]:  training loss=0.5758300423622131                                     \n",
      "epoch 122 [9.77s]:  training loss=0.5765849947929382                                     \n",
      "epoch 123 [9.75s]:  training loss=0.5749954581260681                                     \n",
      "epoch 124 [9.4s]:  training loss=0.5683475732803345                                      \n",
      "epoch 125 [9.43s]: training loss=0.5699693560600281  validation ndcg@10=0.07320973712357258 [0.15s]\n",
      "epoch 126 [9.6s]:  training loss=0.5689225792884827                                      \n",
      "epoch 127 [9.55s]:  training loss=0.5622685551643372                                     \n",
      "epoch 128 [9.6s]:  training loss=0.5711542963981628                                      \n",
      "epoch 129 [9.45s]:  training loss=0.5660783052444458                                     \n",
      "epoch 130 [9.59s]: training loss=0.5701851844787598  validation ndcg@10=0.07172184320629361 [0.14s]\n",
      "epoch 131 [9.38s]:  training loss=0.5609435439109802                                     \n",
      "epoch 132 [9.39s]:  training loss=0.5588725805282593                                     \n",
      "epoch 133 [9.42s]:  training loss=0.5602537393569946                                     \n",
      "epoch 134 [9.31s]:  training loss=0.5635668635368347                                     \n",
      "epoch 135 [9.49s]: training loss=0.5600752234458923  validation ndcg@10=0.07238226094671746 [0.15s]\n",
      "epoch 136 [11.38s]:  training loss=0.5596235394477844                                    \n",
      "epoch 137 [9.75s]:  training loss=0.555748462677002                                      \n",
      "epoch 138 [9.4s]:  training loss=0.5552259683609009                                      \n",
      "epoch 139 [9.38s]:  training loss=0.5578984022140503                                     \n",
      "epoch 140 [9.34s]: training loss=0.5572373867034912  validation ndcg@10=0.07365787435392401 [0.14s]\n",
      "epoch 141 [9.49s]:  training loss=0.5558158755302429                                     \n",
      "epoch 142 [9.47s]:  training loss=0.5531468987464905                                     \n",
      "epoch 143 [9.32s]:  training loss=0.5586980581283569                                     \n",
      "epoch 144 [9.36s]:  training loss=0.5544211864471436                                     \n",
      "epoch 145 [9.41s]: training loss=0.5481002330780029  validation ndcg@10=0.07342524060196197 [0.17s]\n",
      "epoch 146 [9.4s]:  training loss=0.5442091822624207                                      \n",
      "epoch 147 [9.31s]:  training loss=0.5509887337684631                                     \n",
      "epoch 148 [9.48s]:  training loss=0.5483695864677429                                     \n",
      "epoch 149 [9.34s]:  training loss=0.546379566192627                                      \n",
      "epoch 150 [9.32s]: training loss=0.5404535531997681  validation ndcg@10=0.07015662279095669 [0.17s]\n",
      "epoch 151 [9.43s]:  training loss=0.5433239936828613                                     \n",
      "epoch 152 [9.34s]:  training loss=0.5522444844245911                                     \n",
      "epoch 153 [9.27s]:  training loss=0.5436457991600037                                     \n",
      "epoch 154 [9.33s]:  training loss=0.5448117852210999                                     \n",
      "epoch 155 [9.34s]: training loss=0.5463533997535706  validation ndcg@10=0.07230999726677312 [0.16s]\n",
      "epoch 156 [10.22s]:  training loss=0.5425064563751221                                    \n",
      "epoch 157 [9.8s]:  training loss=0.543755829334259                                       \n",
      "epoch 158 [9.71s]:  training loss=0.5372356176376343                                     \n",
      "epoch 159 [10.29s]:  training loss=0.5414472222328186                                    \n",
      "epoch 160 [9.92s]: training loss=0.538274347782135  validation ndcg@10=0.07224945988234512 [0.15s]\n",
      "epoch 161 [10.1s]:  training loss=0.5372706055641174                                     \n",
      "epoch 162 [9.61s]:  training loss=0.5432402491569519                                     \n",
      "epoch 163 [9.51s]:  training loss=0.5373360514640808                                     \n",
      "epoch 164 [9.65s]:  training loss=0.5351625680923462                                     \n",
      "epoch 165 [10.23s]: training loss=0.5330344438552856  validation ndcg@10=0.07084453575706816 [0.16s]\n",
      "epoch 1 [7.67s]:  training loss=0.8291449546813965                                       \n",
      "epoch 2 [8.02s]:  training loss=0.5897487998008728                                       \n",
      "epoch 3 [7.94s]:  training loss=0.5311058759689331                                       \n",
      "epoch 4 [7.79s]:  training loss=0.500543475151062                                        \n",
      "epoch 5 [8.18s]: training loss=0.4604295790195465  validation ndcg@10=0.06312337870835928 [0.12s]\n",
      "epoch 6 [8.23s]:  training loss=0.44168904423713684                                      \n",
      "epoch 7 [8.26s]:  training loss=0.42664408683776855                                      \n",
      "epoch 8 [7.95s]:  training loss=0.42062753438949585                                      \n",
      "epoch 9 [8.09s]:  training loss=0.40805044770240784                                      \n",
      "epoch 10 [7.67s]: training loss=0.38822081685066223  validation ndcg@10=0.07237942570076177 [0.12s]\n",
      "epoch 11 [7.86s]:  training loss=0.37010854482650757                                     \n",
      "epoch 12 [8.06s]:  training loss=0.3654475212097168                                      \n",
      "epoch 13 [7.77s]:  training loss=0.36343538761138916                                     \n",
      "epoch 14 [8.06s]:  training loss=0.35981032252311707                                     \n",
      "epoch 15 [7.69s]: training loss=0.35255739092826843  validation ndcg@10=0.06408458325946972 [0.12s]\n",
      "epoch 16 [8.02s]:  training loss=0.34473171830177307                                     \n",
      "epoch 17 [8.3s]:  training loss=0.34305432438850403                                      \n",
      "epoch 18 [8.42s]:  training loss=0.3399025499820709                                      \n",
      "epoch 19 [7.84s]:  training loss=0.3337812125682831                                      \n",
      "epoch 20 [7.95s]: training loss=0.33769649267196655  validation ndcg@10=0.056864162031075846 [0.13s]\n",
      "epoch 21 [7.95s]:  training loss=0.330056756734848                                       \n",
      "epoch 22 [7.81s]:  training loss=0.3269265294075012                                      \n",
      "epoch 23 [7.99s]:  training loss=0.3251745104789734                                      \n",
      "epoch 24 [7.97s]:  training loss=0.32384175062179565                                     \n",
      "epoch 25 [7.75s]: training loss=0.3216719627380371  validation ndcg@10=0.05662604659490572 [0.12s]\n",
      "epoch 26 [7.94s]:  training loss=0.31878477334976196                                     \n",
      "epoch 27 [7.94s]:  training loss=0.32060083746910095                                     \n",
      "epoch 28 [8.25s]:  training loss=0.31952404975891113                                     \n",
      "epoch 29 [8.09s]:  training loss=0.3182268738746643                                      \n",
      "epoch 30 [7.78s]: training loss=0.3092735707759857  validation ndcg@10=0.06047938342420304 [0.12s]\n",
      "epoch 31 [10.28s]:  training loss=0.308736115694046                                      \n",
      "epoch 32 [7.99s]:  training loss=0.3136383295059204                                      \n",
      "epoch 33 [7.76s]:  training loss=0.30919700860977173                                     \n",
      "epoch 34 [7.92s]:  training loss=0.3073938488960266                                      \n",
      "epoch 35 [7.98s]: training loss=0.3021247982978821  validation ndcg@10=0.059411837651428934 [0.13s]\n",
      "epoch 1 [10.73s]:  training loss=0.8725000023841858                                      \n",
      "epoch 2 [9.79s]:  training loss=0.6423958539962769                                      \n",
      "epoch 3 [9.9s]:  training loss=0.555194616317749                                        \n",
      "epoch 4 [9.78s]:  training loss=0.5209910273551941                                      \n",
      "epoch 5 [9.84s]: training loss=0.5084270238876343  validation ndcg@10=0.0651477439824155 [0.19s]\n",
      "epoch 6 [9.8s]:  training loss=0.5004029870033264                                       \n",
      "epoch 7 [10.01s]:  training loss=0.4915635287761688                                     \n",
      "epoch 8 [10.04s]:  training loss=0.4810176193714142                                     \n",
      "epoch 9 [9.72s]:  training loss=0.46836498379707336                                     \n",
      "epoch 10 [9.83s]: training loss=0.4575176537036896  validation ndcg@10=0.0696450625428324 [0.24s]\n",
      "epoch 11 [9.7s]:  training loss=0.44169971346855164                                     \n",
      "epoch 12 [9.73s]:  training loss=0.42399096488952637                                    \n",
      "epoch 13 [9.89s]:  training loss=0.4116474688053131                                     \n",
      "epoch 14 [9.79s]:  training loss=0.40838173031806946                                    \n",
      "epoch 15 [9.74s]: training loss=0.39683476090431213  validation ndcg@10=0.07593656588967827 [0.17s]\n",
      "epoch 16 [9.99s]:  training loss=0.3852185606956482                                     \n",
      "epoch 17 [9.96s]:  training loss=0.37809786200523376                                    \n",
      "epoch 18 [9.86s]:  training loss=0.3710596561431885                                     \n",
      "epoch 19 [9.77s]:  training loss=0.3716961443424225                                     \n",
      "epoch 20 [9.8s]: training loss=0.35933879017829895  validation ndcg@10=0.0644286682816476 [0.19s]\n",
      "epoch 21 [9.91s]:  training loss=0.35451173782348633                                    \n",
      "epoch 22 [9.95s]:  training loss=0.3516072928905487                                     \n",
      "epoch 23 [9.76s]:  training loss=0.3466271162033081                                     \n",
      "epoch 24 [9.65s]:  training loss=0.3483380675315857                                     \n",
      "epoch 25 [9.8s]: training loss=0.3465921878814697  validation ndcg@10=0.06773334768255088 [0.18s]\n",
      "epoch 26 [9.84s]:  training loss=0.34193509817123413                                    \n",
      "epoch 27 [9.95s]:  training loss=0.3434106707572937                                     \n",
      "epoch 28 [9.61s]:  training loss=0.3424619138240814                                     \n",
      "epoch 29 [9.93s]:  training loss=0.33494266867637634                                    \n",
      "epoch 30 [9.8s]: training loss=0.33463728427886963  validation ndcg@10=0.054560035570778036 [0.23s]\n",
      "epoch 31 [10.04s]:  training loss=0.33817651867866516                                   \n",
      "epoch 32 [10.23s]:  training loss=0.33237922191619873                                   \n",
      "epoch 33 [9.91s]:  training loss=0.3251008093357086                                     \n",
      "epoch 34 [9.92s]:  training loss=0.3371717631816864                                     \n",
      "epoch 35 [9.98s]: training loss=0.32729557156562805  validation ndcg@10=0.055898751572256176 [0.2s]\n",
      "epoch 36 [9.77s]:  training loss=0.33084747195243835                                    \n",
      "epoch 37 [10.22s]:  training loss=0.32684171199798584                                   \n",
      "epoch 38 [9.95s]:  training loss=0.32845377922058105                                    \n",
      "epoch 39 [10.15s]:  training loss=0.32606950402259827                                   \n",
      "epoch 40 [9.95s]: training loss=0.3224472105503082  validation ndcg@10=0.04350504176351359 [0.19s]\n",
      "epoch 1 [3.53s]:  training loss=0.9806181788444519                                      \n",
      "epoch 2 [3.5s]:  training loss=0.970254123210907                                       \n",
      "epoch 3 [3.45s]:  training loss=0.9588212966918945                                     \n",
      "epoch 4 [3.35s]:  training loss=0.947243869304657                                      \n",
      "epoch 5 [3.39s]: training loss=0.9339222311973572  validation ndcg@10=0.04838478126561487 [0.06s]\n",
      "epoch 6 [3.31s]:  training loss=0.9202306270599365                                     \n",
      "epoch 7 [3.33s]:  training loss=0.9044890403747559                                     \n",
      "epoch 8 [3.51s]:  training loss=0.8911734223365784                                     \n",
      "epoch 9 [3.39s]:  training loss=0.875834047794342                                      \n",
      "epoch 10 [3.4s]: training loss=0.8591850996017456  validation ndcg@10=0.06216080450551256 [0.06s]\n",
      "epoch 11 [3.38s]:  training loss=0.8452187180519104                                    \n",
      "epoch 12 [3.61s]:  training loss=0.8245367407798767                                    \n",
      "epoch 13 [3.5s]:  training loss=0.8053901791572571                                     \n",
      "epoch 14 [3.63s]:  training loss=0.7821338176727295                                    \n",
      "epoch 15 [3.69s]: training loss=0.7565145492553711  validation ndcg@10=0.061913355663390804 [0.14s]\n",
      "epoch 16 [5.82s]:  training loss=0.7292563915252686                                    \n",
      "epoch 17 [3.48s]:  training loss=0.7123141288757324                                    \n",
      "epoch 18 [3.45s]:  training loss=0.6964589953422546                                    \n",
      "epoch 19 [3.43s]:  training loss=0.6862998008728027                                    \n",
      "epoch 20 [3.49s]: training loss=0.6806468367576599  validation ndcg@10=0.060558295486185965 [0.06s]\n",
      "epoch 21 [3.43s]:  training loss=0.6677252650260925                                    \n",
      "epoch 22 [3.37s]:  training loss=0.6621553301811218                                    \n",
      "epoch 23 [3.38s]:  training loss=0.6519357562065125                                    \n",
      "epoch 24 [3.37s]:  training loss=0.6503176093101501                                    \n",
      "epoch 25 [3.42s]: training loss=0.6418581604957581  validation ndcg@10=0.06676362552963722 [0.06s]\n",
      "epoch 26 [3.36s]:  training loss=0.6397320628166199                                    \n",
      "epoch 27 [3.4s]:  training loss=0.630492627620697                                      \n",
      "epoch 28 [3.42s]:  training loss=0.6243759393692017                                    \n",
      "epoch 29 [3.54s]:  training loss=0.619839072227478                                     \n",
      "epoch 30 [3.54s]: training loss=0.6193080544471741  validation ndcg@10=0.06843434122551861 [0.07s]\n",
      "epoch 31 [3.42s]:  training loss=0.6109659671783447                                    \n",
      "epoch 32 [3.41s]:  training loss=0.6099132895469666                                    \n",
      "epoch 33 [3.46s]:  training loss=0.6044566035270691                                    \n",
      "epoch 34 [3.37s]:  training loss=0.6048218607902527                                    \n",
      "epoch 35 [3.42s]: training loss=0.5957534909248352  validation ndcg@10=0.07047614018502696 [0.06s]\n",
      "epoch 36 [3.36s]:  training loss=0.5923910140991211                                    \n",
      "epoch 37 [3.35s]:  training loss=0.5854562520980835                                    \n",
      "epoch 38 [3.45s]:  training loss=0.5846510529518127                                    \n",
      "epoch 39 [3.38s]:  training loss=0.5824461579322815                                    \n",
      "epoch 40 [3.37s]: training loss=0.5830296277999878  validation ndcg@10=0.07025877437092075 [0.06s]\n",
      "epoch 41 [3.4s]:  training loss=0.5759926438331604                                     \n",
      "epoch 42 [3.36s]:  training loss=0.5687026977539062                                    \n",
      "epoch 43 [3.43s]:  training loss=0.5638871788978577                                    \n",
      "epoch 44 [3.37s]:  training loss=0.5705040097236633                                    \n",
      "epoch 45 [3.38s]: training loss=0.5569498538970947  validation ndcg@10=0.06752861865266925 [0.06s]\n",
      "epoch 46 [3.34s]:  training loss=0.5570909380912781                                    \n",
      "epoch 47 [3.44s]:  training loss=0.5536134839057922                                    \n",
      "epoch 48 [3.49s]:  training loss=0.5539700984954834                                    \n",
      "epoch 49 [3.46s]:  training loss=0.5468141436576843                                    \n",
      "epoch 50 [3.41s]: training loss=0.5507016777992249  validation ndcg@10=0.06822344390331177 [0.07s]\n",
      "epoch 51 [3.46s]:  training loss=0.5460536479949951                                    \n",
      "epoch 52 [3.38s]:  training loss=0.5424838066101074                                    \n",
      "epoch 53 [3.39s]:  training loss=0.5438064336776733                                    \n",
      "epoch 54 [3.39s]:  training loss=0.5394291281700134                                    \n",
      "epoch 55 [3.38s]: training loss=0.5391007661819458  validation ndcg@10=0.06630541731728884 [0.07s]\n",
      "epoch 56 [3.43s]:  training loss=0.5367194414138794                                    \n",
      "epoch 57 [3.47s]:  training loss=0.5332418084144592                                    \n",
      "epoch 58 [3.37s]:  training loss=0.5319768190383911                                    \n",
      "epoch 59 [3.42s]:  training loss=0.5315120220184326                                    \n",
      "epoch 60 [3.3s]: training loss=0.528041422367096  validation ndcg@10=0.06613445762501852 [0.06s]\n",
      "epoch 1 [21.03s]:  training loss=0.7286975979804993                                    \n",
      "epoch 2 [22.41s]:  training loss=0.9243174195289612                                    \n",
      "epoch 3 [22.57s]:  training loss=1.0578677654266357                                    \n",
      "epoch 4 [22.46s]:  training loss=1.1348011493682861                                    \n",
      "epoch 5 [23.26s]: training loss=1.1906909942626953  validation ndcg@10=0.03652903341288183 [0.27s]\n",
      "epoch 6 [22.5s]:  training loss=1.2260607481002808                                     \n",
      "epoch 7 [22.29s]:  training loss=1.2502774000167847                                    \n",
      "epoch 8 [22.26s]:  training loss=1.309813380241394                                     \n",
      "epoch 9 [22.64s]:  training loss=1.3018420934677124                                    \n",
      "epoch 10 [22.43s]: training loss=1.3508046865463257  validation ndcg@10=0.03275975727119005 [0.31s]\n",
      "epoch 11 [22.31s]:  training loss=1.325478196144104                                    \n",
      "epoch 12 [22.63s]:  training loss=1.3359240293502808                                   \n",
      "epoch 13 [22.69s]:  training loss=1.4057258367538452                                   \n",
      "epoch 14 [25.77s]:  training loss=1.3980516195297241                                   \n",
      "epoch 15 [23.17s]: training loss=1.3765472173690796  validation ndcg@10=0.015330962413949384 [0.33s]\n",
      "epoch 16 [22.55s]:  training loss=1.416445255279541                                    \n",
      "epoch 17 [22.56s]:  training loss=1.4475419521331787                                   \n",
      "epoch 18 [22.59s]:  training loss=1.4482451677322388                                   \n",
      "epoch 19 [23.08s]:  training loss=1.4757037162780762                                   \n",
      "epoch 20 [22.57s]: training loss=1.4361743927001953  validation ndcg@10=0.029026042959803548 [0.3s]\n",
      "epoch 21 [22.7s]:  training loss=1.4135013818740845                                    \n",
      "epoch 22 [23.05s]:  training loss=1.4362914562225342                                   \n",
      "epoch 23 [22.76s]:  training loss=1.4502291679382324                                   \n",
      "epoch 24 [22.79s]:  training loss=1.4924787282943726                                   \n",
      "epoch 25 [22.74s]: training loss=1.4592872858047485  validation ndcg@10=0.02207068764156102 [0.28s]\n",
      "epoch 26 [22.96s]:  training loss=1.4674206972122192                                   \n",
      "epoch 27 [21.93s]:  training loss=1.475356101989746                                    \n",
      "epoch 28 [23.04s]:  training loss=1.4980450868606567                                   \n",
      "epoch 29 [22.84s]:  training loss=1.4682824611663818                                   \n",
      "epoch 30 [22.22s]: training loss=1.5032036304473877  validation ndcg@10=0.008524308799638888 [0.37s]\n",
      "epoch 1 [19.88s]:  training loss=0.5712323784828186                                    \n",
      "epoch 2 [20.73s]:  training loss=0.45129647850990295                                   \n",
      "epoch 3 [20.52s]:  training loss=0.42717042565345764                                   \n",
      "epoch 4 [20.64s]:  training loss=0.4190492033958435                                    \n",
      "epoch 5 [23.02s]: training loss=0.4196520745754242  validation ndcg@10=0.0397298745477851 [0.29s]\n",
      "epoch 6 [20.58s]:  training loss=0.4192328453063965                                    \n",
      "epoch 7 [20.56s]:  training loss=0.41260868310928345                                   \n",
      "epoch 8 [20.68s]:  training loss=0.4178233742713928                                    \n",
      "epoch 9 [20.35s]:  training loss=0.412188857793808                                     \n",
      "epoch 10 [21.48s]: training loss=0.4140298664569855  validation ndcg@10=0.03677588254506273 [0.37s]\n",
      "epoch 11 [20.45s]:  training loss=0.41268137097358704                                  \n",
      "epoch 12 [20.59s]:  training loss=0.40310585498809814                                  \n",
      "epoch 13 [20.39s]:  training loss=0.4095213711261749                                   \n",
      "epoch 14 [20.9s]:  training loss=0.4039633870124817                                    \n",
      "epoch 15 [19.38s]: training loss=0.41079625487327576  validation ndcg@10=0.026309655571360013 [0.34s]\n",
      "epoch 16 [20.54s]:  training loss=0.41175147891044617                                  \n",
      "epoch 17 [20.72s]:  training loss=0.41047465801239014                                  \n",
      "epoch 18 [20.27s]:  training loss=0.40694358944892883                                  \n",
      "epoch 19 [20.63s]:  training loss=0.40872445702552795                                  \n",
      "epoch 20 [19.13s]: training loss=0.4182016849517822  validation ndcg@10=0.01666935131085087 [0.28s]\n",
      "epoch 21 [19.33s]:  training loss=0.41294485330581665                                  \n",
      "epoch 22 [21.01s]:  training loss=0.4130302667617798                                   \n",
      "epoch 23 [21.1s]:  training loss=0.41592562198638916                                   \n",
      "epoch 24 [20.66s]:  training loss=0.40241891145706177                                  \n",
      "epoch 25 [20.22s]: training loss=0.40184181928634644  validation ndcg@10=0.023768102321179475 [0.28s]\n",
      "epoch 26 [20.79s]:  training loss=0.4019802510738373                                   \n",
      "epoch 27 [19.86s]:  training loss=0.41370660066604614                                  \n",
      "epoch 28 [22.36s]:  training loss=0.40533149242401123                                  \n",
      "epoch 29 [20.19s]:  training loss=0.40950414538383484                                  \n",
      "epoch 30 [20.95s]: training loss=0.41278961300849915  validation ndcg@10=0.02063272710858006 [0.28s]\n",
      "epoch 1 [1.41s]:  training loss=0.9587331414222717                                     \n",
      "epoch 2 [1.4s]:  training loss=0.8871451616287231                                      \n",
      "epoch 3 [1.41s]:  training loss=0.7908612489700317                                     \n",
      "epoch 4 [1.33s]:  training loss=0.7266303896903992                                     \n",
      "epoch 5 [1.28s]: training loss=0.698544442653656  validation ndcg@10=0.05016725693118558 [0.03s]\n",
      "epoch 6 [1.31s]:  training loss=0.6641209125518799                                     \n",
      "epoch 7 [1.42s]:  training loss=0.6441847085952759                                     \n",
      "epoch 8 [1.32s]:  training loss=0.6192370653152466                                     \n",
      "epoch 9 [1.31s]:  training loss=0.6040780544281006                                     \n",
      "epoch 10 [1.23s]: training loss=0.5882447957992554  validation ndcg@10=0.05632122492681728 [0.04s]\n",
      "epoch 11 [1.26s]:  training loss=0.5630099773406982                                    \n",
      "epoch 12 [1.25s]:  training loss=0.558838427066803                                     \n",
      "epoch 13 [1.31s]:  training loss=0.5476860404014587                                    \n",
      "epoch 14 [1.31s]:  training loss=0.5402054190635681                                    \n",
      "epoch 15 [1.37s]: training loss=0.5281432867050171  validation ndcg@10=0.05881853743915755 [0.03s]\n",
      "epoch 16 [1.3s]:  training loss=0.523945689201355                                      \n",
      "epoch 17 [1.37s]:  training loss=0.5249500274658203                                    \n",
      "epoch 18 [1.34s]:  training loss=0.5113656520843506                                    \n",
      "epoch 19 [1.27s]:  training loss=0.5127875804901123                                    \n",
      "epoch 20 [1.19s]: training loss=0.5039849877357483  validation ndcg@10=0.06100877484285212 [0.03s]\n",
      "epoch 21 [1.3s]:  training loss=0.5040685534477234                                     \n",
      "epoch 22 [1.31s]:  training loss=0.49970051646232605                                   \n",
      "epoch 23 [1.29s]:  training loss=0.49480971693992615                                   \n",
      "epoch 24 [1.25s]:  training loss=0.4877457618713379                                    \n",
      "epoch 25 [1.28s]: training loss=0.4876526892185211  validation ndcg@10=0.06510426034073943 [0.03s]\n",
      "epoch 26 [1.28s]:  training loss=0.48310527205467224                                   \n",
      "epoch 27 [1.25s]:  training loss=0.4803639352321625                                    \n",
      "epoch 28 [1.25s]:  training loss=0.4715167284011841                                    \n",
      "epoch 29 [1.23s]:  training loss=0.4725192189216614                                    \n",
      "epoch 30 [1.24s]: training loss=0.4698980152606964  validation ndcg@10=0.061122087506922634 [0.03s]\n",
      "epoch 31 [1.29s]:  training loss=0.4708823263645172                                    \n",
      "epoch 32 [1.24s]:  training loss=0.46282893419265747                                   \n",
      "epoch 33 [1.24s]:  training loss=0.46381932497024536                                   \n",
      "epoch 34 [1.25s]:  training loss=0.4651789367198944                                    \n",
      "epoch 35 [1.24s]: training loss=0.45993804931640625  validation ndcg@10=0.06548295650103116 [0.04s]\n",
      "epoch 36 [1.22s]:  training loss=0.453470915555954                                     \n",
      "epoch 37 [1.23s]:  training loss=0.44306468963623047                                   \n",
      "epoch 38 [1.26s]:  training loss=0.44949468970298767                                   \n",
      "epoch 39 [1.24s]:  training loss=0.44924718141555786                                   \n",
      "epoch 40 [1.24s]: training loss=0.4439794421195984  validation ndcg@10=0.06340979686510727 [0.03s]\n",
      "epoch 41 [1.26s]:  training loss=0.44193994998931885                                   \n",
      "epoch 42 [1.23s]:  training loss=0.44263535737991333                                   \n",
      "epoch 43 [1.36s]:  training loss=0.4402705430984497                                    \n",
      "epoch 44 [1.25s]:  training loss=0.4365299940109253                                    \n",
      "epoch 45 [1.22s]: training loss=0.4426347315311432  validation ndcg@10=0.06598708951505691 [0.03s]\n",
      "epoch 46 [1.2s]:  training loss=0.4339524209499359                                     \n",
      "epoch 47 [1.24s]:  training loss=0.4304274022579193                                    \n",
      "epoch 48 [1.27s]:  training loss=0.43956664204597473                                   \n",
      "epoch 49 [1.26s]:  training loss=0.4323810338973999                                    \n",
      "epoch 50 [1.26s]: training loss=0.4335101246833801  validation ndcg@10=0.06278916197032218 [0.03s]\n",
      "epoch 51 [1.28s]:  training loss=0.4281419813632965                                    \n",
      "epoch 52 [1.22s]:  training loss=0.42513954639434814                                   \n",
      "epoch 53 [1.22s]:  training loss=0.4211111068725586                                    \n",
      "epoch 54 [1.22s]:  training loss=0.42574650049209595                                   \n",
      "epoch 55 [1.25s]: training loss=0.4202009439468384  validation ndcg@10=0.0711760179181141 [0.03s]\n",
      "epoch 56 [1.26s]:  training loss=0.4258348047733307                                    \n",
      "epoch 57 [1.25s]:  training loss=0.42135924100875854                                   \n",
      "epoch 58 [1.27s]:  training loss=0.4225721061229706                                    \n",
      "epoch 59 [1.28s]:  training loss=0.418110191822052                                     \n",
      "epoch 60 [1.24s]: training loss=0.4170655906200409  validation ndcg@10=0.06189042735953092 [0.03s]\n",
      "epoch 61 [1.25s]:  training loss=0.41189494729042053                                   \n",
      "epoch 62 [1.22s]:  training loss=0.4108806252479553                                    \n",
      "epoch 63 [1.24s]:  training loss=0.40923872590065                                      \n",
      "epoch 64 [1.27s]:  training loss=0.3995438814163208                                    \n",
      "epoch 65 [1.27s]: training loss=0.3947216272354126  validation ndcg@10=0.06583054774882356 [0.03s]\n",
      "epoch 66 [1.24s]:  training loss=0.3969850242137909                                    \n",
      "epoch 67 [1.28s]:  training loss=0.39911022782325745                                   \n",
      "epoch 68 [1.25s]:  training loss=0.3947930932044983                                    \n",
      "epoch 69 [1.23s]:  training loss=0.3915281593799591                                    \n",
      "epoch 70 [1.22s]: training loss=0.38204747438430786  validation ndcg@10=0.07222859272847805 [0.03s]\n",
      "epoch 71 [1.23s]:  training loss=0.3848739266395569                                    \n",
      "epoch 72 [1.26s]:  training loss=0.3843705654144287                                    \n",
      "epoch 73 [1.26s]:  training loss=0.3875351548194885                                    \n",
      "epoch 74 [1.27s]:  training loss=0.3826389014720917                                    \n",
      "epoch 75 [1.22s]: training loss=0.3817523717880249  validation ndcg@10=0.06707147578606697 [0.03s]\n",
      "epoch 76 [1.31s]:  training loss=0.3765107989311218                                    \n",
      "epoch 77 [1.32s]:  training loss=0.3826864957809448                                    \n",
      "epoch 78 [1.26s]:  training loss=0.3811742961406708                                    \n",
      "epoch 79 [1.25s]:  training loss=0.37876370549201965                                   \n",
      "epoch 80 [1.24s]: training loss=0.3750859200954437  validation ndcg@10=0.06957075011430731 [0.03s]\n",
      "epoch 81 [1.25s]:  training loss=0.3724204897880554                                    \n",
      "epoch 82 [1.19s]:  training loss=0.37074849009513855                                   \n",
      "epoch 83 [1.24s]:  training loss=0.37371373176574707                                   \n",
      "epoch 84 [1.22s]:  training loss=0.3742242157459259                                    \n",
      "epoch 85 [1.28s]: training loss=0.3701918125152588  validation ndcg@10=0.07178686389028806 [0.03s]\n",
      "epoch 86 [1.32s]:  training loss=0.3696346879005432                                    \n",
      "epoch 87 [1.23s]:  training loss=0.37249526381492615                                   \n",
      "epoch 88 [1.28s]:  training loss=0.3663643002510071                                    \n",
      "epoch 89 [1.29s]:  training loss=0.3655078411102295                                    \n",
      "epoch 90 [1.33s]: training loss=0.3653299808502197  validation ndcg@10=0.07439748131481966 [0.03s]\n",
      "epoch 91 [1.34s]:  training loss=0.36536210775375366                                   \n",
      "epoch 92 [1.29s]:  training loss=0.3631085753440857                                    \n",
      "epoch 93 [1.35s]:  training loss=0.3623068928718567                                    \n",
      "epoch 94 [1.33s]:  training loss=0.3623475730419159                                    \n",
      "epoch 95 [1.36s]: training loss=0.36660996079444885  validation ndcg@10=0.06542129559077167 [0.03s]\n",
      "epoch 96 [1.33s]:  training loss=0.35691696405410767                                   \n",
      "epoch 97 [1.27s]:  training loss=0.3617444634437561                                    \n",
      "epoch 98 [1.31s]:  training loss=0.35767561197280884                                   \n",
      "epoch 99 [1.31s]:  training loss=0.35962411761283875                                   \n",
      "epoch 100 [1.3s]: training loss=0.3650369644165039  validation ndcg@10=0.06887090607552944 [0.03s]\n",
      "epoch 101 [1.36s]:  training loss=0.3640247583389282                                   \n",
      "epoch 102 [1.3s]:  training loss=0.35983917117118835                                   \n",
      "epoch 103 [1.23s]:  training loss=0.35545921325683594                                  \n",
      "epoch 104 [1.23s]:  training loss=0.3582896292209625                                   \n",
      "epoch 105 [1.22s]: training loss=0.3494113087654114  validation ndcg@10=0.06437831593552715 [0.03s]\n",
      "epoch 106 [1.22s]:  training loss=0.3574809432029724                                   \n",
      "epoch 107 [1.24s]:  training loss=0.3515392541885376                                   \n",
      "epoch 108 [1.27s]:  training loss=0.36051318049430847                                  \n",
      "epoch 109 [1.28s]:  training loss=0.3542061150074005                                   \n",
      "epoch 110 [1.24s]: training loss=0.35562488436698914  validation ndcg@10=0.06723255828393196 [0.03s]\n",
      "epoch 111 [1.24s]:  training loss=0.3474997878074646                                   \n",
      "epoch 112 [1.26s]:  training loss=0.3477439880371094                                   \n",
      "epoch 113 [1.26s]:  training loss=0.35691338777542114                                  \n",
      "epoch 114 [1.25s]:  training loss=0.34946176409721375                                  \n",
      "epoch 115 [1.19s]: training loss=0.3506363034248352  validation ndcg@10=0.06436005463560791 [0.04s]\n",
      "epoch 1 [22.94s]:  training loss=0.8105410933494568                                     \n",
      "epoch 2 [22.89s]:  training loss=0.5722628235816956                                     \n",
      "epoch 3 [22.87s]:  training loss=0.520479142665863                                      \n",
      "epoch 4 [23.04s]:  training loss=0.4858492314815521                                     \n",
      "epoch 5 [22.75s]: training loss=0.45477527379989624  validation ndcg@10=0.05420830124489465 [0.27s]\n",
      "epoch 6 [22.96s]:  training loss=0.4370826482772827                                     \n",
      "epoch 7 [22.75s]:  training loss=0.4154543876647949                                     \n",
      "epoch 8 [22.72s]:  training loss=0.39673087000846863                                    \n",
      "epoch 9 [22.61s]:  training loss=0.38170918822288513                                    \n",
      "epoch 10 [24.59s]: training loss=0.3725728690624237  validation ndcg@10=0.05606441859595476 [0.26s]\n",
      "epoch 11 [22.95s]:  training loss=0.3708716332912445                                    \n",
      "epoch 12 [22.73s]:  training loss=0.36270126700401306                                   \n",
      "epoch 13 [22.72s]:  training loss=0.3584594428539276                                    \n",
      "epoch 14 [22.55s]:  training loss=0.3492501378059387                                    \n",
      "epoch 15 [22.7s]: training loss=0.34856677055358887  validation ndcg@10=0.06457629170563385 [0.24s]\n",
      "epoch 16 [23.09s]:  training loss=0.33729109168052673                                   \n",
      "epoch 17 [22.73s]:  training loss=0.3352961838245392                                    \n",
      "epoch 18 [22.62s]:  training loss=0.32670336961746216                                   \n",
      "epoch 19 [22.53s]:  training loss=0.33098819851875305                                   \n",
      "epoch 20 [22.91s]: training loss=0.32541996240615845  validation ndcg@10=0.07161542505016648 [0.24s]\n",
      "epoch 21 [22.97s]:  training loss=0.32081007957458496                                   \n",
      "epoch 22 [23.02s]:  training loss=0.32282742857933044                                   \n",
      "epoch 23 [23.41s]:  training loss=0.31969910860061646                                   \n",
      "epoch 24 [23.17s]:  training loss=0.31660181283950806                                   \n",
      "epoch 25 [22.62s]: training loss=0.3121122121810913  validation ndcg@10=0.06977717402860074 [0.27s]\n",
      "epoch 26 [23.06s]:  training loss=0.3135533928871155                                    \n",
      "epoch 27 [22.55s]:  training loss=0.31021109223365784                                   \n",
      "epoch 28 [23.03s]:  training loss=0.3168671429157257                                    \n",
      "epoch 29 [24.66s]:  training loss=0.307312548160553                                     \n",
      "epoch 30 [23.04s]: training loss=0.3086385130882263  validation ndcg@10=0.05471394085856715 [0.26s]\n",
      "epoch 31 [22.66s]:  training loss=0.3061865270137787                                    \n",
      "epoch 32 [22.56s]:  training loss=0.3045700192451477                                    \n",
      "epoch 33 [23.03s]:  training loss=0.3007412850856781                                    \n",
      "epoch 34 [22.66s]:  training loss=0.301617830991745                                     \n",
      "epoch 35 [22.25s]: training loss=0.3031100034713745  validation ndcg@10=0.056822513056875905 [0.29s]\n",
      "epoch 36 [22.05s]:  training loss=0.30580607056617737                                   \n",
      "epoch 37 [22.34s]:  training loss=0.2998795807361603                                    \n",
      "epoch 38 [22.26s]:  training loss=0.29959216713905334                                   \n",
      "epoch 39 [22.32s]:  training loss=0.29791340231895447                                   \n",
      "epoch 40 [22.2s]: training loss=0.2934298515319824  validation ndcg@10=0.04576201189084602 [0.24s]\n",
      "epoch 41 [22.14s]:  training loss=0.29445314407348633                                   \n",
      "epoch 42 [22.38s]:  training loss=0.2991080582141876                                    \n",
      "epoch 43 [22.35s]:  training loss=0.2973611652851105                                    \n",
      "epoch 44 [22.26s]:  training loss=0.291665256023407                                     \n",
      "epoch 45 [22.29s]: training loss=0.2950156629085541  validation ndcg@10=0.05998424225764414 [0.31s]\n",
      "epoch 1 [11.73s]:  training loss=0.9825766682624817                                     \n",
      "epoch 2 [12.37s]:  training loss=0.9745856523513794                                     \n",
      "epoch 3 [12.76s]:  training loss=0.9692262411117554                                     \n",
      "epoch 4 [12.81s]:  training loss=0.9613490104675293                                     \n",
      "epoch 5 [12.9s]: training loss=0.952079713344574  validation ndcg@10=0.041514387453141316 [0.2s]\n",
      "epoch 6 [14.51s]:  training loss=0.946008026599884                                      \n",
      "epoch 7 [11.96s]:  training loss=0.9371343851089478                                     \n",
      "epoch 8 [12.02s]:  training loss=0.9297723770141602                                     \n",
      "epoch 9 [12.45s]:  training loss=0.9200092554092407                                     \n",
      "epoch 10 [11.93s]: training loss=0.913719654083252  validation ndcg@10=0.06392173920925642 [0.19s]\n",
      "epoch 11 [12.28s]:  training loss=0.9022742509841919                                    \n",
      "epoch 12 [12.73s]:  training loss=0.8944812417030334                                    \n",
      "epoch 13 [12.69s]:  training loss=0.8845640420913696                                    \n",
      "epoch 14 [11.79s]:  training loss=0.875253438949585                                     \n",
      "epoch 15 [12.41s]: training loss=0.865272045135498  validation ndcg@10=0.06583113884890604 [0.19s]\n",
      "epoch 16 [12.33s]:  training loss=0.8534743785858154                                    \n",
      "epoch 17 [12.0s]:  training loss=0.8436654210090637                                     \n",
      "epoch 18 [12.25s]:  training loss=0.8341098427772522                                    \n",
      "epoch 19 [12.05s]:  training loss=0.8270553946495056                                    \n",
      "epoch 20 [11.98s]: training loss=0.8131263852119446  validation ndcg@10=0.07241853569484057 [0.19s]\n",
      "epoch 21 [12.91s]:  training loss=0.804131031036377                                     \n",
      "epoch 22 [12.16s]:  training loss=0.7916520237922668                                    \n",
      "epoch 23 [12.4s]:  training loss=0.7775620222091675                                     \n",
      "epoch 24 [12.16s]:  training loss=0.7706160545349121                                    \n",
      "epoch 25 [11.99s]: training loss=0.7593027949333191  validation ndcg@10=0.074474294806834 [0.19s]\n",
      "epoch 26 [12.45s]:  training loss=0.7432704567909241                                    \n",
      "epoch 27 [12.37s]:  training loss=0.7314196825027466                                    \n",
      "epoch 28 [12.12s]:  training loss=0.7205255031585693                                    \n",
      "epoch 29 [12.23s]:  training loss=0.7088767290115356                                    \n",
      "epoch 30 [11.84s]: training loss=0.6965558528900146  validation ndcg@10=0.07387348810420791 [0.19s]\n",
      "epoch 31 [11.87s]:  training loss=0.6852445006370544                                    \n",
      "epoch 32 [11.75s]:  training loss=0.675229012966156                                     \n",
      "epoch 33 [12.38s]:  training loss=0.6692861318588257                                    \n",
      "epoch 34 [12.34s]:  training loss=0.6580417156219482                                    \n",
      "epoch 35 [12.43s]: training loss=0.6508071422576904  validation ndcg@10=0.07231361725672038 [0.21s]\n",
      "epoch 36 [12.15s]:  training loss=0.6450285911560059                                    \n",
      "epoch 37 [12.16s]:  training loss=0.640313446521759                                     \n",
      "epoch 38 [11.91s]:  training loss=0.6369013786315918                                    \n",
      "epoch 39 [12.27s]:  training loss=0.6297838091850281                                    \n",
      "epoch 40 [11.93s]: training loss=0.6188032031059265  validation ndcg@10=0.07040423792728877 [0.18s]\n",
      "epoch 41 [12.24s]:  training loss=0.621761679649353                                     \n",
      "epoch 42 [12.33s]:  training loss=0.6162371635437012                                    \n",
      "epoch 43 [12.02s]:  training loss=0.6185736656188965                                    \n",
      "epoch 44 [14.31s]:  training loss=0.608818531036377                                     \n",
      "epoch 45 [12.41s]: training loss=0.5985180735588074  validation ndcg@10=0.07527410285718358 [0.21s]\n",
      "epoch 46 [11.92s]:  training loss=0.5989094972610474                                    \n",
      "epoch 47 [12.61s]:  training loss=0.5922468304634094                                    \n",
      "epoch 48 [12.72s]:  training loss=0.5944497585296631                                    \n",
      "epoch 49 [12.38s]:  training loss=0.591640293598175                                     \n",
      "epoch 50 [12.32s]: training loss=0.5888756513595581  validation ndcg@10=0.07222881109041424 [0.19s]\n",
      "epoch 51 [12.32s]:  training loss=0.5846834182739258                                    \n",
      "epoch 52 [12.1s]:  training loss=0.5832950472831726                                     \n",
      "epoch 53 [12.19s]:  training loss=0.5778660774230957                                    \n",
      "epoch 54 [11.95s]:  training loss=0.5748326778411865                                    \n",
      "epoch 55 [12.19s]: training loss=0.5777250528335571  validation ndcg@10=0.07445146215078668 [0.23s]\n",
      "epoch 56 [11.79s]:  training loss=0.5705040693283081                                    \n",
      "epoch 57 [12.44s]:  training loss=0.5684388875961304                                    \n",
      "epoch 58 [12.28s]:  training loss=0.5662853717803955                                    \n",
      "epoch 59 [11.86s]:  training loss=0.570015013217926                                     \n",
      "epoch 60 [11.95s]: training loss=0.5654330849647522  validation ndcg@10=0.07233636383467136 [0.19s]\n",
      "epoch 61 [12.18s]:  training loss=0.5598524212837219                                    \n",
      "epoch 62 [12.32s]:  training loss=0.5554726123809814                                    \n",
      "epoch 63 [12.36s]:  training loss=0.5534079670906067                                    \n",
      "epoch 64 [12.33s]:  training loss=0.5578594207763672                                    \n",
      "epoch 65 [12.54s]: training loss=0.5499354004859924  validation ndcg@10=0.07223815630668458 [0.22s]\n",
      "epoch 66 [12.46s]:  training loss=0.5471449494361877                                    \n",
      "epoch 67 [11.82s]:  training loss=0.5495756268501282                                    \n",
      "epoch 68 [12.36s]:  training loss=0.5506210923194885                                    \n",
      "epoch 69 [11.92s]:  training loss=0.5421040654182434                                    \n",
      "epoch 70 [11.9s]: training loss=0.5449513792991638  validation ndcg@10=0.0731371273455696 [0.19s]\n",
      "epoch 1 [36.84s]:  training loss=0.5667821168899536                                     \n",
      "epoch 2 [37.72s]:  training loss=0.47880223393440247                                    \n",
      "epoch 3 [40.55s]:  training loss=0.49240565299987793                                    \n",
      "epoch 4 [41.2s]:  training loss=0.5050777792930603                                      \n",
      "epoch 5 [38.43s]: training loss=0.5079156756401062  validation ndcg@10=0.03872321520261056 [0.56s]\n",
      "epoch 6 [37.9s]:  training loss=0.5107743144035339                                      \n",
      "epoch 7 [39.2s]:  training loss=0.5252862572669983                                      \n",
      "epoch 8 [38.94s]:  training loss=0.5294365286827087                                     \n",
      "epoch 9 [39.23s]:  training loss=0.5172156691551208                                     \n",
      "epoch 10 [38.83s]: training loss=0.5425976514816284  validation ndcg@10=0.03753480685595281 [0.46s]\n",
      "epoch 11 [39.62s]:  training loss=0.5388813614845276                                    \n",
      "epoch 12 [39.21s]:  training loss=0.5383555293083191                                    \n",
      "epoch 13 [42.1s]:  training loss=0.5532963275909424                                     \n",
      "epoch 14 [37.86s]:  training loss=0.5543374419212341                                    \n",
      "epoch 15 [40.02s]: training loss=0.5543702244758606  validation ndcg@10=0.025681848648819536 [0.46s]\n",
      "epoch 16 [38.95s]:  training loss=0.5581017136573792                                    \n",
      "epoch 17 [38.74s]:  training loss=0.5712862610816956                                    \n",
      "epoch 18 [38.2s]:  training loss=0.5532539486885071                                     \n",
      "epoch 19 [39.21s]:  training loss=0.5584546327590942                                    \n",
      "epoch 20 [38.34s]: training loss=0.5670658349990845  validation ndcg@10=0.034348993412978604 [0.46s]\n",
      "epoch 21 [40.25s]:  training loss=0.5644407272338867                                    \n",
      "epoch 22 [38.47s]:  training loss=0.5715687870979309                                    \n",
      "epoch 23 [42.01s]:  training loss=0.5732661485671997                                    \n",
      "epoch 24 [39.32s]:  training loss=0.565416157245636                                     \n",
      "epoch 25 [40.0s]: training loss=0.5824032425880432  validation ndcg@10=0.02093491268492133 [0.46s]\n",
      "epoch 26 [39.0s]:  training loss=0.5577682852745056                                     \n",
      "epoch 27 [40.14s]:  training loss=0.5776839852333069                                    \n",
      "epoch 28 [39.47s]:  training loss=0.5734575390815735                                    \n",
      "epoch 29 [39.73s]:  training loss=0.5727328658103943                                    \n",
      "epoch 30 [39.23s]: training loss=0.5743163824081421  validation ndcg@10=0.029954163088822948 [0.47s]\n",
      "epoch 1 [11.97s]:  training loss=0.7407165169715881                                     \n",
      "epoch 2 [11.31s]:  training loss=0.5253936052322388                                     \n",
      "epoch 3 [11.23s]:  training loss=0.4911530911922455                                     \n",
      "epoch 4 [11.34s]:  training loss=0.4622699022293091                                     \n",
      "epoch 5 [11.09s]: training loss=0.4315567910671234  validation ndcg@10=0.05887524110669732 [0.24s]\n",
      "epoch 6 [11.11s]:  training loss=0.401170015335083                                      \n",
      "epoch 7 [13.35s]:  training loss=0.3877062201499939                                     \n",
      "epoch 8 [11.87s]:  training loss=0.36521854996681213                                    \n",
      "epoch 9 [11.64s]:  training loss=0.3693379759788513                                     \n",
      "epoch 10 [11.46s]: training loss=0.35985684394836426  validation ndcg@10=0.06412887974903847 [0.23s]\n",
      "epoch 11 [11.74s]:  training loss=0.3496662676334381                                    \n",
      "epoch 12 [11.96s]:  training loss=0.3417802155017853                                    \n",
      "epoch 13 [11.29s]:  training loss=0.343253493309021                                     \n",
      "epoch 14 [11.33s]:  training loss=0.34314653277397156                                   \n",
      "epoch 15 [11.76s]: training loss=0.341636598110199  validation ndcg@10=0.04676867643973164 [0.23s]\n",
      "epoch 16 [12.22s]:  training loss=0.34131142497062683                                   \n",
      "epoch 17 [11.58s]:  training loss=0.33009031414985657                                   \n",
      "epoch 18 [11.87s]:  training loss=0.3396788239479065                                    \n",
      "epoch 19 [11.67s]:  training loss=0.3317078649997711                                    \n",
      "epoch 20 [11.47s]: training loss=0.3237621486186981  validation ndcg@10=0.04939290100815064 [0.22s]\n",
      "epoch 21 [11.45s]:  training loss=0.32417282462120056                                   \n",
      "epoch 22 [11.59s]:  training loss=0.3210272789001465                                    \n",
      "epoch 23 [11.49s]:  training loss=0.32456332445144653                                   \n",
      "epoch 24 [11.45s]:  training loss=0.3221319317817688                                    \n",
      "epoch 25 [11.33s]: training loss=0.32270893454551697  validation ndcg@10=0.03924292029300118 [0.22s]\n",
      "epoch 26 [11.51s]:  training loss=0.32686081528663635                                   \n",
      "epoch 27 [11.41s]:  training loss=0.3215342164039612                                    \n",
      "epoch 28 [11.36s]:  training loss=0.32445797324180603                                   \n",
      "epoch 29 [11.58s]:  training loss=0.3225072920322418                                    \n",
      "epoch 30 [11.41s]: training loss=0.31627556681632996  validation ndcg@10=0.04745340615430595 [0.24s]\n",
      "epoch 31 [11.3s]:  training loss=0.31812140345573425                                    \n",
      "epoch 32 [11.43s]:  training loss=0.31739553809165955                                   \n",
      "epoch 33 [11.28s]:  training loss=0.31726694107055664                                   \n",
      "epoch 34 [11.71s]:  training loss=0.31681200861930847                                   \n",
      "epoch 35 [11.58s]: training loss=0.32065287232398987  validation ndcg@10=0.041391912106529136 [0.22s]\n",
      "epoch 1 [2.25s]:  training loss=0.9709795117378235                                      \n",
      "epoch 2 [1.83s]:  training loss=0.9261924028396606                                      \n",
      "epoch 3 [2.11s]:  training loss=0.8604569435119629                                      \n",
      "epoch 4 [2.24s]:  training loss=0.7809090614318848                                      \n",
      "epoch 5 [2.14s]: training loss=0.7308883666992188  validation ndcg@10=0.062184872761841786 [0.06s]\n",
      "epoch 6 [1.97s]:  training loss=0.7065687775611877                                      \n",
      "epoch 7 [1.96s]:  training loss=0.6865260004997253                                      \n",
      "epoch 8 [1.96s]:  training loss=0.6621823906898499                                      \n",
      "epoch 9 [1.88s]:  training loss=0.6504964828491211                                      \n",
      "epoch 10 [1.97s]: training loss=0.6332792043685913  validation ndcg@10=0.07091200328380512 [0.04s]\n",
      "epoch 11 [1.97s]:  training loss=0.6172583699226379                                     \n",
      "epoch 12 [1.83s]:  training loss=0.5987088680267334                                     \n",
      "epoch 13 [1.72s]:  training loss=0.5919153094291687                                     \n",
      "epoch 14 [1.84s]:  training loss=0.5776436924934387                                     \n",
      "epoch 15 [1.8s]: training loss=0.5726901292800903  validation ndcg@10=0.06947498242619088 [0.04s]\n",
      "epoch 16 [1.68s]:  training loss=0.560431957244873                                      \n",
      "epoch 17 [1.7s]:  training loss=0.5508137345314026                                      \n",
      "epoch 18 [1.79s]:  training loss=0.5480342507362366                                     \n",
      "epoch 19 [1.64s]:  training loss=0.5423574447631836                                     \n",
      "epoch 20 [1.75s]: training loss=0.5346987247467041  validation ndcg@10=0.06974004833634223 [0.04s]\n",
      "epoch 21 [1.73s]:  training loss=0.524796187877655                                      \n",
      "epoch 22 [1.66s]:  training loss=0.5230183601379395                                     \n",
      "epoch 23 [1.76s]:  training loss=0.5165712833404541                                     \n",
      "epoch 24 [1.64s]:  training loss=0.515421450138092                                      \n",
      "epoch 25 [1.75s]: training loss=0.5048040747642517  validation ndcg@10=0.07102559884385007 [0.04s]\n",
      "epoch 26 [1.7s]:  training loss=0.5021730065345764                                      \n",
      "epoch 27 [1.65s]:  training loss=0.5012502074241638                                     \n",
      "epoch 28 [1.74s]:  training loss=0.4923762381076813                                     \n",
      "epoch 29 [1.61s]:  training loss=0.49456432461738586                                    \n",
      "epoch 30 [1.74s]: training loss=0.4879765808582306  validation ndcg@10=0.07318195297584192 [0.03s]\n",
      "epoch 31 [1.71s]:  training loss=0.4857698678970337                                     \n",
      "epoch 32 [1.62s]:  training loss=0.4747295081615448                                     \n",
      "epoch 33 [1.77s]:  training loss=0.4801180958747864                                     \n",
      "epoch 34 [1.71s]:  training loss=0.47121739387512207                                    \n",
      "epoch 35 [1.69s]: training loss=0.46938228607177734  validation ndcg@10=0.07718381839302078 [0.04s]\n",
      "epoch 36 [3.59s]:  training loss=0.47101202607154846                                    \n",
      "epoch 37 [1.78s]:  training loss=0.46707549691200256                                    \n",
      "epoch 38 [1.73s]:  training loss=0.4604693055152893                                     \n",
      "epoch 39 [1.8s]:  training loss=0.4615676999092102                                      \n",
      "epoch 40 [1.81s]: training loss=0.46187490224838257  validation ndcg@10=0.07049916871709026 [0.04s]\n",
      "epoch 41 [1.62s]:  training loss=0.45273876190185547                                    \n",
      "epoch 42 [1.76s]:  training loss=0.451913446187973                                      \n",
      "epoch 43 [1.73s]:  training loss=0.45334523916244507                                    \n",
      "epoch 44 [1.7s]:  training loss=0.45687738060951233                                     \n",
      "epoch 45 [1.73s]: training loss=0.4593568444252014  validation ndcg@10=0.06407943047967962 [0.04s]\n",
      "epoch 46 [1.73s]:  training loss=0.4431881308555603                                     \n",
      "epoch 47 [1.7s]:  training loss=0.44641169905662537                                     \n",
      "epoch 48 [1.72s]:  training loss=0.43889227509498596                                    \n",
      "epoch 49 [1.67s]:  training loss=0.4477851390838623                                     \n",
      "epoch 50 [1.73s]: training loss=0.44109833240509033  validation ndcg@10=0.06829280758901649 [0.04s]\n",
      "epoch 51 [1.75s]:  training loss=0.43989691138267517                                    \n",
      "epoch 52 [1.68s]:  training loss=0.4390951097011566                                     \n",
      "epoch 53 [1.77s]:  training loss=0.4369197487831116                                     \n",
      "epoch 54 [1.64s]:  training loss=0.44060277938842773                                    \n",
      "epoch 55 [1.72s]: training loss=0.43065908551216125  validation ndcg@10=0.06332779121285749 [0.04s]\n",
      "epoch 56 [1.69s]:  training loss=0.43282201886177063                                    \n",
      "epoch 57 [1.75s]:  training loss=0.43384474515914917                                    \n",
      "epoch 58 [1.74s]:  training loss=0.43287235498428345                                    \n",
      "epoch 59 [1.72s]:  training loss=0.4348197281360626                                     \n",
      "epoch 60 [1.71s]: training loss=0.4298129677772522  validation ndcg@10=0.06324053745480182 [0.04s]\n",
      "epoch 1 [7.35s]:  training loss=1.3657172918319702                                      \n",
      "epoch 2 [7.41s]:  training loss=2.3309335708618164                                      \n",
      "epoch 3 [7.65s]:  training loss=3.1857705116271973                                      \n",
      "epoch 4 [7.55s]:  training loss=3.653701066970825                                       \n",
      "epoch 5 [7.79s]: training loss=4.042423248291016  validation ndcg@10=0.03687977517331509 [0.13s]\n",
      "epoch 6 [7.63s]:  training loss=4.529143810272217                                       \n",
      "epoch 7 [7.51s]:  training loss=4.783653736114502                                       \n",
      "epoch 8 [7.55s]:  training loss=5.04388952255249                                        \n",
      "epoch 9 [8.03s]:  training loss=5.2603583335876465                                      \n",
      "epoch 10 [7.59s]: training loss=5.429703712463379  validation ndcg@10=0.03569403900025362 [0.13s]\n",
      "epoch 11 [7.67s]:  training loss=5.388131618499756                                      \n",
      "epoch 12 [7.79s]:  training loss=5.577157974243164                                      \n",
      "epoch 13 [7.69s]:  training loss=5.517766952514648                                      \n",
      "epoch 14 [7.7s]:  training loss=5.847594261169434                                       \n",
      "epoch 15 [7.71s]: training loss=5.95942497253418  validation ndcg@10=0.04487687469261076 [0.13s]\n",
      "epoch 16 [7.54s]:  training loss=5.941763877868652                                      \n",
      "epoch 17 [7.8s]:  training loss=5.991582870483398                                       \n",
      "epoch 18 [7.54s]:  training loss=6.0681915283203125                                     \n",
      "epoch 19 [7.72s]:  training loss=5.991161823272705                                      \n",
      "epoch 20 [7.38s]: training loss=6.025932788848877  validation ndcg@10=0.04700783226189694 [0.14s]\n",
      "epoch 21 [7.57s]:  training loss=6.296235084533691                                      \n",
      "epoch 22 [7.54s]:  training loss=6.291386127471924                                      \n",
      "epoch 23 [7.76s]:  training loss=6.352393627166748                                      \n",
      "epoch 24 [7.66s]:  training loss=6.298033714294434                                      \n",
      "epoch 25 [7.59s]: training loss=6.3449506759643555  validation ndcg@10=0.023522384406909593 [0.14s]\n",
      "epoch 26 [7.4s]:  training loss=6.356032848358154                                       \n",
      "epoch 27 [7.65s]:  training loss=6.238589763641357                                      \n",
      "epoch 28 [7.58s]:  training loss=6.186115264892578                                      \n",
      "epoch 29 [7.97s]:  training loss=6.331800937652588                                      \n",
      "epoch 30 [7.88s]: training loss=6.478573322296143  validation ndcg@10=0.03792779699771273 [0.13s]\n",
      "epoch 31 [7.76s]:  training loss=6.129428386688232                                      \n",
      "epoch 32 [7.59s]:  training loss=6.338122367858887                                      \n",
      "epoch 33 [7.81s]:  training loss=6.155858516693115                                      \n",
      "epoch 34 [7.54s]:  training loss=6.293487071990967                                      \n",
      "epoch 35 [7.49s]: training loss=6.266522407531738  validation ndcg@10=0.045037490550600766 [0.14s]\n",
      "epoch 36 [7.4s]:  training loss=6.301702976226807                                       \n",
      "epoch 37 [7.41s]:  training loss=6.250390529632568                                      \n",
      "epoch 38 [7.48s]:  training loss=6.316705703735352                                      \n",
      "epoch 39 [7.52s]:  training loss=6.276989936828613                                      \n",
      "epoch 40 [7.46s]: training loss=6.416849613189697  validation ndcg@10=0.02843192694338372 [0.13s]\n",
      "epoch 41 [7.57s]:  training loss=6.0960564613342285                                     \n",
      "epoch 42 [7.67s]:  training loss=6.154527187347412                                      \n",
      "epoch 43 [7.46s]:  training loss=6.224000453948975                                      \n",
      "epoch 44 [7.56s]:  training loss=6.091305255889893                                      \n",
      "epoch 45 [7.6s]: training loss=6.1595635414123535  validation ndcg@10=0.028312340167389524 [0.13s]\n",
      "epoch 1 [9.86s]:  training loss=0.7011652588844299                                      \n",
      "epoch 2 [12.2s]:  training loss=0.5243332386016846                                      \n",
      "epoch 3 [10.25s]:  training loss=0.47027602791786194                                    \n",
      "epoch 4 [10.3s]:  training loss=0.44112861156463623                                     \n",
      "epoch 5 [9.88s]: training loss=0.4167267382144928  validation ndcg@10=0.05900253010297166 [0.13s]\n",
      "epoch 6 [10.17s]:  training loss=0.39208513498306274                                    \n",
      "epoch 7 [10.82s]:  training loss=0.37623727321624756                                    \n",
      "epoch 8 [10.3s]:  training loss=0.3669353425502777                                      \n",
      "epoch 9 [10.48s]:  training loss=0.35598084330558777                                    \n",
      "epoch 10 [10.3s]: training loss=0.351645290851593  validation ndcg@10=0.058591316830275304 [0.13s]\n",
      "epoch 11 [10.26s]:  training loss=0.34400835633277893                                   \n",
      "epoch 12 [10.27s]:  training loss=0.3414166271686554                                    \n",
      "epoch 13 [10.6s]:  training loss=0.3400459289550781                                     \n",
      "epoch 14 [10.99s]:  training loss=0.33611100912094116                                   \n",
      "epoch 15 [10.7s]: training loss=0.32852470874786377  validation ndcg@10=0.05726901347521941 [0.14s]\n",
      "epoch 16 [10.55s]:  training loss=0.3340425193309784                                    \n",
      "epoch 17 [10.21s]:  training loss=0.3285885453224182                                    \n",
      "epoch 18 [10.26s]:  training loss=0.32268181443214417                                   \n",
      "epoch 19 [10.16s]:  training loss=0.3219921290874481                                    \n",
      "epoch 20 [10.31s]: training loss=0.3206618130207062  validation ndcg@10=0.03942130025143935 [0.13s]\n",
      "epoch 21 [10.18s]:  training loss=0.3208354413509369                                    \n",
      "epoch 22 [10.3s]:  training loss=0.319230854511261                                      \n",
      "epoch 23 [10.28s]:  training loss=0.3139626681804657                                    \n",
      "epoch 24 [10.32s]:  training loss=0.31600460410118103                                   \n",
      "epoch 25 [10.3s]: training loss=0.3124042749404907  validation ndcg@10=0.044618758955840415 [0.16s]\n",
      "epoch 26 [10.37s]:  training loss=0.3171873390674591                                    \n",
      "epoch 27 [10.3s]:  training loss=0.31125199794769287                                    \n",
      "epoch 28 [10.3s]:  training loss=0.31400835514068604                                    \n",
      "epoch 29 [10.09s]:  training loss=0.3089272379875183                                    \n",
      "epoch 30 [10.32s]: training loss=0.31254538893699646  validation ndcg@10=0.037566354715629934 [0.15s]\n",
      "epoch 1 [5.45s]:  training loss=0.5665039420127869                                      \n",
      "epoch 2 [5.38s]:  training loss=0.47489625215530396                                     \n",
      "epoch 3 [5.68s]:  training loss=0.46773669123649597                                     \n",
      "epoch 4 [5.49s]:  training loss=0.46537163853645325                                     \n",
      "epoch 5 [5.63s]: training loss=0.4674835801124573  validation ndcg@10=0.03387603477246351 [0.1s]\n",
      "epoch 6 [5.54s]:  training loss=0.47687211632728577                                     \n",
      "epoch 7 [5.67s]:  training loss=0.4762061536312103                                      \n",
      "epoch 8 [5.4s]:  training loss=0.48210427165031433                                      \n",
      "epoch 9 [5.41s]:  training loss=0.4877551198005676                                      \n",
      "epoch 10 [5.63s]: training loss=0.49029961228370667  validation ndcg@10=0.03807235573011797 [0.1s]\n",
      "epoch 11 [5.52s]:  training loss=0.47384658455848694                                    \n",
      "epoch 12 [5.71s]:  training loss=0.48179197311401367                                    \n",
      "epoch 13 [5.44s]:  training loss=0.49022775888442993                                    \n",
      "epoch 14 [5.42s]:  training loss=0.48183318972587585                                    \n",
      "epoch 15 [5.61s]: training loss=0.48244425654411316  validation ndcg@10=0.027427384198407026 [0.1s]\n",
      "epoch 16 [5.55s]:  training loss=0.48177430033683777                                    \n",
      "epoch 17 [5.51s]:  training loss=0.4875129461288452                                     \n",
      "epoch 18 [5.76s]:  training loss=0.48392805457115173                                    \n",
      "epoch 19 [5.77s]:  training loss=0.47927653789520264                                    \n",
      "epoch 20 [5.91s]: training loss=0.48346835374832153  validation ndcg@10=0.01792009482222459 [0.1s]\n",
      "epoch 21 [5.49s]:  training loss=0.48807859420776367                                    \n",
      "epoch 22 [5.65s]:  training loss=0.4771423935890198                                     \n",
      "epoch 23 [5.51s]:  training loss=0.48018041253089905                                    \n",
      "epoch 24 [5.57s]:  training loss=0.4851909875869751                                     \n",
      "epoch 25 [5.43s]: training loss=0.47764065861701965  validation ndcg@10=0.014804851855119637 [0.1s]\n",
      "epoch 26 [5.47s]:  training loss=0.472583532333374                                      \n",
      "epoch 27 [5.57s]:  training loss=0.4863511025905609                                     \n",
      "epoch 28 [5.77s]:  training loss=0.47482481598854065                                    \n",
      "epoch 29 [5.49s]:  training loss=0.4728074371814728                                     \n",
      "epoch 30 [5.83s]: training loss=0.4775311350822449  validation ndcg@10=0.004471736990151807 [0.1s]\n",
      "epoch 31 [5.46s]:  training loss=0.4752368927001953                                     \n",
      "epoch 32 [5.9s]:  training loss=0.4687623679637909                                      \n",
      "epoch 33 [5.7s]:  training loss=0.4797980785369873                                      \n",
      "epoch 34 [5.68s]:  training loss=0.48409730195999146                                    \n",
      "epoch 35 [5.69s]: training loss=0.48453107476234436  validation ndcg@10=0.00993599696685644 [0.1s]\n",
      "epoch 1 [8.4s]:  training loss=0.5942206978797913                                       \n",
      "epoch 2 [8.16s]:  training loss=0.47716468572616577                                     \n",
      "epoch 3 [10.08s]:  training loss=0.4366346597671509                                     \n",
      "epoch 4 [8.26s]:  training loss=0.4112287163734436                                      \n",
      "epoch 5 [7.92s]: training loss=0.4146538972854614  validation ndcg@10=0.05356774530576548 [0.12s]\n",
      "epoch 6 [7.55s]:  training loss=0.4051401913166046                                      \n",
      "epoch 7 [7.45s]:  training loss=0.402800589799881                                       \n",
      "epoch 8 [7.63s]:  training loss=0.39251160621643066                                     \n",
      "epoch 9 [7.45s]:  training loss=0.3962084949016571                                      \n",
      "epoch 10 [7.45s]: training loss=0.4010930359363556  validation ndcg@10=0.03279928672146982 [0.12s]\n",
      "epoch 11 [7.46s]:  training loss=0.38826850056648254                                    \n",
      "epoch 12 [7.46s]:  training loss=0.3929823338985443                                     \n",
      "epoch 13 [7.53s]:  training loss=0.3870260715484619                                     \n",
      "epoch 14 [7.49s]:  training loss=0.38232529163360596                                    \n",
      "epoch 15 [7.45s]: training loss=0.39806583523750305  validation ndcg@10=0.03430281558066024 [0.12s]\n",
      "epoch 16 [7.55s]:  training loss=0.3945868909358978                                     \n",
      "epoch 17 [7.45s]:  training loss=0.39656251668930054                                    \n",
      "epoch 18 [7.48s]:  training loss=0.39361920952796936                                    \n",
      "epoch 19 [7.43s]:  training loss=0.3912743031978607                                     \n",
      "epoch 20 [7.49s]: training loss=0.3879258632659912  validation ndcg@10=0.021637212227649572 [0.12s]\n",
      "epoch 21 [7.47s]:  training loss=0.38575464487075806                                    \n",
      "epoch 22 [7.41s]:  training loss=0.39013156294822693                                    \n",
      "epoch 23 [7.48s]:  training loss=0.3919026553630829                                     \n",
      "epoch 24 [7.35s]:  training loss=0.3890020251274109                                     \n",
      "epoch 25 [7.46s]: training loss=0.3796789050102234  validation ndcg@10=0.017129844506031777 [0.12s]\n",
      "epoch 26 [7.41s]:  training loss=0.382079154253006                                      \n",
      "epoch 27 [7.48s]:  training loss=0.3868621289730072                                     \n",
      "epoch 28 [7.6s]:  training loss=0.38447633385658264                                     \n",
      "epoch 29 [7.5s]:  training loss=0.38301169872283936                                     \n",
      "epoch 30 [7.46s]: training loss=0.3879055678844452  validation ndcg@10=0.011959157432608312 [0.12s]\n",
      "epoch 1 [4.02s]:  training loss=0.9520543217658997                                      \n",
      "epoch 2 [4.09s]:  training loss=0.8736993670463562                                      \n",
      "epoch 3 [4.07s]:  training loss=0.7824952602386475                                      \n",
      "epoch 4 [4.07s]:  training loss=0.6909854412078857                                      \n",
      "epoch 5 [4.1s]: training loss=0.6233884692192078  validation ndcg@10=0.07282191118522438 [0.07s]\n",
      "epoch 6 [4.08s]:  training loss=0.5879560112953186                                      \n",
      "epoch 7 [4.11s]:  training loss=0.5628412365913391                                      \n",
      "epoch 8 [4.04s]:  training loss=0.5516317486763                                         \n",
      "epoch 9 [4.15s]:  training loss=0.5337849259376526                                      \n",
      "epoch 10 [4.06s]: training loss=0.5215690732002258  validation ndcg@10=0.06855955820334356 [0.08s]\n",
      "epoch 11 [4.1s]:  training loss=0.511648416519165                                       \n",
      "epoch 12 [4.29s]:  training loss=0.5065600872039795                                     \n",
      "epoch 13 [4.1s]:  training loss=0.494845986366272                                       \n",
      "epoch 14 [4.16s]:  training loss=0.4894922375679016                                     \n",
      "epoch 15 [4.2s]: training loss=0.47585514187812805  validation ndcg@10=0.05772055308160208 [0.08s]\n",
      "epoch 16 [4.19s]:  training loss=0.4664580821990967                                     \n",
      "epoch 17 [4.12s]:  training loss=0.4624648094177246                                     \n",
      "epoch 18 [4.21s]:  training loss=0.45955464243888855                                    \n",
      "epoch 19 [4.19s]:  training loss=0.4501041769981384                                     \n",
      "epoch 20 [4.15s]: training loss=0.4409736096858978  validation ndcg@10=0.0613000151491616 [0.07s]\n",
      "epoch 21 [4.17s]:  training loss=0.43995508551597595                                    \n",
      "epoch 22 [4.17s]:  training loss=0.43723204731941223                                    \n",
      "epoch 23 [4.21s]:  training loss=0.4323776364326477                                     \n",
      "epoch 24 [4.23s]:  training loss=0.42388883233070374                                    \n",
      "epoch 25 [4.11s]: training loss=0.42478659749031067  validation ndcg@10=0.06259088944000238 [0.08s]\n",
      "epoch 26 [4.22s]:  training loss=0.42138004302978516                                    \n",
      "epoch 27 [4.3s]:  training loss=0.41810235381126404                                     \n",
      "epoch 28 [4.22s]:  training loss=0.4082578420639038                                     \n",
      "epoch 29 [4.19s]:  training loss=0.4134873151779175                                     \n",
      "epoch 30 [4.18s]: training loss=0.4043668508529663  validation ndcg@10=0.07071741533727045 [0.07s]\n",
      "epoch 1 [1.7s]:  training loss=0.9826436638832092                                       \n",
      "epoch 2 [1.75s]:  training loss=0.9704452753067017                                      \n",
      "epoch 3 [1.61s]:  training loss=0.9554651975631714                                      \n",
      "epoch 4 [1.83s]:  training loss=0.941402792930603                                       \n",
      "epoch 5 [1.69s]: training loss=0.9250285625457764  validation ndcg@10=0.04347324626735363 [0.04s]\n",
      "epoch 6 [1.58s]:  training loss=0.9077902436256409                                      \n",
      "epoch 7 [1.85s]:  training loss=0.8855160474777222                                      \n",
      "epoch 8 [1.58s]:  training loss=0.8623977899551392                                      \n",
      "epoch 9 [1.74s]:  training loss=0.8366284966468811                                      \n",
      "epoch 10 [1.59s]: training loss=0.8103805184364319  validation ndcg@10=0.05745703562537359 [0.04s]\n",
      "epoch 11 [1.61s]:  training loss=0.7789848446846008                                     \n",
      "epoch 12 [1.68s]:  training loss=0.7661941647529602                                     \n",
      "epoch 13 [1.57s]:  training loss=0.7496416568756104                                     \n",
      "epoch 14 [1.65s]:  training loss=0.7385678887367249                                     \n",
      "epoch 15 [1.52s]: training loss=0.7268253564834595  validation ndcg@10=0.0617662540995111 [0.03s]\n",
      "epoch 16 [1.5s]:  training loss=0.7151558995246887                                      \n",
      "epoch 17 [1.66s]:  training loss=0.707079291343689                                      \n",
      "epoch 18 [1.51s]:  training loss=0.6990503668785095                                     \n",
      "epoch 19 [1.63s]:  training loss=0.6940513253211975                                     \n",
      "epoch 20 [1.52s]: training loss=0.6854977607727051  validation ndcg@10=0.06377988159701514 [0.03s]\n",
      "epoch 21 [1.54s]:  training loss=0.675582230091095                                      \n",
      "epoch 22 [1.56s]:  training loss=0.6739907264709473                                     \n",
      "epoch 23 [1.5s]:  training loss=0.6658862829208374                                      \n",
      "epoch 24 [1.63s]:  training loss=0.6589006781578064                                     \n",
      "epoch 25 [1.53s]: training loss=0.6532854437828064  validation ndcg@10=0.060540493771920084 [0.03s]\n",
      "epoch 26 [1.71s]:  training loss=0.6506028771400452                                     \n",
      "epoch 27 [1.56s]:  training loss=0.6440341472625732                                     \n",
      "epoch 28 [1.52s]:  training loss=0.6350076198577881                                     \n",
      "epoch 29 [1.66s]:  training loss=0.6337426900863647                                     \n",
      "epoch 30 [1.55s]: training loss=0.6262146830558777  validation ndcg@10=0.06370330854476954 [0.04s]\n",
      "epoch 31 [1.66s]:  training loss=0.6199915409088135                                     \n",
      "epoch 32 [1.53s]:  training loss=0.6157642602920532                                     \n",
      "epoch 33 [1.5s]:  training loss=0.616513192653656                                       \n",
      "epoch 34 [1.63s]:  training loss=0.6073788404464722                                     \n",
      "epoch 35 [1.51s]: training loss=0.6028806567192078  validation ndcg@10=0.06182853664917588 [0.03s]\n",
      "epoch 36 [1.59s]:  training loss=0.6015632152557373                                     \n",
      "epoch 37 [1.53s]:  training loss=0.5938063263893127                                     \n",
      "epoch 38 [1.65s]:  training loss=0.5921546220779419                                     \n",
      "epoch 39 [1.48s]:  training loss=0.5864396691322327                                     \n",
      "epoch 40 [1.51s]: training loss=0.5824356079101562  validation ndcg@10=0.0636306481087901 [0.04s]\n",
      "epoch 41 [1.62s]:  training loss=0.5758266448974609                                     \n",
      "epoch 42 [1.5s]:  training loss=0.5711977481842041                                      \n",
      "epoch 43 [1.58s]:  training loss=0.5692068338394165                                     \n",
      "epoch 44 [1.51s]:  training loss=0.5689001083374023                                     \n",
      "epoch 45 [1.6s]: training loss=0.5608136653900146  validation ndcg@10=0.06359612231874814 [0.04s]\n",
      "epoch 1 [1.46s]:  training loss=0.9791548848152161                                      \n",
      "epoch 2 [1.46s]:  training loss=0.9658775925636292                                      \n",
      "epoch 3 [1.56s]:  training loss=0.9458665251731873                                      \n",
      "epoch 4 [1.42s]:  training loss=0.925654411315918                                       \n",
      "epoch 5 [1.51s]: training loss=0.9013872146606445  validation ndcg@10=0.05301522610952882 [0.04s]\n",
      "epoch 6 [1.45s]:  training loss=0.8747827410697937                                      \n",
      "epoch 7 [1.42s]:  training loss=0.8400050401687622                                      \n",
      "epoch 8 [1.55s]:  training loss=0.8025344610214233                                      \n",
      "epoch 9 [1.42s]:  training loss=0.7682936787605286                                      \n",
      "epoch 10 [1.52s]: training loss=0.7433732748031616  validation ndcg@10=0.06328308047079885 [0.04s]\n",
      "epoch 11 [1.57s]:  training loss=0.7301110029220581                                     \n",
      "epoch 12 [1.53s]:  training loss=0.715317964553833                                      \n",
      "epoch 13 [1.63s]:  training loss=0.7006269097328186                                     \n",
      "epoch 14 [1.48s]:  training loss=0.6916657090187073                                     \n",
      "epoch 15 [1.52s]: training loss=0.6819581985473633  validation ndcg@10=0.06331089279058577 [0.04s]\n",
      "epoch 16 [1.54s]:  training loss=0.6761153936386108                                     \n",
      "epoch 17 [1.51s]:  training loss=0.6634090542793274                                     \n",
      "epoch 18 [1.58s]:  training loss=0.6594377756118774                                     \n",
      "epoch 19 [1.52s]:  training loss=0.6533832550048828                                     \n",
      "epoch 20 [1.49s]: training loss=0.6463792324066162  validation ndcg@10=0.06546181552048519 [0.03s]\n",
      "epoch 21 [1.6s]:  training loss=0.6342557668685913                                      \n",
      "epoch 22 [1.44s]:  training loss=0.6263603568077087                                     \n",
      "epoch 23 [1.5s]:  training loss=0.6281992793083191                                      \n",
      "epoch 24 [1.54s]:  training loss=0.6196494698524475                                     \n",
      "epoch 25 [1.51s]: training loss=0.6131423115730286  validation ndcg@10=0.0686673031338785 [0.04s]\n",
      "epoch 26 [1.58s]:  training loss=0.6020721793174744                                     \n",
      "epoch 27 [1.42s]:  training loss=0.6003376841545105                                     \n",
      "epoch 28 [1.55s]:  training loss=0.5972198247909546                                     \n",
      "epoch 29 [1.45s]:  training loss=0.5924769043922424                                     \n",
      "epoch 30 [1.4s]: training loss=0.5851792097091675  validation ndcg@10=0.07039413877323555 [0.03s]\n",
      "epoch 31 [1.55s]:  training loss=0.5796896815299988                                     \n",
      "epoch 32 [1.44s]:  training loss=0.5731669664382935                                     \n",
      "epoch 33 [1.51s]:  training loss=0.5694477558135986                                     \n",
      "epoch 34 [1.41s]:  training loss=0.5673342347145081                                     \n",
      "epoch 35 [1.42s]: training loss=0.5626608729362488  validation ndcg@10=0.06989182114462092 [0.03s]\n",
      "epoch 36 [1.54s]:  training loss=0.5628435015678406                                     \n",
      "epoch 37 [1.44s]:  training loss=0.5557435750961304                                     \n",
      "epoch 38 [1.49s]:  training loss=0.5507054924964905                                     \n",
      "epoch 39 [1.45s]:  training loss=0.5488940477371216                                     \n",
      "epoch 40 [1.42s]: training loss=0.5437847375869751  validation ndcg@10=0.07014409657060452 [0.03s]\n",
      "epoch 41 [1.56s]:  training loss=0.5439354777336121                                     \n",
      "epoch 42 [1.42s]:  training loss=0.53536057472229                                       \n",
      "epoch 43 [1.44s]:  training loss=0.5363555550575256                                     \n",
      "epoch 44 [1.53s]:  training loss=0.5351900458335876                                     \n",
      "epoch 45 [1.51s]: training loss=0.5344781279563904  validation ndcg@10=0.07054886570922284 [0.03s]\n",
      "epoch 46 [1.53s]:  training loss=0.5295447707176208                                     \n",
      "epoch 47 [1.46s]:  training loss=0.5272034406661987                                     \n",
      "epoch 48 [1.51s]:  training loss=0.521571934223175                                      \n",
      "epoch 49 [1.47s]:  training loss=0.523241400718689                                      \n",
      "epoch 50 [1.41s]: training loss=0.521989643573761  validation ndcg@10=0.06981362540660313 [0.03s]\n",
      "epoch 51 [1.5s]:  training loss=0.5196002721786499                                      \n",
      "epoch 52 [1.4s]:  training loss=0.5147236585617065                                      \n",
      "epoch 53 [1.48s]:  training loss=0.5096139311790466                                     \n",
      "epoch 54 [1.48s]:  training loss=0.5175511240959167                                     \n",
      "epoch 55 [1.42s]: training loss=0.5076707005500793  validation ndcg@10=0.07076199682349817 [0.03s]\n",
      "epoch 56 [1.49s]:  training loss=0.5107040405273438                                     \n",
      "epoch 57 [1.4s]:  training loss=0.5122145414352417                                      \n",
      "epoch 58 [1.55s]:  training loss=0.5071104168891907                                     \n",
      "epoch 59 [1.42s]:  training loss=0.5056590437889099                                     \n",
      "epoch 60 [1.5s]: training loss=0.5046002864837646  validation ndcg@10=0.06907241652004494 [0.03s]\n",
      "epoch 61 [1.41s]:  training loss=0.5009406208992004                                     \n",
      "epoch 62 [1.46s]:  training loss=0.49243998527526855                                    \n",
      "epoch 63 [1.49s]:  training loss=0.49920734763145447                                    \n",
      "epoch 64 [1.44s]:  training loss=0.49528029561042786                                    \n",
      "epoch 65 [1.51s]: training loss=0.4932439625263214  validation ndcg@10=0.06866678441173993 [0.03s]\n",
      "epoch 66 [1.49s]:  training loss=0.4897804260253906                                     \n",
      "epoch 67 [1.92s]:  training loss=0.4856095314025879                                     \n",
      "epoch 68 [3.21s]:  training loss=0.4914691746234894                                     \n",
      "epoch 69 [1.45s]:  training loss=0.48504897952079773                                    \n",
      "epoch 70 [1.55s]: training loss=0.4801473915576935  validation ndcg@10=0.06432444195576179 [0.04s]\n",
      "epoch 71 [1.55s]:  training loss=0.4819256067276001                                     \n",
      "epoch 72 [1.55s]:  training loss=0.47827988862991333                                    \n",
      "epoch 73 [1.41s]:  training loss=0.47914138436317444                                    \n",
      "epoch 74 [1.48s]:  training loss=0.47741612792015076                                    \n",
      "epoch 75 [1.52s]: training loss=0.47135066986083984  validation ndcg@10=0.06636140770341009 [0.03s]\n",
      "epoch 76 [1.41s]:  training loss=0.4760189354419708                                     \n",
      "epoch 77 [1.58s]:  training loss=0.4695076048374176                                     \n",
      "epoch 78 [1.4s]:  training loss=0.4665790796279907                                      \n",
      "epoch 79 [1.42s]:  training loss=0.4692530333995819                                     \n",
      "epoch 80 [1.48s]: training loss=0.4615521728992462  validation ndcg@10=0.0641460981891318 [0.03s]\n",
      "epoch 1 [2.72s]:  training loss=0.963151752948761                                       \n",
      "epoch 2 [2.65s]:  training loss=0.9150211811065674                                      \n",
      "epoch 3 [2.73s]:  training loss=0.8460745811462402                                      \n",
      "epoch 4 [2.71s]:  training loss=0.7548303008079529                                      \n",
      "epoch 5 [2.63s]: training loss=0.7090104818344116  validation ndcg@10=0.0688220367192546 [0.04s]\n",
      "epoch 6 [2.69s]:  training loss=0.6803847551345825                                      \n",
      "epoch 7 [2.79s]:  training loss=0.6578162312507629                                      \n",
      "epoch 8 [2.68s]:  training loss=0.639754593372345                                       \n",
      "epoch 9 [2.64s]:  training loss=0.6216795444488525                                      \n",
      "epoch 10 [2.74s]: training loss=0.6075848340988159  validation ndcg@10=0.07393408944213516 [0.05s]\n",
      "epoch 11 [2.76s]:  training loss=0.5914625525474548                                     \n",
      "epoch 12 [2.73s]:  training loss=0.579107403755188                                      \n",
      "epoch 13 [2.64s]:  training loss=0.5675346851348877                                     \n",
      "epoch 14 [2.73s]:  training loss=0.5580521821975708                                     \n",
      "epoch 15 [2.74s]: training loss=0.5447763800621033  validation ndcg@10=0.06884998195748447 [0.04s]\n",
      "epoch 16 [2.64s]:  training loss=0.5438987612724304                                     \n",
      "epoch 17 [2.97s]:  training loss=0.5257145762443542                                     \n",
      "epoch 18 [3.0s]:  training loss=0.5254325270652771                                      \n",
      "epoch 19 [2.83s]:  training loss=0.5172989964485168                                     \n",
      "epoch 20 [2.66s]: training loss=0.5071372985839844  validation ndcg@10=0.06881969730142964 [0.04s]\n",
      "epoch 21 [2.67s]:  training loss=0.5042300820350647                                     \n",
      "epoch 22 [2.7s]:  training loss=0.49739134311676025                                     \n",
      "epoch 23 [2.8s]:  training loss=0.48158589005470276                                     \n",
      "epoch 24 [2.67s]:  training loss=0.4785538613796234                                     \n",
      "epoch 25 [2.66s]: training loss=0.474680632352829  validation ndcg@10=0.0708045224496046 [0.05s]\n",
      "epoch 26 [2.66s]:  training loss=0.4698337912559509                                     \n",
      "epoch 27 [2.69s]:  training loss=0.4717690944671631                                     \n",
      "epoch 28 [2.77s]:  training loss=0.4629378318786621                                     \n",
      "epoch 29 [2.61s]:  training loss=0.4584892690181732                                     \n",
      "epoch 30 [2.71s]: training loss=0.45517486333847046  validation ndcg@10=0.06533889733665427 [0.04s]\n",
      "epoch 31 [2.69s]:  training loss=0.46097075939178467                                    \n",
      "epoch 32 [2.69s]:  training loss=0.4471229612827301                                     \n",
      "epoch 33 [2.66s]:  training loss=0.44277888536453247                                    \n",
      "epoch 34 [2.65s]:  training loss=0.4437806308269501                                     \n",
      "epoch 35 [2.71s]: training loss=0.4474623501300812  validation ndcg@10=0.0665725251133637 [0.04s]\n",
      "epoch 1 [2.05s]:  training loss=0.6405889391899109                                      \n",
      "epoch 2 [1.95s]:  training loss=0.6410890817642212                                      \n",
      "epoch 3 [2.0s]:  training loss=0.6597173810005188                                       \n",
      "epoch 4 [1.94s]:  training loss=0.654156506061554                                       \n",
      "epoch 5 [2.0s]: training loss=0.6578288078308105  validation ndcg@10=0.008370126013428387 [0.04s]\n",
      "epoch 6 [1.98s]:  training loss=0.6542940139770508                                      \n",
      "epoch 7 [2.1s]:  training loss=0.6594551205635071                                       \n",
      "epoch 8 [2.04s]:  training loss=0.6645731329917908                                      \n",
      "epoch 9 [1.92s]:  training loss=0.6512601971626282                                      \n",
      "epoch 10 [2.03s]: training loss=0.6378385424613953  validation ndcg@10=0.006214814079533187 [0.04s]\n",
      "epoch 11 [2.01s]:  training loss=0.6621840596199036                                     \n",
      "epoch 12 [2.08s]:  training loss=0.6556149125099182                                     \n",
      "epoch 13 [2.02s]:  training loss=0.6530934572219849                                     \n",
      "epoch 14 [2.07s]:  training loss=0.6673711538314819                                     \n",
      "epoch 15 [2.04s]: training loss=0.6631225943565369  validation ndcg@10=0.0017640547857302523 [0.04s]\n",
      "epoch 16 [1.94s]:  training loss=0.6663932204246521                                     \n",
      "epoch 17 [2.15s]:  training loss=0.6749373078346252                                     \n",
      "epoch 18 [1.96s]:  training loss=0.6684610247612                                        \n",
      "epoch 19 [2.07s]:  training loss=0.6638978123664856                                     \n",
      "epoch 20 [1.93s]: training loss=0.6693897843360901  validation ndcg@10=0.0029200104487481127 [0.04s]\n",
      "epoch 21 [2.09s]:  training loss=0.6654381155967712                                     \n",
      "epoch 22 [1.93s]:  training loss=0.6593133807182312                                     \n",
      "epoch 23 [2.03s]:  training loss=0.6728384494781494                                     \n",
      "epoch 24 [1.96s]:  training loss=0.6662880778312683                                     \n",
      "epoch 25 [2.14s]: training loss=0.656923234462738  validation ndcg@10=0.0013420229652026593 [0.05s]\n",
      "epoch 26 [1.97s]:  training loss=0.677432656288147                                      \n",
      "epoch 27 [2.22s]:  training loss=0.6613839268684387                                     \n",
      "epoch 28 [2.15s]:  training loss=0.6519138216972351                                     \n",
      "epoch 29 [2.13s]:  training loss=0.6791770458221436                                     \n",
      "epoch 30 [2.2s]: training loss=0.6750589609146118  validation ndcg@10=0.002852524676549337 [0.08s]\n",
      "epoch 1 [11.93s]:  training loss=0.9820891618728638                                     \n",
      "epoch 2 [11.72s]:  training loss=0.964820921421051                                    \n",
      "epoch 3 [11.34s]:  training loss=0.9477729797363281                                   \n",
      "epoch 4 [11.55s]:  training loss=0.9293874502182007                                   \n",
      "epoch 5 [11.48s]: training loss=0.908418595790863  validation ndcg@10=0.05782221041835077 [0.21s]\n",
      "epoch 6 [11.43s]:  training loss=0.8844171762466431                                   \n",
      "epoch 7 [11.92s]:  training loss=0.8606985211372375                                   \n",
      "epoch 8 [11.72s]:  training loss=0.8335803747177124                                   \n",
      "epoch 9 [11.47s]:  training loss=0.807695209980011                                    \n",
      "epoch 10 [11.46s]: training loss=0.7761857509613037  validation ndcg@10=0.06450006202397453 [0.19s]\n",
      "epoch 11 [11.46s]:  training loss=0.7396886348724365                                  \n",
      "epoch 12 [11.58s]:  training loss=0.7120088934898376                                  \n",
      "epoch 13 [11.44s]:  training loss=0.6863884925842285                                  \n",
      "epoch 14 [11.53s]:  training loss=0.6645276546478271                                  \n",
      "epoch 15 [11.42s]: training loss=0.6492452025413513  validation ndcg@10=0.06971716719802061 [0.27s]\n",
      "epoch 16 [11.38s]:  training loss=0.6312927007675171                                  \n",
      "epoch 17 [11.41s]:  training loss=0.6266977787017822                                  \n",
      "epoch 18 [11.53s]:  training loss=0.6130257844924927                                  \n",
      "epoch 19 [11.55s]:  training loss=0.6021982431411743                                  \n",
      "epoch 20 [11.53s]: training loss=0.6010120511054993  validation ndcg@10=0.06925285873781918 [0.18s]\n",
      "epoch 21 [11.43s]:  training loss=0.5862682461738586                                  \n",
      "epoch 22 [11.6s]:  training loss=0.582455575466156                                    \n",
      "epoch 23 [11.48s]:  training loss=0.5767204761505127                                  \n",
      "epoch 24 [11.52s]:  training loss=0.567432165145874                                   \n",
      "epoch 25 [14.13s]: training loss=0.5682017207145691  validation ndcg@10=0.06962600274236443 [0.21s]\n",
      "epoch 26 [11.16s]:  training loss=0.5588284730911255                                  \n",
      "epoch 27 [11.39s]:  training loss=0.5576790571212769                                  \n",
      "epoch 28 [12.48s]:  training loss=0.5471928119659424                                  \n",
      "epoch 29 [12.13s]:  training loss=0.5418050289154053                                  \n",
      "epoch 30 [12.16s]: training loss=0.5410625338554382  validation ndcg@10=0.07095638180386692 [0.18s]\n",
      "epoch 31 [12.22s]:  training loss=0.5380557775497437                                  \n",
      "epoch 32 [12.13s]:  training loss=0.5363464951515198                                  \n",
      "epoch 33 [12.21s]:  training loss=0.5322997570037842                                  \n",
      "epoch 34 [12.39s]:  training loss=0.5302923917770386                                  \n",
      "epoch 35 [12.08s]: training loss=0.5225582122802734  validation ndcg@10=0.07218208506071579 [0.26s]\n",
      "epoch 36 [11.88s]:  training loss=0.5261378884315491                                  \n",
      "epoch 37 [12.25s]:  training loss=0.5222474932670593                                  \n",
      "epoch 38 [12.41s]:  training loss=0.515251636505127                                   \n",
      "epoch 39 [12.08s]:  training loss=0.5187984704971313                                  \n",
      "epoch 40 [11.83s]: training loss=0.5170948505401611  validation ndcg@10=0.07340725113571861 [0.18s]\n",
      "epoch 41 [11.86s]:  training loss=0.5156592130661011                                  \n",
      "epoch 42 [11.73s]:  training loss=0.5112444162368774                                  \n",
      "epoch 43 [11.79s]:  training loss=0.5125358700752258                                  \n",
      "epoch 44 [11.85s]:  training loss=0.5067318677902222                                  \n",
      "epoch 45 [11.85s]: training loss=0.5090088844299316  validation ndcg@10=0.07237375364465493 [0.21s]\n",
      "epoch 46 [11.92s]:  training loss=0.5124720335006714                                  \n",
      "epoch 47 [11.89s]:  training loss=0.5058872699737549                                  \n",
      "epoch 48 [11.88s]:  training loss=0.4986926317214966                                  \n",
      "epoch 49 [11.77s]:  training loss=0.5050098299980164                                  \n",
      "epoch 50 [11.79s]: training loss=0.49887901544570923  validation ndcg@10=0.07135880769228953 [0.21s]\n",
      "epoch 51 [11.85s]:  training loss=0.49999934434890747                                 \n",
      "epoch 52 [11.8s]:  training loss=0.49767863750457764                                  \n",
      "epoch 53 [11.78s]:  training loss=0.49162471294403076                                 \n",
      "epoch 54 [11.87s]:  training loss=0.4915270507335663                                  \n",
      "epoch 55 [11.89s]: training loss=0.4963807165622711  validation ndcg@10=0.07267260598078468 [0.19s]\n",
      "epoch 56 [11.86s]:  training loss=0.48287802934646606                                 \n",
      "epoch 57 [11.81s]:  training loss=0.4869164228439331                                  \n",
      "epoch 58 [11.81s]:  training loss=0.4884171783924103                                  \n",
      "epoch 59 [11.8s]:  training loss=0.4799029529094696                                   \n",
      "epoch 60 [11.84s]: training loss=0.48130378127098083  validation ndcg@10=0.0704223196770013 [0.22s]\n",
      "epoch 61 [11.66s]:  training loss=0.47594115138053894                                 \n",
      "epoch 62 [11.83s]:  training loss=0.47634080052375793                                 \n",
      "epoch 63 [11.76s]:  training loss=0.46574077010154724                                 \n",
      "epoch 64 [11.75s]:  training loss=0.46751299500465393                                 \n",
      "epoch 65 [11.81s]: training loss=0.46967989206314087  validation ndcg@10=0.07174667994060475 [0.19s]\n",
      "epoch 1 [1.81s]:  training loss=0.9308074712753296                                      \n",
      "epoch 2 [1.88s]:  training loss=0.8146805763244629                                      \n",
      "epoch 3 [1.83s]:  training loss=0.6867326498031616                                      \n",
      "epoch 4 [1.84s]:  training loss=0.6042457818984985                                      \n",
      "epoch 5 [1.8s]: training loss=0.5582979917526245  validation ndcg@10=0.062129066420027225 [0.05s]\n",
      "epoch 6 [1.87s]:  training loss=0.5339916348457336                                      \n",
      "epoch 7 [1.82s]:  training loss=0.5256292819976807                                      \n",
      "epoch 8 [1.82s]:  training loss=0.505405604839325                                       \n",
      "epoch 9 [1.85s]:  training loss=0.5069001317024231                                      \n",
      "epoch 10 [1.82s]: training loss=0.49620747566223145  validation ndcg@10=0.06683340977403326 [0.05s]\n",
      "epoch 11 [1.89s]:  training loss=0.49599888920783997                                    \n",
      "epoch 12 [1.81s]:  training loss=0.49605393409729004                                    \n",
      "epoch 13 [1.88s]:  training loss=0.49296101927757263                                    \n",
      "epoch 14 [1.86s]:  training loss=0.4831427037715912                                     \n",
      "epoch 15 [1.87s]: training loss=0.4903232753276825  validation ndcg@10=0.07371212572038169 [0.05s]\n",
      "epoch 16 [1.91s]:  training loss=0.481934517621994                                      \n",
      "epoch 17 [1.94s]:  training loss=0.4852023720741272                                     \n",
      "epoch 18 [1.88s]:  training loss=0.4843032956123352                                     \n",
      "epoch 19 [1.82s]:  training loss=0.47434931993484497                                    \n",
      "epoch 20 [1.85s]: training loss=0.4811502695083618  validation ndcg@10=0.07177082993374333 [0.05s]\n",
      "epoch 21 [1.84s]:  training loss=0.4795607626438141                                     \n",
      "epoch 22 [1.91s]:  training loss=0.47114917635917664                                    \n",
      "epoch 23 [1.87s]:  training loss=0.4703766405582428                                     \n",
      "epoch 24 [1.85s]:  training loss=0.4708361327648163                                     \n",
      "epoch 25 [1.96s]: training loss=0.4670572280883789  validation ndcg@10=0.06853928660524994 [0.05s]\n",
      "epoch 26 [1.93s]:  training loss=0.45209941267967224                                    \n",
      "epoch 27 [1.92s]:  training loss=0.452292799949646                                      \n",
      "epoch 28 [1.93s]:  training loss=0.44541385769844055                                    \n",
      "epoch 29 [1.98s]:  training loss=0.4373790919780731                                     \n",
      "epoch 30 [2.02s]: training loss=0.4344128370285034  validation ndcg@10=0.07502456775922751 [0.05s]\n",
      "epoch 31 [1.94s]:  training loss=0.4363751709461212                                     \n",
      "epoch 32 [2.01s]:  training loss=0.42868050932884216                                    \n",
      "epoch 33 [2.0s]:  training loss=0.41969504952430725                                     \n",
      "epoch 34 [1.97s]:  training loss=0.41657212376594543                                    \n",
      "epoch 35 [1.9s]: training loss=0.4134805500507355  validation ndcg@10=0.0632213952625177 [0.06s]\n",
      "epoch 36 [1.88s]:  training loss=0.4170445203781128                                     \n",
      "epoch 37 [1.86s]:  training loss=0.40428096055984497                                    \n",
      "epoch 38 [1.85s]:  training loss=0.3942154347896576                                     \n",
      "epoch 39 [1.88s]:  training loss=0.39219897985458374                                    \n",
      "epoch 40 [1.8s]: training loss=0.39008936285972595  validation ndcg@10=0.06317724126450301 [0.06s]\n",
      "epoch 41 [1.83s]:  training loss=0.38093844056129456                                    \n",
      "epoch 42 [1.83s]:  training loss=0.38118892908096313                                    \n",
      "epoch 43 [1.84s]:  training loss=0.3715391159057617                                     \n",
      "epoch 44 [1.86s]:  training loss=0.37159326672554016                                    \n",
      "epoch 45 [1.8s]: training loss=0.3660195469856262  validation ndcg@10=0.06661502087753157 [0.06s]\n",
      "epoch 46 [1.83s]:  training loss=0.3727322816848755                                     \n",
      "epoch 47 [1.82s]:  training loss=0.3623814284801483                                     \n",
      "epoch 48 [1.83s]:  training loss=0.3628885746002197                                     \n",
      "epoch 49 [1.93s]:  training loss=0.35797321796417236                                    \n",
      "epoch 50 [1.83s]: training loss=0.35366296768188477  validation ndcg@10=0.06286897101809782 [0.05s]\n",
      "epoch 51 [1.8s]:  training loss=0.36153173446655273                                     \n",
      "epoch 52 [1.83s]:  training loss=0.35260650515556335                                    \n",
      "epoch 53 [1.85s]:  training loss=0.35370731353759766                                    \n",
      "epoch 54 [1.89s]:  training loss=0.3479144871234894                                     \n",
      "epoch 55 [1.83s]: training loss=0.34881848096847534  validation ndcg@10=0.05817026664595472 [0.05s]\n",
      "epoch 1 [2.56s]:  training loss=0.9795143604278564                                      \n",
      "epoch 2 [3.73s]:  training loss=0.9659228324890137                                      \n",
      "epoch 3 [2.15s]:  training loss=0.9497128129005432                                      \n",
      "epoch 4 [2.14s]:  training loss=0.9326226711273193                                      \n",
      "epoch 5 [2.13s]: training loss=0.9151650071144104  validation ndcg@10=0.06138595072127726 [0.05s]\n",
      "epoch 6 [2.23s]:  training loss=0.8895355463027954                                      \n",
      "epoch 7 [2.11s]:  training loss=0.8668929934501648                                      \n",
      "epoch 8 [2.25s]:  training loss=0.8381797075271606                                      \n",
      "epoch 9 [2.11s]:  training loss=0.8030206561088562                                      \n",
      "epoch 10 [2.06s]: training loss=0.7698729038238525  validation ndcg@10=0.07328014094163085 [0.04s]\n",
      "epoch 11 [2.08s]:  training loss=0.749233603477478                                      \n",
      "epoch 12 [2.09s]:  training loss=0.7248827815055847                                     \n",
      "epoch 13 [2.1s]:  training loss=0.7115716338157654                                      \n",
      "epoch 14 [2.15s]:  training loss=0.6995396614074707                                     \n",
      "epoch 15 [2.09s]: training loss=0.6906444430351257  validation ndcg@10=0.07299263295336064 [0.04s]\n",
      "epoch 16 [2.1s]:  training loss=0.6773061156272888                                      \n",
      "epoch 17 [2.13s]:  training loss=0.6750162243843079                                     \n",
      "epoch 18 [2.07s]:  training loss=0.6640897393226624                                     \n",
      "epoch 19 [2.06s]:  training loss=0.6547800302505493                                     \n",
      "epoch 20 [2.12s]: training loss=0.6511338353157043  validation ndcg@10=0.07274787378128994 [0.04s]\n",
      "epoch 21 [2.08s]:  training loss=0.6415845155715942                                     \n",
      "epoch 22 [2.1s]:  training loss=0.6374080181121826                                      \n",
      "epoch 23 [2.1s]:  training loss=0.6276413798332214                                      \n",
      "epoch 24 [2.09s]:  training loss=0.6262759566307068                                     \n",
      "epoch 25 [2.08s]: training loss=0.6179161667823792  validation ndcg@10=0.07376140781856841 [0.04s]\n",
      "epoch 26 [2.12s]:  training loss=0.617990255355835                                      \n",
      "epoch 27 [2.11s]:  training loss=0.6048480272293091                                     \n",
      "epoch 28 [2.05s]:  training loss=0.600186288356781                                      \n",
      "epoch 29 [2.07s]:  training loss=0.5979673266410828                                     \n",
      "epoch 30 [2.06s]: training loss=0.5925474166870117  validation ndcg@10=0.07478234093927974 [0.04s]\n",
      "epoch 31 [2.08s]:  training loss=0.5864404439926147                                     \n",
      "epoch 32 [2.08s]:  training loss=0.5851567387580872                                     \n",
      "epoch 33 [2.06s]:  training loss=0.5765393972396851                                     \n",
      "epoch 34 [2.04s]:  training loss=0.5728990435600281                                     \n",
      "epoch 35 [2.17s]: training loss=0.569990336894989  validation ndcg@10=0.07321742165355596 [0.04s]\n",
      "epoch 36 [2.07s]:  training loss=0.5625332593917847                                     \n",
      "epoch 37 [2.11s]:  training loss=0.5552276372909546                                     \n",
      "epoch 38 [2.15s]:  training loss=0.5581308007240295                                     \n",
      "epoch 39 [2.05s]:  training loss=0.5551614165306091                                     \n",
      "epoch 40 [2.1s]: training loss=0.5490421056747437  validation ndcg@10=0.07181836053682362 [0.04s]\n",
      "epoch 41 [2.14s]:  training loss=0.5531179904937744                                     \n",
      "epoch 42 [2.08s]:  training loss=0.5436248779296875                                     \n",
      "epoch 43 [2.11s]:  training loss=0.5427687168121338                                     \n",
      "epoch 44 [2.1s]:  training loss=0.5407307147979736                                      \n",
      "epoch 45 [2.11s]: training loss=0.5367792248725891  validation ndcg@10=0.07098594079946863 [0.04s]\n",
      "epoch 46 [2.09s]:  training loss=0.5316181182861328                                     \n",
      "epoch 47 [2.08s]:  training loss=0.5286773443222046                                     \n",
      "epoch 48 [2.06s]:  training loss=0.5266783833503723                                     \n",
      "epoch 49 [2.12s]:  training loss=0.5237249135971069                                     \n",
      "epoch 50 [2.1s]: training loss=0.5235055685043335  validation ndcg@10=0.07157769161595505 [0.04s]\n",
      "epoch 51 [2.1s]:  training loss=0.5202198028564453                                      \n",
      "epoch 52 [2.09s]:  training loss=0.5216372013092041                                     \n",
      "epoch 53 [2.07s]:  training loss=0.5150853991508484                                     \n",
      "epoch 54 [2.1s]:  training loss=0.5139864683151245                                      \n",
      "epoch 55 [2.04s]: training loss=0.499693363904953  validation ndcg@10=0.07169867907626958 [0.04s]\n",
      "epoch 1 [17.33s]:  training loss=0.7142778635025024                                     \n",
      "epoch 2 [18.11s]:  training loss=0.8683329224586487                                     \n",
      "epoch 3 [19.52s]:  training loss=0.9420533180236816                                     \n",
      "epoch 4 [20.62s]:  training loss=0.9706607460975647                                     \n",
      "epoch 5 [19.02s]: training loss=1.0114845037460327  validation ndcg@10=0.021239650536906504 [0.34s]\n",
      "epoch 6 [20.43s]:  training loss=1.0497671365737915                                     \n",
      "epoch 7 [20.14s]:  training loss=1.0599439144134521                                     \n",
      "epoch 8 [20.17s]:  training loss=1.0774706602096558                                     \n",
      "epoch 9 [20.42s]:  training loss=1.093413233757019                                      \n",
      "epoch 10 [20.54s]: training loss=1.0756052732467651  validation ndcg@10=0.012112317582920034 [0.3s]\n",
      "epoch 11 [19.05s]:  training loss=1.0752184391021729                                    \n",
      "epoch 12 [20.22s]:  training loss=1.1005977392196655                                    \n",
      "epoch 13 [20.59s]:  training loss=1.067989706993103                                     \n",
      "epoch 14 [19.88s]:  training loss=1.109387755393982                                     \n",
      "epoch 15 [19.47s]: training loss=1.0927656888961792  validation ndcg@10=0.013369757962001542 [0.31s]\n",
      "epoch 16 [20.88s]:  training loss=1.124366283416748                                     \n",
      "epoch 17 [20.71s]:  training loss=1.0978370904922485                                    \n",
      "epoch 18 [20.07s]:  training loss=1.1036814451217651                                    \n",
      "epoch 19 [19.66s]:  training loss=1.0887120962142944                                    \n",
      "epoch 20 [20.55s]: training loss=1.1137700080871582  validation ndcg@10=0.006931091740075489 [0.33s]\n",
      "epoch 21 [20.85s]:  training loss=1.1283481121063232                                    \n",
      "epoch 22 [20.87s]:  training loss=1.1379069089889526                                    \n",
      "epoch 23 [20.24s]:  training loss=1.1097805500030518                                    \n",
      "epoch 24 [21.63s]:  training loss=1.131795883178711                                     \n",
      "epoch 25 [19.77s]: training loss=1.143654465675354  validation ndcg@10=0.003507638411070693 [0.34s]\n",
      "epoch 26 [22.22s]:  training loss=1.1056830883026123                                    \n",
      "epoch 27 [19.42s]:  training loss=1.0878428220748901                                    \n",
      "epoch 28 [20.23s]:  training loss=1.0972785949707031                                    \n",
      "epoch 29 [19.9s]:  training loss=1.1397078037261963                                     \n",
      "epoch 30 [19.14s]: training loss=1.1379400491714478  validation ndcg@10=0.001693444437445101 [0.29s]\n",
      "epoch 1 [15.72s]:  training loss=0.8188425898551941                                     \n",
      "epoch 2 [14.07s]:  training loss=0.612108588218689                                      \n",
      "epoch 3 [13.05s]:  training loss=0.549470067024231                                      \n",
      "epoch 4 [12.98s]:  training loss=0.5126922726631165                                     \n",
      "epoch 5 [12.95s]: training loss=0.481891006231308  validation ndcg@10=0.0688287246755805 [0.18s]\n",
      "epoch 6 [12.97s]:  training loss=0.45992547273635864                                    \n",
      "epoch 7 [13.12s]:  training loss=0.44490763545036316                                    \n",
      "epoch 8 [13.07s]:  training loss=0.4213128089904785                                     \n",
      "epoch 9 [13.33s]:  training loss=0.3964926600456238                                     \n",
      "epoch 10 [13.12s]: training loss=0.3961634337902069  validation ndcg@10=0.061628184169223256 [0.14s]\n",
      "epoch 11 [13.0s]:  training loss=0.3796146810054779                                     \n",
      "epoch 12 [13.2s]:  training loss=0.37576285004615784                                    \n",
      "epoch 13 [13.08s]:  training loss=0.36999762058258057                                   \n",
      "epoch 14 [13.04s]:  training loss=0.35856643319129944                                   \n",
      "epoch 15 [13.01s]: training loss=0.35433053970336914  validation ndcg@10=0.044218791527623896 [0.17s]\n",
      "epoch 16 [12.98s]:  training loss=0.3574185371398926                                    \n",
      "epoch 17 [13.15s]:  training loss=0.3514731824398041                                    \n",
      "epoch 18 [13.09s]:  training loss=0.3415515124797821                                    \n",
      "epoch 19 [13.15s]:  training loss=0.34464821219444275                                   \n",
      "epoch 20 [13.09s]: training loss=0.33390146493911743  validation ndcg@10=0.06066263661640256 [0.19s]\n",
      "epoch 21 [13.14s]:  training loss=0.34420010447502136                                   \n",
      "epoch 22 [13.2s]:  training loss=0.335008829832077                                      \n",
      "epoch 23 [13.15s]:  training loss=0.33355358242988586                                   \n",
      "epoch 24 [13.16s]:  training loss=0.3321407735347748                                    \n",
      "epoch 25 [13.05s]: training loss=0.3307119309902191  validation ndcg@10=0.06144716811903792 [0.14s]\n",
      "epoch 26 [13.12s]:  training loss=0.32196250557899475                                   \n",
      "epoch 27 [13.33s]:  training loss=0.3271467685699463                                    \n",
      "epoch 28 [13.23s]:  training loss=0.3219592273235321                                    \n",
      "epoch 29 [13.19s]:  training loss=0.3176810145378113                                    \n",
      "epoch 30 [13.49s]: training loss=0.31426316499710083  validation ndcg@10=0.06468566995573852 [0.15s]\n",
      "epoch 1 [5.37s]:  training loss=0.8262310028076172                                      \n",
      "epoch 2 [5.45s]:  training loss=0.5831989049911499                                      \n",
      "epoch 3 [5.34s]:  training loss=0.5279269814491272                                      \n",
      "epoch 4 [5.32s]:  training loss=0.5088907480239868                                      \n",
      "epoch 5 [5.4s]: training loss=0.471032053232193  validation ndcg@10=0.06632092243125857 [0.08s]\n",
      "epoch 6 [5.57s]:  training loss=0.4422709345817566                                      \n",
      "epoch 7 [5.3s]:  training loss=0.42761847376823425                                      \n",
      "epoch 8 [5.44s]:  training loss=0.41314512491226196                                     \n",
      "epoch 9 [5.36s]:  training loss=0.39009812474250793                                     \n",
      "epoch 10 [5.36s]: training loss=0.3814481794834137  validation ndcg@10=0.06500512279437992 [0.11s]\n",
      "epoch 11 [5.4s]:  training loss=0.37192636728286743                                     \n",
      "epoch 12 [5.42s]:  training loss=0.3722241520881653                                     \n",
      "epoch 13 [5.4s]:  training loss=0.36076927185058594                                     \n",
      "epoch 14 [5.36s]:  training loss=0.3461821675300598                                     \n",
      "epoch 15 [5.31s]: training loss=0.34561383724212646  validation ndcg@10=0.060572193280334974 [0.09s]\n",
      "epoch 16 [5.29s]:  training loss=0.33664265275001526                                    \n",
      "epoch 17 [5.32s]:  training loss=0.334507018327713                                      \n",
      "epoch 18 [5.41s]:  training loss=0.33976852893829346                                    \n",
      "epoch 19 [5.52s]:  training loss=0.3291269540786743                                     \n",
      "epoch 20 [5.58s]: training loss=0.3296707272529602  validation ndcg@10=0.054021681204465245 [0.09s]\n",
      "epoch 21 [5.55s]:  training loss=0.3273932933807373                                     \n",
      "epoch 22 [5.5s]:  training loss=0.32216712832450867                                     \n",
      "epoch 23 [5.52s]:  training loss=0.323871374130249                                      \n",
      "epoch 24 [5.5s]:  training loss=0.3211577832698822                                      \n",
      "epoch 25 [5.45s]: training loss=0.3212403953075409  validation ndcg@10=0.06545775212460331 [0.1s]\n",
      "epoch 26 [7.09s]:  training loss=0.31287893652915955                                    \n",
      "epoch 27 [5.4s]:  training loss=0.31382280588150024                                     \n",
      "epoch 28 [5.56s]:  training loss=0.31337031722068787                                    \n",
      "epoch 29 [5.55s]:  training loss=0.3195214569568634                                     \n",
      "epoch 30 [5.31s]: training loss=0.30898839235305786  validation ndcg@10=0.05910402699067712 [0.08s]\n",
      "epoch 1 [8.57s]:  training loss=0.7556203007698059                                      \n",
      "epoch 2 [9.06s]:  training loss=0.5422543883323669                                      \n",
      "epoch 3 [9.16s]:  training loss=0.5052483677864075                                      \n",
      "epoch 4 [9.01s]:  training loss=0.4790445864200592                                      \n",
      "epoch 5 [8.96s]: training loss=0.4408377408981323  validation ndcg@10=0.06846056551709537 [0.15s]\n",
      "epoch 6 [9.25s]:  training loss=0.42016997933387756                                     \n",
      "epoch 7 [9.19s]:  training loss=0.4018939435482025                                      \n",
      "epoch 8 [8.81s]:  training loss=0.3842052221298218                                      \n",
      "epoch 9 [9.12s]:  training loss=0.36683163046836853                                     \n",
      "epoch 10 [9.37s]: training loss=0.3648777902126312  validation ndcg@10=0.04954977129210493 [0.15s]\n",
      "epoch 11 [9.05s]:  training loss=0.351302832365036                                      \n",
      "epoch 12 [9.16s]:  training loss=0.35239270329475403                                    \n",
      "epoch 13 [9.19s]:  training loss=0.3389551341533661                                     \n",
      "epoch 14 [9.09s]:  training loss=0.3470398485660553                                     \n",
      "epoch 15 [9.22s]: training loss=0.32999107241630554  validation ndcg@10=0.05321472812777105 [0.17s]\n",
      "epoch 16 [9.25s]:  training loss=0.3396480083465576                                     \n",
      "epoch 17 [9.19s]:  training loss=0.33198925852775574                                    \n",
      "epoch 18 [9.23s]:  training loss=0.3323502838611603                                     \n",
      "epoch 19 [9.18s]:  training loss=0.3277628421783447                                     \n",
      "epoch 20 [9.34s]: training loss=0.33009517192840576  validation ndcg@10=0.054645835920793806 [0.15s]\n",
      "epoch 21 [9.14s]:  training loss=0.32437384128570557                                    \n",
      "epoch 22 [9.14s]:  training loss=0.3274807631969452                                     \n",
      "epoch 23 [9.22s]:  training loss=0.32718992233276367                                    \n",
      "epoch 24 [8.75s]:  training loss=0.3182467520236969                                     \n",
      "epoch 25 [9.17s]: training loss=0.32429736852645874  validation ndcg@10=0.05826634923447258 [0.2s]\n",
      "epoch 26 [9.33s]:  training loss=0.3201034963130951                                     \n",
      "epoch 27 [9.4s]:  training loss=0.31913456320762634                                     \n",
      "epoch 28 [9.07s]:  training loss=0.31487080454826355                                    \n",
      "epoch 29 [8.98s]:  training loss=0.3162509500980377                                     \n",
      "epoch 30 [8.96s]: training loss=0.31356802582740784  validation ndcg@10=0.04416757631936786 [0.15s]\n",
      "epoch 1 [3.91s]:  training loss=0.9849129319190979                                      \n",
      "epoch 2 [3.63s]:  training loss=0.9798864722251892                                      \n",
      "epoch 3 [3.55s]:  training loss=0.9754883050918579                                      \n",
      "epoch 4 [3.57s]:  training loss=0.9718345403671265                                      \n",
      "epoch 5 [3.82s]: training loss=0.9658110737800598  validation ndcg@10=0.01837541098846249 [0.08s]\n",
      "epoch 6 [3.63s]:  training loss=0.9613389372825623                                      \n",
      "epoch 7 [3.58s]:  training loss=0.9578952789306641                                      \n",
      "epoch 8 [3.73s]:  training loss=0.9511886835098267                                      \n",
      "epoch 9 [3.54s]:  training loss=0.9478538036346436                                      \n",
      "epoch 10 [3.49s]: training loss=0.9426864385604858  validation ndcg@10=0.03639183796219321 [0.1s]\n",
      "epoch 11 [3.72s]:  training loss=0.9362762570381165                                     \n",
      "epoch 12 [3.42s]:  training loss=0.9321857690811157                                     \n",
      "epoch 13 [3.67s]:  training loss=0.9276440143585205                                     \n",
      "epoch 14 [3.55s]:  training loss=0.9199190139770508                                     \n",
      "epoch 15 [3.56s]: training loss=0.9156655669212341  validation ndcg@10=0.046626338630095514 [0.08s]\n",
      "epoch 16 [3.63s]:  training loss=0.9098519682884216                                     \n",
      "epoch 17 [3.58s]:  training loss=0.9041032791137695                                     \n",
      "epoch 18 [3.54s]:  training loss=0.899548351764679                                      \n",
      "epoch 19 [3.6s]:  training loss=0.893157422542572                                       \n",
      "epoch 20 [3.5s]: training loss=0.8876856565475464  validation ndcg@10=0.05112818225806405 [0.07s]\n",
      "epoch 21 [3.68s]:  training loss=0.8792765140533447                                     \n",
      "epoch 22 [3.52s]:  training loss=0.8718127608299255                                     \n",
      "epoch 23 [3.62s]:  training loss=0.8655179738998413                                     \n",
      "epoch 24 [3.62s]:  training loss=0.8599761724472046                                     \n",
      "epoch 25 [3.71s]: training loss=0.8508195877075195  validation ndcg@10=0.05674822289903846 [0.08s]\n",
      "epoch 26 [3.63s]:  training loss=0.8471012711524963                                     \n",
      "epoch 27 [3.69s]:  training loss=0.8369138836860657                                     \n",
      "epoch 28 [3.7s]:  training loss=0.8295958638191223                                      \n",
      "epoch 29 [3.63s]:  training loss=0.8227735161781311                                     \n",
      "epoch 30 [3.73s]: training loss=0.8175402283668518  validation ndcg@10=0.05792261031035334 [0.07s]\n",
      "epoch 31 [3.57s]:  training loss=0.8080336451530457                                     \n",
      "epoch 32 [3.78s]:  training loss=0.8015713691711426                                     \n",
      "epoch 33 [3.87s]:  training loss=0.7949737310409546                                     \n",
      "epoch 34 [3.63s]:  training loss=0.7864436507225037                                     \n",
      "epoch 35 [3.86s]: training loss=0.7787162065505981  validation ndcg@10=0.059137076332580786 [0.09s]\n",
      "epoch 36 [3.85s]:  training loss=0.7720839381217957                                     \n",
      "epoch 37 [3.85s]:  training loss=0.7612797021865845                                     \n",
      "epoch 38 [3.58s]:  training loss=0.7611613869667053                                     \n",
      "epoch 39 [3.48s]:  training loss=0.7486400604248047                                     \n",
      "epoch 40 [3.69s]: training loss=0.7419764995574951  validation ndcg@10=0.06024180101571153 [0.08s]\n",
      "epoch 41 [3.61s]:  training loss=0.7315062284469604                                     \n",
      "epoch 42 [3.8s]:  training loss=0.7219523191452026                                      \n",
      "epoch 43 [3.62s]:  training loss=0.713905930519104                                      \n",
      "epoch 44 [3.57s]:  training loss=0.7054990530014038                                     \n",
      "epoch 45 [3.56s]: training loss=0.6980428099632263  validation ndcg@10=0.06263964128586469 [0.1s]\n",
      "epoch 46 [3.6s]:  training loss=0.6893824934959412                                      \n",
      "epoch 47 [3.51s]:  training loss=0.6818377375602722                                     \n",
      "epoch 48 [3.46s]:  training loss=0.6792895793914795                                     \n",
      "epoch 49 [3.48s]:  training loss=0.6683539152145386                                     \n",
      "epoch 50 [3.51s]: training loss=0.6649678349494934  validation ndcg@10=0.06149157915688785 [0.11s]\n",
      "epoch 51 [3.68s]:  training loss=0.6620736718177795                                     \n",
      "epoch 52 [3.64s]:  training loss=0.6555961966514587                                     \n",
      "epoch 53 [3.69s]:  training loss=0.652881383895874                                      \n",
      "epoch 54 [3.6s]:  training loss=0.6515105962753296                                      \n",
      "epoch 55 [3.52s]: training loss=0.6444714069366455  validation ndcg@10=0.06268082406780365 [0.08s]\n",
      "epoch 56 [3.65s]:  training loss=0.6389217376708984                                     \n",
      "epoch 57 [3.7s]:  training loss=0.6339339017868042                                      \n",
      "epoch 58 [3.78s]:  training loss=0.6307784914970398                                     \n",
      "epoch 59 [3.64s]:  training loss=0.6275408267974854                                     \n",
      "epoch 60 [3.54s]: training loss=0.624070942401886  validation ndcg@10=0.0653066001001587 [0.08s]\n",
      "epoch 61 [3.9s]:  training loss=0.6224799752235413                                      \n",
      "epoch 62 [3.77s]:  training loss=0.6234087347984314                                     \n",
      "epoch 63 [3.56s]:  training loss=0.6171040534973145                                     \n",
      "epoch 64 [3.52s]:  training loss=0.6184415221214294                                     \n",
      "epoch 65 [3.52s]: training loss=0.6114149689674377  validation ndcg@10=0.06516011571999338 [0.08s]\n",
      "epoch 66 [3.63s]:  training loss=0.6075922846794128                                     \n",
      "epoch 67 [3.52s]:  training loss=0.6047974228858948                                     \n",
      "epoch 68 [3.58s]:  training loss=0.6053289175033569                                     \n",
      "epoch 69 [3.58s]:  training loss=0.598452091217041                                      \n",
      "epoch 70 [3.65s]: training loss=0.599237322807312  validation ndcg@10=0.06372606108898948 [0.09s]\n",
      "epoch 71 [3.7s]:  training loss=0.5976555347442627                                      \n",
      "epoch 72 [3.57s]:  training loss=0.5957617163658142                                     \n",
      "epoch 73 [3.69s]:  training loss=0.5911305546760559                                     \n",
      "epoch 74 [3.81s]:  training loss=0.591810941696167                                      \n",
      "epoch 75 [3.65s]: training loss=0.588738203048706  validation ndcg@10=0.06393658046933895 [0.09s]\n",
      "epoch 76 [3.81s]:  training loss=0.5835250020027161                                     \n",
      "epoch 77 [3.65s]:  training loss=0.5844073295593262                                     \n",
      "epoch 78 [3.61s]:  training loss=0.5814967751502991                                     \n",
      "epoch 79 [3.78s]:  training loss=0.5818032622337341                                     \n",
      "epoch 80 [3.76s]: training loss=0.5784255862236023  validation ndcg@10=0.06275556423818333 [0.09s]\n",
      "epoch 81 [3.67s]:  training loss=0.576109766960144                                      \n",
      "epoch 82 [3.52s]:  training loss=0.572236180305481                                      \n",
      "epoch 83 [3.82s]:  training loss=0.5741505026817322                                     \n",
      "epoch 84 [3.6s]:  training loss=0.5713167786598206                                      \n",
      "epoch 85 [5.56s]: training loss=0.5690467357635498  validation ndcg@10=0.06204172123519093 [0.07s]\n",
      "epoch 1 [12.74s]:  training loss=0.9250890612602234                                     \n",
      "epoch 2 [12.39s]:  training loss=0.755062997341156                                      \n",
      "epoch 3 [12.74s]:  training loss=0.6783900856971741                                     \n",
      "epoch 4 [12.73s]:  training loss=0.6372849941253662                                     \n",
      "epoch 5 [12.81s]: training loss=0.5972567200660706  validation ndcg@10=0.06391946453971539 [0.16s]\n",
      "epoch 6 [12.78s]:  training loss=0.5728461146354675                                     \n",
      "epoch 7 [12.4s]:  training loss=0.5451768636703491                                      \n",
      "epoch 8 [13.18s]:  training loss=0.5310027599334717                                     \n",
      "epoch 9 [13.04s]:  training loss=0.5136867761611938                                     \n",
      "epoch 10 [12.76s]: training loss=0.49728238582611084  validation ndcg@10=0.06431119420773337 [0.17s]\n",
      "epoch 11 [12.64s]:  training loss=0.48332276940345764                                   \n",
      "epoch 12 [12.67s]:  training loss=0.470851331949234                                     \n",
      "epoch 13 [12.78s]:  training loss=0.4635494649410248                                    \n",
      "epoch 14 [12.59s]:  training loss=0.4584957957267761                                    \n",
      "epoch 15 [12.88s]: training loss=0.44883185625076294  validation ndcg@10=0.07260385741305127 [0.17s]\n",
      "epoch 16 [12.74s]:  training loss=0.45000937581062317                                   \n",
      "epoch 17 [12.48s]:  training loss=0.43422287702560425                                   \n",
      "epoch 18 [13.27s]:  training loss=0.4488845765590668                                    \n",
      "epoch 19 [12.75s]:  training loss=0.42891937494277954                                   \n",
      "epoch 20 [13.11s]: training loss=0.4298326373100281  validation ndcg@10=0.06637286973511085 [0.15s]\n",
      "epoch 21 [12.81s]:  training loss=0.4182836711406708                                    \n",
      "epoch 22 [12.75s]:  training loss=0.41217631101608276                                   \n",
      "epoch 23 [12.69s]:  training loss=0.4046720862388611                                    \n",
      "epoch 24 [12.6s]:  training loss=0.3951546549797058                                     \n",
      "epoch 25 [13.09s]: training loss=0.3866369426250458  validation ndcg@10=0.06596678456258649 [0.18s]\n",
      "epoch 26 [12.67s]:  training loss=0.39099523425102234                                   \n",
      "epoch 27 [12.58s]:  training loss=0.3800334334373474                                    \n",
      "epoch 28 [13.37s]:  training loss=0.3868471086025238                                    \n",
      "epoch 29 [12.49s]:  training loss=0.37991148233413696                                   \n",
      "epoch 30 [12.76s]: training loss=0.38384848833084106  validation ndcg@10=0.06441426354585988 [0.18s]\n",
      "epoch 31 [12.73s]:  training loss=0.379146933555603                                     \n",
      "epoch 32 [12.29s]:  training loss=0.37274929881095886                                   \n",
      "epoch 33 [12.91s]:  training loss=0.376864492893219                                     \n",
      "epoch 34 [12.81s]:  training loss=0.37479376792907715                                   \n",
      "epoch 35 [12.73s]: training loss=0.36690178513526917  validation ndcg@10=0.06692362181396502 [0.15s]\n",
      "epoch 36 [12.7s]:  training loss=0.3634908199310303                                     \n",
      "epoch 37 [12.89s]:  training loss=0.367093563079834                                     \n",
      "epoch 38 [12.74s]:  training loss=0.36132630705833435                                   \n",
      "epoch 39 [12.57s]:  training loss=0.36263176798820496                                   \n",
      "epoch 40 [13.05s]: training loss=0.35683292150497437  validation ndcg@10=0.06505572819592685 [0.27s]\n",
      "epoch 1 [6.49s]:  training loss=0.5934323668479919                                      \n",
      "epoch 2 [6.49s]:  training loss=0.5865600109100342                                      \n",
      "epoch 3 [6.23s]:  training loss=0.5715173482894897                                      \n",
      "epoch 4 [6.29s]:  training loss=0.5712192058563232                                      \n",
      "epoch 5 [6.83s]: training loss=0.5670497417449951  validation ndcg@10=0.015676736595993945 [0.13s]\n",
      "epoch 6 [6.42s]:  training loss=0.5532665848731995                                      \n",
      "epoch 7 [6.2s]:  training loss=0.5527309775352478                                       \n",
      "epoch 8 [6.23s]:  training loss=0.5676008462905884                                      \n",
      "epoch 9 [6.21s]:  training loss=0.5669727325439453                                      \n",
      "epoch 10 [6.07s]: training loss=0.5522685647010803  validation ndcg@10=0.009354707176520961 [0.14s]\n",
      "epoch 11 [6.1s]:  training loss=0.558290421962738                                       \n",
      "epoch 12 [7.89s]:  training loss=0.561809241771698                                      \n",
      "epoch 13 [6.28s]:  training loss=0.5619190335273743                                     \n",
      "epoch 14 [6.41s]:  training loss=0.5561750531196594                                     \n",
      "epoch 15 [6.13s]: training loss=0.548122763633728  validation ndcg@10=0.009217687270239706 [0.11s]\n",
      "epoch 16 [6.13s]:  training loss=0.5651813745498657                                     \n",
      "epoch 17 [6.15s]:  training loss=0.5616159439086914                                     \n",
      "epoch 18 [6.23s]:  training loss=0.5597554445266724                                     \n",
      "epoch 19 [6.27s]:  training loss=0.5572713613510132                                     \n",
      "epoch 20 [6.24s]: training loss=0.5574213266372681  validation ndcg@10=0.0053201321027314145 [0.11s]\n",
      "epoch 21 [6.21s]:  training loss=0.5504956841468811                                     \n",
      "epoch 22 [6.25s]:  training loss=0.5544302463531494                                     \n",
      "epoch 23 [6.26s]:  training loss=0.557235836982727                                      \n",
      "epoch 24 [6.2s]:  training loss=0.5573649406433105                                      \n",
      "epoch 25 [6.06s]: training loss=0.5541813373565674  validation ndcg@10=0.004281570851691975 [0.12s]\n",
      "epoch 26 [6.11s]:  training loss=0.5559868216514587                                     \n",
      "epoch 27 [6.24s]:  training loss=0.5683671832084656                                     \n",
      "epoch 28 [6.03s]:  training loss=0.5593855977058411                                     \n",
      "epoch 29 [6.11s]:  training loss=0.555935263633728                                      \n",
      "epoch 30 [6.12s]: training loss=0.5618871450424194  validation ndcg@10=0.0026840459304053187 [0.11s]\n",
      "epoch 1 [8.65s]:  training loss=0.8794670104980469                                      \n",
      "epoch 2 [8.74s]:  training loss=0.6523438096046448                                      \n",
      "epoch 3 [8.73s]:  training loss=0.5581874251365662                                      \n",
      "epoch 4 [8.61s]:  training loss=0.5236665606498718                                      \n",
      "epoch 5 [8.54s]: training loss=0.49722740054130554  validation ndcg@10=0.07274758853098938 [0.14s]\n",
      "epoch 6 [8.7s]:  training loss=0.48557883501052856                                      \n",
      "epoch 7 [8.54s]:  training loss=0.46686074137687683                                     \n",
      "epoch 8 [8.59s]:  training loss=0.4505951702594757                                      \n",
      "epoch 9 [8.62s]:  training loss=0.4348657727241516                                      \n",
      "epoch 10 [8.85s]: training loss=0.4187362790107727  validation ndcg@10=0.067944430774875 [0.15s]\n",
      "epoch 11 [8.78s]:  training loss=0.40311700105667114                                    \n",
      "epoch 12 [8.66s]:  training loss=0.3901795744895935                                     \n",
      "epoch 13 [8.58s]:  training loss=0.3838394284248352                                     \n",
      "epoch 14 [8.45s]:  training loss=0.3717968463897705                                     \n",
      "epoch 15 [8.64s]: training loss=0.36324629187583923  validation ndcg@10=0.06363021725035807 [0.14s]\n",
      "epoch 16 [8.56s]:  training loss=0.3575197458267212                                     \n",
      "epoch 17 [8.97s]:  training loss=0.3505496084690094                                     \n",
      "epoch 18 [8.72s]:  training loss=0.348791241645813                                      \n",
      "epoch 19 [8.71s]:  training loss=0.3441423773765564                                     \n",
      "epoch 20 [8.81s]: training loss=0.34293287992477417  validation ndcg@10=0.07072230333760626 [0.14s]\n",
      "epoch 21 [8.75s]:  training loss=0.3390759527683258                                     \n",
      "epoch 22 [9.03s]:  training loss=0.33739280700683594                                    \n",
      "epoch 23 [8.95s]:  training loss=0.34039491415023804                                    \n",
      "epoch 24 [8.57s]:  training loss=0.3278998136520386                                     \n",
      "epoch 25 [8.73s]: training loss=0.3270229697227478  validation ndcg@10=0.06375201333165777 [0.12s]\n",
      "epoch 26 [8.67s]:  training loss=0.3249410390853882                                     \n",
      "epoch 27 [8.71s]:  training loss=0.3294704556465149                                     \n",
      "epoch 28 [8.51s]:  training loss=0.31398868560791016                                    \n",
      "epoch 29 [8.54s]:  training loss=0.3219457268714905                                     \n",
      "epoch 30 [8.48s]: training loss=0.318093866109848  validation ndcg@10=0.06912004377599444 [0.14s]\n",
      "epoch 1 [3.52s]:  training loss=0.979102373123169                                       \n",
      "epoch 2 [3.48s]:  training loss=0.9629483222961426                                      \n",
      "epoch 3 [3.45s]:  training loss=0.9458242058753967                                      \n",
      "epoch 4 [3.57s]:  training loss=0.9287846684455872                                      \n",
      "epoch 5 [3.5s]: training loss=0.9102205634117126  validation ndcg@10=0.0508549697798358 [0.05s]\n",
      "epoch 6 [3.47s]:  training loss=0.8886288404464722                                      \n",
      "epoch 7 [3.41s]:  training loss=0.8671256303787231                                      \n",
      "epoch 8 [3.43s]:  training loss=0.8436498641967773                                      \n",
      "epoch 9 [3.39s]:  training loss=0.8194338083267212                                      \n",
      "epoch 10 [3.35s]: training loss=0.7898133993148804  validation ndcg@10=0.06755002776261522 [0.05s]\n",
      "epoch 11 [3.37s]:  training loss=0.7498312592506409                                     \n",
      "epoch 12 [3.4s]:  training loss=0.7193429470062256                                      \n",
      "epoch 13 [3.41s]:  training loss=0.7052501440048218                                     \n",
      "epoch 14 [3.42s]:  training loss=0.6872881054878235                                     \n",
      "epoch 15 [3.4s]: training loss=0.6791552305221558  validation ndcg@10=0.07149525725859791 [0.06s]\n",
      "epoch 16 [3.45s]:  training loss=0.6735274195671082                                     \n",
      "epoch 17 [3.5s]:  training loss=0.6612085700035095                                      \n",
      "epoch 18 [3.5s]:  training loss=0.6549750566482544                                      \n",
      "epoch 19 [3.44s]:  training loss=0.6480644941329956                                     \n",
      "epoch 20 [3.45s]: training loss=0.6402170062065125  validation ndcg@10=0.07014965731481879 [0.06s]\n",
      "epoch 21 [3.5s]:  training loss=0.6330555081367493                                      \n",
      "epoch 22 [3.54s]:  training loss=0.6263712048530579                                     \n",
      "epoch 23 [3.49s]:  training loss=0.619496762752533                                      \n",
      "epoch 24 [3.49s]:  training loss=0.6122510433197021                                     \n",
      "epoch 25 [3.47s]: training loss=0.6092879176139832  validation ndcg@10=0.07187399395642967 [0.05s]\n",
      "epoch 26 [3.47s]:  training loss=0.6042263507843018                                     \n",
      "epoch 27 [3.38s]:  training loss=0.5960881114006042                                     \n",
      "epoch 28 [3.44s]:  training loss=0.5880925059318542                                     \n",
      "epoch 29 [3.38s]:  training loss=0.5820512771606445                                     \n",
      "epoch 30 [3.44s]: training loss=0.5814058184623718  validation ndcg@10=0.07107305206597023 [0.06s]\n",
      "epoch 31 [3.46s]:  training loss=0.5781792998313904                                     \n",
      "epoch 32 [3.48s]:  training loss=0.5757255554199219                                     \n",
      "epoch 33 [3.39s]:  training loss=0.5658856630325317                                     \n",
      "epoch 34 [3.46s]:  training loss=0.5645073652267456                                     \n",
      "epoch 35 [3.41s]: training loss=0.5632816553115845  validation ndcg@10=0.06933544103624328 [0.08s]\n",
      "epoch 36 [3.44s]:  training loss=0.5631459355354309                                     \n",
      "epoch 37 [3.45s]:  training loss=0.5581727027893066                                     \n",
      "epoch 38 [3.49s]:  training loss=0.5522129535675049                                     \n",
      "epoch 39 [3.5s]:  training loss=0.5553100109100342                                      \n",
      "epoch 40 [3.43s]: training loss=0.5500219464302063  validation ndcg@10=0.06997283687260203 [0.06s]\n",
      "epoch 41 [3.51s]:  training loss=0.5412386059761047                                     \n",
      "epoch 42 [3.53s]:  training loss=0.54372638463974                                       \n",
      "epoch 43 [3.44s]:  training loss=0.536388099193573                                      \n",
      "epoch 44 [3.49s]:  training loss=0.5319535732269287                                     \n",
      "epoch 45 [3.42s]: training loss=0.5311349630355835  validation ndcg@10=0.07063097322919575 [0.06s]\n",
      "epoch 46 [3.38s]:  training loss=0.527941107749939                                      \n",
      "epoch 47 [3.42s]:  training loss=0.5224831104278564                                     \n",
      "epoch 48 [3.41s]:  training loss=0.5295871496200562                                     \n",
      "epoch 49 [3.38s]:  training loss=0.5233765840530396                                     \n",
      "epoch 50 [3.46s]: training loss=0.5213118195533752  validation ndcg@10=0.07045404612363243 [0.06s]\n",
      "epoch 1 [14.56s]:  training loss=0.9881837964057922                                     \n",
      "epoch 2 [15.49s]:  training loss=0.985134482383728                                      \n",
      "epoch 3 [15.66s]:  training loss=0.9814373254776001                                     \n",
      "epoch 4 [15.76s]:  training loss=0.9766460061073303                                     \n",
      "epoch 5 [15.7s]: training loss=0.9740801453590393  validation ndcg@10=0.006262773837612409 [0.26s]\n",
      "epoch 6 [17.49s]:  training loss=0.9714712500572205                                     \n",
      "epoch 7 [16.24s]:  training loss=0.9689533114433289                                     \n",
      "epoch 8 [15.61s]:  training loss=0.9644638299942017                                     \n",
      "epoch 9 [15.61s]:  training loss=0.9603841304779053                                     \n",
      "epoch 10 [17.23s]: training loss=0.9581836462020874  validation ndcg@10=0.016998465477182773 [0.31s]\n",
      "epoch 11 [16.18s]:  training loss=0.955962598323822                                     \n",
      "epoch 12 [15.54s]:  training loss=0.9516177773475647                                    \n",
      "epoch 13 [15.5s]:  training loss=0.9491156339645386                                     \n",
      "epoch 14 [15.54s]:  training loss=0.9436193704605103                                    \n",
      "epoch 15 [15.55s]: training loss=0.9399431347846985  validation ndcg@10=0.015659167602151036 [0.28s]\n",
      "epoch 16 [16.09s]:  training loss=0.9376150965690613                                    \n",
      "epoch 17 [15.46s]:  training loss=0.9337785840034485                                    \n",
      "epoch 18 [15.69s]:  training loss=0.9284588098526001                                    \n",
      "epoch 19 [16.26s]:  training loss=0.9265639185905457                                    \n",
      "epoch 20 [16.98s]: training loss=0.9204071164131165  validation ndcg@10=0.03687864849497791 [0.26s]\n",
      "epoch 21 [16.26s]:  training loss=0.9177745580673218                                    \n",
      "epoch 22 [15.55s]:  training loss=0.9142520427703857                                    \n",
      "epoch 23 [15.63s]:  training loss=0.9102937579154968                                    \n",
      "epoch 24 [15.45s]:  training loss=0.9064481258392334                                    \n",
      "epoch 25 [15.39s]: training loss=0.8987723588943481  validation ndcg@10=0.039893992261989614 [0.28s]\n",
      "epoch 26 [15.87s]:  training loss=0.8962429761886597                                    \n",
      "epoch 27 [15.67s]:  training loss=0.8907186388969421                                    \n",
      "epoch 28 [15.49s]:  training loss=0.8850021958351135                                    \n",
      "epoch 29 [15.48s]:  training loss=0.8769937753677368                                    \n",
      "epoch 30 [15.26s]: training loss=0.876924455165863  validation ndcg@10=0.05816243670408983 [0.3s]\n",
      "epoch 31 [15.81s]:  training loss=0.8711333870887756                                    \n",
      "epoch 32 [15.34s]:  training loss=0.8666587471961975                                    \n",
      "epoch 33 [15.33s]:  training loss=0.861599326133728                                     \n",
      "epoch 34 [16.11s]:  training loss=0.8539265990257263                                    \n",
      "epoch 35 [15.29s]: training loss=0.851839005947113  validation ndcg@10=0.059977695829619565 [0.24s]\n",
      "epoch 36 [15.47s]:  training loss=0.8402548432350159                                    \n",
      "epoch 37 [15.67s]:  training loss=0.8395938277244568                                    \n",
      "epoch 38 [15.57s]:  training loss=0.8326650857925415                                    \n",
      "epoch 39 [15.75s]:  training loss=0.8264332413673401                                    \n",
      "epoch 40 [15.94s]: training loss=0.8187887668609619  validation ndcg@10=0.06534706892428174 [0.26s]\n",
      "epoch 41 [15.46s]:  training loss=0.8141257762908936                                    \n",
      "epoch 42 [15.5s]:  training loss=0.8093693256378174                                     \n",
      "epoch 43 [15.51s]:  training loss=0.8025496006011963                                    \n",
      "epoch 44 [15.43s]:  training loss=0.7961385250091553                                    \n",
      "epoch 45 [15.67s]: training loss=0.789716362953186  validation ndcg@10=0.06802176245525343 [0.29s]\n",
      "epoch 46 [15.61s]:  training loss=0.7839949131011963                                    \n",
      "epoch 47 [15.44s]:  training loss=0.781378448009491                                     \n",
      "epoch 48 [15.54s]:  training loss=0.7718662619590759                                    \n",
      "epoch 49 [17.64s]:  training loss=0.7644142508506775                                    \n",
      "epoch 50 [16.22s]: training loss=0.7606316208839417  validation ndcg@10=0.06522432845828276 [0.3s]\n",
      "epoch 51 [15.43s]:  training loss=0.7522308826446533                                    \n",
      "epoch 52 [15.71s]:  training loss=0.7507539987564087                                    \n",
      "epoch 53 [16.06s]:  training loss=0.7432874441146851                                    \n",
      "epoch 54 [16.62s]:  training loss=0.7405794262886047                                    \n",
      "epoch 55 [15.97s]: training loss=0.7389325499534607  validation ndcg@10=0.0688323357755365 [0.24s]\n",
      "epoch 56 [15.65s]:  training loss=0.7283390760421753                                    \n",
      "epoch 57 [15.78s]:  training loss=0.7271659970283508                                    \n",
      "epoch 58 [15.59s]:  training loss=0.7243027091026306                                    \n",
      "epoch 59 [15.5s]:  training loss=0.7189793586730957                                     \n",
      "epoch 60 [16.23s]: training loss=0.717580258846283  validation ndcg@10=0.0680437106189466 [0.27s]\n",
      "epoch 61 [16.03s]:  training loss=0.712083101272583                                     \n",
      "epoch 62 [16.57s]:  training loss=0.7054954171180725                                    \n",
      "epoch 63 [15.72s]:  training loss=0.7069392204284668                                    \n",
      "epoch 64 [15.81s]:  training loss=0.7059142589569092                                    \n",
      "epoch 65 [16.0s]: training loss=0.7030584216117859  validation ndcg@10=0.06884515537362262 [0.27s]\n",
      "epoch 66 [16.37s]:  training loss=0.6942427158355713                                    \n",
      "epoch 67 [16.41s]:  training loss=0.6902843713760376                                    \n",
      "epoch 68 [15.99s]:  training loss=0.6839178800582886                                    \n",
      "epoch 69 [16.03s]:  training loss=0.6862223744392395                                    \n",
      "epoch 70 [15.95s]: training loss=0.6802540421485901  validation ndcg@10=0.06932902983031858 [0.24s]\n",
      "epoch 71 [15.96s]:  training loss=0.6810089945793152                                    \n",
      "epoch 72 [16.04s]:  training loss=0.677090048789978                                     \n",
      "epoch 73 [15.84s]:  training loss=0.6754233837127686                                    \n",
      "epoch 74 [15.97s]:  training loss=0.6717616319656372                                    \n",
      "epoch 75 [15.83s]: training loss=0.6692391633987427  validation ndcg@10=0.07023790796507384 [0.27s]\n",
      "epoch 76 [16.22s]:  training loss=0.668944239616394                                     \n",
      "epoch 77 [15.77s]:  training loss=0.6617152094841003                                    \n",
      "epoch 78 [15.6s]:  training loss=0.6640576720237732                                     \n",
      "epoch 79 [15.72s]:  training loss=0.659187376499176                                     \n",
      "epoch 80 [15.42s]: training loss=0.6555290222167969  validation ndcg@10=0.0723837375886977 [0.27s]\n",
      "epoch 81 [15.43s]:  training loss=0.6581751108169556                                    \n",
      "epoch 82 [15.88s]:  training loss=0.654188334941864                                     \n",
      "epoch 83 [15.31s]:  training loss=0.6499213576316833                                    \n",
      "epoch 84 [15.76s]:  training loss=0.6496649384498596                                    \n",
      "epoch 85 [15.21s]: training loss=0.6480123996734619  validation ndcg@10=0.06748945572062522 [0.25s]\n",
      "epoch 86 [16.21s]:  training loss=0.6467307209968567                                    \n",
      "epoch 87 [15.82s]:  training loss=0.6401028633117676                                    \n",
      "epoch 88 [17.21s]:  training loss=0.6381973028182983                                    \n",
      "epoch 89 [16.27s]:  training loss=0.64068204164505                                      \n",
      "epoch 90 [15.95s]: training loss=0.6405766606330872  validation ndcg@10=0.06856299017571818 [0.27s]\n",
      "epoch 91 [15.75s]:  training loss=0.6342028379440308                                    \n",
      "epoch 92 [17.93s]:  training loss=0.6336156129837036                                    \n",
      "epoch 93 [15.56s]:  training loss=0.6351153254508972                                    \n",
      "epoch 94 [16.09s]:  training loss=0.628976583480835                                     \n",
      "epoch 95 [15.94s]: training loss=0.6302248239517212  validation ndcg@10=0.0696830094367924 [0.26s]\n",
      "epoch 96 [15.8s]:  training loss=0.6297265291213989                                     \n",
      "epoch 97 [16.34s]:  training loss=0.6276553869247437                                    \n",
      "epoch 98 [15.86s]:  training loss=0.6244223713874817                                    \n",
      "epoch 99 [16.06s]:  training loss=0.620664656162262                                     \n",
      "epoch 100 [15.78s]: training loss=0.6164191365242004  validation ndcg@10=0.06947076066055967 [0.23s]\n",
      "epoch 101 [16.19s]:  training loss=0.6196703910827637                                   \n",
      "epoch 102 [15.89s]:  training loss=0.6184496879577637                                   \n",
      "epoch 103 [15.99s]:  training loss=0.6164001226425171                                   \n",
      "epoch 104 [16.04s]:  training loss=0.6131201982498169                                   \n",
      "epoch 105 [16.14s]: training loss=0.6130008101463318  validation ndcg@10=0.06930507398837231 [0.26s]\n",
      "epoch 1 [11.75s]:  training loss=0.5820255875587463                                     \n",
      "epoch 2 [11.73s]:  training loss=0.45331689715385437                                    \n",
      "epoch 3 [10.72s]:  training loss=0.42846906185150146                                    \n",
      "epoch 4 [10.44s]:  training loss=0.41939282417297363                                    \n",
      "epoch 5 [10.58s]: training loss=0.42076924443244934  validation ndcg@10=0.04204801727589718 [0.19s]\n",
      "epoch 6 [10.5s]:  training loss=0.41812029480934143                                     \n",
      "epoch 7 [10.57s]:  training loss=0.4163498282432556                                     \n",
      "epoch 8 [10.69s]:  training loss=0.4136824309825897                                     \n",
      "epoch 9 [10.58s]:  training loss=0.41437333822250366                                    \n",
      "epoch 10 [10.37s]: training loss=0.41113778948783875  validation ndcg@10=0.025644471433103142 [0.2s]\n",
      "epoch 11 [10.81s]:  training loss=0.4134005010128021                                    \n",
      "epoch 12 [10.75s]:  training loss=0.4101921617984772                                    \n",
      "epoch 13 [10.78s]:  training loss=0.4206344783306122                                    \n",
      "epoch 14 [10.9s]:  training loss=0.40726378560066223                                    \n",
      "epoch 15 [10.57s]: training loss=0.4136178195476532  validation ndcg@10=0.02482663112037199 [0.18s]\n",
      "epoch 16 [10.75s]:  training loss=0.4109703004360199                                    \n",
      "epoch 17 [10.42s]:  training loss=0.4083448350429535                                    \n",
      "epoch 18 [10.93s]:  training loss=0.4103516936302185                                    \n",
      "epoch 19 [11.1s]:  training loss=0.40501224994659424                                    \n",
      "epoch 20 [10.82s]: training loss=0.4188091456890106  validation ndcg@10=0.03076741028953029 [0.17s]\n",
      "epoch 21 [10.65s]:  training loss=0.3963835537433624                                    \n",
      "epoch 22 [10.49s]:  training loss=0.40434762835502625                                   \n",
      "epoch 23 [10.41s]:  training loss=0.4106764793395996                                    \n",
      "epoch 24 [10.96s]:  training loss=0.4043518304824829                                    \n",
      "epoch 25 [11.15s]: training loss=0.4085122048854828  validation ndcg@10=0.012362195965171494 [0.2s]\n",
      "epoch 26 [10.53s]:  training loss=0.4099927842617035                                    \n",
      "epoch 27 [10.78s]:  training loss=0.4038669764995575                                    \n",
      "epoch 28 [11.04s]:  training loss=0.4050366282463074                                    \n",
      "epoch 29 [11.08s]:  training loss=0.4029715061187744                                    \n",
      "epoch 30 [10.69s]: training loss=0.40244895219802856  validation ndcg@10=0.009194794708296435 [0.19s]\n",
      "epoch 1 [5.9s]:  training loss=0.98319411277771                                         \n",
      "epoch 2 [5.81s]:  training loss=0.9818338751792908                                      \n",
      "epoch 3 [5.61s]:  training loss=0.9794712662696838                                      \n",
      "epoch 4 [5.53s]:  training loss=0.9769112467765808                                      \n",
      "epoch 5 [5.59s]: training loss=0.9736175537109375  validation ndcg@10=0.017365744577614105 [0.07s]\n",
      "epoch 6 [5.45s]:  training loss=0.9714506268501282                                      \n",
      "epoch 7 [5.57s]:  training loss=0.9691024422645569                                      \n",
      "epoch 8 [5.42s]:  training loss=0.9660652875900269                                      \n",
      "epoch 9 [6.99s]:  training loss=0.9639064073562622                                      \n",
      "epoch 10 [5.26s]: training loss=0.9580937027931213  validation ndcg@10=0.022426227512276367 [0.07s]\n",
      "epoch 11 [5.38s]:  training loss=0.9577245712280273                                     \n",
      "epoch 12 [5.32s]:  training loss=0.9537090063095093                                     \n",
      "epoch 13 [5.35s]:  training loss=0.9508966207504272                                     \n",
      "epoch 14 [5.21s]:  training loss=0.9482426643371582                                     \n",
      "epoch 15 [5.29s]: training loss=0.9455071091651917  validation ndcg@10=0.029311538924088748 [0.07s]\n",
      "epoch 16 [5.27s]:  training loss=0.9429810643196106                                     \n",
      "epoch 17 [5.35s]:  training loss=0.9386647939682007                                     \n",
      "epoch 18 [5.25s]:  training loss=0.9343328475952148                                     \n",
      "epoch 19 [5.32s]:  training loss=0.9328289031982422                                     \n",
      "epoch 20 [5.3s]: training loss=0.925999641418457  validation ndcg@10=0.050553978143342324 [0.07s]\n",
      "epoch 21 [5.32s]:  training loss=0.9253378510475159                                     \n",
      "epoch 22 [5.24s]:  training loss=0.9214637279510498                                     \n",
      "epoch 23 [5.33s]:  training loss=0.9165616035461426                                     \n",
      "epoch 24 [5.33s]:  training loss=0.9126995205879211                                     \n",
      "epoch 25 [5.35s]: training loss=0.9083359837532043  validation ndcg@10=0.06329286203878222 [0.1s]\n",
      "epoch 26 [5.44s]:  training loss=0.9039860367774963                                     \n",
      "epoch 27 [5.35s]:  training loss=0.9005148410797119                                     \n",
      "epoch 28 [5.35s]:  training loss=0.8958374261856079                                     \n",
      "epoch 29 [5.28s]:  training loss=0.8927690386772156                                     \n",
      "epoch 30 [5.41s]: training loss=0.8893153667449951  validation ndcg@10=0.0683899918193752 [0.07s]\n",
      "epoch 31 [5.34s]:  training loss=0.8842202425003052                                     \n",
      "epoch 32 [5.37s]:  training loss=0.8818363547325134                                     \n",
      "epoch 33 [5.33s]:  training loss=0.8752957582473755                                     \n",
      "epoch 34 [5.37s]:  training loss=0.8684057593345642                                     \n",
      "epoch 35 [5.28s]: training loss=0.8612965941429138  validation ndcg@10=0.06317476455883506 [0.07s]\n",
      "epoch 36 [5.28s]:  training loss=0.8577766418457031                                     \n",
      "epoch 37 [5.37s]:  training loss=0.8532348871231079                                     \n",
      "epoch 38 [5.35s]:  training loss=0.8459377884864807                                     \n",
      "epoch 39 [5.22s]:  training loss=0.8380995988845825                                     \n",
      "epoch 40 [5.39s]: training loss=0.8366501927375793  validation ndcg@10=0.06740736520272855 [0.09s]\n",
      "epoch 41 [5.34s]:  training loss=0.8280330300331116                                     \n",
      "epoch 42 [5.36s]:  training loss=0.8231061697006226                                     \n",
      "epoch 43 [5.3s]:  training loss=0.8174170255661011                                      \n",
      "epoch 44 [5.34s]:  training loss=0.8122939467430115                                     \n",
      "epoch 45 [5.31s]: training loss=0.8036283850669861  validation ndcg@10=0.0684473742833533 [0.06s]\n",
      "epoch 46 [5.29s]:  training loss=0.7972214221954346                                     \n",
      "epoch 47 [5.36s]:  training loss=0.7942606210708618                                     \n",
      "epoch 48 [5.43s]:  training loss=0.7900424003601074                                     \n",
      "epoch 49 [5.38s]:  training loss=0.7844308018684387                                     \n",
      "epoch 50 [5.26s]: training loss=0.7748597264289856  validation ndcg@10=0.0706643087656701 [0.07s]\n",
      "epoch 51 [5.31s]:  training loss=0.7751889228820801                                     \n",
      "epoch 52 [5.22s]:  training loss=0.7630623579025269                                     \n",
      "epoch 53 [5.33s]:  training loss=0.7650710344314575                                     \n",
      "epoch 54 [5.25s]:  training loss=0.7579330205917358                                     \n",
      "epoch 55 [5.34s]: training loss=0.755943775177002  validation ndcg@10=0.06955522769673787 [0.07s]\n",
      "epoch 56 [5.29s]:  training loss=0.752910852432251                                      \n",
      "epoch 57 [5.29s]:  training loss=0.746728241443634                                      \n",
      "epoch 58 [5.46s]:  training loss=0.7455764412879944                                     \n",
      "epoch 59 [5.32s]:  training loss=0.7399834394454956                                     \n",
      "epoch 60 [5.3s]: training loss=0.736524224281311  validation ndcg@10=0.06833888280559156 [0.07s]\n",
      "epoch 61 [5.26s]:  training loss=0.7318351864814758                                     \n",
      "epoch 62 [5.38s]:  training loss=0.7306106090545654                                     \n",
      "epoch 63 [5.33s]:  training loss=0.7271501421928406                                     \n",
      "epoch 64 [5.27s]:  training loss=0.7253755331039429                                     \n",
      "epoch 65 [5.33s]: training loss=0.7242541313171387  validation ndcg@10=0.06952991428840623 [0.08s]\n",
      "epoch 66 [5.32s]:  training loss=0.7200322151184082                                     \n",
      "epoch 67 [5.25s]:  training loss=0.7158463001251221                                     \n",
      "epoch 68 [5.34s]:  training loss=0.7186374068260193                                     \n",
      "epoch 69 [5.3s]:  training loss=0.7129029631614685                                      \n",
      "epoch 70 [5.26s]: training loss=0.7133567333221436  validation ndcg@10=0.06831480792126413 [0.06s]\n",
      "epoch 71 [5.29s]:  training loss=0.7109899520874023                                     \n",
      "epoch 72 [5.36s]:  training loss=0.7056747078895569                                     \n",
      "epoch 73 [5.27s]:  training loss=0.7045271396636963                                     \n",
      "epoch 74 [5.32s]:  training loss=0.7047258615493774                                     \n",
      "epoch 75 [5.26s]: training loss=0.6997715830802917  validation ndcg@10=0.06539207455465236 [0.11s]\n",
      "epoch 1 [17.57s]:  training loss=0.9150428771972656                                     \n",
      "epoch 2 [16.99s]:  training loss=0.7285690307617188                                     \n",
      "epoch 3 [16.77s]:  training loss=0.6112045645713806                                     \n",
      "epoch 4 [16.76s]:  training loss=0.5651832222938538                                     \n",
      "epoch 5 [17.3s]: training loss=0.543357789516449  validation ndcg@10=0.0700271458863575 [0.3s]\n",
      "epoch 6 [16.51s]:  training loss=0.5226089358329773                                     \n",
      "epoch 7 [16.41s]:  training loss=0.510382354259491                                      \n",
      "epoch 8 [16.52s]:  training loss=0.5034007430076599                                     \n",
      "epoch 9 [16.61s]:  training loss=0.49093833565711975                                    \n",
      "epoch 10 [16.6s]: training loss=0.4735648036003113  validation ndcg@10=0.07398405762648766 [0.29s]\n",
      "epoch 11 [16.63s]:  training loss=0.4570094645023346                                    \n",
      "epoch 12 [16.48s]:  training loss=0.445146381855011                                     \n",
      "epoch 13 [16.55s]:  training loss=0.4412675201892853                                    \n",
      "epoch 14 [16.6s]:  training loss=0.4352644383907318                                     \n",
      "epoch 15 [16.69s]: training loss=0.42399463057518005  validation ndcg@10=0.0661045171951212 [0.28s]\n",
      "epoch 16 [16.64s]:  training loss=0.4129045605659485                                    \n",
      "epoch 17 [16.54s]:  training loss=0.40998247265815735                                   \n",
      "epoch 18 [16.58s]:  training loss=0.4005592465400696                                    \n",
      "epoch 19 [16.56s]:  training loss=0.39254894852638245                                   \n",
      "epoch 20 [18.11s]: training loss=0.3834776282310486  validation ndcg@10=0.06466347428026246 [0.27s]\n",
      "epoch 21 [15.14s]:  training loss=0.3805687129497528                                    \n",
      "epoch 22 [16.22s]:  training loss=0.36961138248443604                                   \n",
      "epoch 23 [16.52s]:  training loss=0.3724370300769806                                    \n",
      "epoch 24 [16.53s]:  training loss=0.36690449714660645                                   \n",
      "epoch 25 [16.48s]: training loss=0.35861557722091675  validation ndcg@10=0.06542946623653703 [0.28s]\n",
      "epoch 26 [16.28s]:  training loss=0.3621308505535126                                    \n",
      "epoch 27 [16.53s]:  training loss=0.3553604483604431                                    \n",
      "epoch 28 [16.37s]:  training loss=0.34996625781059265                                   \n",
      "epoch 29 [16.2s]:  training loss=0.3526967167854309                                     \n",
      "epoch 30 [16.18s]: training loss=0.3547264635562897  validation ndcg@10=0.06842467891514897 [0.22s]\n",
      "epoch 31 [16.05s]:  training loss=0.34614458680152893                                   \n",
      "epoch 32 [15.95s]:  training loss=0.34465596079826355                                   \n",
      "epoch 33 [16.04s]:  training loss=0.3472394049167633                                    \n",
      "epoch 34 [16.07s]:  training loss=0.3432225286960602                                    \n",
      "epoch 35 [16.11s]: training loss=0.340143084526062  validation ndcg@10=0.06125696436676035 [0.22s]\n",
      "epoch 1 [1.99s]:  training loss=0.9834377765655518                                      \n",
      "epoch 2 [2.05s]:  training loss=0.9805850982666016                                      \n",
      "epoch 3 [2.05s]:  training loss=0.9744496941566467                                      \n",
      "epoch 4 [1.99s]:  training loss=0.9693924784660339                                      \n",
      "epoch 5 [2.04s]: training loss=0.9623849391937256  validation ndcg@10=0.04194730141367628 [0.04s]\n",
      "epoch 6 [2.07s]:  training loss=0.9565879106521606                                      \n",
      "epoch 7 [2.06s]:  training loss=0.9508540034294128                                      \n",
      "epoch 8 [2.01s]:  training loss=0.9456945061683655                                      \n",
      "epoch 9 [2.05s]:  training loss=0.9389806389808655                                      \n",
      "epoch 10 [2.06s]: training loss=0.9294677972793579  validation ndcg@10=0.05538324085938897 [0.05s]\n",
      "epoch 11 [2.11s]:  training loss=0.9246456623077393                                     \n",
      "epoch 12 [2.06s]:  training loss=0.9131463766098022                                     \n",
      "epoch 13 [2.19s]:  training loss=0.9063349366188049                                     \n",
      "epoch 14 [2.2s]:  training loss=0.8966770172119141                                      \n",
      "epoch 15 [2.12s]: training loss=0.8857046365737915  validation ndcg@10=0.07199126420860676 [0.05s]\n",
      "epoch 16 [2.13s]:  training loss=0.875210702419281                                      \n",
      "epoch 17 [2.14s]:  training loss=0.8661299347877502                                     \n",
      "epoch 18 [2.13s]:  training loss=0.8521671891212463                                     \n",
      "epoch 19 [2.07s]:  training loss=0.8400185704231262                                     \n",
      "epoch 20 [2.11s]: training loss=0.8267069458961487  validation ndcg@10=0.06950603836444534 [0.05s]\n",
      "epoch 21 [2.14s]:  training loss=0.8155734539031982                                     \n",
      "epoch 22 [2.07s]:  training loss=0.7988309860229492                                     \n",
      "epoch 23 [2.05s]:  training loss=0.7905957698822021                                     \n",
      "epoch 24 [2.01s]:  training loss=0.7787085175514221                                     \n",
      "epoch 25 [2.05s]: training loss=0.7695875763893127  validation ndcg@10=0.06742172797668908 [0.05s]\n",
      "epoch 26 [2.06s]:  training loss=0.7627302408218384                                     \n",
      "epoch 27 [2.06s]:  training loss=0.7543074488639832                                     \n",
      "epoch 28 [2.1s]:  training loss=0.7459591627120972                                      \n",
      "epoch 29 [2.06s]:  training loss=0.7387241125106812                                     \n",
      "epoch 30 [2.11s]: training loss=0.7323196530342102  validation ndcg@10=0.06585300832845385 [0.04s]\n",
      "epoch 31 [2.05s]:  training loss=0.7285774946212769                                     \n",
      "epoch 32 [2.09s]:  training loss=0.727069079875946                                      \n",
      "epoch 33 [2.18s]:  training loss=0.7192034125328064                                     \n",
      "epoch 34 [2.04s]:  training loss=0.7209779620170593                                     \n",
      "epoch 35 [2.04s]: training loss=0.7148182988166809  validation ndcg@10=0.06154632591071516 [0.04s]\n",
      "epoch 36 [2.0s]:  training loss=0.710186779499054                                       \n",
      "epoch 37 [2.04s]:  training loss=0.7041780352592468                                     \n",
      "epoch 38 [2.1s]:  training loss=0.7048665285110474                                      \n",
      "epoch 39 [2.01s]:  training loss=0.695803165435791                                      \n",
      "epoch 40 [2.07s]: training loss=0.6945688724517822  validation ndcg@10=0.0629973613344163 [0.04s]\n",
      "epoch 1 [19.28s]:  training loss=0.6420683860778809                                     \n",
      "epoch 2 [20.52s]:  training loss=0.4924597442150116                                     \n",
      "epoch 3 [21.12s]:  training loss=0.43654587864875793                                    \n",
      "epoch 4 [21.22s]:  training loss=0.4065285921096802                                     \n",
      "epoch 5 [21.03s]: training loss=0.38898780941963196  validation ndcg@10=0.05865899110247171 [0.4s]\n",
      "epoch 6 [20.17s]:  training loss=0.378969669342041                                      \n",
      "epoch 7 [21.01s]:  training loss=0.37462350726127625                                    \n",
      "epoch 8 [21.45s]:  training loss=0.3611747622489929                                     \n",
      "epoch 9 [21.5s]:  training loss=0.3563796281814575                                      \n",
      "epoch 10 [21.09s]: training loss=0.3564397096633911  validation ndcg@10=0.039828634513610336 [0.55s]\n",
      "epoch 11 [21.32s]:  training loss=0.3482452630996704                                    \n",
      "epoch 12 [21.12s]:  training loss=0.35375869274139404                                   \n",
      "epoch 13 [22.76s]:  training loss=0.35040169954299927                                   \n",
      "epoch 14 [20.7s]:  training loss=0.34604039788246155                                    \n",
      "epoch 15 [20.79s]: training loss=0.3516348898410797  validation ndcg@10=0.04591004566679631 [0.4s]\n",
      "epoch 16 [21.08s]:  training loss=0.34457671642303467                                   \n",
      "epoch 17 [21.0s]:  training loss=0.34439486265182495                                    \n",
      "epoch 18 [20.56s]:  training loss=0.34692397713661194                                   \n",
      "epoch 19 [20.0s]:  training loss=0.3458414077758789                                     \n",
      "epoch 20 [19.63s]: training loss=0.3478167653083801  validation ndcg@10=0.041694557053291205 [0.4s]\n",
      "epoch 21 [20.46s]:  training loss=0.33835268020629883                                   \n",
      "epoch 22 [19.95s]:  training loss=0.34429726004600525                                   \n",
      "epoch 23 [19.66s]:  training loss=0.3398245871067047                                    \n",
      "epoch 24 [21.12s]:  training loss=0.3453129529953003                                    \n",
      "epoch 25 [21.06s]: training loss=0.32966116070747375  validation ndcg@10=0.03182295160663743 [0.35s]\n",
      "epoch 26 [21.1s]:  training loss=0.3348615765571594                                     \n",
      "epoch 27 [20.45s]:  training loss=0.3434737026691437                                    \n",
      "epoch 28 [20.35s]:  training loss=0.3436022102832794                                    \n",
      "epoch 29 [20.55s]:  training loss=0.34223130345344543                                   \n",
      "epoch 30 [20.74s]: training loss=0.34001725912094116  validation ndcg@10=0.0399723123536507 [0.39s]\n",
      "epoch 1 [4.02s]:  training loss=0.9836450219154358                                      \n",
      "epoch 2 [3.65s]:  training loss=0.9812831878662109                                      \n",
      "epoch 3 [3.61s]:  training loss=0.9763713479042053                                      \n",
      "epoch 4 [3.58s]:  training loss=0.9743478298187256                                      \n",
      "epoch 5 [3.57s]: training loss=0.9697436690330505  validation ndcg@10=0.019324541975352452 [0.06s]\n",
      "epoch 6 [3.52s]:  training loss=0.9676122069358826                                      \n",
      "epoch 7 [3.48s]:  training loss=0.9648252129554749                                      \n",
      "epoch 8 [3.49s]:  training loss=0.9599682688713074                                      \n",
      "epoch 9 [3.5s]:  training loss=0.9557709693908691                                       \n",
      "epoch 10 [3.55s]: training loss=0.9521392583847046  validation ndcg@10=0.026184938051604337 [0.07s]\n",
      "epoch 11 [3.58s]:  training loss=0.9481862187385559                                     \n",
      "epoch 12 [3.57s]:  training loss=0.945726752281189                                      \n",
      "epoch 13 [3.6s]:  training loss=0.9391961693763733                                      \n",
      "epoch 14 [3.54s]:  training loss=0.9371605515480042                                     \n",
      "epoch 15 [3.67s]: training loss=0.9325999021530151  validation ndcg@10=0.03415956851276099 [0.07s]\n",
      "epoch 16 [3.58s]:  training loss=0.9280450344085693                                     \n",
      "epoch 17 [3.74s]:  training loss=0.9227734208106995                                     \n",
      "epoch 18 [3.83s]:  training loss=0.9203686714172363                                     \n",
      "epoch 19 [3.75s]:  training loss=0.9155812859535217                                     \n",
      "epoch 20 [3.74s]: training loss=0.9104248285293579  validation ndcg@10=0.03396191740661393 [0.08s]\n",
      "epoch 21 [3.56s]:  training loss=0.9052386283874512                                     \n",
      "epoch 22 [3.6s]:  training loss=0.9022537469863892                                      \n",
      "epoch 23 [3.59s]:  training loss=0.8986842632293701                                     \n",
      "epoch 24 [3.57s]:  training loss=0.891522228717804                                      \n",
      "epoch 25 [3.74s]: training loss=0.889126718044281  validation ndcg@10=0.04190062951744681 [0.07s]\n",
      "epoch 26 [3.57s]:  training loss=0.8839127421379089                                     \n",
      "epoch 27 [3.64s]:  training loss=0.8782869577407837                                     \n",
      "epoch 28 [3.8s]:  training loss=0.872568666934967                                       \n",
      "epoch 29 [3.6s]:  training loss=0.867884635925293                                       \n",
      "epoch 30 [3.62s]: training loss=0.8615464568138123  validation ndcg@10=0.045771362203145954 [0.07s]\n",
      "epoch 31 [3.55s]:  training loss=0.857304573059082                                      \n",
      "epoch 32 [3.67s]:  training loss=0.851377010345459                                      \n",
      "epoch 33 [3.61s]:  training loss=0.8465786576271057                                     \n",
      "epoch 34 [3.54s]:  training loss=0.8401334881782532                                     \n",
      "epoch 35 [3.62s]: training loss=0.8356085419654846  validation ndcg@10=0.053724391368078835 [0.06s]\n",
      "epoch 36 [3.52s]:  training loss=0.825671374797821                                      \n",
      "epoch 37 [3.5s]:  training loss=0.8204281330108643                                      \n",
      "epoch 38 [3.56s]:  training loss=0.8169914484024048                                     \n",
      "epoch 39 [3.52s]:  training loss=0.8037629127502441                                     \n",
      "epoch 40 [3.56s]: training loss=0.7972295880317688  validation ndcg@10=0.057665878619427784 [0.06s]\n",
      "epoch 41 [3.61s]:  training loss=0.7878401279449463                                     \n",
      "epoch 42 [3.74s]:  training loss=0.7769095301628113                                     \n",
      "epoch 43 [3.55s]:  training loss=0.7666546702384949                                     \n",
      "epoch 44 [3.56s]:  training loss=0.7588769793510437                                     \n",
      "epoch 45 [3.7s]: training loss=0.7526259422302246  validation ndcg@10=0.05531306203126509 [0.07s]\n",
      "epoch 46 [3.57s]:  training loss=0.7458745241165161                                     \n",
      "epoch 47 [3.59s]:  training loss=0.7400643825531006                                     \n",
      "epoch 48 [3.75s]:  training loss=0.728533148765564                                      \n",
      "epoch 49 [3.59s]:  training loss=0.7253135442733765                                     \n",
      "epoch 50 [3.52s]: training loss=0.7205392122268677  validation ndcg@10=0.057399640313486724 [0.07s]\n",
      "epoch 51 [3.63s]:  training loss=0.7167324423789978                                     \n",
      "epoch 52 [3.56s]:  training loss=0.7159876227378845                                     \n",
      "epoch 53 [3.55s]:  training loss=0.7044596672058105                                     \n",
      "epoch 54 [3.52s]:  training loss=0.7069897055625916                                     \n",
      "epoch 55 [3.57s]: training loss=0.7000165581703186  validation ndcg@10=0.05677074151266358 [0.06s]\n",
      "epoch 56 [3.52s]:  training loss=0.702105700969696                                      \n",
      "epoch 57 [3.53s]:  training loss=0.6960665583610535                                     \n",
      "epoch 58 [3.54s]:  training loss=0.6923229098320007                                     \n",
      "epoch 59 [3.57s]:  training loss=0.6941900849342346                                     \n",
      "epoch 60 [3.58s]: training loss=0.6911768317222595  validation ndcg@10=0.05873788247837578 [0.07s]\n",
      "epoch 61 [3.64s]:  training loss=0.6869664192199707                                     \n",
      "epoch 62 [3.57s]:  training loss=0.6824299693107605                                     \n",
      "epoch 63 [3.62s]:  training loss=0.6783609986305237                                     \n",
      "epoch 64 [3.61s]:  training loss=0.6813034415245056                                     \n",
      "epoch 65 [3.6s]: training loss=0.6741570830345154  validation ndcg@10=0.05846050462997835 [0.06s]\n",
      "epoch 66 [3.56s]:  training loss=0.680176317691803                                      \n",
      "epoch 67 [3.6s]:  training loss=0.6739091277122498                                      \n",
      "epoch 68 [5.43s]:  training loss=0.6713033318519592                                     \n",
      "epoch 69 [3.57s]:  training loss=0.6646788716316223                                     \n",
      "epoch 70 [3.64s]: training loss=0.6672257781028748  validation ndcg@10=0.05964663789900743 [0.07s]\n",
      "epoch 71 [3.58s]:  training loss=0.6670727133750916                                     \n",
      "epoch 72 [3.6s]:  training loss=0.6626602411270142                                      \n",
      "epoch 73 [3.59s]:  training loss=0.6582618951797485                                     \n",
      "epoch 74 [3.61s]:  training loss=0.6616308689117432                                     \n",
      "epoch 75 [3.54s]: training loss=0.661336362361908  validation ndcg@10=0.05995119291574046 [0.07s]\n",
      "epoch 76 [3.59s]:  training loss=0.6557823419570923                                     \n",
      "epoch 77 [3.51s]:  training loss=0.6565104126930237                                     \n",
      "epoch 78 [3.54s]:  training loss=0.6547746062278748                                     \n",
      "epoch 79 [3.55s]:  training loss=0.6502780914306641                                     \n",
      "epoch 80 [3.65s]: training loss=0.6497965455055237  validation ndcg@10=0.0604182194981821 [0.06s]\n",
      "epoch 81 [3.55s]:  training loss=0.647304117679596                                      \n",
      "epoch 82 [3.6s]:  training loss=0.6449712514877319                                      \n",
      "epoch 83 [3.51s]:  training loss=0.6507532596588135                                     \n",
      "epoch 84 [3.51s]:  training loss=0.6424060463905334                                     \n",
      "epoch 85 [3.55s]: training loss=0.6393051147460938  validation ndcg@10=0.060055591221972654 [0.06s]\n",
      "epoch 86 [3.55s]:  training loss=0.6411646008491516                                     \n",
      "epoch 87 [3.52s]:  training loss=0.6407572031021118                                     \n",
      "epoch 88 [3.51s]:  training loss=0.6359521746635437                                     \n",
      "epoch 89 [3.55s]:  training loss=0.6398591995239258                                     \n",
      "epoch 90 [3.62s]: training loss=0.6361286044120789  validation ndcg@10=0.06389415046965448 [0.06s]\n",
      "epoch 91 [3.56s]:  training loss=0.6293644905090332                                     \n",
      "epoch 92 [3.72s]:  training loss=0.6306688189506531                                     \n",
      "epoch 93 [3.62s]:  training loss=0.6261001825332642                                     \n",
      "epoch 94 [3.72s]:  training loss=0.6283206343650818                                     \n",
      "epoch 95 [3.61s]: training loss=0.6249237060546875  validation ndcg@10=0.06482352444441761 [0.06s]\n",
      "epoch 96 [3.56s]:  training loss=0.6181333661079407                                     \n",
      "epoch 97 [3.75s]:  training loss=0.6211792230606079                                     \n",
      "epoch 98 [3.65s]:  training loss=0.619853138923645                                      \n",
      "epoch 99 [3.65s]:  training loss=0.6194794774055481                                     \n",
      "epoch 100 [3.53s]: training loss=0.6172637343406677  validation ndcg@10=0.06518861118083316 [0.07s]\n",
      "epoch 101 [3.55s]:  training loss=0.6146284341812134                                    \n",
      "epoch 102 [3.55s]:  training loss=0.6189610362052917                                    \n",
      "epoch 103 [3.63s]:  training loss=0.6113885641098022                                    \n",
      "epoch 104 [3.47s]:  training loss=0.6077826619148254                                    \n",
      "epoch 105 [3.59s]: training loss=0.6127051711082458  validation ndcg@10=0.06517150177264362 [0.06s]\n",
      "epoch 106 [3.56s]:  training loss=0.6083253026008606                                    \n",
      "epoch 107 [3.6s]:  training loss=0.6032599806785583                                     \n",
      "epoch 108 [3.52s]:  training loss=0.6073072552680969                                    \n",
      "epoch 109 [3.58s]:  training loss=0.6070328950881958                                    \n",
      "epoch 110 [3.53s]: training loss=0.6043737530708313  validation ndcg@10=0.06585347831128162 [0.06s]\n",
      "epoch 111 [3.57s]:  training loss=0.599277675151825                                     \n",
      "epoch 112 [3.54s]:  training loss=0.5993510484695435                                    \n",
      "epoch 113 [3.79s]:  training loss=0.5999569892883301                                    \n",
      "epoch 114 [3.78s]:  training loss=0.6015889048576355                                    \n",
      "epoch 115 [3.77s]: training loss=0.5962029695510864  validation ndcg@10=0.06580172538740502 [0.07s]\n",
      "epoch 116 [3.76s]:  training loss=0.5969517230987549                                    \n",
      "epoch 117 [3.66s]:  training loss=0.5932959318161011                                    \n",
      "epoch 118 [3.59s]:  training loss=0.5958037972450256                                    \n",
      "epoch 119 [3.58s]:  training loss=0.5928454995155334                                    \n",
      "epoch 120 [3.54s]: training loss=0.589400589466095  validation ndcg@10=0.06547538253492016 [0.07s]\n",
      "epoch 121 [3.56s]:  training loss=0.5929627418518066                                    \n",
      "epoch 122 [3.52s]:  training loss=0.5864772200584412                                    \n",
      "epoch 123 [3.58s]:  training loss=0.5860041975975037                                    \n",
      "epoch 124 [3.58s]:  training loss=0.5865134596824646                                    \n",
      "epoch 125 [3.55s]: training loss=0.5861868858337402  validation ndcg@10=0.0653570429766715 [0.06s]\n",
      "epoch 126 [3.56s]:  training loss=0.5836698412895203                                    \n",
      "epoch 127 [3.51s]:  training loss=0.5824905037879944                                    \n",
      "epoch 128 [3.56s]:  training loss=0.5797181725502014                                    \n",
      "epoch 129 [3.7s]:  training loss=0.5806817412376404                                     \n",
      "epoch 130 [3.52s]: training loss=0.5787690877914429  validation ndcg@10=0.06617165013130406 [0.08s]\n",
      "epoch 131 [3.55s]:  training loss=0.5764127373695374                                    \n",
      "epoch 132 [3.54s]:  training loss=0.5702048540115356                                    \n",
      "epoch 133 [3.63s]:  training loss=0.5775511264801025                                    \n",
      "epoch 134 [3.55s]:  training loss=0.5745702385902405                                    \n",
      "epoch 135 [3.52s]: training loss=0.5766372680664062  validation ndcg@10=0.06795631609514745 [0.06s]\n",
      "epoch 136 [3.52s]:  training loss=0.5727138519287109                                    \n",
      "epoch 137 [3.65s]:  training loss=0.5760018229484558                                    \n",
      "epoch 138 [3.52s]:  training loss=0.5696462392807007                                    \n",
      "epoch 139 [3.6s]:  training loss=0.5663335919380188                                     \n",
      "epoch 140 [3.63s]: training loss=0.569491446018219  validation ndcg@10=0.06711116066210046 [0.06s]\n",
      "epoch 141 [3.53s]:  training loss=0.5672702789306641                                    \n",
      "epoch 142 [3.6s]:  training loss=0.567700982093811                                      \n",
      "epoch 143 [3.69s]:  training loss=0.5649597644805908                                    \n",
      "epoch 144 [3.55s]:  training loss=0.5649389624595642                                    \n",
      "epoch 145 [3.67s]: training loss=0.5660107135772705  validation ndcg@10=0.06640041124937489 [0.06s]\n",
      "epoch 146 [3.62s]:  training loss=0.5646893978118896                                    \n",
      "epoch 147 [3.56s]:  training loss=0.5602399706840515                                    \n",
      "epoch 148 [3.62s]:  training loss=0.5597227811813354                                    \n",
      "epoch 149 [3.59s]:  training loss=0.5591181516647339                                    \n",
      "epoch 150 [3.59s]: training loss=0.5568460822105408  validation ndcg@10=0.06714511123025521 [0.06s]\n",
      "epoch 151 [3.59s]:  training loss=0.5591145753860474                                    \n",
      "epoch 152 [3.54s]:  training loss=0.5588658452033997                                    \n",
      "epoch 153 [3.54s]:  training loss=0.5575386881828308                                    \n",
      "epoch 154 [3.55s]:  training loss=0.5535210967063904                                    \n",
      "epoch 155 [3.54s]: training loss=0.5548455715179443  validation ndcg@10=0.06624787262980009 [0.06s]\n",
      "epoch 156 [3.55s]:  training loss=0.5540683269500732                                    \n",
      "epoch 157 [3.54s]:  training loss=0.5512910485267639                                    \n",
      "epoch 158 [3.49s]:  training loss=0.5539159178733826                                    \n",
      "epoch 159 [3.57s]:  training loss=0.5528742074966431                                    \n",
      "epoch 160 [3.61s]: training loss=0.5470293760299683  validation ndcg@10=0.0656397735070877 [0.07s]\n",
      "epoch 1 [12.42s]:  training loss=0.6876878142356873                                     \n",
      "epoch 2 [12.42s]:  training loss=0.5077196955680847                                   \n",
      "epoch 3 [12.16s]:  training loss=0.4546816051006317                                   \n",
      "epoch 4 [12.07s]:  training loss=0.4095005691051483                                   \n",
      "epoch 5 [12.38s]: training loss=0.38494884967803955  validation ndcg@10=0.06582504861492523 [0.14s]\n",
      "epoch 6 [12.06s]:  training loss=0.36729034781455994                                  \n",
      "epoch 7 [12.07s]:  training loss=0.36075079441070557                                  \n",
      "epoch 8 [12.2s]:  training loss=0.35303065180778503                                   \n",
      "epoch 9 [12.12s]:  training loss=0.3426399528980255                                   \n",
      "epoch 10 [12.1s]: training loss=0.34149298071861267  validation ndcg@10=0.05305291996945295 [0.16s]\n",
      "epoch 11 [12.07s]:  training loss=0.34055057168006897                                 \n",
      "epoch 12 [12.23s]:  training loss=0.33702462911605835                                 \n",
      "epoch 13 [12.05s]:  training loss=0.3251804709434509                                  \n",
      "epoch 14 [12.11s]:  training loss=0.33038949966430664                                 \n",
      "epoch 15 [12.08s]: training loss=0.3242362141609192  validation ndcg@10=0.04374551426104089 [0.18s]\n",
      "epoch 16 [12.12s]:  training loss=0.32480183243751526                                 \n",
      "epoch 17 [12.06s]:  training loss=0.3171781003475189                                  \n",
      "epoch 18 [12.12s]:  training loss=0.3178076446056366                                  \n",
      "epoch 19 [11.96s]:  training loss=0.31847190856933594                                 \n",
      "epoch 20 [12.27s]: training loss=0.31744492053985596  validation ndcg@10=0.05426511283271738 [0.15s]\n",
      "epoch 21 [13.92s]:  training loss=0.3176114559173584                                  \n",
      "epoch 22 [12.06s]:  training loss=0.3156149089336395                                  \n",
      "epoch 23 [12.22s]:  training loss=0.3124610483646393                                  \n",
      "epoch 24 [12.12s]:  training loss=0.3171115517616272                                  \n",
      "epoch 25 [12.18s]: training loss=0.3132307231426239  validation ndcg@10=0.055113742647580156 [0.18s]\n",
      "epoch 26 [12.17s]:  training loss=0.3149401843547821                                  \n",
      "epoch 27 [12.21s]:  training loss=0.3108058571815491                                  \n",
      "epoch 28 [12.06s]:  training loss=0.3070455491542816                                  \n",
      "epoch 29 [12.18s]:  training loss=0.304890900850296                                   \n",
      "epoch 30 [12.1s]: training loss=0.30605241656303406  validation ndcg@10=0.0465545459594262 [0.17s]\n",
      "epoch 1 [21.43s]:  training loss=0.756345808506012                                    \n",
      "epoch 2 [20.85s]:  training loss=0.5417289137840271                                   \n",
      "epoch 3 [20.37s]:  training loss=0.5046358704566956                                   \n",
      "epoch 4 [20.48s]:  training loss=0.46538111567497253                                  \n",
      "epoch 5 [20.29s]: training loss=0.43383628129959106  validation ndcg@10=0.07939801730340713 [0.28s]\n",
      "epoch 6 [20.39s]:  training loss=0.4053760766983032                                   \n",
      "epoch 7 [20.47s]:  training loss=0.3860992193222046                                   \n",
      "epoch 8 [20.27s]:  training loss=0.3659401535987854                                   \n",
      "epoch 9 [20.32s]:  training loss=0.3642411530017853                                   \n",
      "epoch 10 [20.45s]: training loss=0.35071662068367004  validation ndcg@10=0.0649075606221638 [0.29s]\n",
      "epoch 11 [20.35s]:  training loss=0.3467293679714203                                  \n",
      "epoch 12 [20.28s]:  training loss=0.34324875473976135                                 \n",
      "epoch 13 [20.49s]:  training loss=0.33858832716941833                                 \n",
      "epoch 14 [20.61s]:  training loss=0.33604443073272705                                 \n",
      "epoch 15 [20.46s]: training loss=0.32958531379699707  validation ndcg@10=0.05589571354215341 [0.29s]\n",
      "epoch 16 [20.48s]:  training loss=0.3309541940689087                                  \n",
      "epoch 17 [20.33s]:  training loss=0.3256344497203827                                  \n",
      "epoch 18 [20.38s]:  training loss=0.32448115944862366                                 \n",
      "epoch 19 [20.41s]:  training loss=0.32160860300064087                                 \n",
      "epoch 20 [20.39s]: training loss=0.3229466378688812  validation ndcg@10=0.0491862072061132 [0.33s]\n",
      "epoch 21 [20.42s]:  training loss=0.3095603287220001                                  \n",
      "epoch 22 [20.24s]:  training loss=0.3078851103782654                                  \n",
      "epoch 23 [20.31s]:  training loss=0.3140321373939514                                  \n",
      "epoch 24 [22.2s]:  training loss=0.3112427294254303                                   \n",
      "epoch 25 [20.71s]: training loss=0.31448304653167725  validation ndcg@10=0.05973546900967178 [0.27s]\n",
      "epoch 26 [20.55s]:  training loss=0.30927491188049316                                 \n",
      "epoch 27 [20.67s]:  training loss=0.31125083565711975                                 \n",
      "epoch 28 [20.99s]:  training loss=0.3098812699317932                                  \n",
      "epoch 29 [20.55s]:  training loss=0.30548056960105896                                 \n",
      "epoch 30 [20.32s]: training loss=0.3061642646789551  validation ndcg@10=0.05492810469324936 [0.33s]\n",
      "epoch 1 [22.19s]:  training loss=0.8013060092926025                                   \n",
      "epoch 2 [25.16s]:  training loss=0.5671524405479431                                   \n",
      "epoch 3 [23.94s]:  training loss=0.5178353786468506                                   \n",
      "epoch 4 [25.02s]:  training loss=0.4908309578895569                                   \n",
      "epoch 5 [24.91s]: training loss=0.45968255400657654  validation ndcg@10=0.07212124767055561 [0.3s]\n",
      "epoch 6 [24.6s]:  training loss=0.4296945631504059                                    \n",
      "epoch 7 [25.01s]:  training loss=0.4061009883880615                                   \n",
      "epoch 8 [24.64s]:  training loss=0.3889428973197937                                   \n",
      "epoch 9 [25.15s]:  training loss=0.372728168964386                                    \n",
      "epoch 10 [24.98s]: training loss=0.3647388815879822  validation ndcg@10=0.06051850450197098 [0.34s]\n",
      "epoch 11 [24.28s]:  training loss=0.3611489534378052                                  \n",
      "epoch 12 [25.33s]:  training loss=0.3532896935939789                                  \n",
      "epoch 13 [24.61s]:  training loss=0.3489316701889038                                  \n",
      "epoch 14 [24.7s]:  training loss=0.3491632640361786                                   \n",
      "epoch 15 [24.49s]: training loss=0.33976900577545166  validation ndcg@10=0.07306761126310697 [0.29s]\n",
      "epoch 16 [24.73s]:  training loss=0.3349683880805969                                  \n",
      "epoch 17 [24.8s]:  training loss=0.34053733944892883                                  \n",
      "epoch 18 [24.01s]:  training loss=0.3329785466194153                                  \n",
      "epoch 19 [24.8s]:  training loss=0.329240620136261                                    \n",
      "epoch 20 [27.05s]: training loss=0.3258623480796814  validation ndcg@10=0.04901224737769071 [0.3s]\n",
      "epoch 21 [25.19s]:  training loss=0.3219354450702667                                  \n",
      "epoch 22 [25.17s]:  training loss=0.3232609033584595                                  \n",
      "epoch 23 [25.78s]:  training loss=0.3234667181968689                                  \n",
      "epoch 24 [25.0s]:  training loss=0.31899508833885193                                  \n",
      "epoch 25 [25.77s]: training loss=0.3187122046947479  validation ndcg@10=0.04637911439121108 [0.34s]\n",
      "epoch 26 [25.81s]:  training loss=0.31228405237197876                                 \n",
      "epoch 27 [25.95s]:  training loss=0.3136928379535675                                  \n",
      "epoch 28 [25.55s]:  training loss=0.3143670856952667                                  \n",
      "epoch 29 [26.11s]:  training loss=0.30911415815353394                                 \n",
      "epoch 30 [25.22s]: training loss=0.3067198097705841  validation ndcg@10=0.05263557970354237 [0.34s]\n",
      "epoch 31 [25.4s]:  training loss=0.3119382858276367                                   \n",
      "epoch 32 [25.36s]:  training loss=0.3036130666732788                                  \n",
      "epoch 33 [25.11s]:  training loss=0.3030318319797516                                  \n",
      "epoch 34 [25.67s]:  training loss=0.30758363008499146                                 \n",
      "epoch 35 [25.16s]: training loss=0.30350762605667114  validation ndcg@10=0.061842099959334984 [0.36s]\n",
      "epoch 36 [25.18s]:  training loss=0.30772578716278076                                 \n",
      "epoch 37 [25.53s]:  training loss=0.3034338355064392                                  \n",
      "epoch 38 [25.44s]:  training loss=0.2993071675300598                                  \n",
      "epoch 39 [25.82s]:  training loss=0.29529544711112976                                 \n",
      "epoch 40 [25.37s]: training loss=0.3008071780204773  validation ndcg@10=0.03907022685911573 [0.36s]\n",
      "epoch 1 [9.57s]:  training loss=0.9827003479003906                                    \n",
      "epoch 2 [8.77s]:  training loss=0.9762710928916931                                    \n",
      "epoch 3 [8.66s]:  training loss=0.9712141156196594                                    \n",
      "epoch 4 [9.52s]:  training loss=0.9646338820457458                                    \n",
      "epoch 5 [9.25s]: training loss=0.9580675363540649  validation ndcg@10=0.034241006960597904 [0.15s]\n",
      "epoch 6 [8.99s]:  training loss=0.9527688026428223                                    \n",
      "epoch 7 [8.62s]:  training loss=0.9443004727363586                                    \n",
      "epoch 8 [9.15s]:  training loss=0.9372433423995972                                    \n",
      "epoch 9 [8.81s]:  training loss=0.9312543272972107                                    \n",
      "epoch 10 [9.08s]: training loss=0.9211164116859436  validation ndcg@10=0.051844364592590536 [0.15s]\n",
      "epoch 11 [9.17s]:  training loss=0.9147287607192993                                   \n",
      "epoch 12 [8.71s]:  training loss=0.9077097177505493                                   \n",
      "epoch 13 [9.0s]:  training loss=0.8996924757957458                                    \n",
      "epoch 14 [8.55s]:  training loss=0.8904910087585449                                   \n",
      "epoch 15 [9.28s]: training loss=0.8841677904129028  validation ndcg@10=0.05615407589679912 [0.16s]\n",
      "epoch 16 [8.94s]:  training loss=0.8768655061721802                                   \n",
      "epoch 17 [8.82s]:  training loss=0.8686172962188721                                   \n",
      "epoch 18 [8.86s]:  training loss=0.8572525978088379                                   \n",
      "epoch 19 [8.75s]:  training loss=0.846962571144104                                    \n",
      "epoch 20 [8.99s]: training loss=0.8385269641876221  validation ndcg@10=0.06152554528144113 [0.2s]\n",
      "epoch 21 [10.99s]:  training loss=0.8303273320198059                                  \n",
      "epoch 22 [8.58s]:  training loss=0.8215752243995667                                   \n",
      "epoch 23 [8.67s]:  training loss=0.809740424156189                                    \n",
      "epoch 24 [8.97s]:  training loss=0.8042469024658203                                   \n",
      "epoch 25 [9.23s]: training loss=0.7928698658943176  validation ndcg@10=0.06633755041160809 [0.2s]\n",
      "epoch 26 [8.88s]:  training loss=0.7808516025543213                                   \n",
      "epoch 27 [8.21s]:  training loss=0.7707266211509705                                   \n",
      "epoch 28 [8.84s]:  training loss=0.765419065952301                                    \n",
      "epoch 29 [9.1s]:  training loss=0.7556842565536499                                    \n",
      "epoch 30 [9.07s]: training loss=0.741753876209259  validation ndcg@10=0.06675002875982784 [0.16s]\n",
      "epoch 31 [8.99s]:  training loss=0.7333690524101257                                   \n",
      "epoch 32 [9.02s]:  training loss=0.7208719253540039                                   \n",
      "epoch 33 [8.62s]:  training loss=0.7144826650619507                                   \n",
      "epoch 34 [8.92s]:  training loss=0.7025635838508606                                   \n",
      "epoch 35 [8.74s]: training loss=0.6943416595458984  validation ndcg@10=0.06577091310122464 [0.15s]\n",
      "epoch 36 [8.89s]:  training loss=0.6849043965339661                                   \n",
      "epoch 37 [9.26s]:  training loss=0.6730676889419556                                   \n",
      "epoch 38 [8.72s]:  training loss=0.6690489053726196                                   \n",
      "epoch 39 [8.69s]:  training loss=0.6611869931221008                                   \n",
      "epoch 40 [8.65s]: training loss=0.6488876342773438  validation ndcg@10=0.06519019646802018 [0.16s]\n",
      "epoch 41 [9.15s]:  training loss=0.6427743434906006                                   \n",
      "epoch 42 [9.2s]:  training loss=0.6380699872970581                                    \n",
      "epoch 43 [8.84s]:  training loss=0.6340596079826355                                   \n",
      "epoch 44 [8.88s]:  training loss=0.6269699335098267                                   \n",
      "epoch 45 [8.9s]: training loss=0.6217072606086731  validation ndcg@10=0.0663734406112662 [0.17s]\n",
      "epoch 46 [8.87s]:  training loss=0.6228387355804443                                   \n",
      "epoch 47 [8.86s]:  training loss=0.6124046444892883                                   \n",
      "epoch 48 [8.85s]:  training loss=0.6124200820922852                                   \n",
      "epoch 49 [8.88s]:  training loss=0.6085246801376343                                   \n",
      "epoch 50 [8.84s]: training loss=0.6064724922180176  validation ndcg@10=0.06710403313221064 [0.16s]\n",
      "epoch 51 [8.86s]:  training loss=0.6042425036430359                                   \n",
      "epoch 52 [8.86s]:  training loss=0.5951961278915405                                   \n",
      "epoch 53 [9.05s]:  training loss=0.5926998853683472                                   \n",
      "epoch 54 [8.55s]:  training loss=0.5924791097640991                                   \n",
      "epoch 55 [8.74s]: training loss=0.5919965505599976  validation ndcg@10=0.06790430643295908 [0.18s]\n",
      "epoch 56 [8.73s]:  training loss=0.590571403503418                                    \n",
      "epoch 57 [8.69s]:  training loss=0.579561710357666                                    \n",
      "epoch 58 [8.73s]:  training loss=0.5798066854476929                                   \n",
      "epoch 59 [8.68s]:  training loss=0.5801175236701965                                   \n",
      "epoch 60 [8.67s]: training loss=0.5686945915222168  validation ndcg@10=0.06911267124408076 [0.16s]\n",
      "epoch 61 [8.79s]:  training loss=0.5761083364486694                                   \n",
      "epoch 62 [9.35s]:  training loss=0.5674835443496704                                   \n",
      "epoch 63 [8.92s]:  training loss=0.5672645568847656                                   \n",
      "epoch 64 [9.14s]:  training loss=0.5677841305732727                                   \n",
      "epoch 65 [8.83s]: training loss=0.5599461197853088  validation ndcg@10=0.06727168543453303 [0.19s]\n",
      "epoch 66 [9.03s]:  training loss=0.5670290589332581                                   \n",
      "epoch 67 [9.45s]:  training loss=0.5593026280403137                                   \n",
      "epoch 68 [8.88s]:  training loss=0.5600000619888306                                   \n",
      "epoch 69 [8.92s]:  training loss=0.5542090535163879                                   \n",
      "epoch 70 [8.94s]: training loss=0.551777720451355  validation ndcg@10=0.06886558913542729 [0.15s]\n",
      "epoch 71 [8.98s]:  training loss=0.5510138273239136                                   \n",
      "epoch 72 [9.0s]:  training loss=0.5534931421279907                                    \n",
      "epoch 73 [9.57s]:  training loss=0.5527177453041077                                   \n",
      "epoch 74 [8.65s]:  training loss=0.549950122833252                                    \n",
      "epoch 75 [9.09s]: training loss=0.5465331673622131  validation ndcg@10=0.06664466878767415 [0.17s]\n",
      "epoch 76 [9.1s]:  training loss=0.5443955659866333                                    \n",
      "epoch 77 [8.57s]:  training loss=0.5414457321166992                                   \n",
      "epoch 78 [8.93s]:  training loss=0.5374791622161865                                   \n",
      "epoch 79 [9.05s]:  training loss=0.5373855233192444                                   \n",
      "epoch 80 [8.92s]: training loss=0.5351577401161194  validation ndcg@10=0.0657291270330237 [0.2s]\n",
      "epoch 81 [8.87s]:  training loss=0.5355700850486755                                   \n",
      "epoch 82 [8.69s]:  training loss=0.5361385941505432                                   \n",
      "epoch 83 [9.0s]:  training loss=0.5368849039077759                                    \n",
      "epoch 84 [8.74s]:  training loss=0.5369372367858887                                   \n",
      "epoch 85 [8.76s]: training loss=0.5309408903121948  validation ndcg@10=0.0668230319976777 [0.16s]\n",
      "epoch 1 [5.87s]:  training loss=0.8980274200439453                                    \n",
      "epoch 2 [5.65s]:  training loss=0.6846078634262085                                    \n",
      "epoch 3 [5.64s]:  training loss=0.5745711922645569                                    \n",
      "epoch 4 [5.65s]:  training loss=0.5358926057815552                                    \n",
      "epoch 5 [5.81s]: training loss=0.5184754133224487  validation ndcg@10=0.06503034957043707 [0.17s]\n",
      "epoch 6 [5.74s]:  training loss=0.501731812953949                                     \n",
      "epoch 7 [5.92s]:  training loss=0.4850795567035675                                    \n",
      "epoch 8 [5.81s]:  training loss=0.470164030790329                                     \n",
      "epoch 9 [5.86s]:  training loss=0.4583924412727356                                    \n",
      "epoch 10 [5.83s]: training loss=0.4432741701602936  validation ndcg@10=0.0716355199799718 [0.1s]\n",
      "epoch 11 [5.72s]:  training loss=0.42793288826942444                                  \n",
      "epoch 12 [5.69s]:  training loss=0.41223034262657166                                  \n",
      "epoch 13 [7.63s]:  training loss=0.4012710154056549                                   \n",
      "epoch 14 [5.82s]:  training loss=0.3846871852874756                                   \n",
      "epoch 15 [5.77s]: training loss=0.3805069327354431  validation ndcg@10=0.06758708866818078 [0.1s]\n",
      "epoch 16 [5.72s]:  training loss=0.3690187633037567                                   \n",
      "epoch 17 [5.69s]:  training loss=0.3634412884712219                                   \n",
      "epoch 18 [5.6s]:  training loss=0.36294856667518616                                   \n",
      "epoch 19 [5.68s]:  training loss=0.35816219449043274                                  \n",
      "epoch 20 [5.66s]: training loss=0.35005176067352295  validation ndcg@10=0.06369916067105513 [0.1s]\n",
      "epoch 21 [5.62s]:  training loss=0.34950459003448486                                  \n",
      "epoch 22 [5.62s]:  training loss=0.34375298023223877                                  \n",
      "epoch 23 [5.66s]:  training loss=0.3430515229701996                                   \n",
      "epoch 24 [5.58s]:  training loss=0.3380434513092041                                   \n",
      "epoch 25 [5.61s]: training loss=0.33379459381103516  validation ndcg@10=0.06056755613833288 [0.09s]\n",
      "epoch 26 [5.71s]:  training loss=0.3358149230480194                                   \n",
      "epoch 27 [5.71s]:  training loss=0.3322654962539673                                   \n",
      "epoch 28 [5.7s]:  training loss=0.3336666524410248                                    \n",
      "epoch 29 [5.75s]:  training loss=0.32429736852645874                                  \n",
      "epoch 30 [5.63s]: training loss=0.3227064311504364  validation ndcg@10=0.05514382444515057 [0.11s]\n",
      "epoch 31 [5.73s]:  training loss=0.3293100595474243                                   \n",
      "epoch 32 [5.56s]:  training loss=0.32355692982673645                                  \n",
      "epoch 33 [5.74s]:  training loss=0.3221065402030945                                   \n",
      "epoch 34 [5.75s]:  training loss=0.3193510174751282                                   \n",
      "epoch 35 [5.74s]: training loss=0.3148176968097687  validation ndcg@10=0.05708940803693578 [0.1s]\n",
      "epoch 1 [35.62s]:  training loss=0.9264622926712036                                   \n",
      "epoch 2 [36.38s]:  training loss=0.7864282727241516                                   \n",
      "epoch 3 [37.6s]:  training loss=0.6621469855308533                                    \n",
      "epoch 4 [38.41s]:  training loss=0.5908844470977783                                   \n",
      "epoch 5 [38.27s]: training loss=0.556789755821228  validation ndcg@10=0.0640972547012563 [0.49s]\n",
      "epoch 6 [37.56s]:  training loss=0.5348284840583801                                   \n",
      "epoch 7 [36.82s]:  training loss=0.5202405452728271                                   \n",
      "epoch 8 [36.45s]:  training loss=0.5083056092262268                                   \n",
      "epoch 9 [36.06s]:  training loss=0.5003882050514221                                   \n",
      "epoch 10 [37.07s]: training loss=0.47807249426841736  validation ndcg@10=0.055302757857562994 [0.46s]\n",
      "epoch 11 [38.14s]:  training loss=0.4620741009712219                                  \n",
      "epoch 12 [38.01s]:  training loss=0.45291945338249207                                 \n",
      "epoch 13 [38.54s]:  training loss=0.44754013419151306                                 \n",
      "epoch 14 [38.97s]:  training loss=0.43644967675209045                                 \n",
      "epoch 15 [37.32s]: training loss=0.43300512433052063  validation ndcg@10=0.06442139184743804 [0.51s]\n",
      "epoch 16 [36.7s]:  training loss=0.4321480691432953                                   \n",
      "epoch 17 [37.2s]:  training loss=0.4244800806045532                                   \n",
      "epoch 18 [37.82s]:  training loss=0.41960254311561584                                 \n",
      "epoch 19 [38.38s]:  training loss=0.4147937595844269                                  \n",
      "epoch 20 [37.68s]: training loss=0.4056721031665802  validation ndcg@10=0.06622405680557714 [0.51s]\n",
      "epoch 21 [37.44s]:  training loss=0.40356454253196716                                 \n",
      "epoch 22 [37.69s]:  training loss=0.3933122456073761                                  \n",
      "epoch 23 [37.95s]:  training loss=0.39414864778518677                                 \n",
      "epoch 24 [37.72s]:  training loss=0.3853822946548462                                  \n",
      "epoch 25 [37.09s]: training loss=0.3813714385032654  validation ndcg@10=0.06806367145281057 [0.46s]\n",
      "epoch 26 [38.44s]:  training loss=0.3774360716342926                                  \n",
      "epoch 27 [36.86s]:  training loss=0.3687337040901184                                  \n",
      "epoch 28 [37.13s]:  training loss=0.36673295497894287                                 \n",
      "epoch 29 [37.75s]:  training loss=0.36679384112358093                                 \n",
      "epoch 30 [38.09s]: training loss=0.358949214220047  validation ndcg@10=0.06669440848965406 [1.26s]\n",
      "epoch 31 [37.53s]:  training loss=0.3621239960193634                                  \n",
      "epoch 32 [37.32s]:  training loss=0.3547682762145996                                  \n",
      "epoch 33 [37.55s]:  training loss=0.34902817010879517                                 \n",
      "epoch 34 [37.8s]:  training loss=0.3541342318058014                                   \n",
      "epoch 35 [37.52s]: training loss=0.3528022766113281  validation ndcg@10=0.06881701076693857 [0.45s]\n",
      "epoch 36 [37.14s]:  training loss=0.34793034195899963                                 \n",
      "epoch 37 [37.53s]:  training loss=0.35065892338752747                                 \n",
      "epoch 38 [37.03s]:  training loss=0.34819069504737854                                 \n",
      "epoch 39 [37.59s]:  training loss=0.34665942192077637                                 \n",
      "epoch 40 [37.44s]: training loss=0.3453119397163391  validation ndcg@10=0.08190379450097597 [0.54s]\n",
      "epoch 41 [37.44s]:  training loss=0.34272298216819763                                 \n",
      "epoch 42 [37.46s]:  training loss=0.3433082401752472                                  \n",
      "epoch 43 [37.83s]:  training loss=0.34497717022895813                                 \n",
      "epoch 44 [37.66s]:  training loss=0.33582502603530884                                 \n",
      "epoch 45 [37.57s]: training loss=0.3374175727367401  validation ndcg@10=0.0723211644964528 [0.49s]\n",
      "epoch 46 [37.43s]:  training loss=0.33906984329223633                                 \n",
      "epoch 47 [39.89s]:  training loss=0.3312697112560272                                  \n",
      "epoch 48 [37.29s]:  training loss=0.3330860435962677                                  \n",
      "epoch 49 [37.21s]:  training loss=0.33493101596832275                                 \n",
      "epoch 50 [36.7s]: training loss=0.3281165659427643  validation ndcg@10=0.06890656940995715 [0.49s]\n",
      "epoch 51 [37.11s]:  training loss=0.3295992612838745                                  \n",
      "epoch 52 [37.32s]:  training loss=0.3338664472103119                                  \n",
      "epoch 53 [37.78s]:  training loss=0.3330439329147339                                  \n",
      "epoch 54 [36.85s]:  training loss=0.32704684138298035                                 \n",
      "epoch 55 [35.86s]: training loss=0.32315880060195923  validation ndcg@10=0.06385967199124132 [0.45s]\n",
      "epoch 56 [36.55s]:  training loss=0.32482147216796875                                 \n",
      "epoch 57 [37.34s]:  training loss=0.31989791989326477                                 \n",
      "epoch 58 [37.96s]:  training loss=0.32140448689460754                                 \n",
      "epoch 59 [37.21s]:  training loss=0.32479146122932434                                 \n",
      "epoch 60 [38.42s]: training loss=0.3161546587944031  validation ndcg@10=0.06338917275279446 [0.46s]\n",
      "epoch 61 [37.95s]:  training loss=0.31912556290626526                                 \n",
      "epoch 62 [37.49s]:  training loss=0.3224698007106781                                  \n",
      "epoch 63 [38.22s]:  training loss=0.3179642856121063                                  \n",
      "epoch 64 [37.99s]:  training loss=0.31620514392852783                                 \n",
      "epoch 65 [40.2s]: training loss=0.315847247838974  validation ndcg@10=0.06684775289950504 [0.43s]\n",
      "epoch 1 [40.72s]:  training loss=0.927432656288147                                     \n",
      "epoch 2 [37.23s]:  training loss=0.7895739078521729                                    \n",
      "epoch 3 [38.36s]:  training loss=0.6599512100219727                                    \n",
      "epoch 4 [36.93s]:  training loss=0.5913538336753845                                    \n",
      "epoch 5 [37.81s]: training loss=0.5575490593910217  validation ndcg@10=0.06611647382352065 [0.43s]\n",
      "epoch 6 [37.5s]:  training loss=0.5370944738388062                                     \n",
      "epoch 7 [37.42s]:  training loss=0.52408766746521                                      \n",
      "epoch 8 [37.95s]:  training loss=0.5042584538459778                                    \n",
      "epoch 9 [37.52s]:  training loss=0.5042613744735718                                    \n",
      "epoch 10 [38.49s]: training loss=0.49222296476364136  validation ndcg@10=0.06100046379266638 [0.43s]\n",
      "epoch 11 [36.82s]:  training loss=0.4832926094532013                                   \n",
      "epoch 12 [37.83s]:  training loss=0.47356709837913513                                  \n",
      "epoch 13 [37.71s]:  training loss=0.4422283172607422                                   \n",
      "epoch 14 [36.98s]:  training loss=0.4357125759124756                                   \n",
      "epoch 15 [37.22s]: training loss=0.4316735565662384  validation ndcg@10=0.06106741750043928 [0.41s]\n",
      "epoch 16 [37.89s]:  training loss=0.4252077639102936                                   \n",
      "epoch 17 [37.24s]:  training loss=0.4243834316730499                                   \n",
      "epoch 18 [37.39s]:  training loss=0.4170173704624176                                   \n",
      "epoch 19 [37.23s]:  training loss=0.40470090508461                                     \n",
      "epoch 20 [37.65s]: training loss=0.40596088767051697  validation ndcg@10=0.06559075046900546 [0.42s]\n",
      "epoch 21 [37.94s]:  training loss=0.4038391411304474                                   \n",
      "epoch 22 [37.71s]:  training loss=0.40114593505859375                                  \n",
      "epoch 23 [37.74s]:  training loss=0.39231178164482117                                  \n",
      "epoch 24 [37.8s]:  training loss=0.3901003301143646                                    \n",
      "epoch 25 [38.07s]: training loss=0.3774820566177368  validation ndcg@10=0.06362967876532678 [0.38s]\n",
      "epoch 26 [38.03s]:  training loss=0.37122875452041626                                  \n",
      "epoch 27 [38.36s]:  training loss=0.3676408529281616                                   \n",
      "epoch 28 [39.37s]:  training loss=0.3621009290218353                                   \n",
      "epoch 29 [37.9s]:  training loss=0.36377161741256714                                   \n",
      "epoch 30 [37.49s]: training loss=0.3650050461292267  validation ndcg@10=0.062345086177660616 [0.39s]\n",
      "100%|██████████| 50/50 [7:38:56<00:00, 550.73s/trial, best loss: -0.08190379450097597] \n"
     ]
    }
   ],
   "source": [
    "tune_pinsage('movielens_100k', use_text_feature=False, use_no_feature=True, use_only_text=False)\n",
    "# no feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters of PinSage Recommender on dataset adobe_core5...\n",
      "use_text_feature=True, use_no_feature=False, use_only_text=True\n",
      "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengxuan_yan/opt/miniconda3/envs/torch/lib/python3.8/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 [23.62s]:  training loss=0.3486691415309906   \n",
      "epoch 2 [23.87s]:  training loss=0.22067128121852875  \n",
      "epoch 3 [24.69s]:  training loss=0.20616216957569122  \n",
      "epoch 4 [24.33s]:  training loss=0.19862930476665497  \n",
      "epoch 5 [24.59s]: training loss=0.1990644931793213  validation ndcg@10=0.019722343761656696 [0.7s]\n",
      "epoch 6 [24.49s]:  training loss=0.20327425003051758  \n",
      "epoch 7 [24.36s]:  training loss=0.19845826923847198  \n",
      "epoch 8 [23.78s]:  training loss=0.2015356868505478   \n",
      "epoch 9 [24.06s]:  training loss=0.19544020295143127  \n",
      "epoch 10 [24.21s]: training loss=0.19767020642757416  validation ndcg@10=0.015980039889260098 [0.75s]\n",
      "epoch 11 [23.93s]:  training loss=0.19936113059520721 \n",
      "epoch 12 [24.59s]:  training loss=0.20640616118907928 \n",
      "epoch 13 [25.29s]:  training loss=0.20818683505058289 \n",
      "epoch 14 [23.39s]:  training loss=0.20791779458522797 \n",
      "epoch 15 [24.47s]: training loss=0.20599789917469025  validation ndcg@10=0.015415416165420434 [0.7s]\n",
      "epoch 16 [23.82s]:  training loss=0.20660263299942017 \n",
      "epoch 17 [24.74s]:  training loss=0.21689744293689728 \n",
      "epoch 18 [23.92s]:  training loss=0.20782651007175446 \n",
      "epoch 19 [24.55s]:  training loss=0.21614190936088562 \n",
      "epoch 20 [23.96s]: training loss=0.2051057070493698  validation ndcg@10=0.01776305312026272 [0.76s]\n",
      "epoch 21 [24.03s]:  training loss=0.20946604013442993 \n",
      "epoch 22 [24.93s]:  training loss=0.2046409249305725  \n",
      "epoch 23 [24.29s]:  training loss=0.20874793827533722 \n",
      "epoch 24 [24.3s]:  training loss=0.20101208984851837  \n",
      "epoch 25 [24.24s]: training loss=0.21228012442588806  validation ndcg@10=0.018338091197219325 [0.7s]\n",
      "epoch 26 [23.95s]:  training loss=0.2177603542804718  \n",
      "epoch 27 [24.28s]:  training loss=0.21231801807880402 \n",
      "epoch 28 [24.18s]:  training loss=0.22105135023593903 \n",
      "epoch 29 [24.11s]:  training loss=0.21711252629756927 \n",
      "epoch 30 [24.65s]: training loss=0.2279999703168869  validation ndcg@10=0.017666227598641854 [0.73s]\n",
      "epoch 1 [44.45s]:  training loss=0.7838965058326721                                   \n",
      "epoch 2 [43.76s]:  training loss=0.6851773858070374                                   \n",
      "epoch 3 [44.44s]:  training loss=0.5845539569854736                                   \n",
      "epoch 4 [44.73s]:  training loss=0.5288466811180115                                   \n",
      "epoch 5 [44.64s]: training loss=0.4816122055053711  validation ndcg@10=0.012395291854789011 [0.99s]\n",
      "epoch 6 [45.35s]:  training loss=0.4472525715827942                                   \n",
      "epoch 7 [44.22s]:  training loss=0.40973177552223206                                  \n",
      "epoch 8 [47.05s]:  training loss=0.37394997477531433                                  \n",
      "epoch 9 [43.88s]:  training loss=0.3517908751964569                                   \n",
      "epoch 10 [43.31s]: training loss=0.3337738811969757  validation ndcg@10=0.02538971108434283 [1.04s]\n",
      "epoch 11 [43.46s]:  training loss=0.3090507984161377                                  \n",
      "epoch 12 [43.78s]:  training loss=0.29860976338386536                                 \n",
      "epoch 13 [43.82s]:  training loss=0.279534250497818                                   \n",
      "epoch 14 [43.39s]:  training loss=0.26898708939552307                                 \n",
      "epoch 15 [44.0s]: training loss=0.25855106115341187  validation ndcg@10=0.02682351553793536 [0.98s]\n",
      "epoch 16 [44.2s]:  training loss=0.24892525374889374                                  \n",
      "epoch 17 [44.0s]:  training loss=0.23974193632602692                                  \n",
      "epoch 18 [43.44s]:  training loss=0.2324129045009613                                  \n",
      "epoch 19 [44.08s]:  training loss=0.22479233145713806                                 \n",
      "epoch 20 [44.14s]: training loss=0.2179335653781891  validation ndcg@10=0.026681061951498334 [1.04s]\n",
      "epoch 21 [44.85s]:  training loss=0.2098492532968521                                  \n",
      "epoch 22 [45.71s]:  training loss=0.20720113813877106                                 \n",
      "epoch 23 [44.01s]:  training loss=0.20016303658485413                                 \n",
      "epoch 24 [43.41s]:  training loss=0.19244395196437836                                 \n",
      "epoch 25 [44.82s]: training loss=0.19214549660682678  validation ndcg@10=0.02730261912809857 [1.2s]\n",
      "epoch 26 [44.75s]:  training loss=0.18763181567192078                                 \n",
      "epoch 27 [44.42s]:  training loss=0.18430009484291077                                 \n",
      "epoch 28 [45.16s]:  training loss=0.17866964638233185                                 \n",
      "epoch 29 [44.05s]:  training loss=0.17721307277679443                                 \n",
      "epoch 30 [44.73s]: training loss=0.17099204659461975  validation ndcg@10=0.027767311014081687 [0.97s]\n",
      "epoch 31 [44.54s]:  training loss=0.16827239096164703                                 \n",
      "epoch 32 [44.15s]:  training loss=0.16330166161060333                                 \n",
      "epoch 33 [45.25s]:  training loss=0.16224156320095062                                 \n",
      "epoch 34 [45.02s]:  training loss=0.1613444983959198                                  \n",
      "epoch 35 [44.32s]: training loss=0.1552726775407791  validation ndcg@10=0.02887881605526244 [1.01s]\n",
      "epoch 36 [44.25s]:  training loss=0.1556766778230667                                  \n",
      "epoch 37 [44.06s]:  training loss=0.15177802741527557                                 \n",
      "epoch 38 [45.98s]:  training loss=0.1522686779499054                                  \n",
      "epoch 39 [44.95s]:  training loss=0.14715011417865753                                 \n",
      "epoch 40 [44.16s]: training loss=0.14671093225479126  validation ndcg@10=0.028836568682982233 [0.98s]\n",
      "epoch 41 [43.84s]:  training loss=0.14365985989570618                                 \n",
      "epoch 42 [43.7s]:  training loss=0.1378801167011261                                   \n",
      "epoch 43 [43.92s]:  training loss=0.137668639421463                                   \n",
      "epoch 44 [44.96s]:  training loss=0.13908225297927856                                 \n",
      "epoch 45 [44.89s]: training loss=0.13369177281856537  validation ndcg@10=0.029270335259044065 [1.05s]\n",
      "epoch 46 [44.29s]:  training loss=0.13425272703170776                                 \n",
      "epoch 47 [44.85s]:  training loss=0.13262702524662018                                 \n",
      "epoch 48 [43.98s]:  training loss=0.1309669017791748                                  \n",
      "epoch 49 [44.59s]:  training loss=0.1274605095386505                                  \n",
      "epoch 50 [44.67s]: training loss=0.12667305767536163  validation ndcg@10=0.029033993043145054 [1.01s]\n",
      "epoch 51 [43.76s]:  training loss=0.12729096412658691                                 \n",
      "epoch 52 [43.7s]:  training loss=0.12364918738603592                                  \n",
      "epoch 53 [45.76s]:  training loss=0.12174829840660095                                 \n",
      "epoch 54 [44.43s]:  training loss=0.12061883509159088                                 \n",
      "epoch 55 [44.85s]: training loss=0.12093337625265121  validation ndcg@10=0.029744170796530613 [1.05s]\n",
      "epoch 56 [44.25s]:  training loss=0.11858963966369629                                 \n",
      "epoch 57 [46.11s]:  training loss=0.11952779442071915                                 \n",
      "epoch 58 [45.02s]:  training loss=0.11396365612745285                                 \n",
      "epoch 59 [44.94s]:  training loss=0.11457536369562149                                 \n",
      "epoch 60 [44.9s]: training loss=0.11469396948814392  validation ndcg@10=0.02991373317577109 [0.98s]\n",
      "epoch 61 [44.7s]:  training loss=0.1131819486618042                                   \n",
      "epoch 62 [45.33s]:  training loss=0.11368410289287567                                 \n",
      "epoch 63 [44.67s]:  training loss=0.10888922959566116                                 \n",
      "epoch 64 [45.07s]:  training loss=0.10951951891183853                                 \n",
      "epoch 65 [44.71s]: training loss=0.10581912845373154  validation ndcg@10=0.029894567854433497 [1.01s]\n",
      "epoch 66 [45.38s]:  training loss=0.10773605853319168                                   \n",
      "epoch 67 [44.65s]:  training loss=0.1055130586028099                                    \n",
      "epoch 68 [44.6s]:  training loss=0.10327132046222687                                    \n",
      "epoch 69 [45.13s]:  training loss=0.10174214839935303                                   \n",
      "epoch 70 [44.76s]: training loss=0.1034000962972641  validation ndcg@10=0.029569570300971636 [1.02s]\n",
      "epoch 71 [46.3s]:  training loss=0.10238008201122284                                    \n",
      "epoch 72 [43.85s]:  training loss=0.10029906034469604                                   \n",
      "epoch 73 [44.2s]:  training loss=0.09824377298355103                                    \n",
      "epoch 74 [44.54s]:  training loss=0.09993148595094681                                   \n",
      "epoch 75 [43.98s]: training loss=0.0968443751335144  validation ndcg@10=0.03011704983178082 [1.0s]\n",
      "epoch 76 [43.97s]:  training loss=0.0963253453373909                                    \n",
      "epoch 77 [44.61s]:  training loss=0.09522885829210281                                   \n",
      "epoch 78 [44.26s]:  training loss=0.09493891894817352                                   \n",
      "epoch 79 [45.38s]:  training loss=0.09239709377288818                                   \n",
      "epoch 80 [43.99s]: training loss=0.09186658263206482  validation ndcg@10=0.03036918689000982 [1.02s]\n",
      "epoch 81 [46.77s]:  training loss=0.09236018359661102                                   \n",
      "epoch 82 [44.34s]:  training loss=0.09172718971967697                                   \n",
      "epoch 83 [44.58s]:  training loss=0.09093447029590607                                   \n",
      "epoch 84 [44.12s]:  training loss=0.09158425778150558                                   \n",
      "epoch 85 [43.83s]: training loss=0.08946844935417175  validation ndcg@10=0.030103766235408547 [0.99s]\n",
      "epoch 86 [44.51s]:  training loss=0.08957894891500473                                   \n",
      "epoch 87 [44.5s]:  training loss=0.08818617463111877                                    \n",
      "epoch 88 [44.28s]:  training loss=0.08508750796318054                                   \n",
      "epoch 89 [42.96s]:  training loss=0.08746230602264404                                   \n",
      "epoch 90 [44.13s]: training loss=0.08403557538986206  validation ndcg@10=0.02977824163247651 [1.0s]\n",
      "epoch 91 [43.8s]:  training loss=0.08471965789794922                                    \n",
      "epoch 92 [44.05s]:  training loss=0.08492806553840637                                   \n",
      "epoch 93 [44.06s]:  training loss=0.0831267237663269                                    \n",
      "epoch 94 [45.4s]:  training loss=0.08088181167840958                                    \n",
      "epoch 95 [44.03s]: training loss=0.08232395350933075  validation ndcg@10=0.029651060773265923 [0.92s]\n",
      "epoch 96 [43.9s]:  training loss=0.08180569857358932                                    \n",
      "epoch 97 [43.93s]:  training loss=0.08085187524557114                                   \n",
      "epoch 98 [44.67s]:  training loss=0.07803163677453995                                   \n",
      "epoch 99 [44.36s]:  training loss=0.0792912095785141                                    \n",
      "epoch 100 [43.86s]: training loss=0.08049045503139496  validation ndcg@10=0.02970071827115631 [0.95s]\n",
      "epoch 101 [44.3s]:  training loss=0.07940854877233505                                   \n",
      "epoch 102 [45.55s]:  training loss=0.07718946784734726                                  \n",
      "epoch 103 [44.43s]:  training loss=0.0773804560303688                                   \n",
      "epoch 104 [43.94s]:  training loss=0.07620944827795029                                  \n",
      "epoch 105 [44.63s]: training loss=0.07845652103424072  validation ndcg@10=0.02981099439918991 [1.0s]\n",
      "epoch 1 [35.88s]:  training loss=0.7939429879188538                                      \n",
      "epoch 2 [32.92s]:  training loss=0.692268431186676                                       \n",
      "epoch 3 [34.14s]:  training loss=0.5879552364349365                                      \n",
      "epoch 4 [34.17s]:  training loss=0.5341838598251343                                      \n",
      "epoch 5 [33.83s]: training loss=0.496370404958725  validation ndcg@10=0.012715296620154366 [0.84s]\n",
      "epoch 6 [33.22s]:  training loss=0.4547981023788452                                      \n",
      "epoch 7 [33.67s]:  training loss=0.4202632009983063                                      \n",
      "epoch 8 [33.44s]:  training loss=0.3845842480659485                                      \n",
      "epoch 9 [33.63s]:  training loss=0.3572666645050049                                      \n",
      "epoch 10 [34.29s]: training loss=0.3377316892147064  validation ndcg@10=0.0235033075365243 [0.85s]\n",
      "epoch 11 [33.71s]:  training loss=0.3122688829898834                                     \n",
      "epoch 12 [34.1s]:  training loss=0.2892453074455261                                      \n",
      "epoch 13 [35.21s]:  training loss=0.2789704203605652                                     \n",
      "epoch 14 [33.59s]:  training loss=0.265506386756897                                      \n",
      "epoch 15 [33.65s]: training loss=0.2541673183441162  validation ndcg@10=0.026171367098161778 [0.78s]\n",
      "epoch 16 [34.0s]:  training loss=0.24612829089164734                                     \n",
      "epoch 17 [34.92s]:  training loss=0.23667271435260773                                    \n",
      "epoch 18 [33.93s]:  training loss=0.22619660198688507                                    \n",
      "epoch 19 [33.71s]:  training loss=0.22152209281921387                                    \n",
      "epoch 20 [33.42s]: training loss=0.2152746170759201  validation ndcg@10=0.02697751702815264 [0.88s]\n",
      "epoch 21 [34.08s]:  training loss=0.2063887119293213                                     \n",
      "epoch 22 [34.48s]:  training loss=0.19844728708267212                                    \n",
      "epoch 23 [33.54s]:  training loss=0.1969316303730011                                     \n",
      "epoch 24 [33.71s]:  training loss=0.1878383606672287                                     \n",
      "epoch 25 [35.15s]: training loss=0.1884755641222  validation ndcg@10=0.027571842139403314 [0.85s]\n",
      "epoch 26 [33.4s]:  training loss=0.17850956320762634                                     \n",
      "epoch 27 [34.5s]:  training loss=0.1815163940191269                                      \n",
      "epoch 28 [34.43s]:  training loss=0.17535500228405                                       \n",
      "epoch 29 [35.03s]:  training loss=0.16978570818901062                                    \n",
      "epoch 30 [33.71s]: training loss=0.16687068343162537  validation ndcg@10=0.028358316428467572 [0.78s]\n",
      "epoch 31 [34.53s]:  training loss=0.16561897099018097                                    \n",
      "epoch 32 [34.42s]:  training loss=0.16044391691684723                                    \n",
      "epoch 33 [34.67s]:  training loss=0.1573806256055832                                     \n",
      "epoch 34 [34.02s]:  training loss=0.15532012283802032                                    \n",
      "epoch 35 [34.21s]: training loss=0.15152443945407867  validation ndcg@10=0.029091055728613965 [0.86s]\n",
      "epoch 36 [32.82s]:  training loss=0.14938762784004211                                    \n",
      "epoch 37 [33.21s]:  training loss=0.1454639434814453                                     \n",
      "epoch 38 [33.97s]:  training loss=0.14386655390262604                                    \n",
      "epoch 39 [33.45s]:  training loss=0.13835662603378296                                    \n",
      "epoch 40 [33.36s]: training loss=0.13946890830993652  validation ndcg@10=0.02913155601725517 [0.84s]\n",
      "epoch 41 [33.08s]:  training loss=0.13526581227779388                                    \n",
      "epoch 42 [33.52s]:  training loss=0.1347871571779251                                     \n",
      "epoch 43 [33.02s]:  training loss=0.13478384912014008                                    \n",
      "epoch 44 [33.27s]:  training loss=0.13154380023479462                                    \n",
      "epoch 45 [33.14s]: training loss=0.12775826454162598  validation ndcg@10=0.029318743735261468 [0.76s]\n",
      "epoch 46 [33.88s]:  training loss=0.12769468128681183                                    \n",
      "epoch 47 [32.74s]:  training loss=0.12526781857013702                                    \n",
      "epoch 48 [32.67s]:  training loss=0.12340134382247925                                    \n",
      "epoch 49 [33.62s]:  training loss=0.12088535726070404                                    \n",
      "epoch 50 [33.02s]: training loss=0.1194080039858818  validation ndcg@10=0.029331747715549018 [0.8s]\n",
      "epoch 51 [34.47s]:  training loss=0.11533777415752411                                    \n",
      "epoch 52 [32.35s]:  training loss=0.11438852548599243                                    \n",
      "epoch 53 [32.59s]:  training loss=0.11677531152963638                                    \n",
      "epoch 54 [32.54s]:  training loss=0.11342031508684158                                    \n",
      "epoch 55 [32.81s]: training loss=0.11261998116970062  validation ndcg@10=0.02944277229376866 [0.79s]\n",
      "epoch 56 [32.38s]:  training loss=0.11271149665117264                                    \n",
      "epoch 57 [32.75s]:  training loss=0.10966131091117859                                    \n",
      "epoch 58 [32.73s]:  training loss=0.10771747678518295                                    \n",
      "epoch 59 [33.34s]:  training loss=0.10856816917657852                                    \n",
      "epoch 60 [33.09s]: training loss=0.10537329316139221  validation ndcg@10=0.028724303354623615 [0.8s]\n",
      "epoch 61 [32.76s]:  training loss=0.1062784269452095                                     \n",
      "epoch 62 [32.72s]:  training loss=0.10033006221055984                                    \n",
      "epoch 63 [33.54s]:  training loss=0.10219366103410721                                    \n",
      "epoch 64 [32.43s]:  training loss=0.10157310217618942                                    \n",
      "epoch 65 [34.07s]: training loss=0.1019229143857956  validation ndcg@10=0.029921110101138355 [0.87s]\n",
      "epoch 66 [33.24s]:  training loss=0.09634541720151901                                    \n",
      "epoch 67 [32.76s]:  training loss=0.09649518877267838                                    \n",
      "epoch 68 [33.53s]:  training loss=0.09620347619056702                                    \n",
      "epoch 69 [33.14s]:  training loss=0.09479369968175888                                    \n",
      "epoch 70 [33.35s]: training loss=0.09550987929105759  validation ndcg@10=0.029421350271321076 [0.87s]\n",
      "epoch 71 [33.45s]:  training loss=0.09556648135185242                                    \n",
      "epoch 72 [32.77s]:  training loss=0.08880190551280975                                    \n",
      "epoch 73 [33.12s]:  training loss=0.09225266426801682                                    \n",
      "epoch 74 [34.21s]:  training loss=0.09019435197114944                                    \n",
      "epoch 75 [33.77s]: training loss=0.08973349630832672  validation ndcg@10=0.029373934407236863 [0.9s]\n",
      "epoch 76 [33.74s]:  training loss=0.08868353068828583                                    \n",
      "epoch 77 [34.82s]:  training loss=0.08743419498205185                                    \n",
      "epoch 78 [33.14s]:  training loss=0.08649926632642746                                    \n",
      "epoch 79 [33.08s]:  training loss=0.08712979406118393                                    \n",
      "epoch 80 [32.83s]: training loss=0.088048554956913  validation ndcg@10=0.0300867807453876 [0.85s]\n",
      "epoch 81 [33.3s]:  training loss=0.0846276506781578                                      \n",
      "epoch 82 [33.98s]:  training loss=0.08484537899494171                                    \n",
      "epoch 83 [33.2s]:  training loss=0.08462591469287872                                     \n",
      "epoch 84 [33.79s]:  training loss=0.08202850073575974                                    \n",
      "epoch 85 [33.26s]: training loss=0.08411934226751328  validation ndcg@10=0.030423714975632905 [0.82s]\n",
      "epoch 86 [33.04s]:  training loss=0.0820978432893753                                     \n",
      "epoch 87 [32.98s]:  training loss=0.08031421154737473                                    \n",
      "epoch 88 [32.82s]:  training loss=0.08189539611339569                                    \n",
      "epoch 89 [32.87s]:  training loss=0.07835894823074341                                    \n",
      "epoch 90 [32.78s]: training loss=0.0777541771531105  validation ndcg@10=0.029961123102479413 [0.79s]\n",
      "epoch 91 [33.92s]:  training loss=0.0785246342420578                                     \n",
      "epoch 92 [33.78s]:  training loss=0.07573162019252777                                    \n",
      "epoch 93 [33.67s]:  training loss=0.0772126317024231                                     \n",
      "epoch 94 [33.57s]:  training loss=0.07827239483594894                                    \n",
      "epoch 95 [33.38s]: training loss=0.07708960771560669  validation ndcg@10=0.03037174314479857 [0.8s]\n",
      "epoch 96 [33.06s]:  training loss=0.07311475276947021                                    \n",
      "epoch 97 [33.94s]:  training loss=0.07551105320453644                                    \n",
      "epoch 98 [34.11s]:  training loss=0.0736314058303833                                     \n",
      "epoch 99 [32.97s]:  training loss=0.07408243417739868                                    \n",
      "epoch 100 [33.85s]: training loss=0.07304874807596207  validation ndcg@10=0.029870647057869397 [0.85s]\n",
      "epoch 101 [32.85s]:  training loss=0.07382585108280182                                   \n",
      "epoch 102 [33.57s]:  training loss=0.07189210504293442                                   \n",
      "epoch 103 [34.5s]:  training loss=0.07150569558143616                                    \n",
      "epoch 104 [31.99s]:  training loss=0.06885413080453873                                   \n",
      "epoch 105 [33.16s]: training loss=0.07014349848031998  validation ndcg@10=0.029882931790823186 [0.86s]\n",
      "epoch 106 [33.02s]:  training loss=0.06937684863805771                                   \n",
      "epoch 107 [33.54s]:  training loss=0.0692468136548996                                    \n",
      "epoch 108 [34.13s]:  training loss=0.06887050718069077                                   \n",
      "epoch 109 [32.67s]:  training loss=0.06665223836898804                                   \n",
      "epoch 110 [33.32s]: training loss=0.06713017076253891  validation ndcg@10=0.029908515227345805 [0.78s]\n",
      "epoch 1 [37.47s]:  training loss=0.37610140442848206                                      \n",
      "epoch 2 [38.09s]:  training loss=0.19754277169704437                                      \n",
      "epoch 3 [38.89s]:  training loss=0.14915503561496735                                      \n",
      "epoch 4 [38.46s]:  training loss=0.1295570582151413                                       \n",
      "epoch 5 [37.99s]: training loss=0.11768370866775513  validation ndcg@10=0.027785511105522714 [1.06s]\n",
      "epoch 6 [38.05s]:  training loss=0.11254389584064484                                      \n",
      "epoch 7 [38.6s]:  training loss=0.10992056876420975                                       \n",
      "epoch 8 [38.09s]:  training loss=0.10128740966320038                                      \n",
      "epoch 9 [38.41s]:  training loss=0.09792546182870865                                      \n",
      "epoch 10 [38.41s]: training loss=0.09835437685251236  validation ndcg@10=0.025161359174292387 [0.94s]\n",
      "epoch 11 [37.99s]:  training loss=0.09414590150117874                                     \n",
      "epoch 12 [37.64s]:  training loss=0.09663159400224686                                     \n",
      "epoch 13 [37.69s]:  training loss=0.09511919319629669                                     \n",
      "epoch 14 [39.35s]:  training loss=0.0922030434012413                                      \n",
      "epoch 15 [38.56s]: training loss=0.0910949781537056  validation ndcg@10=0.025144756323392827 [1.03s]\n",
      "epoch 16 [37.95s]:  training loss=0.08646325767040253                                     \n",
      "epoch 17 [38.78s]:  training loss=0.08581416308879852                                     \n",
      "epoch 18 [38.37s]:  training loss=0.08963640034198761                                     \n",
      "epoch 19 [38.21s]:  training loss=0.0858696848154068                                      \n",
      "epoch 20 [38.44s]: training loss=0.08506264537572861  validation ndcg@10=0.022245928851396063 [0.95s]\n",
      "epoch 21 [37.95s]:  training loss=0.0824764147400856                                      \n",
      "epoch 22 [38.48s]:  training loss=0.08623149245977402                                     \n",
      "epoch 23 [37.66s]:  training loss=0.08435843884944916                                     \n",
      "epoch 24 [37.79s]:  training loss=0.08278457820415497                                     \n",
      "epoch 25 [38.66s]: training loss=0.09111838042736053  validation ndcg@10=0.021658952323565742 [1.05s]\n",
      "epoch 26 [38.37s]:  training loss=0.08375663310289383                                     \n",
      "epoch 27 [37.63s]:  training loss=0.08608990162611008                                     \n",
      "epoch 28 [38.84s]:  training loss=0.08474477380514145                                     \n",
      "epoch 29 [37.61s]:  training loss=0.07971104979515076                                     \n",
      "epoch 30 [37.85s]: training loss=0.08559326827526093  validation ndcg@10=0.02013153652418738 [1.01s]\n",
      "epoch 1 [40.19s]:  training loss=0.31625619530677795                                      \n",
      "epoch 2 [41.16s]:  training loss=0.18178144097328186                                      \n",
      "epoch 3 [41.65s]:  training loss=0.15950962901115417                                      \n",
      "epoch 4 [40.33s]:  training loss=0.15114182233810425                                      \n",
      "epoch 5 [40.61s]: training loss=0.14379048347473145  validation ndcg@10=0.022233241875297627 [1.08s]\n",
      "epoch 6 [41.25s]:  training loss=0.1389991194009781                                       \n",
      "epoch 7 [41.63s]:  training loss=0.14371104538440704                                      \n",
      "epoch 8 [41.26s]:  training loss=0.1414729207754135                                       \n",
      "epoch 9 [41.45s]:  training loss=0.13683514297008514                                      \n",
      "epoch 10 [41.74s]: training loss=0.14100651443004608  validation ndcg@10=0.022244742657587607 [1.09s]\n",
      "epoch 11 [41.83s]:  training loss=0.1347804218530655                                      \n",
      "epoch 12 [41.42s]:  training loss=0.13731983304023743                                     \n",
      "epoch 13 [41.44s]:  training loss=0.14225837588310242                                     \n",
      "epoch 14 [41.36s]:  training loss=0.14124184846878052                                     \n",
      "epoch 15 [42.06s]: training loss=0.13821634650230408  validation ndcg@10=0.020581553542684917 [1.01s]\n",
      "epoch 16 [41.12s]:  training loss=0.13876214623451233                                     \n",
      "epoch 17 [42.43s]:  training loss=0.14277125895023346                                     \n",
      "epoch 18 [42.15s]:  training loss=0.13238663971424103                                     \n",
      "epoch 19 [42.37s]:  training loss=0.13721251487731934                                     \n",
      "epoch 20 [41.65s]: training loss=0.13624562323093414  validation ndcg@10=0.01792173527971432 [1.03s]\n",
      "epoch 21 [43.81s]:  training loss=0.14181390404701233                                     \n",
      "epoch 22 [41.18s]:  training loss=0.1469847410917282                                      \n",
      "epoch 23 [41.32s]:  training loss=0.13937057554721832                                     \n",
      "epoch 24 [42.19s]:  training loss=0.14623329043388367                                     \n",
      "epoch 25 [41.21s]: training loss=0.13607197999954224  validation ndcg@10=0.02111810265230749 [1.07s]\n",
      "epoch 26 [41.78s]:  training loss=0.14255960285663605                                     \n",
      "epoch 27 [41.7s]:  training loss=0.14575664699077606                                      \n",
      "epoch 28 [42.92s]:  training loss=0.14602696895599365                                     \n",
      "epoch 29 [41.02s]:  training loss=0.14176952838897705                                     \n",
      "epoch 30 [42.38s]: training loss=0.1388995349407196  validation ndcg@10=0.019781801729104147 [1.07s]\n",
      "epoch 31 [42.62s]:  training loss=0.14502276480197906                                     \n",
      "epoch 32 [41.72s]:  training loss=0.14502330124378204                                     \n",
      "epoch 33 [41.79s]:  training loss=0.14744055271148682                                     \n",
      "epoch 34 [42.06s]:  training loss=0.14509466290473938                                     \n",
      "epoch 35 [41.62s]: training loss=0.14465172588825226  validation ndcg@10=0.020321776891037785 [0.99s]\n",
      "epoch 1 [18.64s]:  training loss=0.31405940651893616                                      \n",
      "epoch 2 [18.44s]:  training loss=0.28389444947242737                                      \n",
      "epoch 3 [17.95s]:  training loss=0.30380135774612427                                      \n",
      "epoch 4 [18.11s]:  training loss=0.32944878935813904                                      \n",
      "epoch 5 [18.41s]: training loss=0.34757405519485474  validation ndcg@10=0.019052562113143382 [0.62s]\n",
      "epoch 6 [18.15s]:  training loss=0.35101133584976196                                      \n",
      "epoch 7 [18.81s]:  training loss=0.3776604235172272                                       \n",
      "epoch 8 [18.09s]:  training loss=0.37747278809547424                                      \n",
      "epoch 9 [18.23s]:  training loss=0.4160640835762024                                       \n",
      "epoch 10 [19.34s]: training loss=0.4090671241283417  validation ndcg@10=0.020822663846777677 [0.52s]\n",
      "epoch 11 [18.06s]:  training loss=0.4299561083316803                                      \n",
      "epoch 12 [19.11s]:  training loss=0.4281141459941864                                      \n",
      "epoch 13 [18.81s]:  training loss=0.40443721413612366                                     \n",
      "epoch 14 [18.67s]:  training loss=0.42827558517456055                                     \n",
      "epoch 15 [18.88s]: training loss=0.4378291964530945  validation ndcg@10=0.018568868651036567 [0.59s]\n",
      "epoch 16 [19.03s]:  training loss=0.44594910740852356                                     \n",
      "epoch 17 [18.57s]:  training loss=0.4398249685764313                                      \n",
      "epoch 18 [18.44s]:  training loss=0.4475897252559662                                      \n",
      "epoch 19 [18.9s]:  training loss=0.4669153094291687                                       \n",
      "epoch 20 [19.04s]: training loss=0.4892515540122986  validation ndcg@10=0.017931455073343053 [0.53s]\n",
      "epoch 21 [19.02s]:  training loss=0.4750698506832123                                      \n",
      "epoch 22 [18.34s]:  training loss=0.47906389832496643                                     \n",
      "epoch 23 [18.94s]:  training loss=0.5071077942848206                                      \n",
      "epoch 24 [19.0s]:  training loss=0.5040223002433777                                       \n",
      "epoch 25 [18.29s]: training loss=0.49149972200393677  validation ndcg@10=0.018381774441593172 [0.54s]\n",
      "epoch 26 [18.81s]:  training loss=0.49585843086242676                                     \n",
      "epoch 27 [18.37s]:  training loss=0.4885275959968567                                      \n",
      "epoch 28 [18.28s]:  training loss=0.5205004215240479                                      \n",
      "epoch 29 [18.87s]:  training loss=0.47418972849845886                                     \n",
      "epoch 30 [18.22s]: training loss=0.4755885601043701  validation ndcg@10=0.01853657885105895 [0.51s]\n",
      "epoch 31 [18.46s]:  training loss=0.4920377731323242                                      \n",
      "epoch 32 [18.69s]:  training loss=0.53361976146698                                        \n",
      "epoch 33 [18.67s]:  training loss=0.5015770196914673                                      \n",
      "epoch 34 [18.18s]:  training loss=0.5152028203010559                                      \n",
      "epoch 35 [18.68s]: training loss=0.5472342371940613  validation ndcg@10=0.019609824738556773 [0.51s]\n",
      "epoch 1 [24.89s]:  training loss=0.32766783237457275                                      \n",
      "epoch 2 [25.19s]:  training loss=0.26560404896736145                                      \n",
      "epoch 3 [25.17s]:  training loss=0.2631710469722748                                       \n",
      "epoch 4 [25.23s]:  training loss=0.2702803313732147                                       \n",
      "epoch 5 [26.0s]: training loss=0.2775879204273224  validation ndcg@10=0.01663521995870888 [0.78s]\n",
      "epoch 6 [25.7s]:  training loss=0.282241553068161                                         \n",
      "epoch 7 [25.34s]:  training loss=0.28794965147972107                                      \n",
      "epoch 8 [25.66s]:  training loss=0.297916442155838                                        \n",
      "epoch 9 [24.99s]:  training loss=0.2932514548301697                                       \n",
      "epoch 10 [25.37s]: training loss=0.29716062545776367  validation ndcg@10=0.01670993599513658 [0.74s]\n",
      "epoch 11 [25.42s]:  training loss=0.3017616271972656                                      \n",
      "epoch 12 [24.96s]:  training loss=0.31140992045402527                                     \n",
      "epoch 13 [25.55s]:  training loss=0.3078954815864563                                      \n",
      "epoch 14 [25.72s]:  training loss=0.3116942048072815                                      \n",
      "epoch 15 [25.99s]: training loss=0.32412806153297424  validation ndcg@10=0.01674091524359487 [0.79s]\n",
      "epoch 16 [24.86s]:  training loss=0.3189080059528351                                      \n",
      "epoch 17 [25.36s]:  training loss=0.31950631737709045                                     \n",
      "epoch 18 [25.28s]:  training loss=0.32713091373443604                                     \n",
      "epoch 19 [25.48s]:  training loss=0.35174718499183655                                     \n",
      "epoch 20 [25.36s]: training loss=0.35081562399864197  validation ndcg@10=0.016213975489479088 [0.73s]\n",
      "epoch 21 [25.32s]:  training loss=0.33034107089042664                                     \n",
      "epoch 22 [25.48s]:  training loss=0.3343079090118408                                      \n",
      "epoch 23 [25.4s]:  training loss=0.3450184762477875                                       \n",
      "epoch 24 [25.46s]:  training loss=0.344586044549942                                       \n",
      "epoch 25 [26.02s]: training loss=0.35283058881759644  validation ndcg@10=0.015275453879196614 [0.82s]\n",
      "epoch 26 [26.8s]:  training loss=0.33813029527664185                                      \n",
      "epoch 27 [25.62s]:  training loss=0.3379165232181549                                      \n",
      "epoch 28 [25.22s]:  training loss=0.3510012924671173                                      \n",
      "epoch 29 [25.99s]:  training loss=0.3438163995742798                                      \n",
      "epoch 30 [25.42s]: training loss=0.3648369312286377  validation ndcg@10=0.018302771871750872 [0.79s]\n",
      "epoch 31 [25.87s]:  training loss=0.3434910476207733                                      \n",
      "epoch 32 [25.25s]:  training loss=0.37185999751091003                                     \n",
      "epoch 33 [25.53s]:  training loss=0.3365596830844879                                      \n",
      "epoch 34 [25.79s]:  training loss=0.3567590117454529                                      \n",
      "epoch 35 [25.07s]: training loss=0.35359418392181396  validation ndcg@10=0.016641120308761116 [0.74s]\n",
      "epoch 36 [25.64s]:  training loss=0.3665820360183716                                      \n",
      "epoch 37 [25.34s]:  training loss=0.36173877120018005                                     \n",
      "epoch 38 [25.72s]:  training loss=0.3725121319293976                                      \n",
      "epoch 39 [25.71s]:  training loss=0.36617928743362427                                     \n",
      "epoch 40 [25.77s]: training loss=0.37609902024269104  validation ndcg@10=0.019348646818740253 [0.72s]\n",
      "epoch 41 [25.72s]:  training loss=0.3643391728401184                                      \n",
      "epoch 42 [26.09s]:  training loss=0.361428439617157                                       \n",
      "epoch 43 [25.5s]:  training loss=0.3531709909439087                                       \n",
      "epoch 44 [25.2s]:  training loss=0.38618114590644836                                      \n",
      "epoch 45 [25.55s]: training loss=0.3796353042125702  validation ndcg@10=0.01918820440264386 [0.78s]\n",
      "epoch 46 [25.47s]:  training loss=0.3905068337917328                                      \n",
      "epoch 47 [27.37s]:  training loss=0.36629512906074524                                     \n",
      "epoch 48 [25.4s]:  training loss=0.3571953773498535                                       \n",
      "epoch 49 [25.24s]:  training loss=0.38427484035491943                                     \n",
      "epoch 50 [25.19s]: training loss=0.38138073682785034  validation ndcg@10=0.017145350328351537 [0.77s]\n",
      "epoch 51 [25.12s]:  training loss=0.37088727951049805                                     \n",
      "epoch 52 [25.36s]:  training loss=0.3707590103149414                                      \n",
      "epoch 53 [24.97s]:  training loss=0.3828476369380951                                      \n",
      "epoch 54 [24.89s]:  training loss=0.3817162811756134                                      \n",
      "epoch 55 [24.7s]: training loss=0.3940076529979706  validation ndcg@10=0.015020596429342007 [0.71s]\n",
      "epoch 56 [24.65s]:  training loss=0.3865075707435608                                      \n",
      "epoch 57 [25.45s]:  training loss=0.37478408217430115                                     \n",
      "epoch 58 [24.74s]:  training loss=0.3714584410190582                                      \n",
      "epoch 59 [24.64s]:  training loss=0.38498881459236145                                     \n",
      "epoch 60 [25.59s]: training loss=0.37523654103279114  validation ndcg@10=0.015426530264122988 [0.77s]\n",
      "epoch 61 [25.35s]:  training loss=0.4042274057865143                                      \n",
      "epoch 62 [25.0s]:  training loss=0.3987776041030884                                       \n",
      "epoch 63 [25.61s]:  training loss=0.383590430021286                                       \n",
      "epoch 64 [25.52s]:  training loss=0.40758341550827026                                     \n",
      "epoch 65 [25.05s]: training loss=0.37469932436943054  validation ndcg@10=0.017917317412983795 [0.72s]\n",
      "epoch 1 [45.88s]:  training loss=0.41086941957473755                                      \n",
      "epoch 2 [46.33s]:  training loss=0.20735034346580505                                      \n",
      "epoch 3 [45.38s]:  training loss=0.15998785197734833                                      \n",
      "epoch 4 [45.54s]:  training loss=0.13566255569458008                                      \n",
      "epoch 5 [46.91s]: training loss=0.11995205283164978  validation ndcg@10=0.025995069312251717 [1.1s]\n",
      "epoch 6 [45.63s]:  training loss=0.11039270460605621                                      \n",
      "epoch 7 [45.66s]:  training loss=0.10534146428108215                                      \n",
      "epoch 8 [45.35s]:  training loss=0.0967889279127121                                       \n",
      "epoch 9 [45.14s]:  training loss=0.096952885389328                                        \n",
      "epoch 10 [45.34s]: training loss=0.09593561291694641  validation ndcg@10=0.02596125274939868 [1.27s]\n",
      "epoch 11 [45.02s]:  training loss=0.08880798518657684                                     \n",
      "epoch 12 [45.25s]:  training loss=0.08749299496412277                                     \n",
      "epoch 13 [45.31s]:  training loss=0.08288230746984482                                     \n",
      "epoch 14 [46.02s]:  training loss=0.08317424356937408                                     \n",
      "epoch 15 [45.73s]: training loss=0.08250445127487183  validation ndcg@10=0.024104821409249216 [1.29s]\n",
      "epoch 16 [45.18s]:  training loss=0.08275245130062103                                     \n",
      "epoch 17 [45.26s]:  training loss=0.08065836131572723                                     \n",
      "epoch 18 [45.53s]:  training loss=0.08132532238960266                                     \n",
      "epoch 19 [44.92s]:  training loss=0.07556809484958649                                     \n",
      "epoch 20 [44.89s]: training loss=0.07508086413145065  validation ndcg@10=0.023234541195264434 [1.25s]\n",
      "epoch 21 [45.64s]:  training loss=0.07515949755907059                                     \n",
      "epoch 22 [46.59s]:  training loss=0.07323431968688965                                     \n",
      "epoch 23 [45.98s]:  training loss=0.07343229651451111                                     \n",
      "epoch 24 [45.44s]:  training loss=0.07089356333017349                                     \n",
      "epoch 25 [45.62s]: training loss=0.0728006586432457  validation ndcg@10=0.023521518192616074 [1.24s]\n",
      "epoch 26 [45.58s]:  training loss=0.07644455879926682                                     \n",
      "epoch 27 [46.0s]:  training loss=0.07419241964817047                                      \n",
      "epoch 28 [45.28s]:  training loss=0.07325049489736557                                     \n",
      "epoch 29 [45.13s]:  training loss=0.07398347556591034                                     \n",
      "epoch 30 [44.63s]: training loss=0.06943167746067047  validation ndcg@10=0.022879218918693833 [1.25s]\n",
      "epoch 1 [50.84s]:  training loss=0.8407395482063293                                       \n",
      "epoch 2 [51.36s]:  training loss=0.8128023147583008                                       \n",
      "epoch 3 [51.54s]:  training loss=0.7954582571983337                                       \n",
      "epoch 4 [51.71s]:  training loss=0.7773569226264954                                       \n",
      "epoch 5 [50.9s]: training loss=0.7602043747901917  validation ndcg@10=0.007191008860902433 [1.33s]\n",
      "epoch 6 [51.12s]:  training loss=0.7357944846153259                                       \n",
      "epoch 7 [52.45s]:  training loss=0.7094230055809021                                       \n",
      "epoch 8 [51.5s]:  training loss=0.6804406046867371                                        \n",
      "epoch 9 [50.67s]:  training loss=0.6465998888015747                                       \n",
      "epoch 10 [50.68s]: training loss=0.6225564479827881  validation ndcg@10=0.009752317381350007 [1.33s]\n",
      "epoch 11 [50.94s]:  training loss=0.6042048335075378                                      \n",
      "epoch 12 [51.2s]:  training loss=0.5859137177467346                                       \n",
      "epoch 13 [51.65s]:  training loss=0.5718538165092468                                      \n",
      "epoch 14 [51.41s]:  training loss=0.5574870705604553                                      \n",
      "epoch 15 [51.74s]: training loss=0.5494614839553833  validation ndcg@10=0.011246163594422268 [1.37s]\n",
      "epoch 16 [50.93s]:  training loss=0.5347654223442078                                      \n",
      "epoch 17 [52.26s]:  training loss=0.5258879661560059                                      \n",
      "epoch 18 [52.63s]:  training loss=0.5152075290679932                                      \n",
      "epoch 19 [51.07s]:  training loss=0.5062621235847473                                      \n",
      "epoch 20 [50.98s]: training loss=0.49433714151382446  validation ndcg@10=0.012509752834591456 [1.46s]\n",
      "epoch 21 [50.76s]:  training loss=0.4896509647369385                                      \n",
      "epoch 22 [52.13s]:  training loss=0.47697713971138                                        \n",
      "epoch 23 [51.18s]:  training loss=0.469382643699646                                       \n",
      "epoch 24 [51.58s]:  training loss=0.4557103216648102                                      \n",
      "epoch 25 [51.14s]: training loss=0.45234328508377075  validation ndcg@10=0.018831680305336846 [1.34s]\n",
      "epoch 26 [51.09s]:  training loss=0.44662362337112427                                     \n",
      "epoch 27 [50.57s]:  training loss=0.4291149973869324                                      \n",
      "epoch 28 [51.33s]:  training loss=0.42150524258613586                                     \n",
      "epoch 29 [51.34s]:  training loss=0.41976091265678406                                     \n",
      "epoch 30 [51.21s]: training loss=0.41139692068099976  validation ndcg@10=0.022640990831798477 [1.37s]\n",
      "epoch 31 [51.23s]:  training loss=0.4028942286968231                                      \n",
      "epoch 32 [50.79s]:  training loss=0.3919185400009155                                      \n",
      "epoch 33 [51.05s]:  training loss=0.3872988224029541                                      \n",
      "epoch 34 [50.77s]:  training loss=0.38398200273513794                                     \n",
      "epoch 35 [50.74s]: training loss=0.37518978118896484  validation ndcg@10=0.024909053713716562 [1.32s]\n",
      "epoch 36 [51.05s]:  training loss=0.37417522072792053                                     \n",
      "epoch 37 [52.86s]:  training loss=0.36333078145980835                                     \n",
      "epoch 38 [50.86s]:  training loss=0.3586919605731964                                      \n",
      "epoch 39 [50.97s]:  training loss=0.3538113534450531                                      \n",
      "epoch 40 [50.39s]: training loss=0.3556384742259979  validation ndcg@10=0.026314425535611318 [1.38s]\n",
      "epoch 41 [50.35s]:  training loss=0.34114643931388855                                     \n",
      "epoch 42 [51.31s]:  training loss=0.33734130859375                                        \n",
      "epoch 43 [50.47s]:  training loss=0.3336123526096344                                      \n",
      "epoch 44 [50.79s]:  training loss=0.3286832571029663                                      \n",
      "epoch 45 [50.57s]: training loss=0.3244040906429291  validation ndcg@10=0.02632803841933687 [1.42s]\n",
      "epoch 46 [51.53s]:  training loss=0.31994396448135376                                     \n",
      "epoch 47 [51.13s]:  training loss=0.3148963451385498                                      \n",
      "epoch 48 [50.78s]:  training loss=0.31281208992004395                                     \n",
      "epoch 49 [52.88s]:  training loss=0.30987682938575745                                     \n",
      "epoch 50 [51.68s]: training loss=0.30484825372695923  validation ndcg@10=0.026083160327339875 [1.36s]\n",
      "epoch 51 [50.67s]:  training loss=0.30379974842071533                                     \n",
      "epoch 52 [50.89s]:  training loss=0.2927647829055786                                      \n",
      "epoch 53 [50.95s]:  training loss=0.29477646946907043                                     \n",
      "epoch 54 [50.99s]:  training loss=0.29166439175605774                                     \n",
      "epoch 55 [51.28s]: training loss=0.28887608647346497  validation ndcg@10=0.025944183207692595 [1.41s]\n",
      "epoch 56 [50.75s]:  training loss=0.28731411695480347                                     \n",
      "epoch 57 [51.13s]:  training loss=0.2806006371974945                                      \n",
      "epoch 58 [51.16s]:  training loss=0.28409451246261597                                     \n",
      "epoch 59 [50.81s]:  training loss=0.27661415934562683                                     \n",
      "epoch 60 [51.47s]: training loss=0.2735833525657654  validation ndcg@10=0.026135528076946316 [1.39s]\n",
      "epoch 61 [51.88s]:  training loss=0.27164748311042786                                     \n",
      "epoch 62 [52.82s]:  training loss=0.26655280590057373                                     \n",
      "epoch 63 [51.31s]:  training loss=0.2651323974132538                                      \n",
      "epoch 64 [51.63s]:  training loss=0.27043142914772034                                     \n",
      "epoch 65 [51.98s]: training loss=0.266282320022583  validation ndcg@10=0.026080821106003155 [1.39s]\n",
      "epoch 66 [51.49s]:  training loss=0.25996479392051697                                     \n",
      "epoch 67 [51.83s]:  training loss=0.2594057619571686                                      \n",
      "epoch 68 [51.22s]:  training loss=0.25629687309265137                                     \n",
      "epoch 69 [51.24s]:  training loss=0.25545135140419006                                     \n",
      "epoch 70 [51.32s]: training loss=0.252971351146698  validation ndcg@10=0.024967290677552306 [1.4s]\n",
      "epoch 1 [45.85s]:  training loss=0.7628549337387085                                       \n",
      "epoch 2 [45.52s]:  training loss=0.5919527411460876                                       \n",
      "epoch 3 [45.78s]:  training loss=0.4983498156070709                                       \n",
      "epoch 4 [45.35s]:  training loss=0.4276202917098999                                       \n",
      "epoch 5 [44.99s]: training loss=0.3686034083366394  validation ndcg@10=0.02371654971152201 [1.09s]\n",
      "epoch 6 [46.01s]:  training loss=0.33075883984565735                                      \n",
      "epoch 7 [44.33s]:  training loss=0.29055485129356384                                      \n",
      "epoch 8 [45.74s]:  training loss=0.2710740864276886                                       \n",
      "epoch 9 [44.43s]:  training loss=0.25110307335853577                                      \n",
      "epoch 10 [44.89s]: training loss=0.23914773762226105  validation ndcg@10=0.026438997292194413 [1.1s]\n",
      "epoch 11 [44.82s]:  training loss=0.22169895470142365                                     \n",
      "epoch 12 [45.22s]:  training loss=0.21254244446754456                                     \n",
      "epoch 13 [46.08s]:  training loss=0.19987677037715912                                     \n",
      "epoch 14 [44.96s]:  training loss=0.1923157423734665                                      \n",
      "epoch 15 [44.89s]: training loss=0.1853119283914566  validation ndcg@10=0.02789338665538643 [1.08s]\n",
      "epoch 16 [44.79s]:  training loss=0.17179995775222778                                     \n",
      "epoch 17 [45.11s]:  training loss=0.17186662554740906                                     \n",
      "epoch 18 [45.09s]:  training loss=0.16380558907985687                                     \n",
      "epoch 19 [45.15s]:  training loss=0.15973952412605286                                     \n",
      "epoch 20 [45.5s]: training loss=0.15620945394039154  validation ndcg@10=0.028143970094194424 [1.1s]\n",
      "epoch 21 [44.89s]:  training loss=0.14850908517837524                                     \n",
      "epoch 22 [45.13s]:  training loss=0.1465195119380951                                      \n",
      "epoch 23 [46.33s]:  training loss=0.14169645309448242                                     \n",
      "epoch 24 [45.08s]:  training loss=0.13745298981666565                                     \n",
      "epoch 25 [46.98s]: training loss=0.1365959346294403  validation ndcg@10=0.0291516525569428 [1.09s]\n",
      "epoch 26 [45.29s]:  training loss=0.13011714816093445                                     \n",
      "epoch 27 [47.43s]:  training loss=0.12804925441741943                                     \n",
      "epoch 28 [45.88s]:  training loss=0.12753431499004364                                     \n",
      "epoch 29 [45.56s]:  training loss=0.12311699241399765                                     \n",
      "epoch 30 [45.63s]: training loss=0.11578544229269028  validation ndcg@10=0.02904004434530825 [1.13s]\n",
      "epoch 31 [45.49s]:  training loss=0.11225374042987823                                     \n",
      "epoch 32 [45.57s]:  training loss=0.11291797459125519                                     \n",
      "epoch 33 [45.59s]:  training loss=0.1121462732553482                                      \n",
      "epoch 34 [45.4s]:  training loss=0.1118226945400238                                       \n",
      "epoch 35 [45.85s]: training loss=0.10669703036546707  validation ndcg@10=0.02838231042622199 [1.08s]\n",
      "epoch 36 [46.09s]:  training loss=0.10499124974012375                                     \n",
      "epoch 37 [47.06s]:  training loss=0.09996071457862854                                     \n",
      "epoch 38 [45.62s]:  training loss=0.09815291315317154                                     \n",
      "epoch 39 [45.89s]:  training loss=0.09641432762145996                                     \n",
      "epoch 40 [47.23s]: training loss=0.09532361477613449  validation ndcg@10=0.028707264813809828 [1.09s]\n",
      "epoch 41 [45.42s]:  training loss=0.09405483305454254                                     \n",
      "epoch 42 [45.75s]:  training loss=0.09374796599149704                                     \n",
      "epoch 43 [45.76s]:  training loss=0.0918930172920227                                      \n",
      "epoch 44 [44.82s]:  training loss=0.09159176051616669                                     \n",
      "epoch 45 [44.84s]: training loss=0.08875787258148193  validation ndcg@10=0.03009013386612922 [1.06s]\n",
      "epoch 46 [44.44s]:  training loss=0.08901643007993698                                     \n",
      "epoch 47 [45.1s]:  training loss=0.0876152291893959                                       \n",
      "epoch 48 [44.7s]:  training loss=0.08351612836122513                                      \n",
      "epoch 49 [45.46s]:  training loss=0.08241346478462219                                     \n",
      "epoch 50 [44.87s]: training loss=0.08161459118127823  validation ndcg@10=0.029527335123943187 [1.07s]\n",
      "epoch 51 [44.67s]:  training loss=0.07909321039915085                                     \n",
      "epoch 52 [45.43s]:  training loss=0.08233881741762161                                     \n",
      "epoch 53 [44.49s]:  training loss=0.07724122703075409                                     \n",
      "epoch 54 [45.77s]:  training loss=0.07567489147186279                                     \n",
      "epoch 55 [45.01s]: training loss=0.07761584222316742  validation ndcg@10=0.028664010583460926 [1.08s]\n",
      "epoch 56 [45.21s]:  training loss=0.0775359496474266                                      \n",
      "epoch 57 [45.44s]:  training loss=0.07407454401254654                                     \n",
      "epoch 58 [45.03s]:  training loss=0.07476959377527237                                     \n",
      "epoch 59 [45.01s]:  training loss=0.07269258052110672                                     \n",
      "epoch 60 [45.23s]: training loss=0.07185085862874985  validation ndcg@10=0.02960461122971419 [1.07s]\n",
      "epoch 61 [45.95s]:  training loss=0.07223913073539734                                     \n",
      "epoch 62 [45.49s]:  training loss=0.07272691279649734                                     \n",
      "epoch 63 [45.07s]:  training loss=0.07112713158130646                                     \n",
      "epoch 64 [45.42s]:  training loss=0.06984120607376099                                     \n",
      "epoch 65 [45.22s]: training loss=0.06644149869680405  validation ndcg@10=0.029582656703236895 [1.01s]\n",
      "epoch 66 [44.98s]:  training loss=0.06602318584918976                                     \n",
      "epoch 67 [46.15s]:  training loss=0.06604260951280594                                     \n",
      "epoch 68 [46.87s]:  training loss=0.06734160333871841                                     \n",
      "epoch 69 [44.75s]:  training loss=0.06539443880319595                                     \n",
      "epoch 70 [44.67s]: training loss=0.06488653272390366  validation ndcg@10=0.029901619937908797 [1.08s]\n",
      "epoch 1 [22.55s]:  training loss=1.50396728515625                                          \n",
      "epoch 2 [22.83s]:  training loss=2.5097553730010986                                        \n",
      "epoch 3 [22.83s]:  training loss=3.0844690799713135                                        \n",
      "epoch 4 [23.06s]:  training loss=3.553852081298828                                         \n",
      "epoch 5 [23.03s]: training loss=3.6855175495147705  validation ndcg@10=0.016939425299397487 [0.7s]\n",
      "epoch 6 [23.89s]:  training loss=3.786074161529541                                         \n",
      "epoch 7 [23.49s]:  training loss=4.17750358581543                                          \n",
      "epoch 8 [23.39s]:  training loss=4.173006057739258                                         \n",
      "epoch 9 [23.67s]:  training loss=4.463444232940674                                         \n",
      "epoch 10 [23.21s]: training loss=4.485318183898926  validation ndcg@10=0.0185370631479036 [0.7s]\n",
      "epoch 11 [23.13s]:  training loss=4.59618616104126                                         \n",
      "epoch 12 [23.03s]:  training loss=4.617719650268555                                        \n",
      "epoch 13 [23.46s]:  training loss=4.555263042449951                                        \n",
      "epoch 14 [22.6s]:  training loss=4.826323986053467                                         \n",
      "epoch 15 [23.13s]: training loss=4.937122344970703  validation ndcg@10=0.018496025286656344 [0.77s]\n",
      "epoch 16 [23.59s]:  training loss=5.010392665863037                                        \n",
      "epoch 17 [22.79s]:  training loss=5.033863544464111                                        \n",
      "epoch 18 [22.32s]:  training loss=5.265896320343018                                        \n",
      "epoch 19 [23.04s]:  training loss=5.042524337768555                                        \n",
      "epoch 20 [22.91s]: training loss=5.246640682220459  validation ndcg@10=0.020546789837717554 [0.64s]\n",
      "epoch 21 [23.09s]:  training loss=5.173307418823242                                        \n",
      "epoch 22 [23.22s]:  training loss=5.256927013397217                                        \n",
      "epoch 23 [22.66s]:  training loss=5.3379740715026855                                       \n",
      "epoch 24 [23.1s]:  training loss=5.718735218048096                                         \n",
      "epoch 25 [24.33s]: training loss=5.786444664001465  validation ndcg@10=0.01837424225855871 [1.28s]\n",
      "epoch 26 [22.95s]:  training loss=5.243141174316406                                        \n",
      "epoch 27 [23.02s]:  training loss=5.8743791580200195                                       \n",
      "epoch 28 [23.57s]:  training loss=5.407286167144775                                        \n",
      "epoch 29 [23.93s]:  training loss=5.813573837280273                                        \n",
      "epoch 30 [23.93s]: training loss=5.835819721221924  validation ndcg@10=0.018682232733793255 [0.68s]\n",
      "epoch 31 [23.58s]:  training loss=5.525251388549805                                        \n",
      "epoch 32 [24.15s]:  training loss=5.794114112854004                                        \n",
      "epoch 33 [23.83s]:  training loss=5.81723690032959                                         \n",
      "epoch 34 [23.84s]:  training loss=5.93039608001709                                         \n",
      "epoch 35 [23.92s]: training loss=6.5210418701171875  validation ndcg@10=0.02021712746156087 [0.62s]\n",
      "epoch 36 [23.96s]:  training loss=6.277228355407715                                        \n",
      "epoch 37 [24.04s]:  training loss=6.219295024871826                                        \n",
      "epoch 38 [23.63s]:  training loss=6.201624870300293                                        \n",
      "epoch 39 [23.52s]:  training loss=6.306979656219482                                        \n",
      "epoch 40 [24.02s]: training loss=6.8598737716674805  validation ndcg@10=0.01814952276834822 [0.67s]\n",
      "epoch 41 [23.58s]:  training loss=6.432375907897949                                        \n",
      "epoch 42 [24.07s]:  training loss=6.665348529815674                                        \n",
      "epoch 43 [24.08s]:  training loss=6.90078067779541                                         \n",
      "epoch 44 [23.57s]:  training loss=6.910959720611572                                        \n",
      "epoch 45 [23.09s]: training loss=6.570734024047852  validation ndcg@10=0.01949187828067948 [0.65s]\n",
      "epoch 1 [19.2s]:  training loss=2.99647855758667                                           \n",
      "epoch 2 [18.92s]:  training loss=5.844386577606201                                         \n",
      "epoch 3 [18.83s]:  training loss=7.328476428985596                                         \n",
      "epoch 4 [18.76s]:  training loss=8.792609214782715                                         \n",
      "epoch 5 [19.3s]: training loss=8.776701927185059  validation ndcg@10=0.018541772832406357 [0.57s]\n",
      "epoch 6 [19.01s]:  training loss=9.63921070098877                                          \n",
      "epoch 7 [19.23s]:  training loss=10.157204627990723                                        \n",
      "epoch 8 [19.13s]:  training loss=10.644691467285156                                        \n",
      "epoch 9 [19.54s]:  training loss=10.261346817016602                                        \n",
      "epoch 10 [19.52s]: training loss=11.281436920166016  validation ndcg@10=0.01896197202029782 [0.65s]\n",
      "epoch 11 [19.85s]:  training loss=11.847684860229492                                       \n",
      "epoch 12 [19.33s]:  training loss=11.790104866027832                                       \n",
      "epoch 13 [18.93s]:  training loss=11.583022117614746                                       \n",
      "epoch 14 [20.5s]:  training loss=11.320291519165039                                        \n",
      "epoch 15 [19.63s]: training loss=12.765480041503906  validation ndcg@10=0.019519643306188845 [0.56s]\n",
      "epoch 16 [18.73s]:  training loss=12.62646484375                                           \n",
      "epoch 17 [18.75s]:  training loss=12.52741813659668                                        \n",
      "epoch 18 [18.73s]:  training loss=13.070199966430664                                       \n",
      "epoch 19 [18.63s]:  training loss=12.484884262084961                                       \n",
      "epoch 20 [18.98s]: training loss=12.231107711791992  validation ndcg@10=0.019451010472362498 [0.59s]\n",
      "epoch 21 [18.83s]:  training loss=12.744996070861816                                       \n",
      "epoch 22 [18.82s]:  training loss=13.34385871887207                                        \n",
      "epoch 23 [18.75s]:  training loss=13.21316146850586                                        \n",
      "epoch 24 [19.37s]:  training loss=13.372265815734863                                       \n",
      "epoch 25 [18.84s]: training loss=12.811809539794922  validation ndcg@10=0.01783620320902926 [0.64s]\n",
      "epoch 26 [18.79s]:  training loss=14.258820533752441                                       \n",
      "epoch 27 [18.87s]:  training loss=14.928802490234375                                       \n",
      "epoch 28 [19.02s]:  training loss=13.754755020141602                                       \n",
      "epoch 29 [19.87s]:  training loss=14.335456848144531                                       \n",
      "epoch 30 [18.89s]: training loss=13.731315612792969  validation ndcg@10=0.01831143918274447 [0.64s]\n",
      "epoch 31 [18.79s]:  training loss=14.8496732711792                                         \n",
      "epoch 32 [19.02s]:  training loss=14.871238708496094                                       \n",
      "epoch 33 [18.54s]:  training loss=14.427098274230957                                       \n",
      "epoch 34 [18.65s]:  training loss=14.871221542358398                                       \n",
      "epoch 35 [18.42s]: training loss=14.244599342346191  validation ndcg@10=0.01621813011230905 [0.58s]\n",
      "epoch 36 [18.88s]:  training loss=16.208866119384766                                       \n",
      "epoch 37 [18.75s]:  training loss=14.712964057922363                                       \n",
      "epoch 38 [18.76s]:  training loss=16.233205795288086                                       \n",
      "epoch 39 [18.98s]:  training loss=16.51871681213379                                        \n",
      "epoch 40 [19.03s]: training loss=15.326216697692871  validation ndcg@10=0.01809918564266785 [0.61s]\n",
      "epoch 1 [10.63s]:  training loss=0.6223458647727966                                        \n",
      "epoch 2 [10.29s]:  training loss=0.8939935564994812                                        \n",
      "epoch 3 [10.57s]:  training loss=1.1131553649902344                                        \n",
      "epoch 4 [10.34s]:  training loss=1.243335247039795                                         \n",
      "epoch 5 [10.36s]: training loss=1.4313589334487915  validation ndcg@10=0.01798629551316845 [0.43s]\n",
      "epoch 6 [10.47s]:  training loss=1.4808732271194458                                        \n",
      "epoch 7 [10.53s]:  training loss=1.550743818283081                                         \n",
      "epoch 8 [10.36s]:  training loss=1.5737940073013306                                        \n",
      "epoch 9 [10.24s]:  training loss=1.6323339939117432                                        \n",
      "epoch 10 [10.45s]: training loss=1.7188313007354736  validation ndcg@10=0.020358863660388887 [0.43s]\n",
      "epoch 11 [10.43s]:  training loss=1.6597367525100708                                       \n",
      "epoch 12 [10.31s]:  training loss=1.68282151222229                                         \n",
      "epoch 13 [10.44s]:  training loss=1.8433572053909302                                       \n",
      "epoch 14 [10.7s]:  training loss=1.8455675840377808                                        \n",
      "epoch 15 [11.05s]: training loss=1.8600438833236694  validation ndcg@10=0.018267354866629438 [0.4s]\n",
      "epoch 16 [10.66s]:  training loss=1.9041188955307007                                       \n",
      "epoch 17 [10.52s]:  training loss=1.9977343082427979                                       \n",
      "epoch 18 [10.77s]:  training loss=2.005934000015259                                        \n",
      "epoch 19 [10.33s]:  training loss=2.025886297225952                                        \n",
      "epoch 20 [10.65s]: training loss=1.9519928693771362  validation ndcg@10=0.020570193839083102 [0.43s]\n",
      "epoch 21 [10.29s]:  training loss=2.076458692550659                                        \n",
      "epoch 22 [10.38s]:  training loss=2.1404342651367188                                       \n",
      "epoch 23 [10.34s]:  training loss=2.044957399368286                                        \n",
      "epoch 24 [10.42s]:  training loss=2.1297736167907715                                       \n",
      "epoch 25 [10.48s]: training loss=2.175039768218994  validation ndcg@10=0.01887126492442443 [0.41s]\n",
      "epoch 26 [10.78s]:  training loss=2.104482650756836                                        \n",
      "epoch 27 [10.48s]:  training loss=2.1483170986175537                                       \n",
      "epoch 28 [10.39s]:  training loss=2.175126075744629                                        \n",
      "epoch 29 [10.32s]:  training loss=2.191796064376831                                        \n",
      "epoch 30 [10.51s]: training loss=2.19838809967041  validation ndcg@10=0.020644399068670213 [0.41s]\n",
      "epoch 31 [10.64s]:  training loss=2.1296372413635254                                       \n",
      "epoch 32 [10.35s]:  training loss=2.1896591186523438                                       \n",
      "epoch 33 [10.48s]:  training loss=2.257507562637329                                        \n",
      "epoch 34 [10.51s]:  training loss=2.243781328201294                                        \n",
      "epoch 35 [10.56s]: training loss=2.2896335124969482  validation ndcg@10=0.01953417675102038 [0.38s]\n",
      "epoch 36 [10.59s]:  training loss=2.3103506565093994                                       \n",
      "epoch 37 [10.46s]:  training loss=2.467750310897827                                        \n",
      "epoch 38 [10.4s]:  training loss=2.4076409339904785                                        \n",
      "epoch 39 [10.32s]:  training loss=2.358842372894287                                        \n",
      "epoch 40 [10.49s]: training loss=2.384150981903076  validation ndcg@10=0.019828053212494184 [0.53s]\n",
      "epoch 41 [10.58s]:  training loss=2.3064050674438477                                       \n",
      "epoch 42 [10.24s]:  training loss=2.3751447200775146                                       \n",
      "epoch 43 [10.74s]:  training loss=2.4122121334075928                                       \n",
      "epoch 44 [10.51s]:  training loss=2.4553306102752686                                       \n",
      "epoch 45 [10.55s]: training loss=2.4745383262634277  validation ndcg@10=0.018444963002491484 [0.4s]\n",
      "epoch 46 [10.44s]:  training loss=2.249284029006958                                        \n",
      "epoch 47 [10.33s]:  training loss=2.545097589492798                                        \n",
      "epoch 48 [10.32s]:  training loss=2.6641552448272705                                       \n",
      "epoch 49 [10.67s]:  training loss=2.428893566131592                                        \n",
      "epoch 50 [10.97s]: training loss=2.4744837284088135  validation ndcg@10=0.018410856614326152 [0.43s]\n",
      "epoch 51 [10.31s]:  training loss=2.4591076374053955                                       \n",
      "epoch 52 [10.37s]:  training loss=2.6227080821990967                                       \n",
      "epoch 53 [10.29s]:  training loss=2.5282299518585205                                       \n",
      "epoch 54 [10.2s]:  training loss=2.62026047706604                                          \n",
      "epoch 55 [10.41s]: training loss=2.555687665939331  validation ndcg@10=0.01849162020550711 [0.42s]\n",
      "epoch 1 [5.74s]:  training loss=0.7184991240501404                                         \n",
      "epoch 2 [5.48s]:  training loss=1.0572153329849243                                         \n",
      "epoch 3 [5.39s]:  training loss=1.3174363374710083                                         \n",
      "epoch 4 [5.3s]:  training loss=1.5222129821777344                                          \n",
      "epoch 5 [5.34s]: training loss=1.6037769317626953  validation ndcg@10=0.013298742201811788 [0.33s]\n",
      "epoch 6 [5.33s]:  training loss=1.8005471229553223                                         \n",
      "epoch 7 [5.29s]:  training loss=1.8084138631820679                                         \n",
      "epoch 8 [5.53s]:  training loss=1.911280632019043                                          \n",
      "epoch 9 [5.59s]:  training loss=1.9630649089813232                                         \n",
      "epoch 10 [5.7s]: training loss=1.952661156654358  validation ndcg@10=0.01727276871865 [0.33s]\n",
      "epoch 11 [5.56s]:  training loss=1.9641995429992676                                        \n",
      "epoch 12 [5.32s]:  training loss=1.9935561418533325                                        \n",
      "epoch 13 [5.47s]:  training loss=2.060176134109497                                         \n",
      "epoch 14 [5.59s]:  training loss=2.0702919960021973                                        \n",
      "epoch 15 [5.47s]: training loss=2.12668514251709  validation ndcg@10=0.018681681358911403 [0.33s]\n",
      "epoch 16 [5.44s]:  training loss=2.0758843421936035                                        \n",
      "epoch 17 [5.55s]:  training loss=2.137294292449951                                         \n",
      "epoch 18 [5.55s]:  training loss=2.1831302642822266                                        \n",
      "epoch 19 [5.54s]:  training loss=2.1641135215759277                                        \n",
      "epoch 20 [5.52s]: training loss=2.1788599491119385  validation ndcg@10=0.019622253699139676 [0.33s]\n",
      "epoch 21 [5.92s]:  training loss=2.193035125732422                                         \n",
      "epoch 22 [5.66s]:  training loss=2.2058093547821045                                        \n",
      "epoch 23 [5.84s]:  training loss=2.2393946647644043                                        \n",
      "epoch 24 [5.61s]:  training loss=2.258906841278076                                         \n",
      "epoch 25 [5.47s]: training loss=2.3729946613311768  validation ndcg@10=0.017783125497203185 [0.35s]\n",
      "epoch 26 [5.56s]:  training loss=2.397099018096924                                         \n",
      "epoch 27 [5.51s]:  training loss=2.362562894821167                                         \n",
      "epoch 28 [5.59s]:  training loss=2.3062734603881836                                        \n",
      "epoch 29 [5.73s]:  training loss=2.41049861907959                                          \n",
      "epoch 30 [5.89s]: training loss=2.4894442558288574  validation ndcg@10=0.018625693945138636 [0.33s]\n",
      "epoch 31 [5.88s]:  training loss=2.2823874950408936                                        \n",
      "epoch 32 [5.68s]:  training loss=2.4552085399627686                                        \n",
      "epoch 33 [5.66s]:  training loss=2.4268057346343994                                        \n",
      "epoch 34 [5.63s]:  training loss=2.5358400344848633                                        \n",
      "epoch 35 [5.89s]: training loss=2.531806230545044  validation ndcg@10=0.01848945337465686 [0.33s]\n",
      "epoch 36 [5.83s]:  training loss=2.513047218322754                                         \n",
      "epoch 37 [5.47s]:  training loss=2.570244073867798                                         \n",
      "epoch 38 [5.39s]:  training loss=2.515793561935425                                         \n",
      "epoch 39 [5.57s]:  training loss=2.4780969619750977                                        \n",
      "epoch 40 [5.49s]: training loss=2.5799965858459473  validation ndcg@10=0.017202389873056424 [0.37s]\n",
      "epoch 41 [5.59s]:  training loss=2.5731632709503174                                        \n",
      "epoch 42 [6.51s]:  training loss=2.4145395755767822                                        \n",
      "epoch 43 [5.71s]:  training loss=2.586153030395508                                         \n",
      "epoch 44 [5.79s]:  training loss=2.5768380165100098                                        \n",
      "epoch 45 [5.49s]: training loss=2.5706870555877686  validation ndcg@10=0.02001246302306854 [0.33s]\n",
      "epoch 46 [5.7s]:  training loss=2.475400924682617                                          \n",
      "epoch 47 [5.45s]:  training loss=2.663908004760742                                         \n",
      "epoch 48 [5.37s]:  training loss=2.348855495452881                                         \n",
      "epoch 49 [5.57s]:  training loss=2.615098237991333                                         \n",
      "epoch 50 [5.49s]: training loss=2.5328238010406494  validation ndcg@10=0.016683611738928236 [0.35s]\n",
      "epoch 51 [5.87s]:  training loss=2.6275405883789062                                        \n",
      "epoch 52 [5.71s]:  training loss=2.4867818355560303                                        \n",
      "epoch 53 [5.51s]:  training loss=2.519331693649292                                         \n",
      "epoch 54 [5.37s]:  training loss=2.6495723724365234                                        \n",
      "epoch 55 [5.53s]: training loss=2.595576763153076  validation ndcg@10=0.021144003421941063 [0.37s]\n",
      "epoch 56 [5.59s]:  training loss=2.679704427719116                                         \n",
      "epoch 57 [5.63s]:  training loss=2.613992691040039                                         \n",
      "epoch 58 [5.47s]:  training loss=2.883842945098877                                         \n",
      "epoch 59 [5.44s]:  training loss=2.8258187770843506                                        \n",
      "epoch 60 [5.42s]: training loss=2.7882750034332275  validation ndcg@10=0.018514580594437415 [0.35s]\n",
      "epoch 61 [5.45s]:  training loss=2.6758792400360107                                        \n",
      "epoch 62 [5.64s]:  training loss=2.6266043186187744                                        \n",
      "epoch 63 [5.69s]:  training loss=2.867612838745117                                         \n",
      "epoch 64 [5.56s]:  training loss=2.801107406616211                                         \n",
      "epoch 65 [5.49s]: training loss=2.7776801586151123  validation ndcg@10=0.019821494556407075 [0.34s]\n",
      "epoch 66 [5.59s]:  training loss=2.7132010459899902                                        \n",
      "epoch 67 [5.45s]:  training loss=2.659520149230957                                         \n",
      "epoch 68 [5.46s]:  training loss=2.7878847122192383                                        \n",
      "epoch 69 [5.51s]:  training loss=2.7226412296295166                                        \n",
      "epoch 70 [5.48s]: training loss=2.7087714672088623  validation ndcg@10=0.019928605801661326 [0.33s]\n",
      "epoch 71 [5.42s]:  training loss=2.7248153686523438                                        \n",
      "epoch 72 [5.44s]:  training loss=2.6874163150787354                                        \n",
      "epoch 73 [5.7s]:  training loss=2.885974168777466                                          \n",
      "epoch 74 [5.5s]:  training loss=2.71738600730896                                           \n",
      "epoch 75 [5.7s]: training loss=2.89778733253479  validation ndcg@10=0.018710711331957484 [0.38s]\n",
      "epoch 76 [5.86s]:  training loss=2.676486015319824                                         \n",
      "epoch 77 [5.5s]:  training loss=2.7677597999572754                                         \n",
      "epoch 78 [5.47s]:  training loss=2.87196683883667                                          \n",
      "epoch 79 [5.41s]:  training loss=2.8558883666992188                                        \n",
      "epoch 80 [5.32s]: training loss=2.7955543994903564  validation ndcg@10=0.019959199741688836 [0.34s]\n",
      "epoch 1 [49.73s]:  training loss=0.8278840184211731                                        \n",
      "epoch 2 [47.16s]:  training loss=0.7784910202026367                                        \n",
      "epoch 3 [47.07s]:  training loss=0.7347830533981323                                        \n",
      "epoch 4 [48.46s]:  training loss=0.6769933700561523                                        \n",
      "epoch 5 [47.53s]: training loss=0.6224207878112793  validation ndcg@10=0.00960137904793808 [1.06s]\n",
      "epoch 6 [47.78s]:  training loss=0.5837297439575195                                        \n",
      "epoch 7 [47.63s]:  training loss=0.5544092059135437                                        \n",
      "epoch 8 [47.85s]:  training loss=0.529295802116394                                         \n",
      "epoch 9 [47.56s]:  training loss=0.5066410303115845                                        \n",
      "epoch 10 [47.12s]: training loss=0.48735159635543823  validation ndcg@10=0.014134960169422787 [1.15s]\n",
      "epoch 11 [48.76s]:  training loss=0.4672355651855469                                       \n",
      "epoch 12 [48.4s]:  training loss=0.4470585584640503                                        \n",
      "epoch 13 [47.92s]:  training loss=0.4288245439529419                                       \n",
      "epoch 14 [47.38s]:  training loss=0.40761125087738037                                      \n",
      "epoch 15 [47.83s]: training loss=0.39345410466194153  validation ndcg@10=0.02362612331038784 [1.2s]\n",
      "epoch 16 [48.58s]:  training loss=0.37656232714653015                                      \n",
      "epoch 17 [47.74s]:  training loss=0.36670657992362976                                      \n",
      "epoch 18 [47.4s]:  training loss=0.35387182235717773                                       \n",
      "epoch 19 [47.66s]:  training loss=0.3467831015586853                                       \n",
      "epoch 20 [47.8s]: training loss=0.3337518274784088  validation ndcg@10=0.025715859299310858 [1.27s]\n",
      "epoch 21 [47.62s]:  training loss=0.3254329264163971                                       \n",
      "epoch 22 [47.72s]:  training loss=0.3167751431465149                                       \n",
      "epoch 23 [47.85s]:  training loss=0.30572518706321716                                      \n",
      "epoch 24 [48.53s]:  training loss=0.30000802874565125                                      \n",
      "epoch 25 [47.35s]: training loss=0.28776752948760986  validation ndcg@10=0.024614911924122965 [1.13s]\n",
      "epoch 26 [47.61s]:  training loss=0.2821718752384186                                       \n",
      "epoch 27 [49.82s]:  training loss=0.2751604914665222                                       \n",
      "epoch 28 [49.26s]:  training loss=0.26806238293647766                                      \n",
      "epoch 29 [49.11s]:  training loss=0.26529598236083984                                      \n",
      "epoch 30 [49.46s]: training loss=0.25612103939056396  validation ndcg@10=0.023957244052003064 [1.11s]\n",
      "epoch 31 [48.78s]:  training loss=0.25687289237976074                                      \n",
      "epoch 32 [48.45s]:  training loss=0.25039270520210266                                      \n",
      "epoch 33 [48.44s]:  training loss=0.2444416880607605                                       \n",
      "epoch 34 [48.36s]:  training loss=0.2409299612045288                                       \n",
      "epoch 35 [49.52s]: training loss=0.23576444387435913  validation ndcg@10=0.024701320178540844 [1.16s]\n",
      "epoch 36 [49.09s]:  training loss=0.23267176747322083                                      \n",
      "epoch 37 [48.86s]:  training loss=0.22980289161205292                                      \n",
      "epoch 38 [49.15s]:  training loss=0.2258995622396469                                       \n",
      "epoch 39 [48.97s]:  training loss=0.21796193718910217                                      \n",
      "epoch 40 [49.13s]: training loss=0.21852010488510132  validation ndcg@10=0.024751511900452588 [1.13s]\n",
      "epoch 41 [48.56s]:  training loss=0.216458261013031                                        \n",
      "epoch 42 [49.87s]:  training loss=0.2153269499540329                                       \n",
      "epoch 43 [49.03s]:  training loss=0.20987458527088165                                      \n",
      "epoch 44 [49.22s]:  training loss=0.20765472948551178                                      \n",
      "epoch 45 [49.38s]: training loss=0.20465265214443207  validation ndcg@10=0.025285219156740542 [1.16s]\n",
      "epoch 1 [18.67s]:  training loss=0.6965173482894897                                        \n",
      "epoch 2 [19.05s]:  training loss=0.4880039691925049                                        \n",
      "epoch 3 [18.66s]:  training loss=0.38251540064811707                                       \n",
      "epoch 4 [18.74s]:  training loss=0.3145146369934082                                        \n",
      "epoch 5 [18.75s]: training loss=0.2783251106739044  validation ndcg@10=0.025244497050984305 [0.53s]\n",
      "epoch 6 [18.37s]:  training loss=0.2502121329307556                                        \n",
      "epoch 7 [18.71s]:  training loss=0.2295207679271698                                        \n",
      "epoch 8 [18.58s]:  training loss=0.21633410453796387                                       \n",
      "epoch 9 [19.73s]:  training loss=0.20026636123657227                                       \n",
      "epoch 10 [18.2s]: training loss=0.186679407954216  validation ndcg@10=0.02825177433804517 [0.55s]\n",
      "epoch 11 [18.28s]:  training loss=0.17935295403003693                                      \n",
      "epoch 12 [18.27s]:  training loss=0.16935938596725464                                      \n",
      "epoch 13 [18.2s]:  training loss=0.16210997104644775                                       \n",
      "epoch 14 [18.03s]:  training loss=0.15396563708782196                                      \n",
      "epoch 15 [18.15s]: training loss=0.15172676742076874  validation ndcg@10=0.028652491379653238 [0.53s]\n",
      "epoch 16 [18.33s]:  training loss=0.14607882499694824                                      \n",
      "epoch 17 [18.19s]:  training loss=0.13967257738113403                                      \n",
      "epoch 18 [18.24s]:  training loss=0.13099581003189087                                      \n",
      "epoch 19 [18.19s]:  training loss=0.12999922037124634                                      \n",
      "epoch 20 [18.56s]: training loss=0.12285293638706207  validation ndcg@10=0.029594156677692534 [0.48s]\n",
      "epoch 21 [18.33s]:  training loss=0.12046828866004944                                      \n",
      "epoch 22 [18.07s]:  training loss=0.11689524352550507                                      \n",
      "epoch 23 [18.14s]:  training loss=0.11305765807628632                                      \n",
      "epoch 24 [18.31s]:  training loss=0.11130878329277039                                      \n",
      "epoch 25 [18.42s]: training loss=0.10501652210950851  validation ndcg@10=0.028979753822264594 [0.55s]\n",
      "epoch 26 [18.27s]:  training loss=0.10367543250322342                                      \n",
      "epoch 27 [18.33s]:  training loss=0.10298566520214081                                      \n",
      "epoch 28 [18.32s]:  training loss=0.09894977509975433                                      \n",
      "epoch 29 [18.43s]:  training loss=0.09613975137472153                                      \n",
      "epoch 30 [18.39s]: training loss=0.09530530869960785  validation ndcg@10=0.029578059648096365 [0.54s]\n",
      "epoch 31 [18.87s]:  training loss=0.09279496222734451                                      \n",
      "epoch 32 [19.83s]:  training loss=0.09198643267154694                                      \n",
      "epoch 33 [18.33s]:  training loss=0.08736053854227066                                      \n",
      "epoch 34 [18.46s]:  training loss=0.08663841336965561                                      \n",
      "epoch 35 [18.11s]: training loss=0.08281747251749039  validation ndcg@10=0.029838320002662882 [0.54s]\n",
      "epoch 36 [18.35s]:  training loss=0.08309593796730042                                      \n",
      "epoch 37 [18.4s]:  training loss=0.08373382687568665                                       \n",
      "epoch 38 [18.38s]:  training loss=0.08004342019557953                                      \n",
      "epoch 39 [18.49s]:  training loss=0.08078670501708984                                      \n",
      "epoch 40 [18.18s]: training loss=0.07793411612510681  validation ndcg@10=0.02985804665157317 [0.56s]\n",
      "epoch 41 [18.39s]:  training loss=0.07793224602937698                                      \n",
      "epoch 42 [18.37s]:  training loss=0.07534566521644592                                      \n",
      "epoch 43 [18.55s]:  training loss=0.07610137015581131                                      \n",
      "epoch 44 [18.72s]:  training loss=0.07416374981403351                                      \n",
      "epoch 45 [18.56s]: training loss=0.06903035938739777  validation ndcg@10=0.029462941057136628 [0.51s]\n",
      "epoch 46 [18.6s]:  training loss=0.07177096605300903                                       \n",
      "epoch 47 [18.17s]:  training loss=0.068200021982193                                        \n",
      "epoch 48 [18.74s]:  training loss=0.06851377338171005                                      \n",
      "epoch 49 [19.25s]:  training loss=0.06781288981437683                                      \n",
      "epoch 50 [18.3s]: training loss=0.0684705525636673  validation ndcg@10=0.029701751759635735 [0.48s]\n",
      "epoch 51 [18.56s]:  training loss=0.06564070284366608                                      \n",
      "epoch 52 [18.56s]:  training loss=0.06425553560256958                                      \n",
      "epoch 53 [18.43s]:  training loss=0.0642089769244194                                       \n",
      "epoch 54 [18.45s]:  training loss=0.06475003808736801                                      \n",
      "epoch 55 [18.58s]: training loss=0.062091294676065445  validation ndcg@10=0.030194945696342387 [0.49s]\n",
      "epoch 56 [18.78s]:  training loss=0.06434129178524017                                      \n",
      "epoch 57 [18.34s]:  training loss=0.06140881031751633                                      \n",
      "epoch 58 [18.44s]:  training loss=0.061653077602386475                                     \n",
      "epoch 59 [18.66s]:  training loss=0.0602373443543911                                       \n",
      "epoch 60 [18.86s]: training loss=0.06012992560863495  validation ndcg@10=0.028926563017688915 [0.55s]\n",
      "epoch 61 [19.28s]:  training loss=0.06074158474802971                                      \n",
      "epoch 62 [18.65s]:  training loss=0.057009853422641754                                     \n",
      "epoch 63 [18.98s]:  training loss=0.05818381905555725                                      \n",
      "epoch 64 [21.36s]:  training loss=0.055474597960710526                                     \n",
      "epoch 65 [20.66s]: training loss=0.05602993816137314  validation ndcg@10=0.028856505694014323 [0.7s]\n",
      "epoch 66 [23.19s]:  training loss=0.05383831262588501                                      \n",
      "epoch 67 [21.29s]:  training loss=0.0558074414730072                                       \n",
      "epoch 68 [19.95s]:  training loss=0.05445386841893196                                      \n",
      "epoch 69 [19.52s]:  training loss=0.052284687757492065                                     \n",
      "epoch 70 [19.65s]: training loss=0.05466516315937042  validation ndcg@10=0.028003975127815706 [0.51s]\n",
      "epoch 71 [19.38s]:  training loss=0.053543347865343094                                     \n",
      "epoch 72 [19.51s]:  training loss=0.052427731454372406                                     \n",
      "epoch 73 [20.76s]:  training loss=0.05383323132991791                                      \n",
      "epoch 74 [19.84s]:  training loss=0.048641204833984375                                     \n",
      "epoch 75 [19.3s]: training loss=0.05055921524763107  validation ndcg@10=0.02842170961232454 [0.55s]\n",
      "epoch 76 [19.3s]:  training loss=0.050719548016786575                                      \n",
      "epoch 77 [19.09s]:  training loss=0.05153810977935791                                      \n",
      "epoch 78 [19.43s]:  training loss=0.04874001070857048                                      \n",
      "epoch 79 [19.01s]:  training loss=0.04961933568120003                                      \n",
      "epoch 80 [18.76s]: training loss=0.04896840080618858  validation ndcg@10=0.028605127189759768 [0.49s]\n",
      "epoch 1 [21.22s]:  training loss=0.411843478679657                                         \n",
      "epoch 2 [19.41s]:  training loss=0.20996743440628052                                       \n",
      "epoch 3 [20.6s]:  training loss=0.15523278713226318                                        \n",
      "epoch 4 [21.84s]:  training loss=0.12818939983844757                                       \n",
      "epoch 5 [22.07s]: training loss=0.11267734318971634  validation ndcg@10=0.026109422851091333 [0.53s]\n",
      "epoch 6 [20.06s]:  training loss=0.10007975995540619                                       \n",
      "epoch 7 [20.27s]:  training loss=0.0926455482840538                                        \n",
      "epoch 8 [21.53s]:  training loss=0.08264478296041489                                       \n",
      "epoch 9 [19.98s]:  training loss=0.07984054833650589                                       \n",
      "epoch 10 [21.66s]: training loss=0.07365933805704117  validation ndcg@10=0.027721947811397113 [0.56s]\n",
      "epoch 11 [20.72s]:  training loss=0.07327743619680405                                      \n",
      "epoch 12 [21.59s]:  training loss=0.06882843375205994                                      \n",
      "epoch 13 [19.28s]:  training loss=0.06783674657344818                                      \n",
      "epoch 14 [21.9s]:  training loss=0.06673769652843475                                       \n",
      "epoch 15 [22.92s]: training loss=0.06233501061797142  validation ndcg@10=0.025605688631509308 [0.73s]\n",
      "epoch 16 [22.18s]:  training loss=0.06237681582570076                                      \n",
      "epoch 17 [20.65s]:  training loss=0.060186076909303665                                     \n",
      "epoch 18 [22.85s]:  training loss=0.05792324244976044                                      \n",
      "epoch 19 [23.32s]:  training loss=0.05778547376394272                                      \n",
      "epoch 20 [20.73s]: training loss=0.05674504116177559  validation ndcg@10=0.02532563480054946 [0.61s]\n",
      "epoch 21 [19.1s]:  training loss=0.05374212563037872                                       \n",
      "epoch 22 [19.2s]:  training loss=0.0556429922580719                                        \n",
      "epoch 23 [21.22s]:  training loss=0.056404076516628265                                     \n",
      "epoch 24 [18.59s]:  training loss=0.05178353935480118                                      \n",
      "epoch 25 [21.76s]: training loss=0.05168584734201431  validation ndcg@10=0.023724070992636057 [0.56s]\n",
      "epoch 26 [20.78s]:  training loss=0.05369258671998978                                      \n",
      "epoch 27 [20.0s]:  training loss=0.05260821804404259                                       \n",
      "epoch 28 [22.61s]:  training loss=0.05297917127609253                                      \n",
      "epoch 29 [22.02s]:  training loss=0.05160359665751457                                      \n",
      "epoch 30 [20.78s]: training loss=0.05139394849538803  validation ndcg@10=0.02331059039888369 [0.57s]\n",
      "epoch 31 [21.66s]:  training loss=0.050334442406892776                                     \n",
      "epoch 32 [20.56s]:  training loss=0.04892357811331749                                      \n",
      "epoch 33 [21.78s]:  training loss=0.05186907947063446                                      \n",
      "epoch 34 [20.27s]:  training loss=0.052034471184015274                                     \n",
      "epoch 35 [21.06s]: training loss=0.053638771176338196  validation ndcg@10=0.024267798474824093 [0.71s]\n",
      "epoch 1 [30.38s]:  training loss=0.6824994683265686                                        \n",
      "epoch 2 [29.65s]:  training loss=0.47242045402526855                                       \n",
      "epoch 3 [30.52s]:  training loss=0.3724367320537567                                        \n",
      "epoch 4 [35.39s]:  training loss=0.30629798769950867                                       \n",
      "epoch 5 [29.73s]: training loss=0.25985461473464966  validation ndcg@10=0.025031834730354943 [0.91s]\n",
      "epoch 6 [28.32s]:  training loss=0.2264792025089264                                        \n",
      "epoch 7 [28.15s]:  training loss=0.20599889755249023                                       \n",
      "epoch 8 [27.99s]:  training loss=0.18384778499603271                                       \n",
      "epoch 9 [26.76s]:  training loss=0.16893164813518524                                       \n",
      "epoch 10 [31.03s]: training loss=0.15645529329776764  validation ndcg@10=0.024451208752758066 [0.85s]\n",
      "epoch 11 [32.9s]:  training loss=0.14795444905757904                                       \n",
      "epoch 12 [38.04s]:  training loss=0.1366073489189148                                       \n",
      "epoch 13 [33.92s]:  training loss=0.135149747133255                                        \n",
      "epoch 14 [29.73s]:  training loss=0.12792038917541504                                      \n",
      "epoch 15 [32.8s]: training loss=0.12425461411476135  validation ndcg@10=0.02614753355360655 [1.28s]\n",
      "epoch 16 [36.44s]:  training loss=0.12008693814277649                                      \n",
      "epoch 17 [40.7s]:  training loss=0.11726681143045425                                       \n",
      "epoch 18 [40.87s]:  training loss=0.11365561187267303                                      \n",
      "epoch 19 [41.24s]:  training loss=0.11303538084030151                                      \n",
      "epoch 20 [39.52s]: training loss=0.10833229124546051  validation ndcg@10=0.025100925212882323 [1.15s]\n",
      "epoch 21 [42.12s]:  training loss=0.10262129455804825                                      \n",
      "epoch 22 [48.18s]:  training loss=0.10241212695837021                                      \n",
      "epoch 23 [38.68s]:  training loss=0.10085920244455338                                      \n",
      "epoch 24 [34.0s]:  training loss=0.10052159428596497                                       \n",
      "epoch 25 [39.11s]: training loss=0.10066806524991989  validation ndcg@10=0.024060321481706753 [1.19s]\n",
      "epoch 26 [40.19s]:  training loss=0.09776682406663895                                      \n",
      "epoch 27 [43.27s]:  training loss=0.0952180027961731                                       \n",
      "epoch 28 [37.97s]:  training loss=0.09505278617143631                                      \n",
      "epoch 29 [38.47s]:  training loss=0.09231685847043991                                      \n",
      "epoch 30 [38.15s]: training loss=0.09110977500677109  validation ndcg@10=0.025182607572845975 [1.23s]\n",
      "epoch 31 [41.11s]:  training loss=0.09144047647714615                                      \n",
      "epoch 32 [40.79s]:  training loss=0.09162157773971558                                      \n",
      "epoch 33 [48.54s]:  training loss=0.088713139295578                                        \n",
      "epoch 34 [56.24s]:  training loss=0.09009134769439697                                      \n",
      "epoch 35 [45.39s]: training loss=0.08804626017808914  validation ndcg@10=0.024107894704711485 [1.9s]\n",
      "epoch 36 [59.84s]:  training loss=0.09016665071249008                                      \n",
      "epoch 37 [43.6s]:  training loss=0.08973357826471329                                       \n",
      "epoch 38 [51.03s]:  training loss=0.08777591586112976                                      \n",
      "epoch 39 [55.54s]:  training loss=0.08623457700014114                                      \n",
      "epoch 40 [58.7s]: training loss=0.08667716383934021  validation ndcg@10=0.02347767511053156 [1.73s]\n",
      "epoch 1 [40.19s]:  training loss=0.7384799718856812                                        \n",
      "epoch 2 [37.66s]:  training loss=0.5500780940055847                                        \n",
      "epoch 3 [37.28s]:  training loss=0.45597612857818604                                       \n",
      "epoch 4 [34.99s]:  training loss=0.3834662139415741                                        \n",
      "epoch 5 [37.02s]: training loss=0.3335185647010803  validation ndcg@10=0.026140758597279195 [0.91s]\n",
      "epoch 6 [35.41s]:  training loss=0.2938844859600067                                        \n",
      "epoch 7 [35.87s]:  training loss=0.2685226500034332                                        \n",
      "epoch 8 [36.17s]:  training loss=0.24700145423412323                                       \n",
      "epoch 9 [36.46s]:  training loss=0.2285299152135849                                        \n",
      "epoch 10 [36.5s]: training loss=0.21550820767879486  validation ndcg@10=0.027153270351886173 [0.85s]\n",
      "epoch 11 [38.37s]:  training loss=0.19977432489395142                                      \n",
      "epoch 12 [37.42s]:  training loss=0.19158807396888733                                      \n",
      "epoch 13 [38.16s]:  training loss=0.17819951474666595                                      \n",
      "epoch 14 [35.5s]:  training loss=0.17515452206134796                                       \n",
      "epoch 15 [37.8s]: training loss=0.16414262354373932  validation ndcg@10=0.0273399202911429 [0.91s]\n",
      "epoch 16 [36.64s]:  training loss=0.15812724828720093                                      \n",
      "epoch 17 [36.36s]:  training loss=0.15318354964256287                                      \n",
      "epoch 18 [38.05s]:  training loss=0.14942678809165955                                      \n",
      "epoch 19 [36.8s]:  training loss=0.14267690479755402                                       \n",
      "epoch 20 [35.0s]: training loss=0.14116746187210083  validation ndcg@10=0.029594572429643265 [1.15s]\n",
      "epoch 21 [37.35s]:  training loss=0.13382555544376373                                      \n",
      "epoch 22 [35.64s]:  training loss=0.1316910833120346                                       \n",
      "epoch 23 [35.42s]:  training loss=0.12516768276691437                                      \n",
      "epoch 24 [37.96s]:  training loss=0.12403609603643417                                      \n",
      "epoch 25 [38.84s]: training loss=0.11994548887014389  validation ndcg@10=0.029576257486612887 [1.12s]\n",
      "epoch 26 [40.43s]:  training loss=0.11426983028650284                                      \n",
      "epoch 27 [44.18s]:  training loss=0.1138865128159523                                       \n",
      "epoch 28 [41.36s]:  training loss=0.11336825788021088                                      \n",
      "epoch 29 [40.21s]:  training loss=0.10720324516296387                                      \n",
      "epoch 30 [35.85s]: training loss=0.10884886234998703  validation ndcg@10=0.029730577445978477 [0.78s]\n",
      "epoch 31 [29.33s]:  training loss=0.10402791947126389                                      \n",
      "epoch 32 [29.25s]:  training loss=0.10031542181968689                                      \n",
      "epoch 33 [28.01s]:  training loss=0.09720804542303085                                      \n",
      "epoch 34 [23.7s]:  training loss=0.09662780910730362                                       \n",
      "epoch 35 [18.52s]: training loss=0.09371088445186615  validation ndcg@10=0.029108502859771512 [0.6s]\n",
      "epoch 36 [18.75s]:  training loss=0.09603433310985565                                      \n",
      "epoch 37 [18.75s]:  training loss=0.09211168438196182                                      \n",
      "epoch 38 [18.9s]:  training loss=0.09278031438589096                                       \n",
      "epoch 39 [18.87s]:  training loss=0.0908060222864151                                       \n",
      "epoch 40 [18.51s]: training loss=0.08639979362487793  validation ndcg@10=0.02979455988808729 [0.61s]\n",
      "epoch 41 [19.02s]:  training loss=0.08593331277370453                                      \n",
      "epoch 42 [18.57s]:  training loss=0.080324187874794                                        \n",
      "epoch 43 [19.32s]:  training loss=0.08063004165887833                                      \n",
      "epoch 44 [18.37s]:  training loss=0.07778993248939514                                      \n",
      "epoch 45 [18.37s]: training loss=0.08205080777406693  validation ndcg@10=0.02924051055041661 [0.57s]\n",
      "epoch 46 [20.32s]:  training loss=0.07843836396932602                                      \n",
      "epoch 47 [18.89s]:  training loss=0.07736597955226898                                      \n",
      "epoch 48 [19.24s]:  training loss=0.07555202394723892                                      \n",
      "epoch 49 [18.7s]:  training loss=0.07427702099084854                                       \n",
      "epoch 50 [18.73s]: training loss=0.07295099645853043  validation ndcg@10=0.0298749135482637 [0.57s]\n",
      "epoch 51 [18.51s]:  training loss=0.07155289500951767                                      \n",
      "epoch 52 [18.43s]:  training loss=0.0721253901720047                                       \n",
      "epoch 53 [18.78s]:  training loss=0.06737061589956284                                      \n",
      "epoch 54 [19.15s]:  training loss=0.06992249935865402                                      \n",
      "epoch 55 [18.51s]: training loss=0.06766194850206375  validation ndcg@10=0.029333706869732728 [0.62s]\n",
      "epoch 56 [18.8s]:  training loss=0.06810352951288223                                       \n",
      "epoch 57 [19.01s]:  training loss=0.06715402007102966                                      \n",
      "epoch 58 [18.26s]:  training loss=0.0659513846039772                                       \n",
      "epoch 59 [19.17s]:  training loss=0.06485149264335632                                      \n",
      "epoch 60 [19.28s]: training loss=0.06456104665994644  validation ndcg@10=0.029134269934047927 [0.6s]\n",
      "epoch 61 [18.55s]:  training loss=0.06177730858325958                                      \n",
      "epoch 62 [18.7s]:  training loss=0.060669273138046265                                      \n",
      "epoch 63 [19.97s]:  training loss=0.06308197230100632                                      \n",
      "epoch 64 [18.75s]:  training loss=0.05933897942304611                                      \n",
      "epoch 65 [18.41s]: training loss=0.06303911656141281  validation ndcg@10=0.028786261761557425 [0.57s]\n",
      "epoch 66 [18.58s]:  training loss=0.058805182576179504                                     \n",
      "epoch 67 [18.44s]:  training loss=0.06195281445980072                                      \n",
      "epoch 68 [18.62s]:  training loss=0.05995909869670868                                      \n",
      "epoch 69 [19.05s]:  training loss=0.057364627718925476                                     \n",
      "epoch 70 [19.24s]: training loss=0.05739232525229454  validation ndcg@10=0.029858129500017577 [0.6s]\n",
      "epoch 71 [18.36s]:  training loss=0.05555182695388794                                      \n",
      "epoch 72 [18.52s]:  training loss=0.056282710283994675                                     \n",
      "epoch 73 [18.84s]:  training loss=0.05657918006181717                                      \n",
      "epoch 74 [18.85s]:  training loss=0.05394481495022774                                      \n",
      "epoch 75 [18.55s]: training loss=0.05390280485153198  validation ndcg@10=0.029113807881422416 [0.57s]\n",
      "epoch 1 [8.56s]:  training loss=0.46266019344329834                                        \n",
      "epoch 2 [8.51s]:  training loss=0.22935821115970612                                        \n",
      "epoch 3 [8.69s]:  training loss=0.17423686385154724                                        \n",
      "epoch 4 [8.56s]:  training loss=0.1448432058095932                                         \n",
      "epoch 5 [8.67s]: training loss=0.12921197712421417  validation ndcg@10=0.023341449517817726 [0.43s]\n",
      "epoch 6 [8.48s]:  training loss=0.11620406061410904                                        \n",
      "epoch 7 [8.51s]:  training loss=0.11211446672677994                                        \n",
      "epoch 8 [8.19s]:  training loss=0.10311546921730042                                        \n",
      "epoch 9 [8.42s]:  training loss=0.09637629240751266                                        \n",
      "epoch 10 [8.5s]: training loss=0.09439690411090851  validation ndcg@10=0.02550467765882519 [0.46s]\n",
      "epoch 11 [9.86s]:  training loss=0.09307188540697098                                       \n",
      "epoch 12 [8.47s]:  training loss=0.09045100957155228                                       \n",
      "epoch 13 [8.23s]:  training loss=0.08892849087715149                                       \n",
      "epoch 14 [8.19s]:  training loss=0.08631494641304016                                       \n",
      "epoch 15 [8.29s]: training loss=0.08570904284715652  validation ndcg@10=0.02567232363775859 [0.4s]\n",
      "epoch 16 [8.56s]:  training loss=0.07886307686567307                                       \n",
      "epoch 17 [8.37s]:  training loss=0.08356957137584686                                       \n",
      "epoch 18 [8.35s]:  training loss=0.08352820575237274                                       \n",
      "epoch 19 [8.4s]:  training loss=0.08339494466781616                                        \n",
      "epoch 20 [8.59s]: training loss=0.07919608056545258  validation ndcg@10=0.02408815517359542 [0.45s]\n",
      "epoch 21 [8.74s]:  training loss=0.07911451160907745                                       \n",
      "epoch 22 [8.15s]:  training loss=0.07959281653165817                                       \n",
      "epoch 23 [8.47s]:  training loss=0.07812156528234482                                       \n",
      "epoch 24 [8.45s]:  training loss=0.07704588770866394                                       \n",
      "epoch 25 [8.72s]: training loss=0.07858025282621384  validation ndcg@10=0.023312414641579883 [0.41s]\n",
      "epoch 26 [8.62s]:  training loss=0.0759347602725029                                        \n",
      "epoch 27 [8.37s]:  training loss=0.0787016972899437                                        \n",
      "epoch 28 [8.42s]:  training loss=0.07898569852113724                                       \n",
      "epoch 29 [8.1s]:  training loss=0.07772793620824814                                        \n",
      "epoch 30 [8.62s]: training loss=0.07933895289897919  validation ndcg@10=0.022729724242176865 [0.41s]\n",
      "epoch 31 [8.5s]:  training loss=0.07457207888364792                                        \n",
      "epoch 32 [8.53s]:  training loss=0.07795418053865433                                       \n",
      "epoch 33 [8.21s]:  training loss=0.07643789798021317                                       \n",
      "epoch 34 [8.15s]:  training loss=0.08145781606435776                                       \n",
      "epoch 35 [9.27s]: training loss=0.07428082823753357  validation ndcg@10=0.02401700247955136 [0.44s]\n",
      "epoch 36 [9.07s]:  training loss=0.07952212542295456                                       \n",
      "epoch 37 [8.98s]:  training loss=0.07473097741603851                                       \n",
      "epoch 38 [8.62s]:  training loss=0.07456589490175247                                       \n",
      "epoch 39 [9.0s]:  training loss=0.07649548351764679                                        \n",
      "epoch 40 [8.9s]: training loss=0.07498663663864136  validation ndcg@10=0.022924313103843696 [0.41s]\n",
      "epoch 1 [27.48s]:  training loss=0.7879813313484192                                        \n",
      "epoch 2 [27.42s]:  training loss=0.6701558232307434                                       \n",
      "epoch 3 [27.9s]:  training loss=0.5692980289459229                                        \n",
      "epoch 4 [26.56s]:  training loss=0.5082922577857971                                       \n",
      "epoch 5 [26.21s]: training loss=0.4540945589542389  validation ndcg@10=0.017178329018940647 [0.66s]\n",
      "epoch 6 [26.56s]:  training loss=0.40799394249916077                                      \n",
      "epoch 7 [26.35s]:  training loss=0.3729845881462097                                       \n",
      "epoch 8 [26.64s]:  training loss=0.34286871552467346                                      \n",
      "epoch 9 [26.46s]:  training loss=0.3172198235988617                                       \n",
      "epoch 10 [27.33s]: training loss=0.3002953827381134  validation ndcg@10=0.024031635901231595 [0.7s]\n",
      "epoch 11 [26.51s]:  training loss=0.27840539813041687                                     \n",
      "epoch 12 [26.36s]:  training loss=0.2712685465812683                                      \n",
      "epoch 13 [26.52s]:  training loss=0.25851160287857056                                     \n",
      "epoch 14 [25.57s]:  training loss=0.24482567608356476                                     \n",
      "epoch 15 [25.88s]: training loss=0.2364373654127121  validation ndcg@10=0.026042330690928116 [0.63s]\n",
      "epoch 16 [27.2s]:  training loss=0.22962599992752075                                      \n",
      "epoch 17 [25.34s]:  training loss=0.22197002172470093                                     \n",
      "epoch 18 [25.78s]:  training loss=0.21492935717105865                                     \n",
      "epoch 19 [26.37s]:  training loss=0.2089039534330368                                      \n",
      "epoch 20 [25.89s]: training loss=0.2000589519739151  validation ndcg@10=0.027441185004636864 [0.62s]\n",
      "epoch 21 [26.65s]:  training loss=0.19618363678455353                                     \n",
      "epoch 22 [26.48s]:  training loss=0.18951822817325592                                     \n",
      "epoch 23 [26.23s]:  training loss=0.18914809823036194                                     \n",
      "epoch 24 [25.89s]:  training loss=0.1850125640630722                                      \n",
      "epoch 25 [26.06s]: training loss=0.17690503597259521  validation ndcg@10=0.028709768508844775 [0.64s]\n",
      "epoch 26 [26.7s]:  training loss=0.17204950749874115                                      \n",
      "epoch 27 [27.35s]:  training loss=0.17387133836746216                                     \n",
      "epoch 28 [28.03s]:  training loss=0.16560882329940796                                     \n",
      "epoch 29 [25.89s]:  training loss=0.16397489607334137                                     \n",
      "epoch 30 [25.72s]: training loss=0.16075965762138367  validation ndcg@10=0.029089268007305942 [0.65s]\n",
      "epoch 31 [26.91s]:  training loss=0.15803968906402588                                     \n",
      "epoch 32 [26.37s]:  training loss=0.15298932790756226                                     \n",
      "epoch 33 [26.49s]:  training loss=0.1499853879213333                                      \n",
      "epoch 34 [26.5s]:  training loss=0.15207023918628693                                      \n",
      "epoch 35 [26.66s]: training loss=0.14589285850524902  validation ndcg@10=0.02952331785761362 [0.68s]\n",
      "epoch 36 [26.49s]:  training loss=0.1437816619873047                                      \n",
      "epoch 37 [26.26s]:  training loss=0.14117464423179626                                     \n",
      "epoch 38 [26.35s]:  training loss=0.13701055943965912                                     \n",
      "epoch 39 [26.38s]:  training loss=0.13583414256572723                                     \n",
      "epoch 40 [26.14s]: training loss=0.13607348501682281  validation ndcg@10=0.02905397681804364 [0.62s]\n",
      "epoch 41 [27.67s]:  training loss=0.13023704290390015                                     \n",
      "epoch 42 [25.84s]:  training loss=0.1339917629957199                                      \n",
      "epoch 43 [26.16s]:  training loss=0.1293729841709137                                      \n",
      "epoch 44 [26.48s]:  training loss=0.1272350549697876                                      \n",
      "epoch 45 [26.05s]: training loss=0.12891344726085663  validation ndcg@10=0.030389719780717594 [0.62s]\n",
      "epoch 46 [26.15s]:  training loss=0.12506689131259918                                     \n",
      "epoch 47 [25.79s]:  training loss=0.12318510562181473                                     \n",
      "epoch 48 [26.06s]:  training loss=0.12289885431528091                                     \n",
      "epoch 49 [25.93s]:  training loss=0.12108006328344345                                     \n",
      "epoch 50 [25.79s]: training loss=0.12155357003211975  validation ndcg@10=0.030302600191911503 [0.6s]\n",
      "epoch 51 [26.59s]:  training loss=0.11600250750780106                                     \n",
      "epoch 52 [26.09s]:  training loss=0.11324086785316467                                     \n",
      "epoch 53 [25.8s]:  training loss=0.11440545320510864                                      \n",
      "epoch 54 [27.03s]:  training loss=0.11102946847677231                                     \n",
      "epoch 55 [24.54s]: training loss=0.10862690955400467  validation ndcg@10=0.03085043538861164 [0.65s]\n",
      "epoch 56 [26.45s]:  training loss=0.1087590754032135                                      \n",
      "epoch 57 [26.74s]:  training loss=0.11142110079526901                                     \n",
      "epoch 58 [26.76s]:  training loss=0.1063055619597435                                      \n",
      "epoch 59 [26.97s]:  training loss=0.10718465596437454                                     \n",
      "epoch 60 [27.34s]: training loss=0.10717420279979706  validation ndcg@10=0.030137481152024453 [0.65s]\n",
      "epoch 61 [27.3s]:  training loss=0.10519525408744812                                      \n",
      "epoch 62 [26.27s]:  training loss=0.10390795767307281                                     \n",
      "epoch 63 [26.82s]:  training loss=0.10350716859102249                                     \n",
      "epoch 64 [27.21s]:  training loss=0.09980536252260208                                     \n",
      "epoch 65 [27.38s]: training loss=0.10165401548147202  validation ndcg@10=0.030017083993650884 [0.7s]\n",
      "epoch 66 [27.44s]:  training loss=0.10015996545553207                                     \n",
      "epoch 67 [27.04s]:  training loss=0.09856606274843216                                     \n",
      "epoch 68 [27.37s]:  training loss=0.0964960977435112                                      \n",
      "epoch 69 [27.14s]:  training loss=0.09582389891147614                                     \n",
      "epoch 70 [26.74s]: training loss=0.09540724754333496  validation ndcg@10=0.02931418678040359 [0.69s]\n",
      "epoch 71 [27.67s]:  training loss=0.09678000956773758                                     \n",
      "epoch 72 [28.2s]:  training loss=0.09144823998212814                                      \n",
      "epoch 73 [27.45s]:  training loss=0.09364637732505798                                     \n",
      "epoch 74 [27.7s]:  training loss=0.09556702524423599                                      \n",
      "epoch 75 [27.46s]: training loss=0.09096172451972961  validation ndcg@10=0.029835000221167203 [0.63s]\n",
      "epoch 76 [28.57s]:  training loss=0.08945135772228241                                     \n",
      "epoch 77 [26.56s]:  training loss=0.08829345554113388                                     \n",
      "epoch 78 [27.02s]:  training loss=0.08764322847127914                                     \n",
      "epoch 79 [26.53s]:  training loss=0.09051021933555603                                     \n",
      "epoch 80 [27.21s]: training loss=0.09136243164539337  validation ndcg@10=0.02976740309877983 [0.69s]\n",
      "epoch 1 [26.93s]:  training loss=0.8036921620368958                                       \n",
      "epoch 2 [27.44s]:  training loss=0.7335617542266846                                       \n",
      "epoch 3 [27.08s]:  training loss=0.6449239253997803                                       \n",
      "epoch 4 [27.7s]:  training loss=0.5793453454971313                                        \n",
      "epoch 5 [26.81s]: training loss=0.5423718094825745  validation ndcg@10=0.012042877240593516 [0.69s]\n",
      "epoch 6 [27.24s]:  training loss=0.4974043667316437                                       \n",
      "epoch 7 [26.96s]:  training loss=0.45134249329566956                                      \n",
      "epoch 8 [27.34s]:  training loss=0.4220888018608093                                       \n",
      "epoch 9 [27.53s]:  training loss=0.3969148099422455                                       \n",
      "epoch 10 [27.69s]: training loss=0.37359529733657837  validation ndcg@10=0.02318110135138551 [0.65s]\n",
      "epoch 11 [26.77s]:  training loss=0.35257893800735474                                     \n",
      "epoch 12 [27.43s]:  training loss=0.335966020822525                                       \n",
      "epoch 13 [27.1s]:  training loss=0.31932875514030457                                      \n",
      "epoch 14 [27.33s]:  training loss=0.30841484665870667                                     \n",
      "epoch 15 [27.29s]: training loss=0.2961912751197815  validation ndcg@10=0.025102890030141944 [0.69s]\n",
      "epoch 16 [26.56s]:  training loss=0.28803056478500366                                      \n",
      "epoch 17 [26.88s]:  training loss=0.2730500102043152                                       \n",
      "epoch 18 [26.26s]:  training loss=0.2690160572528839                                       \n",
      "epoch 19 [27.16s]:  training loss=0.25998079776763916                                      \n",
      "epoch 20 [26.9s]: training loss=0.25062501430511475  validation ndcg@10=0.025980764971487377 [0.67s]\n",
      "epoch 21 [26.69s]:  training loss=0.24641165137290955                                      \n",
      "epoch 22 [27.15s]:  training loss=0.24001821875572205                                      \n",
      "epoch 23 [25.15s]:  training loss=0.2345949411392212                                       \n",
      "epoch 24 [25.72s]:  training loss=0.23373080790042877                                      \n",
      "epoch 25 [26.85s]: training loss=0.22439803183078766  validation ndcg@10=0.026575401613242945 [0.73s]\n",
      "epoch 26 [26.88s]:  training loss=0.22314991056919098                                      \n",
      "epoch 27 [26.88s]:  training loss=0.21800537407398224                                      \n",
      "epoch 28 [26.67s]:  training loss=0.21043378114700317                                      \n",
      "epoch 29 [26.29s]:  training loss=0.20889323949813843                                      \n",
      "epoch 30 [26.76s]: training loss=0.20363888144493103  validation ndcg@10=0.027337334931693158 [0.62s]\n",
      "epoch 31 [26.82s]:  training loss=0.19936957955360413                                      \n",
      "epoch 32 [27.34s]:  training loss=0.19484663009643555                                      \n",
      "epoch 33 [26.41s]:  training loss=0.195321187376976                                        \n",
      "epoch 34 [25.78s]:  training loss=0.18818020820617676                                      \n",
      "epoch 35 [25.73s]: training loss=0.18735620379447937  validation ndcg@10=0.028507764321886318 [0.66s]\n",
      "epoch 36 [26.13s]:  training loss=0.18441987037658691                                      \n",
      "epoch 37 [26.57s]:  training loss=0.17969825863838196                                      \n",
      "epoch 38 [26.96s]:  training loss=0.1778198629617691                                       \n",
      "epoch 39 [26.22s]:  training loss=0.1733298897743225                                       \n",
      "epoch 40 [26.2s]: training loss=0.17221947014331818  validation ndcg@10=0.02865435839349851 [0.67s]\n",
      "epoch 41 [26.04s]:  training loss=0.17262758314609528                                      \n",
      "epoch 42 [25.98s]:  training loss=0.1679808497428894                                       \n",
      "epoch 43 [26.42s]:  training loss=0.16418687999248505                                      \n",
      "epoch 44 [26.32s]:  training loss=0.15496554970741272                                      \n",
      "epoch 45 [26.7s]: training loss=0.16229434311389923  validation ndcg@10=0.029299668666442406 [0.61s]\n",
      "epoch 46 [25.89s]:  training loss=0.15792031586170197                                      \n",
      "epoch 47 [26.44s]:  training loss=0.15616053342819214                                      \n",
      "epoch 48 [26.42s]:  training loss=0.15422789752483368                                      \n",
      "epoch 49 [26.87s]:  training loss=0.15263457596302032                                      \n",
      "epoch 50 [26.36s]: training loss=0.1491706371307373  validation ndcg@10=0.029506738016292226 [0.62s]\n",
      "epoch 51 [26.12s]:  training loss=0.14778853952884674                                      \n",
      "epoch 52 [26.86s]:  training loss=0.1442548930644989                                       \n",
      "epoch 53 [27.24s]:  training loss=0.1434938907623291                                       \n",
      "epoch 54 [26.92s]:  training loss=0.1436682492494583                                       \n",
      "epoch 55 [26.54s]: training loss=0.1392618864774704  validation ndcg@10=0.02989165387523979 [0.65s]\n",
      "epoch 56 [26.1s]:  training loss=0.1408277153968811                                        \n",
      "epoch 57 [27.1s]:  training loss=0.13694646954536438                                       \n",
      "epoch 58 [26.67s]:  training loss=0.1380717158317566                                       \n",
      "epoch 59 [26.94s]:  training loss=0.13490045070648193                                      \n",
      "epoch 60 [26.07s]: training loss=0.13828426599502563  validation ndcg@10=0.029478078010232372 [0.7s]\n",
      "epoch 61 [25.29s]:  training loss=0.13189075887203217                                      \n",
      "epoch 62 [26.46s]:  training loss=0.13419385254383087                                      \n",
      "epoch 63 [26.67s]:  training loss=0.12955692410469055                                      \n",
      "epoch 64 [26.67s]:  training loss=0.12764589488506317                                      \n",
      "epoch 65 [25.51s]: training loss=0.12843365967273712  validation ndcg@10=0.029970015694742606 [0.63s]\n",
      "epoch 66 [26.26s]:  training loss=0.12703385949134827                                      \n",
      "epoch 67 [26.17s]:  training loss=0.1280583292245865                                       \n",
      "epoch 68 [26.19s]:  training loss=0.12510031461715698                                      \n",
      "epoch 69 [26.65s]:  training loss=0.12402918189764023                                      \n",
      "epoch 70 [26.01s]: training loss=0.12213222682476044  validation ndcg@10=0.030249165009473642 [0.62s]\n",
      "epoch 71 [26.26s]:  training loss=0.12176371365785599                                      \n",
      "epoch 72 [26.62s]:  training loss=0.12217045575380325                                      \n",
      "epoch 73 [26.64s]:  training loss=0.12159055471420288                                      \n",
      "epoch 74 [26.8s]:  training loss=0.11807766556739807                                       \n",
      "epoch 75 [26.24s]: training loss=0.1190677061676979  validation ndcg@10=0.030535363674922333 [0.67s]\n",
      "epoch 76 [26.32s]:  training loss=0.1190444752573967                                       \n",
      "epoch 77 [26.43s]:  training loss=0.11713932454586029                                      \n",
      "epoch 78 [26.71s]:  training loss=0.11159256845712662                                      \n",
      "epoch 79 [26.7s]:  training loss=0.10966047644615173                                       \n",
      "epoch 80 [27.36s]: training loss=0.11266051232814789  validation ndcg@10=0.03044406577895867 [0.66s]\n",
      "epoch 81 [25.57s]:  training loss=0.10866405069828033                                      \n",
      "epoch 82 [25.69s]:  training loss=0.11064232140779495                                      \n",
      "epoch 83 [26.28s]:  training loss=0.10913053900003433                                      \n",
      "epoch 84 [26.03s]:  training loss=0.1084754690527916                                       \n",
      "epoch 85 [25.79s]: training loss=0.10965418815612793  validation ndcg@10=0.030441466972530598 [0.66s]\n",
      "epoch 86 [25.74s]:  training loss=0.10657899081707001                                      \n",
      "epoch 87 [26.23s]:  training loss=0.1068299412727356                                       \n",
      "epoch 88 [26.34s]:  training loss=0.1056029424071312                                       \n",
      "epoch 89 [26.27s]:  training loss=0.1050434559583664                                       \n",
      "epoch 90 [26.26s]: training loss=0.10504553467035294  validation ndcg@10=0.03055593269458591 [0.66s]\n",
      "epoch 91 [26.08s]:  training loss=0.10542663186788559                                      \n",
      "epoch 92 [26.16s]:  training loss=0.1068183109164238                                       \n",
      "epoch 93 [26.22s]:  training loss=0.10199657082557678                                      \n",
      "epoch 94 [26.56s]:  training loss=0.10013897716999054                                      \n",
      "epoch 95 [26.21s]: training loss=0.10168057680130005  validation ndcg@10=0.030796687693425903 [0.63s]\n",
      "epoch 96 [26.02s]:  training loss=0.1012008860707283                                       \n",
      "epoch 97 [25.67s]:  training loss=0.0991649180650711                                       \n",
      "epoch 98 [26.15s]:  training loss=0.09999949485063553                                      \n",
      "epoch 99 [27.05s]:  training loss=0.09782566130161285                                      \n",
      "epoch 100 [26.38s]: training loss=0.09792831540107727  validation ndcg@10=0.03050478291613764 [0.67s]\n",
      "epoch 101 [26.65s]:  training loss=0.09710368514060974                                     \n",
      "epoch 102 [25.59s]:  training loss=0.09639047831296921                                     \n",
      "epoch 103 [25.96s]:  training loss=0.0943632572889328                                      \n",
      "epoch 104 [25.91s]:  training loss=0.09437520056962967                                     \n",
      "epoch 105 [26.1s]: training loss=0.09345067292451859  validation ndcg@10=0.030039579882228853 [0.67s]\n",
      "epoch 106 [25.96s]:  training loss=0.09385643899440765                                     \n",
      "epoch 107 [26.13s]:  training loss=0.09534070640802383                                     \n",
      "epoch 108 [25.49s]:  training loss=0.09106002002954483                                     \n",
      "epoch 109 [26.54s]:  training loss=0.09322217851877213                                     \n",
      "epoch 110 [25.83s]: training loss=0.09652406722307205  validation ndcg@10=0.030205521265979213 [0.63s]\n",
      "epoch 111 [25.89s]:  training loss=0.09066134691238403                                     \n",
      "epoch 112 [25.96s]:  training loss=0.08963794261217117                                     \n",
      "epoch 113 [26.47s]:  training loss=0.09238164871931076                                     \n",
      "epoch 114 [25.75s]:  training loss=0.0898999273777008                                      \n",
      "epoch 115 [27.03s]: training loss=0.09046585857868195  validation ndcg@10=0.03021235766934969 [0.68s]\n",
      "epoch 116 [25.84s]:  training loss=0.08904333412647247                                     \n",
      "epoch 117 [26.37s]:  training loss=0.08892043679952621                                     \n",
      "epoch 118 [25.42s]:  training loss=0.08606594800949097                                     \n",
      "epoch 119 [25.82s]:  training loss=0.0864202156662941                                      \n",
      "epoch 120 [26.44s]: training loss=0.08694195002317429  validation ndcg@10=0.029641496126279282 [0.67s]\n",
      "epoch 1 [26.95s]:  training loss=0.8312336802482605                                        \n",
      "epoch 2 [27.95s]:  training loss=0.8053351044654846                                        \n",
      "epoch 3 [27.22s]:  training loss=0.7930539846420288                                        \n",
      "epoch 4 [26.94s]:  training loss=0.7789274454116821                                        \n",
      "epoch 5 [27.27s]: training loss=0.763503909111023  validation ndcg@10=0.008418322729938426 [0.68s]\n",
      "epoch 6 [26.56s]:  training loss=0.749731719493866                                         \n",
      "epoch 7 [26.7s]:  training loss=0.7324100136756897                                         \n",
      "epoch 8 [26.19s]:  training loss=0.7105976343154907                                        \n",
      "epoch 9 [26.74s]:  training loss=0.686964750289917                                         \n",
      "epoch 10 [26.63s]: training loss=0.6634717583656311  validation ndcg@10=0.008443730444979975 [0.66s]\n",
      "epoch 11 [26.77s]:  training loss=0.6439709067344666                                       \n",
      "epoch 12 [26.66s]:  training loss=0.6238393783569336                                       \n",
      "epoch 13 [26.67s]:  training loss=0.6113077402114868                                       \n",
      "epoch 14 [27.26s]:  training loss=0.59487384557724                                         \n",
      "epoch 15 [27.24s]: training loss=0.5794804692268372  validation ndcg@10=0.008935184820464886 [0.75s]\n",
      "epoch 16 [29.38s]:  training loss=0.5717275738716125                                       \n",
      "epoch 17 [27.09s]:  training loss=0.5616664290428162                                       \n",
      "epoch 18 [26.72s]:  training loss=0.553735077381134                                        \n",
      "epoch 19 [26.96s]:  training loss=0.5420268774032593                                       \n",
      "epoch 20 [26.59s]: training loss=0.5394662022590637  validation ndcg@10=0.01071490062690613 [0.68s]\n",
      "epoch 21 [26.27s]:  training loss=0.5252521634101868                                       \n",
      "epoch 22 [27.39s]:  training loss=0.5174962878227234                                       \n",
      "epoch 23 [27.83s]:  training loss=0.5088473558425903                                       \n",
      "epoch 24 [26.72s]:  training loss=0.4956173002719879                                       \n",
      "epoch 25 [26.79s]: training loss=0.48999103903770447  validation ndcg@10=0.012390923713697643 [0.69s]\n",
      "epoch 26 [26.61s]:  training loss=0.4790222942829132                                       \n",
      "epoch 27 [26.87s]:  training loss=0.47084319591522217                                      \n",
      "epoch 28 [26.78s]:  training loss=0.45617562532424927                                      \n",
      "epoch 29 [26.68s]:  training loss=0.4486745595932007                                       \n",
      "epoch 30 [26.9s]: training loss=0.44065168499946594  validation ndcg@10=0.016170341809306384 [0.69s]\n",
      "epoch 31 [26.67s]:  training loss=0.43070903420448303                                      \n",
      "epoch 32 [26.99s]:  training loss=0.4267638027667999                                       \n",
      "epoch 33 [26.86s]:  training loss=0.4172201454639435                                       \n",
      "epoch 34 [27.06s]:  training loss=0.40783584117889404                                      \n",
      "epoch 35 [27.35s]: training loss=0.40374958515167236  validation ndcg@10=0.019676349327096762 [0.67s]\n",
      "epoch 36 [27.5s]:  training loss=0.38852205872535706                                       \n",
      "epoch 37 [26.99s]:  training loss=0.3860083520412445                                       \n",
      "epoch 38 [27.1s]:  training loss=0.3783213496208191                                        \n",
      "epoch 39 [27.28s]:  training loss=0.3754412531852722                                       \n",
      "epoch 40 [26.79s]: training loss=0.3710775077342987  validation ndcg@10=0.021576781765509572 [0.64s]\n",
      "epoch 41 [26.75s]:  training loss=0.3638061285018921                                       \n",
      "epoch 42 [27.02s]:  training loss=0.3602261245250702                                       \n",
      "epoch 43 [27.33s]:  training loss=0.3561246693134308                                       \n",
      "epoch 44 [25.66s]:  training loss=0.3538808524608612                                       \n",
      "epoch 45 [27.07s]: training loss=0.34395429491996765  validation ndcg@10=0.02237282353599475 [0.64s]\n",
      "epoch 46 [26.55s]:  training loss=0.3412662148475647                                       \n",
      "epoch 47 [26.5s]:  training loss=0.3406957983970642                                        \n",
      "epoch 48 [26.98s]:  training loss=0.3328666388988495                                       \n",
      "epoch 49 [26.71s]:  training loss=0.3269147574901581                                       \n",
      "epoch 50 [26.67s]: training loss=0.3258220851421356  validation ndcg@10=0.022760977735007064 [0.63s]\n",
      "epoch 51 [26.56s]:  training loss=0.3222322165966034                                       \n",
      "epoch 52 [26.85s]:  training loss=0.3206915855407715                                       \n",
      "epoch 53 [27.15s]:  training loss=0.3177344799041748                                       \n",
      "epoch 54 [26.91s]:  training loss=0.31230589747428894                                      \n",
      "epoch 55 [27.02s]: training loss=0.3117850422859192  validation ndcg@10=0.023450302687766246 [0.73s]\n",
      "epoch 56 [27.22s]:  training loss=0.31386739015579224                                      \n",
      "epoch 57 [27.03s]:  training loss=0.3080112934112549                                       \n",
      "epoch 58 [26.47s]:  training loss=0.30545327067375183                                      \n",
      "epoch 59 [26.87s]:  training loss=0.29947423934936523                                      \n",
      "epoch 60 [26.96s]: training loss=0.2964569330215454  validation ndcg@10=0.02420069600422111 [0.68s]\n",
      "epoch 61 [26.94s]:  training loss=0.2941405475139618                                       \n",
      "epoch 62 [26.97s]:  training loss=0.2907228171825409                                       \n",
      "epoch 63 [26.95s]:  training loss=0.28887879848480225                                      \n",
      "epoch 64 [26.51s]:  training loss=0.28876516222953796                                      \n",
      "epoch 65 [26.78s]: training loss=0.28328174352645874  validation ndcg@10=0.024379449937862396 [0.68s]\n",
      "epoch 66 [27.89s]:  training loss=0.28706488013267517                                      \n",
      "epoch 67 [26.72s]:  training loss=0.28276383876800537                                      \n",
      "epoch 68 [26.83s]:  training loss=0.2796085774898529                                       \n",
      "epoch 69 [26.62s]:  training loss=0.2790610194206238                                       \n",
      "epoch 70 [26.63s]: training loss=0.2738804817199707  validation ndcg@10=0.02495840535893812 [0.66s]\n",
      "epoch 71 [26.42s]:  training loss=0.27423688769340515                                      \n",
      "epoch 72 [26.34s]:  training loss=0.27339842915534973                                      \n",
      "epoch 73 [26.57s]:  training loss=0.2711854577064514                                       \n",
      "epoch 74 [26.74s]:  training loss=0.26651671528816223                                      \n",
      "epoch 75 [26.46s]: training loss=0.2658187747001648  validation ndcg@10=0.025250185243673808 [0.69s]\n",
      "epoch 76 [26.78s]:  training loss=0.25975725054740906                                      \n",
      "epoch 77 [26.91s]:  training loss=0.263603538274765                                        \n",
      "epoch 78 [26.73s]:  training loss=0.26135188341140747                                      \n",
      "epoch 79 [26.4s]:  training loss=0.25799059867858887                                       \n",
      "epoch 80 [27.26s]: training loss=0.2531611919403076  validation ndcg@10=0.025669848734150867 [0.69s]\n",
      "epoch 81 [26.57s]:  training loss=0.2565218508243561                                       \n",
      "epoch 82 [27.03s]:  training loss=0.25262802839279175                                      \n",
      "epoch 83 [27.11s]:  training loss=0.25428178906440735                                      \n",
      "epoch 84 [26.7s]:  training loss=0.24761925637722015                                       \n",
      "epoch 85 [27.01s]: training loss=0.24681322276592255  validation ndcg@10=0.025718743935298646 [0.65s]\n",
      "epoch 86 [26.64s]:  training loss=0.2481636255979538                                       \n",
      "epoch 87 [27.78s]:  training loss=0.24781976640224457                                      \n",
      "epoch 88 [26.86s]:  training loss=0.24571414291858673                                      \n",
      "epoch 89 [26.88s]:  training loss=0.24333593249320984                                      \n",
      "epoch 90 [26.9s]: training loss=0.24399133026599884  validation ndcg@10=0.02572908295730149 [0.65s]\n",
      "epoch 91 [26.75s]:  training loss=0.24273601174354553                                      \n",
      "epoch 92 [27.14s]:  training loss=0.23710481822490692                                      \n",
      "epoch 93 [27.07s]:  training loss=0.23492883145809174                                      \n",
      "epoch 94 [26.87s]:  training loss=0.23696483671665192                                      \n",
      "epoch 95 [26.74s]: training loss=0.23418626189231873  validation ndcg@10=0.026052972575094337 [0.69s]\n",
      "epoch 96 [26.95s]:  training loss=0.232655331492424                                        \n",
      "epoch 97 [27.54s]:  training loss=0.23366469144821167                                      \n",
      "epoch 98 [26.98s]:  training loss=0.22811990976333618                                      \n",
      "epoch 99 [26.94s]:  training loss=0.23044279217720032                                      \n",
      "epoch 100 [26.84s]: training loss=0.22868658602237701  validation ndcg@10=0.02677363776684901 [0.64s]\n",
      "epoch 101 [26.7s]:  training loss=0.22829337418079376                                      \n",
      "epoch 102 [27.1s]:  training loss=0.22657552361488342                                      \n",
      "epoch 103 [26.88s]:  training loss=0.2241322100162506                                      \n",
      "epoch 104 [26.78s]:  training loss=0.22300276160240173                                     \n",
      "epoch 105 [26.77s]: training loss=0.2249954342842102  validation ndcg@10=0.026567534604076355 [0.67s]\n",
      "epoch 106 [26.57s]:  training loss=0.22075076401233673                                     \n",
      "epoch 107 [26.65s]:  training loss=0.21942411363124847                                     \n",
      "epoch 108 [26.46s]:  training loss=0.218869149684906                                       \n",
      "epoch 109 [27.8s]:  training loss=0.21765924990177155                                      \n",
      "epoch 110 [26.52s]: training loss=0.21894435584545135  validation ndcg@10=0.026615004075073986 [0.7s]\n",
      "epoch 111 [27.03s]:  training loss=0.21540631353855133                                     \n",
      "epoch 112 [26.68s]:  training loss=0.2135351151227951                                      \n",
      "epoch 113 [27.74s]:  training loss=0.21753472089767456                                     \n",
      "epoch 114 [26.99s]:  training loss=0.21569468080997467                                     \n",
      "epoch 115 [27.07s]: training loss=0.21024779975414276  validation ndcg@10=0.026688783444780043 [0.63s]\n",
      "epoch 116 [27.01s]:  training loss=0.2097763866186142                                      \n",
      "epoch 117 [27.52s]:  training loss=0.2104000449180603                                      \n",
      "epoch 118 [27.21s]:  training loss=0.2086011916399002                                      \n",
      "epoch 119 [26.8s]:  training loss=0.20809312164783478                                      \n",
      "epoch 120 [26.85s]: training loss=0.2060530036687851  validation ndcg@10=0.026957930286559188 [0.64s]\n",
      "epoch 121 [26.72s]:  training loss=0.2073238492012024                                      \n",
      "epoch 122 [27.26s]:  training loss=0.20655500888824463                                     \n",
      "epoch 123 [27.0s]:  training loss=0.2036987841129303                                       \n",
      "epoch 124 [27.06s]:  training loss=0.20219548046588898                                     \n",
      "epoch 125 [27.14s]: training loss=0.1980070024728775  validation ndcg@10=0.027371213266166892 [0.7s]\n",
      "epoch 126 [27.02s]:  training loss=0.20027336478233337                                     \n",
      "epoch 127 [27.06s]:  training loss=0.20340248942375183                                     \n",
      "epoch 128 [26.72s]:  training loss=0.1980029046535492                                      \n",
      "epoch 129 [27.06s]:  training loss=0.19930490851402283                                     \n",
      "epoch 130 [26.77s]: training loss=0.19868510961532593  validation ndcg@10=0.0278392004234791 [0.71s]\n",
      "epoch 131 [27.77s]:  training loss=0.1984584778547287                                      \n",
      "epoch 132 [27.16s]:  training loss=0.19558919966220856                                     \n",
      "epoch 133 [26.75s]:  training loss=0.19636914134025574                                     \n",
      "epoch 134 [26.82s]:  training loss=0.19125676155090332                                     \n",
      "epoch 135 [26.58s]: training loss=0.1941554993391037  validation ndcg@10=0.027658249844851643 [0.63s]\n",
      "epoch 136 [26.47s]:  training loss=0.19351030886173248                                     \n",
      "epoch 137 [27.24s]:  training loss=0.19449637830257416                                     \n",
      "epoch 138 [27.22s]:  training loss=0.18900060653686523                                     \n",
      "epoch 139 [26.58s]:  training loss=0.1903892159461975                                      \n",
      "epoch 140 [26.19s]: training loss=0.190586119890213  validation ndcg@10=0.027005552421677573 [0.68s]\n",
      "epoch 141 [26.87s]:  training loss=0.18867036700248718                                     \n",
      "epoch 142 [26.73s]:  training loss=0.18657422065734863                                     \n",
      "epoch 143 [26.62s]:  training loss=0.1859336495399475                                      \n",
      "epoch 144 [26.68s]:  training loss=0.18723072111606598                                     \n",
      "epoch 145 [26.85s]: training loss=0.18706323206424713  validation ndcg@10=0.02763554250908025 [0.66s]\n",
      "epoch 146 [26.76s]:  training loss=0.18716076016426086                                     \n",
      "epoch 147 [26.61s]:  training loss=0.18422845005989075                                     \n",
      "epoch 148 [26.84s]:  training loss=0.1848842352628708                                      \n",
      "epoch 149 [26.42s]:  training loss=0.1833527833223343                                      \n",
      "epoch 150 [26.04s]: training loss=0.18291425704956055  validation ndcg@10=0.02770666985159643 [0.7s]\n",
      "epoch 151 [26.86s]:  training loss=0.18481995165348053                                     \n",
      "epoch 152 [27.45s]:  training loss=0.18141162395477295                                     \n",
      "epoch 153 [27.15s]:  training loss=0.1821655035018921                                      \n",
      "epoch 154 [27.55s]:  training loss=0.17996175587177277                                     \n",
      "epoch 155 [27.06s]: training loss=0.17899195849895477  validation ndcg@10=0.02752805572214208 [0.69s]\n",
      "epoch 1 [20.26s]:  training loss=0.8376765847206116                                        \n",
      "epoch 2 [21.08s]:  training loss=0.7965874671936035                                        \n",
      "epoch 3 [21.11s]:  training loss=0.7602807879447937                                        \n",
      "epoch 4 [21.57s]:  training loss=0.7260356545448303                                        \n",
      "epoch 5 [20.72s]: training loss=0.6829218864440918  validation ndcg@10=0.009692802261399682 [0.57s]\n",
      "epoch 6 [21.06s]:  training loss=0.6412158608436584                                        \n",
      "epoch 7 [21.07s]:  training loss=0.6067876815795898                                        \n",
      "epoch 8 [20.73s]:  training loss=0.5845582485198975                                        \n",
      "epoch 9 [20.71s]:  training loss=0.5607094764709473                                        \n",
      "epoch 10 [20.53s]: training loss=0.544204831123352  validation ndcg@10=0.012007978528748109 [0.54s]\n",
      "epoch 11 [20.57s]:  training loss=0.5216376781463623                                       \n",
      "epoch 12 [20.39s]:  training loss=0.5014600157737732                                       \n",
      "epoch 13 [20.19s]:  training loss=0.4753205180168152                                       \n",
      "epoch 14 [19.65s]:  training loss=0.46037915349006653                                      \n",
      "epoch 15 [21.16s]: training loss=0.4411770701408386  validation ndcg@10=0.019417470301106472 [0.56s]\n",
      "epoch 16 [20.05s]:  training loss=0.4335145652294159                                       \n",
      "epoch 17 [21.02s]:  training loss=0.4111146330833435                                       \n",
      "epoch 18 [20.3s]:  training loss=0.4047965109348297                                        \n",
      "epoch 19 [20.77s]:  training loss=0.3906283378601074                                       \n",
      "epoch 20 [20.67s]: training loss=0.38273367285728455  validation ndcg@10=0.022480779876627166 [0.64s]\n",
      "epoch 21 [20.9s]:  training loss=0.3723072111606598                                        \n",
      "epoch 22 [20.5s]:  training loss=0.36157000064849854                                       \n",
      "epoch 23 [21.16s]:  training loss=0.3470250070095062                                       \n",
      "epoch 24 [20.34s]:  training loss=0.34218165278434753                                      \n",
      "epoch 25 [20.19s]: training loss=0.3360428810119629  validation ndcg@10=0.02333538531840991 [0.62s]\n",
      "epoch 26 [20.41s]:  training loss=0.32816484570503235                                      \n",
      "epoch 27 [21.73s]:  training loss=0.3242686092853546                                       \n",
      "epoch 28 [20.59s]:  training loss=0.31796902418136597                                      \n",
      "epoch 29 [20.29s]:  training loss=0.309518039226532                                        \n",
      "epoch 30 [21.0s]: training loss=0.3054110109806061  validation ndcg@10=0.023897348703033842 [0.61s]\n",
      "epoch 31 [21.05s]:  training loss=0.29629722237586975                                      \n",
      "epoch 32 [20.29s]:  training loss=0.29463762044906616                                      \n",
      "epoch 33 [21.19s]:  training loss=0.28738394379615784                                      \n",
      "epoch 34 [21.82s]:  training loss=0.2832675576210022                                       \n",
      "epoch 35 [22.2s]: training loss=0.28437915444374084  validation ndcg@10=0.025038537120604398 [0.63s]\n",
      "epoch 36 [22.07s]:  training loss=0.27496615052223206                                      \n",
      "epoch 37 [22.77s]:  training loss=0.2706263065338135                                       \n",
      "epoch 38 [22.13s]:  training loss=0.26746654510498047                                      \n",
      "epoch 39 [20.39s]:  training loss=0.26544055342674255                                      \n",
      "epoch 40 [20.71s]: training loss=0.2564801871776581  validation ndcg@10=0.026311067055681408 [0.58s]\n",
      "epoch 41 [20.6s]:  training loss=0.25834786891937256                                       \n",
      "epoch 42 [20.42s]:  training loss=0.2504056692123413                                       \n",
      "epoch 43 [20.29s]:  training loss=0.25079062581062317                                      \n",
      "epoch 44 [20.61s]:  training loss=0.24647323787212372                                      \n",
      "epoch 45 [20.45s]: training loss=0.2422141283750534  validation ndcg@10=0.025860875362415128 [0.58s]\n",
      "epoch 46 [20.79s]:  training loss=0.2389935702085495                                       \n",
      "epoch 47 [20.98s]:  training loss=0.23813007771968842                                      \n",
      "epoch 48 [20.91s]:  training loss=0.23250631988048553                                      \n",
      "epoch 49 [20.86s]:  training loss=0.22827768325805664                                      \n",
      "epoch 50 [20.89s]: training loss=0.23247067630290985  validation ndcg@10=0.02702729335450984 [0.55s]\n",
      "epoch 51 [21.09s]:  training loss=0.2263488471508026                                       \n",
      "epoch 52 [21.03s]:  training loss=0.22494550049304962                                      \n",
      "epoch 53 [20.73s]:  training loss=0.22291840612888336                                      \n",
      "epoch 54 [20.65s]:  training loss=0.21960493922233582                                      \n",
      "epoch 55 [20.8s]: training loss=0.21471595764160156  validation ndcg@10=0.027238405824219845 [0.76s]\n",
      "epoch 56 [21.84s]:  training loss=0.2098240852355957                                       \n",
      "epoch 57 [20.25s]:  training loss=0.21002140641212463                                      \n",
      "epoch 58 [20.48s]:  training loss=0.2111385315656662                                       \n",
      "epoch 59 [20.51s]:  training loss=0.20953962206840515                                      \n",
      "epoch 60 [20.63s]: training loss=0.20453602075576782  validation ndcg@10=0.027593411384277348 [0.59s]\n",
      "epoch 61 [20.64s]:  training loss=0.20271115005016327                                      \n",
      "epoch 62 [20.34s]:  training loss=0.20051206648349762                                      \n",
      "epoch 63 [20.72s]:  training loss=0.20013374090194702                                      \n",
      "epoch 64 [20.21s]:  training loss=0.19608327746391296                                      \n",
      "epoch 65 [20.91s]: training loss=0.1972963511943817  validation ndcg@10=0.0279754964321287 [0.6s]\n",
      "epoch 66 [20.7s]:  training loss=0.19681023061275482                                       \n",
      "epoch 67 [20.36s]:  training loss=0.18979395925998688                                      \n",
      "epoch 68 [20.91s]:  training loss=0.192988321185112                                        \n",
      "epoch 69 [21.12s]:  training loss=0.18734703958034515                                      \n",
      "epoch 70 [20.73s]: training loss=0.19143210351467133  validation ndcg@10=0.027794608476306527 [0.59s]\n",
      "epoch 71 [20.39s]:  training loss=0.1897965520620346                                       \n",
      "epoch 72 [20.68s]:  training loss=0.18219898641109467                                      \n",
      "epoch 73 [20.9s]:  training loss=0.18400897085666656                                       \n",
      "epoch 74 [20.34s]:  training loss=0.18473005294799805                                      \n",
      "epoch 75 [20.64s]: training loss=0.18349319696426392  validation ndcg@10=0.028021822402026518 [0.56s]\n",
      "epoch 76 [20.61s]:  training loss=0.18027138710021973                                      \n",
      "epoch 77 [20.83s]:  training loss=0.17853815853595734                                      \n",
      "epoch 78 [20.79s]:  training loss=0.17676903307437897                                      \n",
      "epoch 79 [20.75s]:  training loss=0.1760130375623703                                       \n",
      "epoch 80 [20.06s]: training loss=0.17635704576969147  validation ndcg@10=0.028246496533195754 [0.59s]\n",
      "epoch 81 [20.92s]:  training loss=0.17374858260154724                                      \n",
      "epoch 82 [20.77s]:  training loss=0.1700318604707718                                       \n",
      "epoch 83 [21.51s]:  training loss=0.1713639199733734                                       \n",
      "epoch 84 [21.51s]:  training loss=0.16883574426174164                                      \n",
      "epoch 85 [20.68s]: training loss=0.16737745702266693  validation ndcg@10=0.028574026095078527 [0.6s]\n",
      "epoch 86 [20.63s]:  training loss=0.1679474264383316                                       \n",
      "epoch 87 [20.87s]:  training loss=0.1645607203245163                                       \n",
      "epoch 88 [20.41s]:  training loss=0.16244350373744965                                      \n",
      "epoch 89 [21.29s]:  training loss=0.16416090726852417                                      \n",
      "epoch 90 [21.15s]: training loss=0.16303996741771698  validation ndcg@10=0.02863831935325754 [0.58s]\n",
      "epoch 91 [20.77s]:  training loss=0.160145565867424                                        \n",
      "epoch 92 [20.38s]:  training loss=0.15965726971626282                                      \n",
      "epoch 93 [21.65s]:  training loss=0.15918821096420288                                      \n",
      "epoch 94 [21.44s]:  training loss=0.15683574974536896                                      \n",
      "epoch 95 [21.46s]: training loss=0.15842846035957336  validation ndcg@10=0.028775844463971616 [0.59s]\n",
      "epoch 96 [20.77s]:  training loss=0.15569241344928741                                      \n",
      "epoch 97 [20.58s]:  training loss=0.1581936627626419                                       \n",
      "epoch 98 [21.15s]:  training loss=0.1520201712846756                                       \n",
      "epoch 99 [21.37s]:  training loss=0.1497769057750702                                       \n",
      "epoch 100 [20.9s]: training loss=0.15175651013851166  validation ndcg@10=0.028857314823809887 [0.55s]\n",
      "epoch 101 [20.55s]:  training loss=0.15275751054286957                                     \n",
      "epoch 102 [20.9s]:  training loss=0.1505889743566513                                       \n",
      "epoch 103 [20.77s]:  training loss=0.14684191346168518                                     \n",
      "epoch 104 [20.87s]:  training loss=0.15187124907970428                                     \n",
      "epoch 105 [20.76s]: training loss=0.14993172883987427  validation ndcg@10=0.029109097026196525 [0.56s]\n",
      "epoch 106 [20.58s]:  training loss=0.14671088755130768                                     \n",
      "epoch 107 [20.24s]:  training loss=0.14548051357269287                                     \n",
      "epoch 108 [21.57s]:  training loss=0.1470031440258026                                      \n",
      "epoch 109 [21.81s]:  training loss=0.1441616714000702                                      \n",
      "epoch 110 [21.27s]: training loss=0.14516617357730865  validation ndcg@10=0.029298758447028787 [0.57s]\n",
      "epoch 111 [20.64s]:  training loss=0.14021797478199005                                     \n",
      "epoch 112 [21.76s]:  training loss=0.14399723708629608                                     \n",
      "epoch 113 [20.37s]:  training loss=0.14147409796714783                                     \n",
      "epoch 114 [20.71s]:  training loss=0.14017494022846222                                     \n",
      "epoch 115 [21.64s]: training loss=0.1408894956111908  validation ndcg@10=0.02936614810997985 [0.59s]\n",
      "epoch 116 [20.73s]:  training loss=0.13903479278087616                                     \n",
      "epoch 117 [21.55s]:  training loss=0.13960391283035278                                     \n",
      "epoch 118 [20.76s]:  training loss=0.13853424787521362                                     \n",
      "epoch 119 [20.77s]:  training loss=0.13803808391094208                                     \n",
      "epoch 120 [20.77s]: training loss=0.1369156688451767  validation ndcg@10=0.02904666906134145 [0.63s]\n",
      "epoch 121 [20.98s]:  training loss=0.13192521035671234                                     \n",
      "epoch 122 [20.24s]:  training loss=0.13610000908374786                                     \n",
      "epoch 123 [20.37s]:  training loss=0.13767996430397034                                     \n",
      "epoch 124 [21.14s]:  training loss=0.13404373824596405                                     \n",
      "epoch 125 [20.75s]: training loss=0.13524070382118225  validation ndcg@10=0.02926647208781374 [0.57s]\n",
      "epoch 126 [21.07s]:  training loss=0.1308581531047821                                      \n",
      "epoch 127 [20.26s]:  training loss=0.1338992863893509                                      \n",
      "epoch 128 [19.98s]:  training loss=0.13343898952007294                                     \n",
      "epoch 129 [20.19s]:  training loss=0.1288486123085022                                      \n",
      "epoch 130 [20.3s]: training loss=0.13033297657966614  validation ndcg@10=0.029887969615441223 [0.61s]\n",
      "epoch 131 [20.33s]:  training loss=0.12967804074287415                                     \n",
      "epoch 132 [19.84s]:  training loss=0.12858927249908447                                     \n",
      "epoch 133 [20.19s]:  training loss=0.12588275969028473                                     \n",
      "epoch 134 [20.62s]:  training loss=0.12906348705291748                                     \n",
      "epoch 135 [21.46s]: training loss=0.12901705503463745  validation ndcg@10=0.02982257585341655 [0.57s]\n",
      "epoch 136 [19.84s]:  training loss=0.12655624747276306                                     \n",
      "epoch 137 [20.69s]:  training loss=0.12811517715454102                                     \n",
      "epoch 138 [20.43s]:  training loss=0.1264464408159256                                      \n",
      "epoch 139 [20.13s]:  training loss=0.12511245906352997                                     \n",
      "epoch 140 [20.47s]: training loss=0.12352060526609421  validation ndcg@10=0.0299845649980207 [0.59s]\n",
      "epoch 141 [21.18s]:  training loss=0.12482330203056335                                     \n",
      "epoch 142 [19.9s]:  training loss=0.1242947205901146                                       \n",
      "epoch 143 [20.28s]:  training loss=0.12400500476360321                                     \n",
      "epoch 144 [20.26s]:  training loss=0.12205909937620163                                     \n",
      "epoch 145 [20.45s]: training loss=0.12129900604486465  validation ndcg@10=0.029726568054936717 [0.6s]\n",
      "epoch 146 [19.97s]:  training loss=0.12344375252723694                                     \n",
      "epoch 147 [20.33s]:  training loss=0.12320340424776077                                     \n",
      "epoch 148 [20.19s]:  training loss=0.12068046629428864                                     \n",
      "epoch 149 [20.92s]:  training loss=0.12120139598846436                                     \n",
      "epoch 150 [20.13s]: training loss=0.11876478046178818  validation ndcg@10=0.02990337656207799 [0.56s]\n",
      "epoch 151 [20.08s]:  training loss=0.11746648699045181                                     \n",
      "epoch 152 [20.31s]:  training loss=0.11635022610425949                                     \n",
      "epoch 153 [20.13s]:  training loss=0.11813219636678696                                     \n",
      "epoch 154 [20.33s]:  training loss=0.11852680891752243                                     \n",
      "epoch 155 [20.08s]: training loss=0.11645805090665817  validation ndcg@10=0.03002615211671631 [0.62s]\n",
      "epoch 156 [20.05s]:  training loss=0.11576586216688156                                     \n",
      "epoch 157 [20.04s]:  training loss=0.11718565225601196                                     \n",
      "epoch 158 [20.12s]:  training loss=0.11567165702581406                                     \n",
      "epoch 159 [20.17s]:  training loss=0.11274363845586777                                     \n",
      "epoch 160 [19.97s]: training loss=0.11470970511436462  validation ndcg@10=0.029887164837535965 [0.56s]\n",
      "epoch 161 [20.12s]:  training loss=0.11576420813798904                                     \n",
      "epoch 162 [20.99s]:  training loss=0.11332137137651443                                     \n",
      "epoch 163 [20.86s]:  training loss=0.11382011324167252                                     \n",
      "epoch 164 [21.55s]:  training loss=0.11488219350576401                                     \n",
      "epoch 165 [21.34s]: training loss=0.112711101770401  validation ndcg@10=0.030329534973829658 [0.61s]\n",
      "epoch 166 [20.06s]:  training loss=0.11229993402957916                                     \n",
      "epoch 167 [20.24s]:  training loss=0.11199517548084259                                     \n",
      "epoch 168 [19.95s]:  training loss=0.11191493272781372                                     \n",
      "epoch 169 [20.28s]:  training loss=0.11248257011175156                                     \n",
      "epoch 170 [21.24s]: training loss=0.1084747388958931  validation ndcg@10=0.030475882082521003 [0.58s]\n",
      "epoch 171 [20.31s]:  training loss=0.10836070030927658                                     \n",
      "epoch 172 [20.22s]:  training loss=0.11121585965156555                                     \n",
      "epoch 173 [20.42s]:  training loss=0.1068749874830246                                      \n",
      "epoch 174 [20.16s]:  training loss=0.10662110894918442                                     \n",
      "epoch 175 [20.1s]: training loss=0.10882538557052612  validation ndcg@10=0.030286031067284665 [0.55s]\n",
      "epoch 176 [20.07s]:  training loss=0.10874922573566437                                     \n",
      "epoch 177 [20.26s]:  training loss=0.10896433889865875                                     \n",
      "epoch 178 [19.66s]:  training loss=0.10945715010166168                                     \n",
      "epoch 179 [20.81s]:  training loss=0.10864508897066116                                     \n",
      "epoch 180 [20.12s]: training loss=0.10823789238929749  validation ndcg@10=0.030245524698726642 [0.61s]\n",
      "epoch 181 [20.47s]:  training loss=0.10867764800786972                                     \n",
      "epoch 182 [20.22s]:  training loss=0.10424621403217316                                     \n",
      "epoch 183 [20.26s]:  training loss=0.10538192093372345                                     \n",
      "epoch 184 [20.41s]:  training loss=0.10701863467693329                                     \n",
      "epoch 185 [20.15s]: training loss=0.10690611600875854  validation ndcg@10=0.030526691802477333 [0.6s]\n",
      "epoch 186 [20.22s]:  training loss=0.10635576397180557                                     \n",
      "epoch 187 [20.74s]:  training loss=0.10498297214508057                                     \n",
      "epoch 188 [21.2s]:  training loss=0.10336440056562424                                      \n",
      "epoch 189 [20.87s]:  training loss=0.105904221534729                                       \n",
      "epoch 190 [20.39s]: training loss=0.10294214636087418  validation ndcg@10=0.030108273637767007 [0.56s]\n",
      "epoch 191 [20.14s]:  training loss=0.09923946857452393                                     \n",
      "epoch 192 [20.4s]:  training loss=0.10241833329200745                                      \n",
      "epoch 193 [20.5s]:  training loss=0.10310706496238708                                      \n",
      "epoch 194 [20.48s]:  training loss=0.10286594182252884                                     \n",
      "epoch 195 [20.29s]: training loss=0.10162164270877838  validation ndcg@10=0.03023349782920455 [0.59s]\n",
      "epoch 196 [20.23s]:  training loss=0.09993688762187958                                     \n",
      "epoch 197 [20.48s]:  training loss=0.10210896283388138                                     \n",
      "epoch 198 [20.3s]:  training loss=0.10073656588792801                                      \n",
      "epoch 199 [20.89s]:  training loss=0.10202888399362564                                     \n",
      "epoch 200 [18.63s]: training loss=0.09796362370252609  validation ndcg@10=0.02999982142167585 [0.53s]\n",
      "epoch 1 [23.72s]:  training loss=0.6460131406784058                                        \n",
      "epoch 2 [24.42s]:  training loss=0.41616207361221313                                       \n",
      "epoch 3 [23.86s]:  training loss=0.31589555740356445                                       \n",
      "epoch 4 [24.33s]:  training loss=0.2650817632675171                                        \n",
      "epoch 5 [24.65s]: training loss=0.23164883255958557  validation ndcg@10=0.026890025377949656 [0.59s]\n",
      "epoch 6 [24.33s]:  training loss=0.21318049728870392                                       \n",
      "epoch 7 [24.06s]:  training loss=0.1890883892774582                                        \n",
      "epoch 8 [24.88s]:  training loss=0.17993684113025665                                       \n",
      "epoch 9 [24.79s]:  training loss=0.16431239247322083                                       \n",
      "epoch 10 [24.4s]: training loss=0.15421336889266968  validation ndcg@10=0.028045392189473942 [0.59s]\n",
      "epoch 11 [24.52s]:  training loss=0.14358510076999664                                      \n",
      "epoch 12 [24.88s]:  training loss=0.1376633197069168                                       \n",
      "epoch 13 [25.3s]:  training loss=0.12822750210762024                                       \n",
      "epoch 14 [24.02s]:  training loss=0.12292379885911942                                      \n",
      "epoch 15 [25.58s]: training loss=0.11793959140777588  validation ndcg@10=0.028447751889485562 [0.64s]\n",
      "epoch 16 [24.3s]:  training loss=0.11473796516656876                                       \n",
      "epoch 17 [23.76s]:  training loss=0.10836431384086609                                      \n",
      "epoch 18 [24.63s]:  training loss=0.10480424761772156                                      \n",
      "epoch 19 [24.35s]:  training loss=0.10309473425149918                                      \n",
      "epoch 20 [24.16s]: training loss=0.09734531491994858  validation ndcg@10=0.028367226569346166 [0.59s]\n",
      "epoch 21 [24.23s]:  training loss=0.09362432360649109                                      \n",
      "epoch 22 [24.27s]:  training loss=0.09457458555698395                                      \n",
      "epoch 23 [24.23s]:  training loss=0.09226836264133453                                      \n",
      "epoch 24 [24.09s]:  training loss=0.0900678038597107                                       \n",
      "epoch 25 [23.8s]: training loss=0.08596674352884293  validation ndcg@10=0.027751165983060547 [0.65s]\n",
      "epoch 26 [24.28s]:  training loss=0.08218421787023544                                      \n",
      "epoch 27 [24.28s]:  training loss=0.0809820368885994                                       \n",
      "epoch 28 [24.41s]:  training loss=0.07653651386499405                                      \n",
      "epoch 29 [25.78s]:  training loss=0.0766737312078476                                       \n",
      "epoch 30 [24.11s]: training loss=0.07497914135456085  validation ndcg@10=0.02843488501158042 [0.58s]\n",
      "epoch 31 [24.39s]:  training loss=0.07422433793544769                                      \n",
      "epoch 32 [24.28s]:  training loss=0.07234583050012589                                      \n",
      "epoch 33 [24.53s]:  training loss=0.0718250498175621                                       \n",
      "epoch 34 [24.36s]:  training loss=0.06995651870965958                                      \n",
      "epoch 35 [25.46s]: training loss=0.06625502556562424  validation ndcg@10=0.028593299476054743 [0.59s]\n",
      "epoch 36 [24.48s]:  training loss=0.0639951303601265                                       \n",
      "epoch 37 [24.4s]:  training loss=0.06510788202285767                                       \n",
      "epoch 38 [24.56s]:  training loss=0.061542898416519165                                     \n",
      "epoch 39 [23.94s]:  training loss=0.0649544894695282                                       \n",
      "epoch 40 [24.7s]: training loss=0.06420709192752838  validation ndcg@10=0.029029617950553904 [0.66s]\n",
      "epoch 41 [24.49s]:  training loss=0.06301401555538177                                      \n",
      "epoch 42 [25.78s]:  training loss=0.0574413537979126                                       \n",
      "epoch 43 [24.49s]:  training loss=0.05904120206832886                                      \n",
      "epoch 44 [24.16s]:  training loss=0.056822676211595535                                     \n",
      "epoch 45 [24.51s]: training loss=0.05950286611914635  validation ndcg@10=0.028866388672304344 [0.58s]\n",
      "epoch 46 [24.4s]:  training loss=0.05762241408228874                                       \n",
      "epoch 47 [23.71s]:  training loss=0.05496346950531006                                      \n",
      "epoch 48 [25.0s]:  training loss=0.05613469332456589                                       \n",
      "epoch 49 [24.55s]:  training loss=0.055477507412433624                                     \n",
      "epoch 50 [24.18s]: training loss=0.05210164189338684  validation ndcg@10=0.028437802874468506 [0.61s]\n",
      "epoch 51 [24.39s]:  training loss=0.05249554291367531                                      \n",
      "epoch 52 [24.42s]:  training loss=0.05054450407624245                                      \n",
      "epoch 53 [23.85s]:  training loss=0.05186101421713829                                      \n",
      "epoch 54 [24.78s]:  training loss=0.05006123334169388                                      \n",
      "epoch 55 [24.59s]: training loss=0.04890526458621025  validation ndcg@10=0.02692775266288121 [0.66s]\n",
      "epoch 56 [25.27s]:  training loss=0.04831787571310997                                      \n",
      "epoch 57 [24.39s]:  training loss=0.04824553057551384                                      \n",
      "epoch 58 [24.57s]:  training loss=0.046468596905469894                                     \n",
      "epoch 59 [24.59s]:  training loss=0.04599140211939812                                      \n",
      "epoch 60 [24.54s]: training loss=0.047946713864803314  validation ndcg@10=0.027454199326154085 [0.58s]\n",
      "epoch 61 [24.0s]:  training loss=0.04509945958852768                                       \n",
      "epoch 62 [23.89s]:  training loss=0.04647025093436241                                      \n",
      "epoch 63 [23.92s]:  training loss=0.04544994607567787                                      \n",
      "epoch 64 [24.78s]:  training loss=0.046514999121427536                                     \n",
      "epoch 65 [24.54s]: training loss=0.0427154079079628  validation ndcg@10=0.026694732373019267 [0.59s]\n",
      "epoch 1 [20.28s]:  training loss=0.5419899225234985                                        \n",
      "epoch 2 [20.34s]:  training loss=0.29873794317245483                                       \n",
      "epoch 3 [20.22s]:  training loss=0.2286832630634308                                        \n",
      "epoch 4 [20.2s]:  training loss=0.1867475062608719                                         \n",
      "epoch 5 [20.19s]: training loss=0.1639852672815323  validation ndcg@10=0.026078608879086758 [0.54s]\n",
      "epoch 6 [21.34s]:  training loss=0.1519361436367035                                        \n",
      "epoch 7 [19.96s]:  training loss=0.13888660073280334                                       \n",
      "epoch 8 [19.91s]:  training loss=0.1273539960384369                                        \n",
      "epoch 9 [19.78s]:  training loss=0.1159236803650856                                        \n",
      "epoch 10 [19.93s]: training loss=0.1060587540268898  validation ndcg@10=0.028498663135060637 [0.52s]\n",
      "epoch 11 [20.01s]:  training loss=0.10456779599189758                                      \n",
      "epoch 12 [19.79s]:  training loss=0.09568975120782852                                      \n",
      "epoch 13 [20.22s]:  training loss=0.091096892952919                                        \n",
      "epoch 14 [18.74s]:  training loss=0.08725830167531967                                      \n",
      "epoch 15 [19.33s]: training loss=0.08480503410100937  validation ndcg@10=0.027217806784103887 [0.53s]\n",
      "epoch 16 [18.75s]:  training loss=0.0814850926399231                                       \n",
      "epoch 17 [18.66s]:  training loss=0.07664058357477188                                      \n",
      "epoch 18 [19.57s]:  training loss=0.07343488931655884                                      \n",
      "epoch 19 [18.78s]:  training loss=0.0721813216805458                                       \n",
      "epoch 20 [18.63s]: training loss=0.06743335723876953  validation ndcg@10=0.02764144682828559 [0.51s]\n",
      "epoch 21 [18.69s]:  training loss=0.06526987999677658                                      \n",
      "epoch 22 [18.63s]:  training loss=0.06499753892421722                                      \n",
      "epoch 23 [18.59s]:  training loss=0.061760857701301575                                     \n",
      "epoch 24 [20.14s]:  training loss=0.061977729201316833                                     \n",
      "epoch 25 [18.66s]: training loss=0.05911923199892044  validation ndcg@10=0.025835053039958866 [0.53s]\n",
      "epoch 26 [18.53s]:  training loss=0.05816779285669327                                      \n",
      "epoch 27 [18.69s]:  training loss=0.05675989389419556                                      \n",
      "epoch 28 [18.61s]:  training loss=0.05804119631648064                                      \n",
      "epoch 29 [18.66s]:  training loss=0.05368746817111969                                      \n",
      "epoch 30 [18.65s]: training loss=0.05320648103952408  validation ndcg@10=0.026992172543382206 [0.57s]\n",
      "epoch 31 [18.57s]:  training loss=0.05261801555752754                                      \n",
      "epoch 32 [18.71s]:  training loss=0.05091243237257004                                      \n",
      "epoch 33 [18.52s]:  training loss=0.05129503086209297                                      \n",
      "epoch 34 [18.7s]:  training loss=0.05032990500330925                                       \n",
      "epoch 35 [18.7s]: training loss=0.04813264682888985  validation ndcg@10=0.02561006169776119 [0.51s]\n",
      "epoch 1 [18.62s]:  training loss=0.7722687125205994                                        \n",
      "epoch 2 [19.1s]:  training loss=0.6136659979820251                                         \n",
      "epoch 3 [19.68s]:  training loss=0.5186925530433655                                        \n",
      "epoch 4 [19.49s]:  training loss=0.44456812739372253                                       \n",
      "epoch 5 [19.19s]: training loss=0.39240914583206177  validation ndcg@10=0.021746594239293468 [0.55s]\n",
      "epoch 6 [19.62s]:  training loss=0.34343716502189636                                       \n",
      "epoch 7 [20.93s]:  training loss=0.31291913986206055                                       \n",
      "epoch 8 [19.46s]:  training loss=0.29137519001960754                                       \n",
      "epoch 9 [19.71s]:  training loss=0.2709922194480896                                        \n",
      "epoch 10 [19.75s]: training loss=0.25699055194854736  validation ndcg@10=0.02550051163887829 [0.56s]\n",
      "epoch 11 [20.64s]:  training loss=0.24097713828086853                                      \n",
      "epoch 12 [19.89s]:  training loss=0.22735442221164703                                      \n",
      "epoch 13 [19.78s]:  training loss=0.21809029579162598                                      \n",
      "epoch 14 [19.18s]:  training loss=0.2064100205898285                                       \n",
      "epoch 15 [20.2s]: training loss=0.19983550906181335  validation ndcg@10=0.02807718700224719 [0.56s]\n",
      "epoch 16 [19.31s]:  training loss=0.18960580229759216                                      \n",
      "epoch 17 [19.78s]:  training loss=0.1833159476518631                                       \n",
      "epoch 18 [19.88s]:  training loss=0.18105797469615936                                      \n",
      "epoch 19 [19.71s]:  training loss=0.1740487515926361                                       \n",
      "epoch 20 [19.63s]: training loss=0.1642606109380722  validation ndcg@10=0.028391236057573505 [0.58s]\n",
      "epoch 21 [19.78s]:  training loss=0.16161465644836426                                      \n",
      "epoch 22 [19.64s]:  training loss=0.15993349254131317                                      \n",
      "epoch 23 [19.5s]:  training loss=0.15366485714912415                                       \n",
      "epoch 24 [21.18s]:  training loss=0.15116021037101746                                      \n",
      "epoch 25 [19.65s]: training loss=0.14554305374622345  validation ndcg@10=0.028780320651753642 [0.62s]\n",
      "epoch 26 [19.64s]:  training loss=0.1420363485813141                                       \n",
      "epoch 27 [19.68s]:  training loss=0.13816243410110474                                      \n",
      "epoch 28 [19.78s]:  training loss=0.13604140281677246                                      \n",
      "epoch 29 [19.74s]:  training loss=0.13235874474048615                                      \n",
      "epoch 30 [19.61s]: training loss=0.12966451048851013  validation ndcg@10=0.029850061756616017 [0.56s]\n",
      "epoch 31 [19.72s]:  training loss=0.12894830107688904                                      \n",
      "epoch 32 [19.75s]:  training loss=0.12471948564052582                                      \n",
      "epoch 33 [19.93s]:  training loss=0.12189635634422302                                      \n",
      "epoch 34 [19.3s]:  training loss=0.12005078792572021                                       \n",
      "epoch 35 [20.19s]: training loss=0.11817395687103271  validation ndcg@10=0.0300282161630139 [0.57s]\n",
      "epoch 36 [19.54s]:  training loss=0.11360405385494232                                      \n",
      "epoch 37 [19.7s]:  training loss=0.1130644828081131                                        \n",
      "epoch 38 [19.99s]:  training loss=0.11424902826547623                                      \n",
      "epoch 39 [19.75s]:  training loss=0.11014819890260696                                      \n",
      "epoch 40 [19.7s]: training loss=0.1068834587931633  validation ndcg@10=0.02849447023879038 [0.62s]\n",
      "epoch 41 [20.1s]:  training loss=0.10390130430459976                                       \n",
      "epoch 42 [20.7s]:  training loss=0.10393165051937103                                       \n",
      "epoch 43 [19.88s]:  training loss=0.10155656188726425                                      \n",
      "epoch 44 [19.56s]:  training loss=0.10244155675172806                                      \n",
      "epoch 45 [19.73s]: training loss=0.1006062850356102  validation ndcg@10=0.029245279895704496 [0.61s]\n",
      "epoch 46 [19.8s]:  training loss=0.09804239869117737                                       \n",
      "epoch 47 [19.71s]:  training loss=0.09711317718029022                                      \n",
      "epoch 48 [19.44s]:  training loss=0.0974876880645752                                       \n",
      "epoch 49 [19.85s]:  training loss=0.09297769516706467                                      \n",
      "epoch 50 [19.49s]: training loss=0.09242680668830872  validation ndcg@10=0.02925196364897577 [0.55s]\n",
      "epoch 51 [19.66s]:  training loss=0.09428814798593521                                      \n",
      "epoch 52 [19.73s]:  training loss=0.09186165034770966                                      \n",
      "epoch 53 [19.52s]:  training loss=0.0892157256603241                                       \n",
      "epoch 54 [19.82s]:  training loss=0.0872737243771553                                       \n",
      "epoch 55 [19.52s]: training loss=0.08833569288253784  validation ndcg@10=0.029471636110836145 [0.63s]\n",
      "epoch 56 [19.74s]:  training loss=0.08711637556552887                                      \n",
      "epoch 57 [19.69s]:  training loss=0.08465556055307388                                      \n",
      "epoch 58 [19.76s]:  training loss=0.0803370252251625                                       \n",
      "epoch 59 [21.04s]:  training loss=0.08382977545261383                                      \n",
      "epoch 60 [19.5s]: training loss=0.08296124637126923  validation ndcg@10=0.03011014157225275 [0.55s]\n",
      "epoch 61 [19.81s]:  training loss=0.08041007816791534                                      \n",
      "epoch 62 [19.73s]:  training loss=0.07969190925359726                                      \n",
      "epoch 63 [19.32s]:  training loss=0.07912404835224152                                      \n",
      "epoch 64 [19.91s]:  training loss=0.0791826844215393                                       \n",
      "epoch 65 [19.98s]: training loss=0.07866236567497253  validation ndcg@10=0.02915253938181429 [0.61s]\n",
      "epoch 66 [20.19s]:  training loss=0.07765401154756546                                      \n",
      "epoch 67 [19.94s]:  training loss=0.07702239602804184                                      \n",
      "epoch 68 [19.55s]:  training loss=0.07670427113771439                                      \n",
      "epoch 69 [20.0s]:  training loss=0.07387936860322952                                       \n",
      "epoch 70 [19.65s]: training loss=0.07315032184123993  validation ndcg@10=0.02896093456954589 [0.54s]\n",
      "epoch 71 [19.61s]:  training loss=0.07375191897153854                                      \n",
      "epoch 72 [19.53s]:  training loss=0.07263065129518509                                      \n",
      "epoch 73 [19.62s]:  training loss=0.07086449861526489                                      \n",
      "epoch 74 [19.64s]:  training loss=0.06981287896633148                                      \n",
      "epoch 75 [19.47s]: training loss=0.07068929821252823  validation ndcg@10=0.02896362545963145 [0.55s]\n",
      "epoch 76 [20.61s]:  training loss=0.06824555993080139                                      \n",
      "epoch 77 [19.38s]:  training loss=0.0687895342707634                                       \n",
      "epoch 78 [19.24s]:  training loss=0.0681573748588562                                       \n",
      "epoch 79 [19.47s]:  training loss=0.07097236812114716                                      \n",
      "epoch 80 [19.54s]: training loss=0.06996281445026398  validation ndcg@10=0.027127263017185318 [0.62s]\n",
      "epoch 81 [19.27s]:  training loss=0.06617432087659836                                      \n",
      "epoch 82 [19.83s]:  training loss=0.06404095143079758                                      \n",
      "epoch 83 [19.82s]:  training loss=0.06639990210533142                                      \n",
      "epoch 84 [19.78s]:  training loss=0.06432849913835526                                      \n",
      "epoch 85 [19.5s]: training loss=0.06309076398611069  validation ndcg@10=0.027836976233600128 [0.54s]\n",
      "epoch 1 [25.64s]:  training loss=0.7895063161849976                                        \n",
      "epoch 2 [26.93s]:  training loss=0.718116819858551                                         \n",
      "epoch 3 [26.92s]:  training loss=0.6277722120285034                                        \n",
      "epoch 4 [26.86s]:  training loss=0.5655902028083801                                        \n",
      "epoch 5 [26.72s]: training loss=0.5195518136024475  validation ndcg@10=0.012204341503147155 [0.59s]\n",
      "epoch 6 [28.91s]:  training loss=0.47625818848609924                                       \n",
      "epoch 7 [26.04s]:  training loss=0.4427986741065979                                        \n",
      "epoch 8 [28.62s]:  training loss=0.40904372930526733                                       \n",
      "epoch 9 [28.85s]:  training loss=0.3836479187011719                                        \n",
      "epoch 10 [27.85s]: training loss=0.3622175455093384  validation ndcg@10=0.022401112810342596 [0.62s]\n",
      "epoch 11 [28.13s]:  training loss=0.3438303768634796                                       \n",
      "epoch 12 [28.63s]:  training loss=0.32141411304473877                                      \n",
      "epoch 13 [27.72s]:  training loss=0.30714505910873413                                      \n",
      "epoch 14 [28.08s]:  training loss=0.29436978697776794                                      \n",
      "epoch 15 [27.83s]: training loss=0.28162309527397156  validation ndcg@10=0.024826137614596334 [0.68s]\n",
      "epoch 16 [28.11s]:  training loss=0.2757079303264618                                       \n",
      "epoch 17 [27.82s]:  training loss=0.2648007571697235                                       \n",
      "epoch 18 [27.75s]:  training loss=0.2567111551761627                                       \n",
      "epoch 19 [28.51s]:  training loss=0.2525158226490021                                       \n",
      "epoch 20 [28.27s]: training loss=0.24649207293987274  validation ndcg@10=0.02599798356769893 [0.62s]\n",
      "epoch 21 [27.76s]:  training loss=0.23677770793437958                                      \n",
      "epoch 22 [28.33s]:  training loss=0.2313629686832428                                       \n",
      "epoch 23 [27.91s]:  training loss=0.2223302125930786                                       \n",
      "epoch 24 [27.86s]:  training loss=0.21876926720142365                                      \n",
      "epoch 25 [29.4s]: training loss=0.21870221197605133  validation ndcg@10=0.026853620662665334 [0.63s]\n",
      "epoch 26 [28.48s]:  training loss=0.21315926313400269                                      \n",
      "epoch 27 [27.78s]:  training loss=0.2095351368188858                                       \n",
      "epoch 28 [28.43s]:  training loss=0.204253152012825                                        \n",
      "epoch 29 [28.29s]:  training loss=0.19911691546440125                                      \n",
      "epoch 30 [28.14s]: training loss=0.19296474754810333  validation ndcg@10=0.02715420260853802 [0.6s]\n",
      "epoch 31 [28.05s]:  training loss=0.19453811645507812                                      \n",
      "epoch 32 [28.3s]:  training loss=0.18877069652080536                                       \n",
      "epoch 33 [28.53s]:  training loss=0.18696467578411102                                      \n",
      "epoch 34 [28.05s]:  training loss=0.18345578014850616                                      \n",
      "epoch 35 [28.41s]: training loss=0.18162478506565094  validation ndcg@10=0.027493373253400642 [0.61s]\n",
      "epoch 36 [28.02s]:  training loss=0.17780451476573944                                      \n",
      "epoch 37 [28.01s]:  training loss=0.17373017966747284                                      \n",
      "epoch 38 [27.7s]:  training loss=0.16959145665168762                                       \n",
      "epoch 39 [28.26s]:  training loss=0.1701449602842331                                       \n",
      "epoch 40 [28.8s]: training loss=0.16724713146686554  validation ndcg@10=0.027975192875179765 [0.68s]\n",
      "epoch 41 [28.41s]:  training loss=0.16350780427455902                                      \n",
      "epoch 42 [28.1s]:  training loss=0.1629721224308014                                        \n",
      "epoch 43 [29.5s]:  training loss=0.16000895202159882                                       \n",
      "epoch 44 [28.06s]:  training loss=0.1561909168958664                                       \n",
      "epoch 45 [28.4s]: training loss=0.1565714329481125  validation ndcg@10=0.028622730834979793 [0.59s]\n",
      "epoch 46 [28.43s]:  training loss=0.15494291484355927                                      \n",
      "epoch 47 [28.02s]:  training loss=0.14890487492084503                                      \n",
      "epoch 48 [28.08s]:  training loss=0.14945648610591888                                      \n",
      "epoch 49 [28.32s]:  training loss=0.1464659720659256                                       \n",
      "epoch 50 [28.28s]: training loss=0.14597724378108978  validation ndcg@10=0.02839401767667154 [0.62s]\n",
      "epoch 51 [28.1s]:  training loss=0.14304590225219727                                       \n",
      "epoch 52 [28.05s]:  training loss=0.13870462775230408                                      \n",
      "epoch 53 [28.35s]:  training loss=0.14003485441207886                                      \n",
      "epoch 54 [28.25s]:  training loss=0.1417156159877777                                       \n",
      "epoch 55 [29.28s]: training loss=0.1363479048013687  validation ndcg@10=0.028531380588087754 [0.65s]\n",
      "epoch 56 [28.12s]:  training loss=0.13440266251564026                                      \n",
      "epoch 57 [28.44s]:  training loss=0.13556411862373352                                      \n",
      "epoch 58 [28.39s]:  training loss=0.12892982363700867                                      \n",
      "epoch 59 [27.94s]:  training loss=0.1336487978696823                                       \n",
      "epoch 60 [28.34s]: training loss=0.1285480409860611  validation ndcg@10=0.02929319609582733 [0.61s]\n",
      "epoch 61 [28.22s]:  training loss=0.12916724383831024                                      \n",
      "epoch 62 [29.09s]:  training loss=0.12952226400375366                                      \n",
      "epoch 63 [27.96s]:  training loss=0.12269939482212067                                      \n",
      "epoch 64 [29.29s]:  training loss=0.12783829867839813                                      \n",
      "epoch 65 [27.77s]: training loss=0.1240188255906105  validation ndcg@10=0.029703758394902163 [0.59s]\n",
      "epoch 66 [28.34s]:  training loss=0.12277012318372726                                      \n",
      "epoch 67 [28.52s]:  training loss=0.11904850602149963                                      \n",
      "epoch 68 [27.76s]:  training loss=0.1230388879776001                                       \n",
      "epoch 69 [28.28s]:  training loss=0.12022627890110016                                      \n",
      "epoch 70 [28.76s]: training loss=0.12044666707515717  validation ndcg@10=0.029425516905597415 [0.63s]\n",
      "epoch 71 [28.15s]:  training loss=0.11789273470640182                                      \n",
      "epoch 72 [28.1s]:  training loss=0.11765485256910324                                       \n",
      "epoch 73 [28.28s]:  training loss=0.11347496509552002                                      \n",
      "epoch 74 [27.74s]:  training loss=0.1140100285410881                                       \n",
      "epoch 75 [28.79s]: training loss=0.11377973854541779  validation ndcg@10=0.029813476962448033 [0.63s]\n",
      "epoch 76 [28.07s]:  training loss=0.11120565235614777                                      \n",
      "epoch 77 [27.98s]:  training loss=0.11105603724718094                                      \n",
      "epoch 78 [28.26s]:  training loss=0.11330220848321915                                      \n",
      "epoch 79 [28.06s]:  training loss=0.11080389469861984                                      \n",
      "epoch 80 [28.76s]: training loss=0.10762673616409302  validation ndcg@10=0.029884204491956556 [0.62s]\n",
      "epoch 81 [27.44s]:  training loss=0.10915933549404144                                      \n",
      "epoch 82 [27.97s]:  training loss=0.10690420120954514                                      \n",
      "epoch 83 [28.5s]:  training loss=0.10605339705944061                                       \n",
      "epoch 84 [27.89s]:  training loss=0.10440552979707718                                      \n",
      "epoch 85 [27.9s]: training loss=0.1024109423160553  validation ndcg@10=0.029928146824103326 [0.6s]\n",
      "epoch 86 [28.03s]:  training loss=0.10249631851911545                                      \n",
      "epoch 87 [28.5s]:  training loss=0.10465317964553833                                       \n",
      "epoch 88 [28.63s]:  training loss=0.10464724898338318                                      \n",
      "epoch 89 [28.41s]:  training loss=0.10422437638044357                                      \n",
      "epoch 90 [28.14s]: training loss=0.09919989854097366  validation ndcg@10=0.030150423416362326 [0.59s]\n",
      "epoch 91 [28.54s]:  training loss=0.0975874662399292                                       \n",
      "epoch 92 [28.38s]:  training loss=0.0979277715086937                                       \n",
      "epoch 93 [28.54s]:  training loss=0.10042650252580643                                      \n",
      "epoch 94 [28.18s]:  training loss=0.09835270047187805                                      \n",
      "epoch 95 [28.05s]: training loss=0.10029520094394684  validation ndcg@10=0.029951474659786034 [0.65s]\n",
      "epoch 96 [28.25s]:  training loss=0.09875544160604477                                      \n",
      "epoch 97 [28.4s]:  training loss=0.09675843268632889                                       \n",
      "epoch 98 [27.82s]:  training loss=0.09465846419334412                                      \n",
      "epoch 99 [28.44s]:  training loss=0.09671492129564285                                      \n",
      "epoch 100 [28.54s]: training loss=0.09195563197135925  validation ndcg@10=0.030140212745108117 [0.68s]\n",
      "epoch 101 [27.81s]:  training loss=0.09112502634525299                                     \n",
      "epoch 102 [28.98s]:  training loss=0.09276071190834045                                     \n",
      "epoch 103 [28.75s]:  training loss=0.09093357622623444                                     \n",
      "epoch 104 [28.03s]:  training loss=0.0947919562458992                                      \n",
      "epoch 105 [27.63s]: training loss=0.09187506139278412  validation ndcg@10=0.030106873210966115 [0.64s]\n",
      "epoch 106 [28.34s]:  training loss=0.08999140560626984                                     \n",
      "epoch 107 [28.28s]:  training loss=0.09007730334997177                                     \n",
      "epoch 108 [28.33s]:  training loss=0.08935524523258209                                     \n",
      "epoch 109 [28.67s]:  training loss=0.08916334062814713                                     \n",
      "epoch 110 [28.22s]: training loss=0.08911369740962982  validation ndcg@10=0.029821442557283355 [0.6s]\n",
      "epoch 111 [28.19s]:  training loss=0.090375155210495                                       \n",
      "epoch 112 [28.5s]:  training loss=0.0890459269285202                                       \n",
      "epoch 113 [28.18s]:  training loss=0.0859893411397934                                      \n",
      "epoch 114 [27.97s]:  training loss=0.08621544390916824                                     \n",
      "epoch 115 [28.61s]: training loss=0.08669247478246689  validation ndcg@10=0.029794140373652807 [0.61s]\n",
      "epoch 1 [27.89s]:  training loss=0.5182988047599792                                        \n",
      "epoch 2 [29.28s]:  training loss=0.276974618434906                                         \n",
      "epoch 3 [28.01s]:  training loss=0.21503590047359467                                       \n",
      "epoch 4 [28.55s]:  training loss=0.1820298433303833                                        \n",
      "epoch 5 [28.25s]: training loss=0.1613115668296814  validation ndcg@10=0.02730254951595782 [0.72s]\n",
      "epoch 6 [28.4s]:  training loss=0.14049719274044037                                        \n",
      "epoch 7 [29.37s]:  training loss=0.12630021572113037                                       \n",
      "epoch 8 [28.17s]:  training loss=0.11905848234891891                                       \n",
      "epoch 9 [28.94s]:  training loss=0.11119651049375534                                       \n",
      "epoch 10 [29.07s]: training loss=0.10492700338363647  validation ndcg@10=0.028356754938632284 [0.71s]\n",
      "epoch 11 [29.13s]:  training loss=0.09569593518972397                                      \n",
      "epoch 12 [28.91s]:  training loss=0.0915047749876976                                       \n",
      "epoch 13 [29.07s]:  training loss=0.08998795598745346                                      \n",
      "epoch 14 [28.56s]:  training loss=0.08233771473169327                                      \n",
      "epoch 15 [28.91s]: training loss=0.07898840308189392  validation ndcg@10=0.02666663134891257 [0.69s]\n",
      "epoch 16 [29.02s]:  training loss=0.07685589045286179                                      \n",
      "epoch 17 [29.19s]:  training loss=0.07465874403715134                                      \n",
      "epoch 18 [28.69s]:  training loss=0.07061102241277695                                      \n",
      "epoch 19 [28.79s]:  training loss=0.06883931159973145                                      \n",
      "epoch 20 [29.38s]: training loss=0.0639885663986206  validation ndcg@10=0.028048253425103507 [0.7s]\n",
      "epoch 21 [28.3s]:  training loss=0.06529036909341812                                       \n",
      "epoch 22 [28.67s]:  training loss=0.061048056930303574                                     \n",
      "epoch 23 [28.12s]:  training loss=0.05798017606139183                                      \n",
      "epoch 24 [29.04s]:  training loss=0.06164019927382469                                      \n",
      "epoch 25 [29.14s]: training loss=0.05763021111488342  validation ndcg@10=0.02535536406508892 [0.68s]\n",
      "epoch 26 [28.48s]:  training loss=0.05683179199695587                                      \n",
      "epoch 27 [28.62s]:  training loss=0.05472508817911148                                      \n",
      "epoch 28 [28.24s]:  training loss=0.05530707910656929                                      \n",
      "epoch 29 [28.53s]:  training loss=0.05216798186302185                                      \n",
      "epoch 30 [28.74s]: training loss=0.052655842155218124  validation ndcg@10=0.02599406262211191 [0.69s]\n",
      "epoch 31 [28.36s]:  training loss=0.05211152508854866                                      \n",
      "epoch 32 [28.37s]:  training loss=0.04948195442557335                                      \n",
      "epoch 33 [28.55s]:  training loss=0.04967594891786575                                      \n",
      "epoch 34 [28.63s]:  training loss=0.04735357686877251                                      \n",
      "epoch 35 [28.17s]: training loss=0.04859798401594162  validation ndcg@10=0.026302565150570105 [0.66s]\n",
      "epoch 1 [25.47s]:  training loss=0.8371720314025879                                        \n",
      "epoch 2 [25.61s]:  training loss=0.8123998045921326                                        \n",
      "epoch 3 [25.35s]:  training loss=0.794452965259552                                         \n",
      "epoch 4 [26.61s]:  training loss=0.779585063457489                                         \n",
      "epoch 5 [25.65s]: training loss=0.7623636722564697  validation ndcg@10=0.010473250813809735 [0.62s]\n",
      "epoch 6 [25.09s]:  training loss=0.7483619451522827                                        \n",
      "epoch 7 [25.74s]:  training loss=0.7306046485900879                                        \n",
      "epoch 8 [25.38s]:  training loss=0.7095616459846497                                        \n",
      "epoch 9 [25.42s]:  training loss=0.6847955584526062                                        \n",
      "epoch 10 [25.79s]: training loss=0.6594170331954956  validation ndcg@10=0.011229544844800468 [0.7s]\n",
      "epoch 11 [25.11s]:  training loss=0.6343671083450317                                       \n",
      "epoch 12 [25.36s]:  training loss=0.6198124885559082                                       \n",
      "epoch 13 [24.86s]:  training loss=0.602124035358429                                        \n",
      "epoch 14 [24.75s]:  training loss=0.5917967557907104                                       \n",
      "epoch 15 [24.85s]: training loss=0.5745345950126648  validation ndcg@10=0.010716500782108493 [0.62s]\n",
      "epoch 16 [24.73s]:  training loss=0.5614221692085266                                       \n",
      "epoch 17 [24.84s]:  training loss=0.5490656495094299                                       \n",
      "epoch 18 [24.47s]:  training loss=0.542629063129425                                        \n",
      "epoch 19 [25.19s]:  training loss=0.5349214673042297                                       \n",
      "epoch 20 [24.74s]: training loss=0.5267956256866455  validation ndcg@10=0.011904475968083939 [0.66s]\n",
      "epoch 21 [24.71s]:  training loss=0.5147170424461365                                       \n",
      "epoch 22 [25.14s]:  training loss=0.5026106834411621                                       \n",
      "epoch 23 [24.79s]:  training loss=0.495250940322876                                        \n",
      "epoch 24 [26.23s]:  training loss=0.48377490043640137                                      \n",
      "epoch 25 [24.36s]: training loss=0.475805401802063  validation ndcg@10=0.01579506954468849 [0.61s]\n",
      "epoch 26 [24.89s]:  training loss=0.46217870712280273                                      \n",
      "epoch 27 [25.27s]:  training loss=0.45093274116516113                                      \n",
      "epoch 28 [25.09s]:  training loss=0.4405622184276581                                       \n",
      "epoch 29 [25.02s]:  training loss=0.4332573711872101                                       \n",
      "epoch 30 [24.9s]: training loss=0.4265980124473572  validation ndcg@10=0.01846917890346151 [0.65s]\n",
      "epoch 31 [24.65s]:  training loss=0.4197300672531128                                       \n",
      "epoch 32 [25.72s]:  training loss=0.41952475905418396                                      \n",
      "epoch 33 [24.24s]:  training loss=0.4086231291294098                                       \n",
      "epoch 34 [24.85s]:  training loss=0.4002296030521393                                       \n",
      "epoch 35 [24.96s]: training loss=0.3946278691291809  validation ndcg@10=0.01987053832698511 [0.6s]\n",
      "epoch 36 [24.32s]:  training loss=0.3889559805393219                                       \n",
      "epoch 37 [24.5s]:  training loss=0.3871282637119293                                        \n",
      "epoch 38 [24.85s]:  training loss=0.38019609451293945                                      \n",
      "epoch 39 [24.73s]:  training loss=0.37639227509498596                                      \n",
      "epoch 40 [24.0s]: training loss=0.370158314704895  validation ndcg@10=0.021340303450152334 [0.67s]\n",
      "epoch 41 [25.28s]:  training loss=0.36594924330711365                                      \n",
      "epoch 42 [25.33s]:  training loss=0.35889390110969543                                      \n",
      "epoch 43 [25.3s]:  training loss=0.35594478249549866                                       \n",
      "epoch 44 [25.6s]:  training loss=0.3477267026901245                                        \n",
      "epoch 45 [25.66s]: training loss=0.34603700041770935  validation ndcg@10=0.022847684177460456 [0.63s]\n",
      "epoch 46 [24.56s]:  training loss=0.34110960364341736                                      \n",
      "epoch 47 [24.61s]:  training loss=0.34150201082229614                                      \n",
      "epoch 48 [24.55s]:  training loss=0.3365555703639984                                       \n",
      "epoch 49 [24.61s]:  training loss=0.3308539092540741                                       \n",
      "epoch 50 [24.53s]: training loss=0.3223479390144348  validation ndcg@10=0.02335946073662709 [0.63s]\n",
      "epoch 51 [24.58s]:  training loss=0.31605565547943115                                      \n",
      "epoch 52 [24.98s]:  training loss=0.3224498927593231                                       \n",
      "epoch 53 [24.95s]:  training loss=0.3129684627056122                                       \n",
      "epoch 54 [25.76s]:  training loss=0.3119562268257141                                       \n",
      "epoch 55 [24.64s]: training loss=0.3053315579891205  validation ndcg@10=0.023941034553748262 [0.62s]\n",
      "epoch 56 [24.73s]:  training loss=0.30556535720825195                                      \n",
      "epoch 57 [25.04s]:  training loss=0.30149972438812256                                      \n",
      "epoch 58 [25.02s]:  training loss=0.29808804392814636                                      \n",
      "epoch 59 [24.68s]:  training loss=0.2984783351421356                                       \n",
      "epoch 60 [25.16s]: training loss=0.2973344027996063  validation ndcg@10=0.02475163037148128 [0.64s]\n",
      "epoch 61 [25.13s]:  training loss=0.2897346019744873                                       \n",
      "epoch 62 [25.09s]:  training loss=0.29195305705070496                                      \n",
      "epoch 63 [24.66s]:  training loss=0.2875309884548187                                       \n",
      "epoch 64 [24.63s]:  training loss=0.2810504734516144                                       \n",
      "epoch 65 [25.32s]: training loss=0.2830289304256439  validation ndcg@10=0.025152422626267153 [0.67s]\n",
      "epoch 66 [26.07s]:  training loss=0.2802554666996002                                       \n",
      "epoch 67 [24.9s]:  training loss=0.27676665782928467                                       \n",
      "epoch 68 [25.14s]:  training loss=0.2729666531085968                                       \n",
      "epoch 69 [24.89s]:  training loss=0.27336928248405457                                      \n",
      "epoch 70 [24.91s]: training loss=0.2730976939201355  validation ndcg@10=0.0253066933970999 [0.61s]\n",
      "epoch 71 [25.15s]:  training loss=0.2668236792087555                                       \n",
      "epoch 72 [25.33s]:  training loss=0.2659170925617218                                       \n",
      "epoch 73 [25.21s]:  training loss=0.26494333148002625                                      \n",
      "epoch 74 [25.36s]:  training loss=0.2675049901008606                                       \n",
      "epoch 75 [25.6s]: training loss=0.25842148065567017  validation ndcg@10=0.02591375690089318 [0.65s]\n",
      "epoch 76 [25.63s]:  training loss=0.2593521475791931                                       \n",
      "epoch 77 [24.93s]:  training loss=0.25894641876220703                                      \n",
      "epoch 78 [24.92s]:  training loss=0.25440454483032227                                      \n",
      "epoch 79 [24.43s]:  training loss=0.2535313069820404                                       \n",
      "epoch 80 [24.72s]: training loss=0.25092801451683044  validation ndcg@10=0.02629509837030205 [0.69s]\n",
      "epoch 81 [24.7s]:  training loss=0.25030097365379333                                       \n",
      "epoch 82 [24.6s]:  training loss=0.24443264305591583                                       \n",
      "epoch 83 [25.0s]:  training loss=0.24392275512218475                                       \n",
      "epoch 84 [24.88s]:  training loss=0.24438107013702393                                      \n",
      "epoch 85 [24.91s]: training loss=0.2456171065568924  validation ndcg@10=0.026602820698327093 [0.62s]\n",
      "epoch 86 [24.78s]:  training loss=0.24247194826602936                                      \n",
      "epoch 87 [25.7s]:  training loss=0.23517869412899017                                       \n",
      "epoch 88 [24.44s]:  training loss=0.23876895010471344                                      \n",
      "epoch 89 [24.67s]:  training loss=0.23584285378456116                                      \n",
      "epoch 90 [24.77s]: training loss=0.2343425750732422  validation ndcg@10=0.027169417979269413 [0.68s]\n",
      "epoch 91 [24.69s]:  training loss=0.23317918181419373                                      \n",
      "epoch 92 [25.15s]:  training loss=0.23561877012252808                                      \n",
      "epoch 93 [24.77s]:  training loss=0.2313392460346222                                       \n",
      "epoch 94 [25.04s]:  training loss=0.2317742556333542                                       \n",
      "epoch 95 [25.1s]: training loss=0.2264651209115982  validation ndcg@10=0.02704700747691812 [0.64s]\n",
      "epoch 96 [24.74s]:  training loss=0.2315952479839325                                       \n",
      "epoch 97 [25.11s]:  training loss=0.22570505738258362                                      \n",
      "epoch 98 [25.13s]:  training loss=0.22343987226486206                                      \n",
      "epoch 99 [24.88s]:  training loss=0.22587239742279053                                      \n",
      "epoch 100 [24.54s]: training loss=0.22325804829597473  validation ndcg@10=0.027206782317833477 [0.61s]\n",
      "epoch 101 [24.71s]:  training loss=0.21836115419864655                                     \n",
      "epoch 102 [24.75s]:  training loss=0.2190171629190445                                      \n",
      "epoch 103 [25.06s]:  training loss=0.21967844665050507                                     \n",
      "epoch 104 [25.21s]:  training loss=0.2173781841993332                                      \n",
      "epoch 105 [25.04s]: training loss=0.21702471375465393  validation ndcg@10=0.027255930858585582 [0.61s]\n",
      "epoch 106 [25.36s]:  training loss=0.21178558468818665                                     \n",
      "epoch 107 [26.45s]:  training loss=0.21634258329868317                                     \n",
      "epoch 108 [25.12s]:  training loss=0.21397484838962555                                     \n",
      "epoch 109 [25.38s]:  training loss=0.2094661295413971                                      \n",
      "epoch 110 [25.09s]: training loss=0.21380721032619476  validation ndcg@10=0.027284144396949542 [0.63s]\n",
      "epoch 111 [25.09s]:  training loss=0.20855480432510376                                     \n",
      "epoch 112 [24.91s]:  training loss=0.21196341514587402                                     \n",
      "epoch 113 [24.45s]:  training loss=0.207842618227005                                       \n",
      "epoch 114 [24.56s]:  training loss=0.20766641199588776                                     \n",
      "epoch 115 [24.57s]: training loss=0.20833875238895416  validation ndcg@10=0.027572492280214665 [0.66s]\n",
      "epoch 116 [24.59s]:  training loss=0.2059299200773239                                      \n",
      "epoch 117 [24.67s]:  training loss=0.2048700749874115                                      \n",
      "epoch 118 [24.75s]:  training loss=0.20569342374801636                                     \n",
      "epoch 119 [25.77s]:  training loss=0.20112477242946625                                     \n",
      "epoch 120 [24.98s]: training loss=0.19818618893623352  validation ndcg@10=0.027273279079323465 [0.6s]\n",
      "epoch 121 [25.11s]:  training loss=0.2001032829284668                                      \n",
      "epoch 122 [24.85s]:  training loss=0.19782215356826782                                     \n",
      "epoch 123 [24.95s]:  training loss=0.20174148678779602                                     \n",
      "epoch 124 [24.84s]:  training loss=0.19718731939792633                                     \n",
      "epoch 125 [24.69s]: training loss=0.2009965032339096  validation ndcg@10=0.02780130382717136 [0.62s]\n",
      "epoch 126 [25.23s]:  training loss=0.1988981068134308                                      \n",
      "epoch 127 [24.58s]:  training loss=0.19721657037734985                                     \n",
      "epoch 128 [25.82s]:  training loss=0.19364292919635773                                     \n",
      "epoch 129 [25.33s]:  training loss=0.19473400712013245                                     \n",
      "epoch 130 [24.97s]: training loss=0.19154508411884308  validation ndcg@10=0.027680139594514014 [0.61s]\n",
      "epoch 131 [24.54s]:  training loss=0.19511522352695465                                     \n",
      "epoch 132 [24.7s]:  training loss=0.19162039458751678                                      \n",
      "epoch 133 [24.71s]:  training loss=0.18883097171783447                                     \n",
      "epoch 134 [24.64s]:  training loss=0.18836985528469086                                     \n",
      "epoch 135 [24.59s]: training loss=0.18598231673240662  validation ndcg@10=0.027559302381190587 [0.59s]\n",
      "epoch 136 [25.19s]:  training loss=0.18624190986156464                                     \n",
      "epoch 137 [24.95s]:  training loss=0.1855754405260086                                      \n",
      "epoch 138 [25.2s]:  training loss=0.18539699912071228                                      \n",
      "epoch 139 [24.91s]:  training loss=0.1852107048034668                                      \n",
      "epoch 140 [25.07s]: training loss=0.18604959547519684  validation ndcg@10=0.028020323102587823 [0.64s]\n",
      "epoch 141 [25.58s]:  training loss=0.18564338982105255                                     \n",
      "epoch 142 [24.73s]:  training loss=0.18505024909973145                                     \n",
      "epoch 143 [24.98s]:  training loss=0.178583562374115                                       \n",
      "epoch 144 [25.09s]:  training loss=0.1817437708377838                                      \n",
      "epoch 145 [25.04s]: training loss=0.1782807856798172  validation ndcg@10=0.02804685004002431 [0.62s]\n",
      "epoch 146 [24.67s]:  training loss=0.179839625954628                                       \n",
      "epoch 147 [25.09s]:  training loss=0.17962761223316193                                     \n",
      "epoch 148 [24.55s]:  training loss=0.17827817797660828                                     \n",
      "epoch 149 [25.43s]:  training loss=0.1778091937303543                                      \n",
      "epoch 150 [24.33s]: training loss=0.1748969405889511  validation ndcg@10=0.02827593018372921 [0.66s]\n",
      "epoch 151 [24.36s]:  training loss=0.1731899529695511                                      \n",
      "epoch 152 [25.08s]:  training loss=0.1745707243680954                                      \n",
      "epoch 153 [25.12s]:  training loss=0.17491060495376587                                     \n",
      "epoch 154 [24.41s]:  training loss=0.1779928058385849                                      \n",
      "epoch 155 [25.02s]: training loss=0.17677170038223267  validation ndcg@10=0.02841314159446886 [0.63s]\n",
      "epoch 156 [24.93s]:  training loss=0.16839082539081573                                     \n",
      "epoch 157 [24.98s]:  training loss=0.1718156486749649                                      \n",
      "epoch 158 [24.79s]:  training loss=0.1709717959165573                                      \n",
      "epoch 159 [24.56s]:  training loss=0.16737152636051178                                     \n",
      "epoch 160 [24.83s]: training loss=0.17210154235363007  validation ndcg@10=0.028557579649310032 [0.69s]\n",
      "epoch 161 [24.88s]:  training loss=0.17056040465831757                                     \n",
      "epoch 162 [25.02s]:  training loss=0.17004379630088806                                     \n",
      "epoch 163 [25.03s]:  training loss=0.16979071497917175                                     \n",
      "epoch 164 [24.4s]:  training loss=0.16629058122634888                                      \n",
      "epoch 165 [24.54s]: training loss=0.16598813235759735  validation ndcg@10=0.028589734813719708 [0.64s]\n",
      "epoch 166 [24.54s]:  training loss=0.16607719659805298                                     \n",
      "epoch 167 [24.88s]:  training loss=0.16583847999572754                                     \n",
      "epoch 168 [24.66s]:  training loss=0.16565939784049988                                     \n",
      "epoch 169 [25.03s]:  training loss=0.16693353652954102                                     \n",
      "epoch 170 [25.75s]: training loss=0.16632238030433655  validation ndcg@10=0.02863181819301193 [0.65s]\n",
      "epoch 171 [24.17s]:  training loss=0.16204892098903656                                     \n",
      "epoch 172 [25.08s]:  training loss=0.16478069126605988                                     \n",
      "epoch 173 [24.8s]:  training loss=0.16400529444217682                                      \n",
      "epoch 174 [25.07s]:  training loss=0.16067278385162354                                     \n",
      "epoch 175 [24.95s]: training loss=0.16234035789966583  validation ndcg@10=0.028735067286594196 [0.66s]\n",
      "epoch 176 [24.91s]:  training loss=0.1621282994747162                                      \n",
      "epoch 177 [25.21s]:  training loss=0.16222628951072693                                     \n",
      "epoch 178 [25.65s]:  training loss=0.1590466946363449                                      \n",
      "epoch 179 [25.81s]:  training loss=0.15970969200134277                                     \n",
      "epoch 180 [25.17s]: training loss=0.15971814095973969  validation ndcg@10=0.029249274794905987 [0.64s]\n",
      "epoch 181 [25.07s]:  training loss=0.15761211514472961                                     \n",
      "epoch 182 [25.75s]:  training loss=0.15605400502681732                                     \n",
      "epoch 183 [25.25s]:  training loss=0.15511105954647064                                     \n",
      "epoch 184 [25.21s]:  training loss=0.15977393090724945                                     \n",
      "epoch 185 [25.91s]: training loss=0.15732403099536896  validation ndcg@10=0.028872776903980754 [0.61s]\n",
      "epoch 186 [25.07s]:  training loss=0.15492062270641327                                     \n",
      "epoch 187 [25.05s]:  training loss=0.15212447941303253                                     \n",
      "epoch 188 [24.65s]:  training loss=0.1541096568107605                                      \n",
      "epoch 189 [24.64s]:  training loss=0.1531628966331482                                      \n",
      "epoch 190 [24.98s]: training loss=0.15587466955184937  validation ndcg@10=0.028962973822995713 [0.65s]\n",
      "epoch 191 [26.16s]:  training loss=0.15450046956539154                                     \n",
      "epoch 192 [24.89s]:  training loss=0.15305280685424805                                     \n",
      "epoch 193 [25.2s]:  training loss=0.15222404897212982                                      \n",
      "epoch 194 [25.32s]:  training loss=0.15112464129924774                                     \n",
      "epoch 195 [25.36s]: training loss=0.14968648552894592  validation ndcg@10=0.029106553961845187 [0.63s]\n",
      "epoch 196 [25.08s]:  training loss=0.15031611919403076                                     \n",
      "epoch 197 [24.54s]:  training loss=0.1499570608139038                                      \n",
      "epoch 198 [24.92s]:  training loss=0.1479659378528595                                      \n",
      "epoch 199 [25.09s]:  training loss=0.14860020577907562                                     \n",
      "epoch 200 [24.79s]: training loss=0.1489771008491516  validation ndcg@10=0.029469749967292874 [0.6s]\n",
      "epoch 1 [29.38s]:  training loss=0.7560109496116638                                        \n",
      "epoch 2 [30.1s]:  training loss=0.5905018448829651                                         \n",
      "epoch 3 [30.31s]:  training loss=0.4965076744556427                                        \n",
      "epoch 4 [30.28s]:  training loss=0.4148445129394531                                        \n",
      "epoch 5 [30.57s]: training loss=0.36671552062034607  validation ndcg@10=0.02102485693956369 [0.76s]\n",
      "epoch 6 [29.92s]:  training loss=0.32541170716285706                                       \n",
      "epoch 7 [29.9s]:  training loss=0.30466681718826294                                        \n",
      "epoch 8 [30.07s]:  training loss=0.2870180010795593                                        \n",
      "epoch 9 [29.84s]:  training loss=0.2647862434387207                                        \n",
      "epoch 10 [31.36s]: training loss=0.25150731205940247  validation ndcg@10=0.024999231728142828 [0.68s]\n",
      "epoch 11 [30.05s]:  training loss=0.2408548891544342                                       \n",
      "epoch 12 [30.02s]:  training loss=0.23171384632587433                                      \n",
      "epoch 13 [30.31s]:  training loss=0.21745021641254425                                      \n",
      "epoch 14 [29.98s]:  training loss=0.2115035206079483                                       \n",
      "epoch 15 [30.18s]: training loss=0.20359861850738525  validation ndcg@10=0.027011731319324123 [0.71s]\n",
      "epoch 16 [30.12s]:  training loss=0.19479836523532867                                      \n",
      "epoch 17 [29.59s]:  training loss=0.1861741989850998                                       \n",
      "epoch 18 [30.9s]:  training loss=0.18547625839710236                                       \n",
      "epoch 19 [30.3s]:  training loss=0.17871473729610443                                       \n",
      "epoch 20 [30.23s]: training loss=0.1737188696861267  validation ndcg@10=0.02847538866044645 [0.75s]\n",
      "epoch 21 [29.96s]:  training loss=0.16605441272258759                                      \n",
      "epoch 22 [30.32s]:  training loss=0.16370317339897156                                      \n",
      "epoch 23 [30.49s]:  training loss=0.15742695331573486                                      \n",
      "epoch 24 [30.82s]:  training loss=0.15610875189304352                                      \n",
      "epoch 25 [30.17s]: training loss=0.14877024292945862  validation ndcg@10=0.028448644529473596 [0.71s]\n",
      "epoch 26 [30.01s]:  training loss=0.1456311196088791                                       \n",
      "epoch 27 [30.78s]:  training loss=0.1429228037595749                                       \n",
      "epoch 28 [28.24s]:  training loss=0.14119210839271545                                      \n",
      "epoch 29 [30.18s]:  training loss=0.13865593075752258                                      \n",
      "epoch 30 [30.11s]: training loss=0.1368861049413681  validation ndcg@10=0.029594741033466147 [0.71s]\n",
      "epoch 31 [29.95s]:  training loss=0.13070595264434814                                      \n",
      "epoch 32 [30.27s]:  training loss=0.13259856402873993                                      \n",
      "epoch 33 [29.99s]:  training loss=0.1290959268808365                                       \n",
      "epoch 34 [29.74s]:  training loss=0.1250150054693222                                       \n",
      "epoch 35 [30.62s]: training loss=0.12177522480487823  validation ndcg@10=0.0295552778968644 [0.75s]\n",
      "epoch 36 [30.63s]:  training loss=0.12244394421577454                                      \n",
      "epoch 37 [30.46s]:  training loss=0.11959343403577805                                      \n",
      "epoch 38 [30.27s]:  training loss=0.11617592722177505                                      \n",
      "epoch 39 [30.21s]:  training loss=0.11532003432512283                                      \n",
      "epoch 40 [30.05s]: training loss=0.11460897326469421  validation ndcg@10=0.030244335414178803 [0.76s]\n",
      "epoch 41 [30.32s]:  training loss=0.1086229607462883                                       \n",
      "epoch 42 [31.28s]:  training loss=0.10915420949459076                                      \n",
      "epoch 43 [30.34s]:  training loss=0.10786621272563934                                      \n",
      "epoch 44 [30.29s]:  training loss=0.10411150753498077                                      \n",
      "epoch 45 [30.49s]: training loss=0.10210370272397995  validation ndcg@10=0.03070276441749797 [0.67s]\n",
      "epoch 46 [31.07s]:  training loss=0.10431751608848572                                      \n",
      "epoch 47 [30.7s]:  training loss=0.09917215257883072                                       \n",
      "epoch 48 [30.32s]:  training loss=0.10124257951974869                                      \n",
      "epoch 49 [30.43s]:  training loss=0.09954239428043365                                      \n",
      "epoch 50 [30.03s]: training loss=0.09646385908126831  validation ndcg@10=0.0302408947084073 [0.7s]\n",
      "epoch 51 [30.43s]:  training loss=0.09512586891651154                                      \n",
      "epoch 52 [30.37s]:  training loss=0.09670530259609222                                      \n",
      "epoch 53 [30.46s]:  training loss=0.0908423513174057                                       \n",
      "epoch 54 [30.3s]:  training loss=0.09350267052650452                                       \n",
      "epoch 55 [30.25s]: training loss=0.08852257579565048  validation ndcg@10=0.030258203111410135 [0.73s]\n",
      "epoch 56 [30.58s]:  training loss=0.09071407467126846                                      \n",
      "epoch 57 [30.38s]:  training loss=0.08954363316297531                                      \n",
      "epoch 58 [30.53s]:  training loss=0.08891132473945618                                      \n",
      "epoch 59 [30.6s]:  training loss=0.08628875017166138                                       \n",
      "epoch 60 [31.08s]: training loss=0.08517999202013016  validation ndcg@10=0.029259109273585167 [0.67s]\n",
      "epoch 61 [30.39s]:  training loss=0.08446592837572098                                      \n",
      "epoch 62 [30.54s]:  training loss=0.08364754915237427                                      \n",
      "epoch 63 [30.31s]:  training loss=0.08078692108392715                                      \n",
      "epoch 64 [31.57s]:  training loss=0.08056426793336868                                      \n",
      "epoch 65 [29.96s]: training loss=0.08184139430522919  validation ndcg@10=0.02995466766376004 [0.72s]\n",
      "epoch 66 [30.49s]:  training loss=0.08197443187236786                                      \n",
      "epoch 67 [30.42s]:  training loss=0.08252628147602081                                      \n",
      "epoch 68 [30.33s]:  training loss=0.07978746294975281                                      \n",
      "epoch 69 [30.58s]:  training loss=0.08113306015729904                                      \n",
      "epoch 70 [30.56s]: training loss=0.07600238919258118  validation ndcg@10=0.0299520948381374 [0.69s]\n",
      "epoch 1 [14.32s]:  training loss=0.823268473148346                                         \n",
      "epoch 2 [14.57s]:  training loss=0.7696991562843323                                        \n",
      "epoch 3 [14.82s]:  training loss=0.7136878371238708                                        \n",
      "epoch 4 [14.51s]:  training loss=0.6440470218658447                                        \n",
      "epoch 5 [14.67s]: training loss=0.588680624961853  validation ndcg@10=0.010291337140863811 [0.48s]\n",
      "epoch 6 [14.87s]:  training loss=0.5558792352676392                                        \n",
      "epoch 7 [14.26s]:  training loss=0.5237089991569519                                        \n",
      "epoch 8 [14.5s]:  training loss=0.4922176003456116                                         \n",
      "epoch 9 [14.76s]:  training loss=0.45768001675605774                                       \n",
      "epoch 10 [14.35s]: training loss=0.43893998861312866  validation ndcg@10=0.017587665205165318 [0.49s]\n",
      "epoch 11 [14.79s]:  training loss=0.4166959822177887                                       \n",
      "epoch 12 [14.65s]:  training loss=0.3998803496360779                                       \n",
      "epoch 13 [14.86s]:  training loss=0.38464534282684326                                      \n",
      "epoch 14 [14.8s]:  training loss=0.371682345867157                                         \n",
      "epoch 15 [14.84s]: training loss=0.3531191647052765  validation ndcg@10=0.022485023593000256 [0.49s]\n",
      "epoch 16 [14.25s]:  training loss=0.34457287192344666                                      \n",
      "epoch 17 [14.54s]:  training loss=0.3299206793308258                                       \n",
      "epoch 18 [14.4s]:  training loss=0.31968557834625244                                       \n",
      "epoch 19 [14.4s]:  training loss=0.30953705310821533                                       \n",
      "epoch 20 [14.58s]: training loss=0.3095453083515167  validation ndcg@10=0.023678553686450405 [0.46s]\n",
      "epoch 21 [14.81s]:  training loss=0.30227935314178467                                      \n",
      "epoch 22 [14.38s]:  training loss=0.2893865406513214                                       \n",
      "epoch 23 [15.26s]:  training loss=0.2836405336856842                                       \n",
      "epoch 24 [14.23s]:  training loss=0.27766692638397217                                      \n",
      "epoch 25 [14.45s]: training loss=0.27044448256492615  validation ndcg@10=0.02456367616236668 [0.45s]\n",
      "epoch 26 [14.17s]:  training loss=0.26273229718208313                                      \n",
      "epoch 27 [14.04s]:  training loss=0.25512510538101196                                      \n",
      "epoch 28 [14.33s]:  training loss=0.25355517864227295                                      \n",
      "epoch 29 [14.31s]:  training loss=0.2501737177371979                                       \n",
      "epoch 30 [14.29s]: training loss=0.24401697516441345  validation ndcg@10=0.026016646311758505 [0.5s]\n",
      "epoch 31 [14.69s]:  training loss=0.2405904233455658                                       \n",
      "epoch 32 [14.37s]:  training loss=0.2361399233341217                                       \n",
      "epoch 33 [14.24s]:  training loss=0.23482438921928406                                      \n",
      "epoch 34 [14.4s]:  training loss=0.22765037417411804                                       \n",
      "epoch 35 [14.36s]: training loss=0.22511737048625946  validation ndcg@10=0.02679427329819046 [0.47s]\n",
      "epoch 36 [14.74s]:  training loss=0.22152750194072723                                      \n",
      "epoch 37 [14.4s]:  training loss=0.21470944583415985                                       \n",
      "epoch 38 [14.3s]:  training loss=0.21656382083892822                                       \n",
      "epoch 39 [14.19s]:  training loss=0.2089826911687851                                       \n",
      "epoch 40 [14.27s]: training loss=0.2080903947353363  validation ndcg@10=0.02682179409995499 [0.49s]\n",
      "epoch 41 [14.36s]:  training loss=0.20543299615383148                                      \n",
      "epoch 42 [14.46s]:  training loss=0.200827494263649                                        \n",
      "epoch 43 [14.07s]:  training loss=0.2026401311159134                                       \n",
      "epoch 44 [14.61s]:  training loss=0.19667333364486694                                      \n",
      "epoch 45 [14.17s]: training loss=0.19453871250152588  validation ndcg@10=0.02786900972447234 [0.48s]\n",
      "epoch 46 [14.29s]:  training loss=0.18801438808441162                                      \n",
      "epoch 47 [14.43s]:  training loss=0.18762119114398956                                      \n",
      "epoch 48 [14.36s]:  training loss=0.18532297015190125                                      \n",
      "epoch 49 [14.52s]:  training loss=0.18476398289203644                                      \n",
      "epoch 50 [14.57s]: training loss=0.18153609335422516  validation ndcg@10=0.02833183757103094 [0.45s]\n",
      "epoch 51 [14.69s]:  training loss=0.17905567586421967                                      \n",
      "epoch 52 [15.15s]:  training loss=0.18122410774230957                                      \n",
      "epoch 53 [14.16s]:  training loss=0.17695339024066925                                      \n",
      "epoch 54 [14.46s]:  training loss=0.17689435184001923                                      \n",
      "epoch 55 [14.42s]: training loss=0.17171508073806763  validation ndcg@10=0.02929232421353959 [0.49s]\n",
      "epoch 56 [14.45s]:  training loss=0.1722458451986313                                       \n",
      "epoch 57 [14.36s]:  training loss=0.16856235265731812                                      \n",
      "epoch 58 [14.35s]:  training loss=0.16515091061592102                                      \n",
      "epoch 59 [14.68s]:  training loss=0.16498631238937378                                      \n",
      "epoch 60 [15.51s]: training loss=0.16270646452903748  validation ndcg@10=0.029145348289567826 [0.48s]\n",
      "epoch 61 [14.65s]:  training loss=0.16106142103672028                                      \n",
      "epoch 62 [14.26s]:  training loss=0.1630132645368576                                       \n",
      "epoch 63 [14.82s]:  training loss=0.1571066975593567                                       \n",
      "epoch 64 [14.51s]:  training loss=0.15854203701019287                                      \n",
      "epoch 65 [14.04s]: training loss=0.15626132488250732  validation ndcg@10=0.02961662438599371 [0.45s]\n",
      "epoch 66 [14.33s]:  training loss=0.15646955370903015                                      \n",
      "epoch 67 [14.37s]:  training loss=0.15152639150619507                                      \n",
      "epoch 68 [13.99s]:  training loss=0.15228012204170227                                      \n",
      "epoch 69 [14.45s]:  training loss=0.14734110236167908                                      \n",
      "epoch 70 [14.28s]: training loss=0.14822104573249817  validation ndcg@10=0.030036930033109307 [0.45s]\n",
      "epoch 71 [14.55s]:  training loss=0.1483052372932434                                       \n",
      "epoch 72 [14.72s]:  training loss=0.14725720882415771                                      \n",
      "epoch 73 [14.53s]:  training loss=0.14749301970005035                                      \n",
      "epoch 74 [14.26s]:  training loss=0.14095358550548553                                      \n",
      "epoch 75 [14.56s]: training loss=0.1423162817955017  validation ndcg@10=0.029963576589386573 [0.47s]\n",
      "epoch 76 [14.32s]:  training loss=0.14256148040294647                                      \n",
      "epoch 77 [14.02s]:  training loss=0.14164157211780548                                      \n",
      "epoch 78 [14.23s]:  training loss=0.14147524535655975                                      \n",
      "epoch 79 [14.53s]:  training loss=0.13942331075668335                                      \n",
      "epoch 80 [14.68s]: training loss=0.13657507300376892  validation ndcg@10=0.03010368269583523 [0.49s]\n",
      "epoch 81 [14.22s]:  training loss=0.13797366619110107                                      \n",
      "epoch 82 [14.05s]:  training loss=0.13806580007076263                                      \n",
      "epoch 83 [14.16s]:  training loss=0.13467487692832947                                      \n",
      "epoch 84 [14.02s]:  training loss=0.1328764259815216                                       \n",
      "epoch 85 [14.05s]: training loss=0.13053004443645477  validation ndcg@10=0.030120944556347465 [0.51s]\n",
      "epoch 86 [14.21s]:  training loss=0.13169293105602264                                      \n",
      "epoch 87 [13.88s]:  training loss=0.13483291864395142                                      \n",
      "epoch 88 [14.27s]:  training loss=0.130707249045372                                        \n",
      "epoch 89 [15.14s]:  training loss=0.12756329774856567                                      \n",
      "epoch 90 [14.82s]: training loss=0.12796060740947723  validation ndcg@10=0.030384017668934643 [0.47s]\n",
      "epoch 91 [14.24s]:  training loss=0.1260988712310791                                       \n",
      "epoch 92 [14.1s]:  training loss=0.1266520917415619                                        \n",
      "epoch 93 [14.07s]:  training loss=0.12601600587368011                                      \n",
      "epoch 94 [14.17s]:  training loss=0.12499309331178665                                      \n",
      "epoch 95 [14.38s]: training loss=0.12288534641265869  validation ndcg@10=0.03069734752616433 [0.43s]\n",
      "epoch 96 [14.2s]:  training loss=0.12273658812046051                                       \n",
      "epoch 97 [15.37s]:  training loss=0.12010911107063293                                      \n",
      "epoch 98 [14.2s]:  training loss=0.12325398623943329                                       \n",
      "epoch 99 [14.17s]:  training loss=0.12099716812372208                                      \n",
      "epoch 100 [14.18s]: training loss=0.11893769353628159  validation ndcg@10=0.03011479212797907 [0.44s]\n",
      "epoch 101 [14.16s]:  training loss=0.1227455660700798                                      \n",
      "epoch 102 [14.18s]:  training loss=0.11585258692502975                                     \n",
      "epoch 103 [14.23s]:  training loss=0.11927518248558044                                     \n",
      "epoch 104 [14.36s]:  training loss=0.11582464724779129                                     \n",
      "epoch 105 [14.1s]: training loss=0.11436482518911362  validation ndcg@10=0.03027511431536174 [0.42s]\n",
      "epoch 106 [13.91s]:  training loss=0.11569280922412872                                     \n",
      "epoch 107 [14.3s]:  training loss=0.11628350615501404                                      \n",
      "epoch 108 [13.81s]:  training loss=0.11402163654565811                                     \n",
      "epoch 109 [14.12s]:  training loss=0.11219225823879242                                     \n",
      "epoch 110 [14.33s]: training loss=0.11072234064340591  validation ndcg@10=0.0302954220358294 [0.48s]\n",
      "epoch 111 [14.29s]:  training loss=0.11101380735635757                                     \n",
      "epoch 112 [14.08s]:  training loss=0.11295950412750244                                     \n",
      "epoch 113 [14.31s]:  training loss=0.10908380895853043                                     \n",
      "epoch 114 [14.44s]:  training loss=0.10770097374916077                                     \n",
      "epoch 115 [14.52s]: training loss=0.10845106840133667  validation ndcg@10=0.030466190301748765 [0.47s]\n",
      "epoch 116 [14.58s]:  training loss=0.1072559580206871                                      \n",
      "epoch 117 [14.05s]:  training loss=0.10820309817790985                                     \n",
      "epoch 118 [14.45s]:  training loss=0.10613908618688583                                     \n",
      "epoch 119 [14.25s]:  training loss=0.10739857703447342                                     \n",
      "epoch 120 [14.42s]: training loss=0.10657495260238647  validation ndcg@10=0.030345398060065924 [0.47s]\n",
      "epoch 1 [21.22s]:  training loss=0.8432013988494873                                        \n",
      "epoch 2 [21.23s]:  training loss=0.8088175058364868                                        \n",
      "epoch 3 [20.41s]:  training loss=0.7885708212852478                                        \n",
      "epoch 4 [20.37s]:  training loss=0.7678714990615845                                        \n",
      "epoch 5 [21.18s]: training loss=0.7406302690505981  validation ndcg@10=0.011935829282640394 [0.58s]\n",
      "epoch 6 [20.89s]:  training loss=0.7143076062202454                                        \n",
      "epoch 7 [20.35s]:  training loss=0.679914116859436                                         \n",
      "epoch 8 [20.15s]:  training loss=0.65206378698349                                          \n",
      "epoch 9 [20.49s]:  training loss=0.624976634979248                                         \n",
      "epoch 10 [21.0s]: training loss=0.603420078754425  validation ndcg@10=0.010767522066713543 [0.63s]\n",
      "epoch 11 [20.43s]:  training loss=0.5852087140083313                                       \n",
      "epoch 12 [19.62s]:  training loss=0.5649042725563049                                       \n",
      "epoch 13 [19.61s]:  training loss=0.5459285378456116                                       \n",
      "epoch 14 [19.65s]:  training loss=0.5313217043876648                                       \n",
      "epoch 15 [19.69s]: training loss=0.5098041892051697  validation ndcg@10=0.014753057334976288 [0.62s]\n",
      "epoch 16 [19.84s]:  training loss=0.4948907792568207                                       \n",
      "epoch 17 [19.6s]:  training loss=0.48008009791374207                                       \n",
      "epoch 18 [20.06s]:  training loss=0.46612146496772766                                      \n",
      "epoch 19 [19.29s]:  training loss=0.4541272222995758                                       \n",
      "epoch 20 [19.61s]: training loss=0.44173192977905273  validation ndcg@10=0.018276317307910118 [0.58s]\n",
      "epoch 21 [20.29s]:  training loss=0.4358517825603485                                       \n",
      "epoch 22 [19.88s]:  training loss=0.42062899470329285                                      \n",
      "epoch 23 [19.81s]:  training loss=0.4112147092819214                                       \n",
      "epoch 24 [19.77s]:  training loss=0.40366432070732117                                      \n",
      "epoch 25 [20.37s]: training loss=0.39826565980911255  validation ndcg@10=0.020869207079755932 [0.58s]\n",
      "epoch 26 [20.14s]:  training loss=0.3949359655380249                                       \n",
      "epoch 27 [20.13s]:  training loss=0.3830792307853699                                       \n",
      "epoch 28 [20.01s]:  training loss=0.3778213858604431                                       \n",
      "epoch 29 [20.6s]:  training loss=0.3706599771976471                                        \n",
      "epoch 30 [20.51s]: training loss=0.3583560883998871  validation ndcg@10=0.023220955565646694 [0.54s]\n",
      "epoch 31 [19.99s]:  training loss=0.3521421551704407                                       \n",
      "epoch 32 [20.55s]:  training loss=0.3449818789958954                                       \n",
      "epoch 33 [20.19s]:  training loss=0.3389464318752289                                       \n",
      "epoch 34 [19.88s]:  training loss=0.3367459774017334                                       \n",
      "epoch 35 [19.67s]: training loss=0.331209659576416  validation ndcg@10=0.0238715500396608 [0.54s]\n",
      "epoch 36 [19.61s]:  training loss=0.3306296169757843                                       \n",
      "epoch 37 [18.22s]:  training loss=0.3207384943962097                                       \n",
      "epoch 38 [18.34s]:  training loss=0.3158913552761078                                       \n",
      "epoch 39 [18.5s]:  training loss=0.3117385506629944                                        \n",
      "epoch 40 [18.51s]: training loss=0.3087711036205292  validation ndcg@10=0.024156615400689557 [0.55s]\n",
      "epoch 41 [18.4s]:  training loss=0.3037462532520294                                        \n",
      "epoch 42 [18.32s]:  training loss=0.298160582780838                                        \n",
      "epoch 43 [18.11s]:  training loss=0.29467836022377014                                      \n",
      "epoch 44 [18.62s]:  training loss=0.29331403970718384                                      \n",
      "epoch 45 [18.6s]: training loss=0.29260528087615967  validation ndcg@10=0.024888968264205797 [0.54s]\n",
      "epoch 46 [18.29s]:  training loss=0.2830085754394531                                       \n",
      "epoch 47 [18.18s]:  training loss=0.27904629707336426                                      \n",
      "epoch 48 [19.14s]:  training loss=0.2793617844581604                                       \n",
      "epoch 49 [18.26s]:  training loss=0.27680492401123047                                      \n",
      "epoch 50 [18.38s]: training loss=0.2714192271232605  validation ndcg@10=0.025186613849640008 [0.61s]\n",
      "epoch 51 [18.47s]:  training loss=0.27075573801994324                                      \n",
      "epoch 52 [18.43s]:  training loss=0.26737678050994873                                      \n",
      "epoch 53 [18.55s]:  training loss=0.26422420144081116                                      \n",
      "epoch 54 [18.48s]:  training loss=0.2647435665130615                                       \n",
      "epoch 55 [18.41s]: training loss=0.25798967480659485  validation ndcg@10=0.026182166933401498 [0.56s]\n",
      "epoch 56 [18.46s]:  training loss=0.2555569112300873                                       \n",
      "epoch 57 [18.64s]:  training loss=0.25525447726249695                                      \n",
      "epoch 58 [18.16s]:  training loss=0.25355616211891174                                      \n",
      "epoch 59 [18.82s]:  training loss=0.2501457929611206                                       \n",
      "epoch 60 [18.28s]: training loss=0.24712258577346802  validation ndcg@10=0.02682578825366963 [0.57s]\n",
      "epoch 61 [19.07s]:  training loss=0.2415827214717865                                       \n",
      "epoch 62 [19.73s]:  training loss=0.24075007438659668                                      \n",
      "epoch 63 [18.75s]:  training loss=0.23864395916461945                                      \n",
      "epoch 64 [18.47s]:  training loss=0.24148471653461456                                      \n",
      "epoch 65 [18.48s]: training loss=0.2353416234254837  validation ndcg@10=0.027307326917214703 [0.54s]\n",
      "epoch 66 [18.53s]:  training loss=0.23237591981887817                                      \n",
      "epoch 67 [18.51s]:  training loss=0.23405292630195618                                      \n",
      "epoch 68 [18.26s]:  training loss=0.2295035421848297                                       \n",
      "epoch 69 [18.26s]:  training loss=0.23270735144615173                                      \n",
      "epoch 70 [18.12s]: training loss=0.22780464589595795  validation ndcg@10=0.027634125121296815 [0.54s]\n",
      "epoch 71 [18.72s]:  training loss=0.22267931699752808                                      \n",
      "epoch 72 [18.1s]:  training loss=0.22304025292396545                                       \n",
      "epoch 73 [18.11s]:  training loss=0.22248505055904388                                      \n",
      "epoch 74 [18.87s]:  training loss=0.22043846547603607                                      \n",
      "epoch 75 [18.83s]: training loss=0.21969084441661835  validation ndcg@10=0.02798974531557797 [0.55s]\n",
      "epoch 76 [18.81s]:  training loss=0.21765179932117462                                      \n",
      "epoch 77 [18.93s]:  training loss=0.21694833040237427                                      \n",
      "epoch 78 [19.07s]:  training loss=0.21487504243850708                                      \n",
      "epoch 79 [18.05s]:  training loss=0.21341626346111298                                      \n",
      "epoch 80 [18.93s]: training loss=0.2122424840927124  validation ndcg@10=0.02773940685111893 [0.53s]\n",
      "epoch 81 [18.2s]:  training loss=0.20846393704414368                                       \n",
      "epoch 82 [18.04s]:  training loss=0.20639990270137787                                      \n",
      "epoch 83 [18.25s]:  training loss=0.20982593297958374                                      \n",
      "epoch 84 [18.4s]:  training loss=0.2063186913728714                                        \n",
      "epoch 85 [18.27s]: training loss=0.20199818909168243  validation ndcg@10=0.02826501492581968 [0.54s]\n",
      "epoch 86 [18.22s]:  training loss=0.20206250250339508                                      \n",
      "epoch 87 [19.78s]:  training loss=0.20172657072544098                                      \n",
      "epoch 88 [18.19s]:  training loss=0.20171858370304108                                      \n",
      "epoch 89 [18.33s]:  training loss=0.2003180980682373                                       \n",
      "epoch 90 [18.58s]: training loss=0.19980022311210632  validation ndcg@10=0.028316771461296613 [0.56s]\n",
      "epoch 91 [18.95s]:  training loss=0.19542010128498077                                      \n",
      "epoch 92 [18.09s]:  training loss=0.1932433396577835                                       \n",
      "epoch 93 [18.3s]:  training loss=0.19435209035873413                                       \n",
      "epoch 94 [18.25s]:  training loss=0.1920662820339203                                       \n",
      "epoch 95 [18.59s]: training loss=0.19282305240631104  validation ndcg@10=0.028249646622258616 [0.54s]\n",
      "epoch 96 [18.15s]:  training loss=0.18962453305721283                                      \n",
      "epoch 97 [18.14s]:  training loss=0.19084033370018005                                      \n",
      "epoch 98 [18.5s]:  training loss=0.18955951929092407                                       \n",
      "epoch 99 [18.63s]:  training loss=0.19036684930324554                                      \n",
      "epoch 100 [18.66s]: training loss=0.18603505194187164  validation ndcg@10=0.028499910542015693 [0.54s]\n",
      "epoch 101 [18.5s]:  training loss=0.18607065081596375                                      \n",
      "epoch 102 [18.83s]:  training loss=0.18555133044719696                                     \n",
      "epoch 103 [18.54s]:  training loss=0.18089929223060608                                     \n",
      "epoch 104 [18.6s]:  training loss=0.18264521658420563                                      \n",
      "epoch 105 [18.78s]: training loss=0.17847707867622375  validation ndcg@10=0.028914786999827226 [0.54s]\n",
      "epoch 106 [18.24s]:  training loss=0.17955973744392395                                     \n",
      "epoch 107 [18.09s]:  training loss=0.1800624281167984                                      \n",
      "epoch 108 [18.42s]:  training loss=0.17762987315654755                                     \n",
      "epoch 109 [18.42s]:  training loss=0.17193202674388885                                     \n",
      "epoch 110 [18.45s]: training loss=0.17532400786876678  validation ndcg@10=0.028768888458461827 [0.54s]\n",
      "epoch 111 [18.48s]:  training loss=0.17789500951766968                                     \n",
      "epoch 112 [19.59s]:  training loss=0.1783851981163025                                      \n",
      "epoch 113 [18.34s]:  training loss=0.1767660230398178                                      \n",
      "epoch 114 [18.14s]:  training loss=0.17037928104400635                                     \n",
      "epoch 115 [18.52s]: training loss=0.17182518541812897  validation ndcg@10=0.029294551186872998 [0.54s]\n",
      "epoch 116 [18.79s]:  training loss=0.17313911020755768                                     \n",
      "epoch 117 [18.75s]:  training loss=0.17300912737846375                                     \n",
      "epoch 118 [18.77s]:  training loss=0.17113113403320312                                     \n",
      "epoch 119 [19.09s]:  training loss=0.16640540957450867                                     \n",
      "epoch 120 [19.37s]: training loss=0.16314697265625  validation ndcg@10=0.02931563450463581 [0.56s]\n",
      "epoch 121 [18.85s]:  training loss=0.16975848376750946                                     \n",
      "epoch 122 [18.8s]:  training loss=0.16806896030902863                                      \n",
      "epoch 123 [18.5s]:  training loss=0.16660477221012115                                      \n",
      "epoch 124 [18.51s]:  training loss=0.1630810797214508                                      \n",
      "epoch 125 [18.82s]: training loss=0.1612750142812729  validation ndcg@10=0.029645532359407187 [0.54s]\n",
      "epoch 126 [18.31s]:  training loss=0.16397255659103394                                     \n",
      "epoch 127 [18.59s]:  training loss=0.15931016206741333                                     \n",
      "epoch 128 [18.67s]:  training loss=0.1625053435564041                                      \n",
      "epoch 129 [18.01s]:  training loss=0.1627480685710907                                      \n",
      "epoch 130 [18.6s]: training loss=0.1602284163236618  validation ndcg@10=0.030236670440540042 [0.57s]\n",
      "epoch 131 [18.65s]:  training loss=0.15512320399284363                                     \n",
      "epoch 132 [18.54s]:  training loss=0.15864376723766327                                     \n",
      "epoch 133 [18.42s]:  training loss=0.15571872889995575                                     \n",
      "epoch 134 [18.63s]:  training loss=0.15778054296970367                                     \n",
      "epoch 135 [18.59s]: training loss=0.15432466566562653  validation ndcg@10=0.029790461965972386 [0.55s]\n",
      "epoch 136 [19.79s]:  training loss=0.15318962931632996                                     \n",
      "epoch 137 [18.68s]:  training loss=0.153233602643013                                       \n",
      "epoch 138 [18.73s]:  training loss=0.1505165547132492                                      \n",
      "epoch 139 [18.56s]:  training loss=0.15314863622188568                                     \n",
      "epoch 140 [18.42s]: training loss=0.15236403048038483  validation ndcg@10=0.029678917823354366 [0.53s]\n",
      "epoch 141 [18.33s]:  training loss=0.15311846137046814                                     \n",
      "epoch 142 [18.54s]:  training loss=0.15126124024391174                                     \n",
      "epoch 143 [18.17s]:  training loss=0.14892327785491943                                     \n",
      "epoch 144 [18.32s]:  training loss=0.1488148421049118                                      \n",
      "epoch 145 [18.6s]: training loss=0.14955227077007294  validation ndcg@10=0.02991126487127031 [0.53s]\n",
      "epoch 146 [19.1s]:  training loss=0.14872238039970398                                      \n",
      "epoch 147 [18.48s]:  training loss=0.14723044633865356                                     \n",
      "epoch 148 [18.96s]:  training loss=0.14517702162265778                                     \n",
      "epoch 149 [18.97s]:  training loss=0.15033400058746338                                     \n",
      "epoch 150 [18.47s]: training loss=0.14684246480464935  validation ndcg@10=0.030132844985235566 [0.55s]\n",
      "epoch 151 [18.4s]:  training loss=0.1422879993915558                                       \n",
      "epoch 152 [18.2s]:  training loss=0.14549975097179413                                      \n",
      "epoch 153 [18.5s]:  training loss=0.14646846055984497                                      \n",
      "epoch 154 [18.37s]:  training loss=0.14372092485427856                                     \n",
      "epoch 155 [18.26s]: training loss=0.14233483374118805  validation ndcg@10=0.029653849664115873 [0.55s]\n",
      "epoch 1 [22.3s]:  training loss=0.5847914218902588                                         \n",
      "epoch 2 [22.56s]:  training loss=0.340781033039093                                         \n",
      "epoch 3 [22.22s]:  training loss=0.25759246945381165                                       \n",
      "epoch 4 [23.9s]:  training loss=0.2152215987443924                                         \n",
      "epoch 5 [22.66s]: training loss=0.18804773688316345  validation ndcg@10=0.027019303987909617 [0.59s]\n",
      "epoch 6 [22.18s]:  training loss=0.1705787181854248                                        \n",
      "epoch 7 [21.82s]:  training loss=0.15390309691429138                                       \n",
      "epoch 8 [21.83s]:  training loss=0.14258217811584473                                       \n",
      "epoch 9 [22.29s]:  training loss=0.13153044879436493                                       \n",
      "epoch 10 [21.81s]: training loss=0.12551501393318176  validation ndcg@10=0.028604831543526506 [0.58s]\n",
      "epoch 11 [22.36s]:  training loss=0.11652181297540665                                      \n",
      "epoch 12 [22.29s]:  training loss=0.10866393893957138                                      \n",
      "epoch 13 [22.36s]:  training loss=0.10366608202457428                                      \n",
      "epoch 14 [22.35s]:  training loss=0.09862566739320755                                      \n",
      "epoch 15 [21.91s]: training loss=0.09605340659618378  validation ndcg@10=0.029065054180176166 [0.58s]\n",
      "epoch 16 [22.16s]:  training loss=0.09078182280063629                                      \n",
      "epoch 17 [22.1s]:  training loss=0.08677049726247787                                       \n",
      "epoch 18 [22.19s]:  training loss=0.08364421129226685                                      \n",
      "epoch 19 [22.89s]:  training loss=0.08036019653081894                                      \n",
      "epoch 20 [22.1s]: training loss=0.0791907086968422  validation ndcg@10=0.027897654221556805 [0.57s]\n",
      "epoch 21 [22.03s]:  training loss=0.0769953578710556                                       \n",
      "epoch 22 [22.09s]:  training loss=0.07383829355239868                                      \n",
      "epoch 23 [22.08s]:  training loss=0.07107176631689072                                      \n",
      "epoch 24 [23.32s]:  training loss=0.06685099750757217                                      \n",
      "epoch 25 [21.81s]: training loss=0.06994113326072693  validation ndcg@10=0.0278899033706849 [0.59s]\n",
      "epoch 26 [22.37s]:  training loss=0.06705610454082489                                      \n",
      "epoch 27 [22.18s]:  training loss=0.06450239568948746                                      \n",
      "epoch 28 [22.7s]:  training loss=0.06395991891622543                                       \n",
      "epoch 29 [22.41s]:  training loss=0.060317687690258026                                     \n",
      "epoch 30 [22.59s]: training loss=0.05799439176917076  validation ndcg@10=0.026694265684468334 [0.59s]\n",
      "epoch 31 [22.62s]:  training loss=0.058262430131435394                                     \n",
      "epoch 32 [22.3s]:  training loss=0.055090948939323425                                      \n",
      "epoch 33 [22.31s]:  training loss=0.05453414469957352                                      \n",
      "epoch 34 [22.49s]:  training loss=0.05662228912115097                                      \n",
      "epoch 35 [22.45s]: training loss=0.05480851233005524  validation ndcg@10=0.027461703650237534 [0.58s]\n",
      "epoch 36 [21.64s]:  training loss=0.0534236840903759                                       \n",
      "epoch 37 [22.07s]:  training loss=0.053780216723680496                                     \n",
      "epoch 38 [22.22s]:  training loss=0.051468927413225174                                     \n",
      "epoch 39 [22.04s]:  training loss=0.05158436298370361                                      \n",
      "epoch 40 [21.89s]: training loss=0.04960170388221741  validation ndcg@10=0.026750490129002668 [0.59s]\n",
      "epoch 1 [21.48s]:  training loss=0.8007700443267822                                        \n",
      "epoch 2 [23.13s]:  training loss=0.7415372729301453                                       \n",
      "epoch 3 [25.45s]:  training loss=0.6554996371269226                                       \n",
      "epoch 4 [22.87s]:  training loss=0.5896552205085754                                       \n",
      "epoch 5 [22.15s]: training loss=0.5466166138648987  validation ndcg@10=0.010619612667018835 [0.58s]\n",
      "epoch 6 [21.91s]:  training loss=0.4940975606441498                                       \n",
      "epoch 7 [21.05s]:  training loss=0.4561150074005127                                       \n",
      "epoch 8 [21.24s]:  training loss=0.4258454740047455                                       \n",
      "epoch 9 [21.27s]:  training loss=0.39915600419044495                                      \n",
      "epoch 10 [20.93s]: training loss=0.3811337947845459  validation ndcg@10=0.02066850080697583 [0.57s]\n",
      "epoch 11 [21.13s]:  training loss=0.3622645139694214                                      \n",
      "epoch 12 [21.04s]:  training loss=0.3423895835876465                                      \n",
      "epoch 13 [21.02s]:  training loss=0.32723891735076904                                     \n",
      "epoch 14 [21.18s]:  training loss=0.30909454822540283                                     \n",
      "epoch 15 [21.3s]: training loss=0.2975313067436218  validation ndcg@10=0.02381177839154509 [0.58s]\n",
      "epoch 16 [21.15s]:  training loss=0.2873392105102539                                      \n",
      "epoch 17 [21.08s]:  training loss=0.2738800644874573                                      \n",
      "epoch 18 [20.89s]:  training loss=0.27046075463294983                                     \n",
      "epoch 19 [21.09s]:  training loss=0.25993502140045166                                     \n",
      "epoch 20 [21.02s]: training loss=0.25230035185813904  validation ndcg@10=0.025417022697895617 [0.55s]\n",
      "epoch 21 [21.02s]:  training loss=0.24544554948806763                                     \n",
      "epoch 22 [21.0s]:  training loss=0.2406434416770935                                       \n",
      "epoch 23 [21.08s]:  training loss=0.23318003118038177                                     \n",
      "epoch 24 [22.31s]:  training loss=0.22774924337863922                                     \n",
      "epoch 25 [20.96s]: training loss=0.2230224460363388  validation ndcg@10=0.026366256181679135 [0.64s]\n",
      "epoch 26 [20.99s]:  training loss=0.2169346809387207                                      \n",
      "epoch 27 [21.01s]:  training loss=0.21305277943611145                                     \n",
      "epoch 28 [20.92s]:  training loss=0.21247537434101105                                     \n",
      "epoch 29 [21.35s]:  training loss=0.20754477381706238                                     \n",
      "epoch 30 [20.93s]: training loss=0.2012026309967041  validation ndcg@10=0.02780758496047485 [0.56s]\n",
      "epoch 31 [21.69s]:  training loss=0.19653040170669556                                     \n",
      "epoch 32 [21.26s]:  training loss=0.19064147770404816                                     \n",
      "epoch 33 [20.98s]:  training loss=0.18892937898635864                                     \n",
      "epoch 34 [21.02s]:  training loss=0.1883474886417389                                      \n",
      "epoch 35 [21.01s]: training loss=0.18766070902347565  validation ndcg@10=0.027777539982093143 [0.55s]\n",
      "epoch 36 [21.03s]:  training loss=0.18009673058986664                                     \n",
      "epoch 37 [21.08s]:  training loss=0.1793328821659088                                      \n",
      "epoch 38 [20.93s]:  training loss=0.17564882338047028                                     \n",
      "epoch 39 [20.91s]:  training loss=0.17569392919540405                                     \n",
      "epoch 40 [21.02s]: training loss=0.1717153638601303  validation ndcg@10=0.028480298142674082 [0.57s]\n",
      "epoch 41 [21.25s]:  training loss=0.16661988198757172                                     \n",
      "epoch 42 [21.12s]:  training loss=0.1677536815404892                                      \n",
      "epoch 43 [21.25s]:  training loss=0.16212762892246246                                     \n",
      "epoch 44 [20.99s]:  training loss=0.16076533496379852                                     \n",
      "epoch 45 [22.45s]: training loss=0.16176295280456543  validation ndcg@10=0.028735597932695197 [0.59s]\n",
      "epoch 46 [21.42s]:  training loss=0.15646295249462128                                     \n",
      "epoch 47 [21.04s]:  training loss=0.15696129202842712                                     \n",
      "epoch 48 [20.99s]:  training loss=0.15287452936172485                                     \n",
      "epoch 49 [21.01s]:  training loss=0.1502349078655243                                      \n",
      "epoch 50 [21.15s]: training loss=0.15326613187789917  validation ndcg@10=0.02980462946317677 [0.56s]\n",
      "epoch 51 [20.96s]:  training loss=0.14932288229465485                                     \n",
      "epoch 52 [20.88s]:  training loss=0.14762787520885468                                     \n",
      "epoch 53 [20.9s]:  training loss=0.1418062299489975                                       \n",
      "epoch 54 [21.05s]:  training loss=0.14065302908420563                                     \n",
      "epoch 55 [20.91s]: training loss=0.14302381873130798  validation ndcg@10=0.029559449102826412 [0.56s]\n",
      "epoch 56 [21.04s]:  training loss=0.13911712169647217                                     \n",
      "epoch 57 [21.55s]:  training loss=0.13816332817077637                                     \n",
      "epoch 58 [21.07s]:  training loss=0.13696913421154022                                     \n",
      "epoch 59 [20.94s]:  training loss=0.13266003131866455                                     \n",
      "epoch 60 [20.97s]: training loss=0.13610278069972992  validation ndcg@10=0.02960050480585479 [0.55s]\n",
      "epoch 61 [20.92s]:  training loss=0.1328619122505188                                      \n",
      "epoch 62 [20.9s]:  training loss=0.13204340636730194                                      \n",
      "epoch 63 [21.08s]:  training loss=0.13088838756084442                                     \n",
      "epoch 64 [21.1s]:  training loss=0.12768220901489258                                      \n",
      "epoch 65 [20.92s]: training loss=0.1280430555343628  validation ndcg@10=0.02986027722125543 [0.55s]\n",
      "epoch 66 [20.82s]:  training loss=0.1260373741388321                                      \n",
      "epoch 67 [22.48s]:  training loss=0.12818126380443573                                     \n",
      "epoch 68 [20.97s]:  training loss=0.12538708746433258                                     \n",
      "epoch 69 [21.05s]:  training loss=0.12229994684457779                                     \n",
      "epoch 70 [21.09s]: training loss=0.12164952605962753  validation ndcg@10=0.03033357963141197 [0.56s]\n",
      "epoch 71 [21.1s]:  training loss=0.1224314421415329                                       \n",
      "epoch 72 [20.98s]:  training loss=0.12025121599435806                                     \n",
      "epoch 73 [21.08s]:  training loss=0.11901659518480301                                     \n",
      "epoch 74 [20.92s]:  training loss=0.11854087561368942                                     \n",
      "epoch 75 [20.91s]: training loss=0.11283354461193085  validation ndcg@10=0.030097486546102503 [0.61s]\n",
      "epoch 76 [21.1s]:  training loss=0.11359766125679016                                      \n",
      "epoch 77 [21.02s]:  training loss=0.11202356219291687                                     \n",
      "epoch 78 [20.95s]:  training loss=0.1127268373966217                                      \n",
      "epoch 79 [21.12s]:  training loss=0.11142226308584213                                     \n",
      "epoch 80 [21.06s]: training loss=0.10904457420110703  validation ndcg@10=0.030135853007153324 [0.55s]\n",
      "epoch 81 [21.09s]:  training loss=0.11185972392559052                                     \n",
      "epoch 82 [21.07s]:  training loss=0.10934693366289139                                     \n",
      "epoch 83 [21.67s]:  training loss=0.10624726861715317                                     \n",
      "epoch 84 [21.22s]:  training loss=0.10839862376451492                                     \n",
      "epoch 85 [21.12s]: training loss=0.10610537230968475  validation ndcg@10=0.03056828489218066 [0.56s]\n",
      "epoch 86 [21.01s]:  training loss=0.10270434617996216                                     \n",
      "epoch 87 [21.01s]:  training loss=0.10730630904436111                                     \n",
      "epoch 88 [21.21s]:  training loss=0.10649175941944122                                     \n",
      "epoch 89 [22.42s]:  training loss=0.1031556949019432                                      \n",
      "epoch 90 [21.1s]: training loss=0.10210607945919037  validation ndcg@10=0.030711240177779063 [0.55s]\n",
      "epoch 91 [21.11s]:  training loss=0.10455348342657089                                     \n",
      "epoch 92 [21.02s]:  training loss=0.10199253261089325                                     \n",
      "epoch 93 [21.01s]:  training loss=0.09953464567661285                                     \n",
      "epoch 94 [21.18s]:  training loss=0.10150120407342911                                     \n",
      "epoch 95 [21.13s]: training loss=0.09825539588928223  validation ndcg@10=0.03055882313944 [0.56s]\n",
      "epoch 96 [20.87s]:  training loss=0.09746002405881882                                     \n",
      "epoch 97 [21.39s]:  training loss=0.09599719941616058                                     \n",
      "epoch 98 [21.15s]:  training loss=0.09631679952144623                                     \n",
      "epoch 99 [21.06s]:  training loss=0.09698436409235                                        \n",
      "epoch 100 [21.05s]: training loss=0.0968712717294693  validation ndcg@10=0.030517180659411863 [0.57s]\n",
      "epoch 101 [21.15s]:  training loss=0.09520217031240463                                    \n",
      "epoch 102 [21.03s]:  training loss=0.09469617903232574                                    \n",
      "epoch 103 [21.3s]:  training loss=0.09385630488395691                                     \n",
      "epoch 104 [20.98s]:  training loss=0.09452489018440247                                    \n",
      "epoch 105 [21.14s]: training loss=0.09402784705162048  validation ndcg@10=0.03046202358142238 [0.6s]\n",
      "epoch 106 [20.98s]:  training loss=0.09257347881793976                                    \n",
      "epoch 107 [21.15s]:  training loss=0.0897996574640274                                     \n",
      "epoch 108 [21.56s]:  training loss=0.09086520224809647                                    \n",
      "epoch 109 [21.38s]:  training loss=0.08889990299940109                                    \n",
      "epoch 110 [22.32s]: training loss=0.0907180905342102  validation ndcg@10=0.03028489121173572 [0.55s]\n",
      "epoch 111 [21.07s]:  training loss=0.09160985797643661                                    \n",
      "epoch 112 [20.97s]:  training loss=0.086878202855587                                      \n",
      "epoch 113 [21.08s]:  training loss=0.09014897793531418                                    \n",
      "epoch 114 [21.0s]:  training loss=0.08844123780727386                                     \n",
      "epoch 115 [21.03s]: training loss=0.08516573905944824  validation ndcg@10=0.03049165168215802 [0.55s]\n",
      "epoch 1 [15.78s]:  training loss=0.7538820505142212                                       \n",
      "epoch 2 [15.81s]:  training loss=0.5802649855613708                                       \n",
      "epoch 3 [16.08s]:  training loss=0.4781491160392761                                       \n",
      "epoch 4 [15.95s]:  training loss=0.40170419216156006                                      \n",
      "epoch 5 [16.99s]: training loss=0.3552168309688568  validation ndcg@10=0.022526241338002927 [0.49s]\n",
      "epoch 6 [16.53s]:  training loss=0.3181980848312378                                       \n",
      "epoch 7 [16.39s]:  training loss=0.29342782497406006                                      \n",
      "epoch 8 [16.35s]:  training loss=0.2722049057483673                                       \n",
      "epoch 9 [15.93s]:  training loss=0.2551819980144501                                       \n",
      "epoch 10 [16.34s]: training loss=0.24500350654125214  validation ndcg@10=0.02519987614670289 [0.49s]\n",
      "epoch 11 [16.26s]:  training loss=0.22818118333816528                                     \n",
      "epoch 12 [15.9s]:  training loss=0.21568140387535095                                      \n",
      "epoch 13 [16.27s]:  training loss=0.20950523018836975                                     \n",
      "epoch 14 [16.65s]:  training loss=0.19991539418697357                                     \n",
      "epoch 15 [15.94s]: training loss=0.19238577783107758  validation ndcg@10=0.027372283889461634 [0.47s]\n",
      "epoch 16 [16.31s]:  training loss=0.18479205667972565                                     \n",
      "epoch 17 [16.08s]:  training loss=0.17605279386043549                                     \n",
      "epoch 18 [16.46s]:  training loss=0.17393043637275696                                     \n",
      "epoch 19 [16.34s]:  training loss=0.1657421737909317                                      \n",
      "epoch 20 [17.42s]: training loss=0.1646537333726883  validation ndcg@10=0.02896201402222575 [0.53s]\n",
      "epoch 21 [16.33s]:  training loss=0.16047948598861694                                     \n",
      "epoch 22 [16.04s]:  training loss=0.1524195522069931                                      \n",
      "epoch 23 [15.91s]:  training loss=0.1505032628774643                                      \n",
      "epoch 24 [16.16s]:  training loss=0.14516086876392365                                     \n",
      "epoch 25 [16.79s]: training loss=0.13952526450157166  validation ndcg@10=0.029654867684073532 [0.47s]\n",
      "epoch 26 [16.25s]:  training loss=0.14309662580490112                                     \n",
      "epoch 27 [15.99s]:  training loss=0.13146047294139862                                     \n",
      "epoch 28 [16.27s]:  training loss=0.13314448297023773                                     \n",
      "epoch 29 [16.24s]:  training loss=0.12791338562965393                                     \n",
      "epoch 30 [16.26s]: training loss=0.12557554244995117  validation ndcg@10=0.02976392502193505 [0.47s]\n",
      "epoch 31 [16.1s]:  training loss=0.12411154806613922                                      \n",
      "epoch 32 [15.99s]:  training loss=0.12244662642478943                                     \n",
      "epoch 33 [16.07s]:  training loss=0.11980944126844406                                     \n",
      "epoch 34 [16.05s]:  training loss=0.11555041372776031                                     \n",
      "epoch 35 [16.23s]: training loss=0.1138068214058876  validation ndcg@10=0.028639205012212038 [0.47s]\n",
      "epoch 36 [16.22s]:  training loss=0.11361998319625854                                     \n",
      "epoch 37 [16.14s]:  training loss=0.11060602217912674                                     \n",
      "epoch 38 [16.25s]:  training loss=0.10423821955919266                                     \n",
      "epoch 39 [15.97s]:  training loss=0.10417179018259048                                     \n",
      "epoch 40 [16.37s]: training loss=0.1051468700170517  validation ndcg@10=0.02970070019007421 [0.47s]\n",
      "epoch 41 [16.27s]:  training loss=0.10004878789186478                                     \n",
      "epoch 42 [16.11s]:  training loss=0.10217582434415817                                     \n",
      "epoch 43 [16.3s]:  training loss=0.100538469851017                                        \n",
      "epoch 44 [16.1s]:  training loss=0.09888128191232681                                      \n",
      "epoch 45 [16.3s]: training loss=0.0978500172495842  validation ndcg@10=0.030090307831650582 [0.48s]\n",
      "epoch 46 [16.0s]:  training loss=0.09725874662399292                                      \n",
      "epoch 47 [17.44s]:  training loss=0.09609916061162949                                     \n",
      "epoch 48 [16.02s]:  training loss=0.09285161644220352                                     \n",
      "epoch 49 [16.09s]:  training loss=0.0943925529718399                                      \n",
      "epoch 50 [15.74s]: training loss=0.0899188220500946  validation ndcg@10=0.030176209235736703 [0.5s]\n",
      "epoch 51 [16.21s]:  training loss=0.08791497349739075                                     \n",
      "epoch 52 [16.18s]:  training loss=0.0898623839020729                                      \n",
      "epoch 53 [16.15s]:  training loss=0.08696331083774567                                     \n",
      "epoch 54 [16.01s]:  training loss=0.08495190739631653                                     \n",
      "epoch 55 [15.86s]: training loss=0.0849728137254715  validation ndcg@10=0.02913610448327347 [0.51s]\n",
      "epoch 56 [16.16s]:  training loss=0.08463834971189499                                     \n",
      "epoch 57 [16.4s]:  training loss=0.08290517330169678                                      \n",
      "epoch 58 [16.56s]:  training loss=0.08219033479690552                                     \n",
      "epoch 59 [16.54s]:  training loss=0.0778636485338211                                      \n",
      "epoch 60 [16.14s]: training loss=0.07741568237543106  validation ndcg@10=0.0300326907425491 [0.48s]\n",
      "epoch 61 [16.09s]:  training loss=0.0766509622335434                                      \n",
      "epoch 62 [16.29s]:  training loss=0.07924187183380127                                     \n",
      "epoch 63 [16.18s]:  training loss=0.07723283022642136                                     \n",
      "epoch 64 [16.25s]:  training loss=0.07733749598264694                                     \n",
      "epoch 65 [16.27s]: training loss=0.07676756381988525  validation ndcg@10=0.029255233217254827 [0.5s]\n",
      "epoch 66 [15.9s]:  training loss=0.07409340888261795                                      \n",
      "epoch 67 [16.58s]:  training loss=0.07538976520299911                                     \n",
      "epoch 68 [16.14s]:  training loss=0.07125034928321838                                     \n",
      "epoch 69 [16.17s]:  training loss=0.07244548946619034                                     \n",
      "epoch 70 [16.16s]: training loss=0.07151970267295837  validation ndcg@10=0.029517255229793656 [0.47s]\n",
      "epoch 71 [16.16s]:  training loss=0.07169732451438904                                     \n",
      "epoch 72 [16.4s]:  training loss=0.07119926065206528                                      \n",
      "epoch 73 [15.99s]:  training loss=0.07140138000249863                                     \n",
      "epoch 74 [18.01s]:  training loss=0.06762777268886566                                     \n",
      "epoch 75 [16.11s]: training loss=0.06780533492565155  validation ndcg@10=0.029055371677014306 [0.48s]\n",
      "epoch 1 [27.91s]:  training loss=0.8150656819343567                                       \n",
      "epoch 2 [28.22s]:  training loss=0.7796241641044617                                       \n",
      "epoch 3 [28.66s]:  training loss=0.751381516456604                                        \n",
      "epoch 4 [28.43s]:  training loss=0.716739296913147                                        \n",
      "epoch 5 [28.45s]: training loss=0.6717254519462585  validation ndcg@10=0.010357755837204575 [0.64s]\n",
      "epoch 6 [28.53s]:  training loss=0.6275036334991455                                       \n",
      "epoch 7 [28.43s]:  training loss=0.5897780656814575                                       \n",
      "epoch 8 [27.84s]:  training loss=0.5672946572303772                                       \n",
      "epoch 9 [28.14s]:  training loss=0.5466375350952148                                       \n",
      "epoch 10 [29.2s]: training loss=0.5280234217643738  validation ndcg@10=0.010302041557563832 [0.64s]\n",
      "epoch 11 [28.86s]:  training loss=0.5128617286682129                                      \n",
      "epoch 12 [28.54s]:  training loss=0.4897863268852234                                      \n",
      "epoch 13 [28.53s]:  training loss=0.47033631801605225                                     \n",
      "epoch 14 [28.37s]:  training loss=0.44938793778419495                                     \n",
      "epoch 15 [29.7s]: training loss=0.43373677134513855  validation ndcg@10=0.018223504038690743 [0.68s]\n",
      "epoch 16 [29.14s]:  training loss=0.42047351598739624                                     \n",
      "epoch 17 [28.41s]:  training loss=0.4027072787284851                                      \n",
      "epoch 18 [28.59s]:  training loss=0.3915513753890991                                      \n",
      "epoch 19 [28.13s]:  training loss=0.3785536587238312                                      \n",
      "epoch 20 [28.46s]: training loss=0.3671192228794098  validation ndcg@10=0.0217466133516853 [0.72s]\n",
      "epoch 21 [28.38s]:  training loss=0.3538375496864319                                      \n",
      "epoch 22 [28.22s]:  training loss=0.3460935950279236                                      \n",
      "epoch 23 [28.27s]:  training loss=0.3387231230735779                                      \n",
      "epoch 24 [28.28s]:  training loss=0.3339293599128723                                      \n",
      "epoch 25 [28.36s]: training loss=0.3216843008995056  validation ndcg@10=0.022876444503917827 [0.68s]\n",
      "epoch 26 [28.29s]:  training loss=0.31747594475746155                                     \n",
      "epoch 27 [28.48s]:  training loss=0.3079760670661926                                      \n",
      "epoch 28 [28.51s]:  training loss=0.30519112944602966                                     \n",
      "epoch 29 [29.25s]:  training loss=0.30013036727905273                                     \n",
      "epoch 30 [28.04s]: training loss=0.29475653171539307  validation ndcg@10=0.024392483557275243 [0.63s]\n",
      "epoch 31 [30.2s]:  training loss=0.29035821557044983                                      \n",
      "epoch 32 [28.25s]:  training loss=0.2829306125640869                                      \n",
      "epoch 33 [28.32s]:  training loss=0.2781982123851776                                      \n",
      "epoch 34 [28.22s]:  training loss=0.2739512324333191                                      \n",
      "epoch 35 [28.63s]: training loss=0.2748326361179352  validation ndcg@10=0.024824064484855225 [0.64s]\n",
      "epoch 36 [28.43s]:  training loss=0.2718556821346283                                      \n",
      "epoch 37 [28.03s]:  training loss=0.26738494634628296                                     \n",
      "epoch 38 [28.11s]:  training loss=0.2626371681690216                                      \n",
      "epoch 39 [28.0s]:  training loss=0.25508981943130493                                      \n",
      "epoch 40 [28.48s]: training loss=0.25665780901908875  validation ndcg@10=0.0256661542051206 [0.62s]\n",
      "epoch 41 [28.48s]:  training loss=0.2530626654624939                                      \n",
      "epoch 42 [28.24s]:  training loss=0.24909143149852753                                     \n",
      "epoch 43 [28.16s]:  training loss=0.24466335773468018                                     \n",
      "epoch 44 [28.59s]:  training loss=0.24218595027923584                                     \n",
      "epoch 45 [28.34s]: training loss=0.2438083291053772  validation ndcg@10=0.026226446527427314 [0.62s]\n",
      "epoch 46 [28.36s]:  training loss=0.23361451923847198                                     \n",
      "epoch 47 [30.1s]:  training loss=0.2363589107990265                                       \n",
      "epoch 48 [29.4s]:  training loss=0.23212935030460358                                      \n",
      "epoch 49 [28.2s]:  training loss=0.23011012375354767                                      \n",
      "epoch 50 [28.0s]: training loss=0.23031963407993317  validation ndcg@10=0.026722963069341316 [0.64s]\n",
      "epoch 51 [28.13s]:  training loss=0.22382062673568726                                     \n",
      "epoch 52 [28.07s]:  training loss=0.22185416519641876                                     \n",
      "epoch 53 [28.35s]:  training loss=0.2227770984172821                                      \n",
      "epoch 54 [28.56s]:  training loss=0.21786077320575714                                     \n",
      "epoch 55 [28.58s]: training loss=0.21453090012073517  validation ndcg@10=0.026663104320512176 [0.64s]\n",
      "epoch 56 [28.61s]:  training loss=0.21427874267101288                                     \n",
      "epoch 57 [28.17s]:  training loss=0.21104232966899872                                     \n",
      "epoch 58 [28.29s]:  training loss=0.20861393213272095                                     \n",
      "epoch 59 [28.62s]:  training loss=0.20809461176395416                                     \n",
      "epoch 60 [28.74s]: training loss=0.20347559452056885  validation ndcg@10=0.027448235943272073 [0.71s]\n",
      "epoch 61 [28.43s]:  training loss=0.2066068947315216                                      \n",
      "epoch 62 [29.9s]:  training loss=0.204924538731575                                        \n",
      "epoch 63 [28.58s]:  training loss=0.20009997487068176                                     \n",
      "epoch 64 [27.8s]:  training loss=0.20055030286312103                                      \n",
      "epoch 65 [28.88s]: training loss=0.19854331016540527  validation ndcg@10=0.02747426201356527 [0.65s]\n",
      "epoch 66 [28.65s]:  training loss=0.19304560124874115                                     \n",
      "epoch 67 [29.09s]:  training loss=0.19363167881965637                                     \n",
      "epoch 68 [28.04s]:  training loss=0.1901889592409134                                      \n",
      "epoch 69 [28.66s]:  training loss=0.19039347767829895                                     \n",
      "epoch 70 [28.43s]: training loss=0.19302158057689667  validation ndcg@10=0.027866705809684134 [0.65s]\n",
      "epoch 71 [29.03s]:  training loss=0.18863418698310852                                     \n",
      "epoch 72 [28.39s]:  training loss=0.19070111215114594                                     \n",
      "epoch 73 [28.57s]:  training loss=0.18342000246047974                                     \n",
      "epoch 74 [29.06s]:  training loss=0.18224410712718964                                     \n",
      "epoch 75 [28.91s]: training loss=0.18465398252010345  validation ndcg@10=0.027814126036738367 [0.63s]\n",
      "epoch 76 [29.08s]:  training loss=0.17859549820423126                                     \n",
      "epoch 77 [28.8s]:  training loss=0.18121759593486786                                      \n",
      "epoch 78 [30.18s]:  training loss=0.17983174324035645                                     \n",
      "epoch 79 [28.81s]:  training loss=0.17549964785575867                                     \n",
      "epoch 80 [28.52s]: training loss=0.1774119883775711  validation ndcg@10=0.02808285703001057 [0.63s]\n",
      "epoch 81 [28.41s]:  training loss=0.17366936802864075                                     \n",
      "epoch 82 [28.43s]:  training loss=0.17364412546157837                                     \n",
      "epoch 83 [28.49s]:  training loss=0.17175117135047913                                     \n",
      "epoch 84 [28.06s]:  training loss=0.17084211111068726                                     \n",
      "epoch 85 [28.14s]: training loss=0.16990190744400024  validation ndcg@10=0.02841010096678897 [0.63s]\n",
      "epoch 86 [30.55s]:  training loss=0.16944696009159088                                     \n",
      "epoch 87 [28.58s]:  training loss=0.16672344505786896                                     \n",
      "epoch 88 [28.23s]:  training loss=0.1661599576473236                                      \n",
      "epoch 89 [27.85s]:  training loss=0.1666834056377411                                      \n",
      "epoch 90 [28.63s]: training loss=0.16479580104351044  validation ndcg@10=0.028761283933455027 [0.67s]\n",
      "epoch 91 [28.37s]:  training loss=0.1682078093290329                                      \n",
      "epoch 92 [28.14s]:  training loss=0.1648668795824051                                      \n",
      "epoch 93 [28.67s]:  training loss=0.16153445839881897                                     \n",
      "epoch 94 [29.76s]:  training loss=0.1621703803539276                                      \n",
      "epoch 95 [28.27s]: training loss=0.1569766104221344  validation ndcg@10=0.029131992383867926 [0.62s]\n",
      "epoch 96 [28.28s]:  training loss=0.16018079221248627                                     \n",
      "epoch 97 [28.28s]:  training loss=0.15417219698429108                                     \n",
      "epoch 98 [28.25s]:  training loss=0.15543249249458313                                     \n",
      "epoch 99 [28.43s]:  training loss=0.15697874128818512                                     \n",
      "epoch 100 [28.05s]: training loss=0.15378719568252563  validation ndcg@10=0.029192427176233212 [0.62s]\n",
      "epoch 101 [28.38s]:  training loss=0.15151134133338928                                    \n",
      "epoch 102 [28.73s]:  training loss=0.15032587945461273                                    \n",
      "epoch 103 [28.58s]:  training loss=0.14920136332511902                                    \n",
      "epoch 104 [28.7s]:  training loss=0.15217149257659912                                     \n",
      "epoch 105 [28.93s]: training loss=0.1487695574760437  validation ndcg@10=0.029129507955081794 [0.65s]\n",
      "epoch 106 [29.11s]:  training loss=0.14918109774589539                                    \n",
      "epoch 107 [28.27s]:  training loss=0.1481941193342209                                     \n",
      "epoch 108 [28.24s]:  training loss=0.14767245948314667                                    \n",
      "epoch 109 [29.35s]:  training loss=0.14602729678153992                                    \n",
      "epoch 110 [27.99s]: training loss=0.14568422734737396  validation ndcg@10=0.02965246889878116 [0.62s]\n",
      "epoch 111 [28.38s]:  training loss=0.142296701669693                                      \n",
      "epoch 112 [28.37s]:  training loss=0.14373838901519775                                    \n",
      "epoch 113 [28.27s]:  training loss=0.14175978302955627                                    \n",
      "epoch 114 [28.14s]:  training loss=0.14308594167232513                                    \n",
      "epoch 115 [27.91s]: training loss=0.1427222639322281  validation ndcg@10=0.02952129396361304 [0.62s]\n",
      "epoch 116 [28.41s]:  training loss=0.14068937301635742                                    \n",
      "epoch 117 [28.62s]:  training loss=0.14158311486244202                                    \n",
      "epoch 118 [28.53s]:  training loss=0.1416330188512802                                     \n",
      "epoch 119 [29.0s]:  training loss=0.14032578468322754                                     \n",
      "epoch 120 [28.32s]: training loss=0.14126253128051758  validation ndcg@10=0.02933828353239936 [0.61s]\n",
      "epoch 121 [28.78s]:  training loss=0.13369332253932953                                    \n",
      "epoch 122 [28.37s]:  training loss=0.1359557807445526                                     \n",
      "epoch 123 [27.95s]:  training loss=0.1363460123538971                                     \n",
      "epoch 124 [29.73s]:  training loss=0.13669484853744507                                    \n",
      "epoch 125 [28.74s]: training loss=0.13275891542434692  validation ndcg@10=0.029401970073706554 [0.71s]\n",
      "epoch 126 [29.95s]:  training loss=0.1350003480911255                                     \n",
      "epoch 127 [29.36s]:  training loss=0.13455942273139954                                    \n",
      "epoch 128 [30.09s]:  training loss=0.13089638948440552                                    \n",
      "epoch 129 [29.77s]:  training loss=0.1310710608959198                                     \n",
      "epoch 130 [29.45s]: training loss=0.13255460560321808  validation ndcg@10=0.02988101553256061 [0.68s]\n",
      "epoch 131 [30.29s]:  training loss=0.1325158029794693                                     \n",
      "epoch 132 [29.13s]:  training loss=0.1331442892551422                                     \n",
      "epoch 133 [30.2s]:  training loss=0.13049206137657166                                     \n",
      "epoch 134 [29.67s]:  training loss=0.12587133049964905                                    \n",
      "epoch 135 [29.42s]: training loss=0.1293686330318451  validation ndcg@10=0.030156731644596677 [0.71s]\n",
      "epoch 136 [29.23s]:  training loss=0.12626078724861145                                    \n",
      "epoch 137 [29.7s]:  training loss=0.12914535403251648                                     \n",
      "epoch 138 [29.24s]:  training loss=0.12564505636692047                                    \n",
      "epoch 139 [29.97s]:  training loss=0.12548169493675232                                    \n",
      "epoch 140 [29.84s]: training loss=0.12304206192493439  validation ndcg@10=0.030401591357611602 [0.7s]\n",
      "epoch 141 [29.86s]:  training loss=0.12286718934774399                                    \n",
      "epoch 142 [29.9s]:  training loss=0.12684161961078644                                     \n",
      "epoch 143 [30.39s]:  training loss=0.12446127831935883                                    \n",
      "epoch 144 [29.56s]:  training loss=0.1222129538655281                                     \n",
      "epoch 145 [29.52s]: training loss=0.12282613664865494  validation ndcg@10=0.02999992285606922 [0.68s]\n",
      "epoch 146 [30.48s]:  training loss=0.12316820025444031                                    \n",
      "epoch 147 [29.46s]:  training loss=0.11938155442476273                                    \n",
      "epoch 148 [29.31s]:  training loss=0.12553973495960236                                    \n",
      "epoch 149 [29.59s]:  training loss=0.12096723169088364                                    \n",
      "epoch 150 [29.51s]: training loss=0.11874018609523773  validation ndcg@10=0.0302356410618365 [0.64s]\n",
      "epoch 151 [30.5s]:  training loss=0.1182267889380455                                      \n",
      "epoch 152 [30.11s]:  training loss=0.11922533810138702                                    \n",
      "epoch 153 [29.62s]:  training loss=0.11411146074533463                                    \n",
      "epoch 154 [29.75s]:  training loss=0.11576933413743973                                    \n",
      "epoch 155 [30.07s]: training loss=0.11770478636026382  validation ndcg@10=0.0301919101244574 [0.7s]\n",
      "epoch 156 [29.56s]:  training loss=0.11549073457717896                                    \n",
      "epoch 157 [29.84s]:  training loss=0.11617817729711533                                    \n",
      "epoch 158 [29.86s]:  training loss=0.11642063409090042                                    \n",
      "epoch 159 [29.88s]:  training loss=0.11681815981864929                                    \n",
      "epoch 160 [29.89s]: training loss=0.11586833000183105  validation ndcg@10=0.030185941610288168 [0.7s]\n",
      "epoch 161 [30.72s]:  training loss=0.11384379118680954                                    \n",
      "epoch 162 [29.52s]:  training loss=0.11475064605474472                                    \n",
      "epoch 163 [29.31s]:  training loss=0.11264558881521225                                    \n",
      "epoch 164 [30.23s]:  training loss=0.1160673275589943                                     \n",
      "epoch 165 [30.07s]: training loss=0.1124301329255104  validation ndcg@10=0.030428058466409582 [0.65s]\n",
      "epoch 166 [31.07s]:  training loss=0.11349207162857056                                    \n",
      "epoch 167 [29.92s]:  training loss=0.11354158073663712                                    \n",
      "epoch 168 [29.59s]:  training loss=0.11020474135875702                                    \n",
      "epoch 169 [30.16s]:  training loss=0.11133372783660889                                    \n",
      "epoch 170 [29.89s]: training loss=0.11044846475124359  validation ndcg@10=0.030276548947653616 [0.72s]\n",
      "epoch 171 [29.25s]:  training loss=0.10896605998277664                                    \n",
      "epoch 172 [30.09s]:  training loss=0.11137934029102325                                    \n",
      "epoch 173 [30.02s]:  training loss=0.10774718225002289                                    \n",
      "epoch 174 [29.87s]:  training loss=0.10770683735609055                                    \n",
      "epoch 175 [29.52s]: training loss=0.11030027270317078  validation ndcg@10=0.030786725208580536 [0.64s]\n",
      "epoch 176 [29.56s]:  training loss=0.10769828408956528                                    \n",
      "epoch 177 [29.68s]:  training loss=0.11030188202857971                                    \n",
      "epoch 178 [30.01s]:  training loss=0.10600145161151886                                    \n",
      "epoch 179 [29.97s]:  training loss=0.11018732935190201                                    \n",
      "epoch 180 [30.15s]: training loss=0.10716447234153748  validation ndcg@10=0.030432235289966723 [0.71s]\n",
      "epoch 181 [30.06s]:  training loss=0.10814740508794785                                    \n",
      "epoch 182 [29.3s]:  training loss=0.10764627903699875                                     \n",
      "epoch 183 [29.43s]:  training loss=0.10659907758235931                                    \n",
      "epoch 184 [29.02s]:  training loss=0.10574202984571457                                    \n",
      "epoch 185 [30.08s]: training loss=0.1056535467505455  validation ndcg@10=0.03071565295310556 [0.68s]\n",
      "epoch 186 [31.19s]:  training loss=0.1058427095413208                                     \n",
      "epoch 187 [29.89s]:  training loss=0.10075321793556213                                    \n",
      "epoch 188 [29.91s]:  training loss=0.10193400830030441                                    \n",
      "epoch 189 [29.66s]:  training loss=0.10207259654998779                                    \n",
      "epoch 190 [29.65s]: training loss=0.10492256283760071  validation ndcg@10=0.030704796369374353 [0.68s]\n",
      "epoch 191 [30.43s]:  training loss=0.10422848910093307                                    \n",
      "epoch 192 [30.6s]:  training loss=0.09983358532190323                                     \n",
      "epoch 193 [29.83s]:  training loss=0.10376926511526108                                    \n",
      "epoch 194 [29.79s]:  training loss=0.0987112894654274                                     \n",
      "epoch 195 [31.73s]: training loss=0.09968005120754242  validation ndcg@10=0.03049528923194761 [0.69s]\n",
      "epoch 196 [34.51s]:  training loss=0.10273817181587219                                    \n",
      "epoch 197 [31.37s]:  training loss=0.10347581654787064                                    \n",
      "epoch 198 [29.7s]:  training loss=0.10106079280376434                                     \n",
      "epoch 199 [29.49s]:  training loss=0.09747610986232758                                    \n",
      "epoch 200 [31.06s]: training loss=0.1013595387339592  validation ndcg@10=0.030583736895729827 [0.67s]\n",
      "epoch 1 [32.56s]:  training loss=0.5055121183395386                                        \n",
      "epoch 2 [32.76s]:  training loss=0.27060672640800476                                       \n",
      "epoch 3 [34.65s]:  training loss=0.20441006124019623                                       \n",
      "epoch 4 [34.52s]:  training loss=0.17267920076847076                                       \n",
      "epoch 5 [34.6s]: training loss=0.15108917653560638  validation ndcg@10=0.02792557481559027 [0.98s]\n",
      "epoch 6 [34.56s]:  training loss=0.1314091682434082                                        \n",
      "epoch 7 [36.28s]:  training loss=0.12024950236082077                                       \n",
      "epoch 8 [34.91s]:  training loss=0.10977498441934586                                       \n",
      "epoch 9 [34.7s]:  training loss=0.1003628820180893                                         \n",
      "epoch 10 [34.35s]: training loss=0.09243052452802658  validation ndcg@10=0.027856801119657763 [0.91s]\n",
      "epoch 11 [34.44s]:  training loss=0.08946558833122253                                      \n",
      "epoch 12 [34.48s]:  training loss=0.08472699671983719                                      \n",
      "epoch 13 [35.0s]:  training loss=0.08271125704050064                                       \n",
      "epoch 14 [35.03s]:  training loss=0.07524387538433075                                      \n",
      "epoch 15 [34.95s]: training loss=0.07554052025079727  validation ndcg@10=0.02704994899805247 [0.96s]\n",
      "epoch 16 [34.68s]:  training loss=0.07057206332683563                                      \n",
      "epoch 17 [34.32s]:  training loss=0.07395458966493607                                      \n",
      "epoch 18 [34.59s]:  training loss=0.06545339524745941                                      \n",
      "epoch 19 [34.46s]:  training loss=0.0682278722524643                                       \n",
      "epoch 20 [34.66s]: training loss=0.06521481275558472  validation ndcg@10=0.026283798506175068 [0.88s]\n",
      "epoch 21 [34.38s]:  training loss=0.06309244781732559                                      \n",
      "epoch 22 [34.88s]:  training loss=0.06254929304122925                                      \n",
      "epoch 23 [34.47s]:  training loss=0.061729684472084045                                     \n",
      "epoch 24 [36.11s]:  training loss=0.05985429137945175                                      \n",
      "epoch 25 [34.56s]: training loss=0.06079333275556564  validation ndcg@10=0.025491736385870746 [0.97s]\n",
      "epoch 26 [34.86s]:  training loss=0.058207545429468155                                     \n",
      "epoch 27 [34.55s]:  training loss=0.05438941344618797                                      \n",
      "epoch 28 [35.02s]:  training loss=0.0544576570391655                                       \n",
      "epoch 29 [35.64s]:  training loss=0.056888263672590256                                     \n",
      "epoch 30 [34.88s]: training loss=0.05664275586605072  validation ndcg@10=0.026659687568783084 [0.87s]\n",
      "epoch 1 [37.98s]:  training loss=37.10515213012695                                         \n",
      "epoch 2 [37.39s]:  training loss=67.83741760253906                                        \n",
      "epoch 3 [37.7s]:  training loss=80.97832489013672                                         \n",
      "epoch 4 [37.81s]:  training loss=88.39149475097656                                        \n",
      "epoch 5 [38.34s]: training loss=95.11019897460938  validation ndcg@10=0.017924627125776946 [1.01s]\n",
      "epoch 6 [38.95s]:  training loss=90.61787414550781                                        \n",
      "epoch 7 [38.42s]:  training loss=103.20471954345703                                       \n",
      "epoch 8 [38.54s]:  training loss=107.84967803955078                                       \n",
      "epoch 9 [38.81s]:  training loss=103.51412963867188                                       \n",
      "epoch 10 [38.77s]: training loss=109.49822998046875  validation ndcg@10=0.018808209759180836 [0.95s]\n",
      "epoch 11 [39.56s]:  training loss=109.9110336303711                                       \n",
      "epoch 12 [38.28s]:  training loss=118.87350463867188                                      \n",
      "epoch 13 [38.98s]:  training loss=120.22098541259766                                      \n",
      "epoch 14 [38.28s]:  training loss=110.12081146240234                                      \n",
      "epoch 15 [38.29s]: training loss=111.43096160888672  validation ndcg@10=0.018975459554505206 [1.0s]\n",
      "epoch 16 [38.87s]:  training loss=124.06072235107422                                      \n",
      "epoch 17 [38.33s]:  training loss=122.24271392822266                                      \n",
      "epoch 18 [37.96s]:  training loss=125.5776138305664                                       \n",
      "epoch 19 [38.31s]:  training loss=124.97301483154297                                      \n",
      "epoch 20 [38.06s]: training loss=139.39718627929688  validation ndcg@10=0.01806854703616362 [1.05s]\n",
      "epoch 21 [38.5s]:  training loss=137.35800170898438                                       \n",
      "epoch 22 [38.26s]:  training loss=140.81280517578125                                      \n",
      "epoch 23 [38.35s]:  training loss=123.7048110961914                                       \n",
      "epoch 24 [38.16s]:  training loss=137.99932861328125                                      \n",
      "epoch 25 [38.55s]: training loss=139.0558319091797  validation ndcg@10=0.020113375175032377 [1.02s]\n",
      "epoch 26 [38.3s]:  training loss=137.61651611328125                                       \n",
      "epoch 27 [40.34s]:  training loss=136.45008850097656                                      \n",
      "epoch 28 [38.73s]:  training loss=133.9090576171875                                       \n",
      "epoch 29 [38.7s]:  training loss=152.0214080810547                                        \n",
      "epoch 30 [38.84s]: training loss=141.6702423095703  validation ndcg@10=0.019514597317932224 [0.94s]\n",
      "epoch 31 [38.3s]:  training loss=147.60244750976562                                       \n",
      "epoch 32 [38.76s]:  training loss=148.24749755859375                                      \n",
      "epoch 33 [38.57s]:  training loss=154.6277313232422                                       \n",
      "epoch 34 [37.93s]:  training loss=145.67982482910156                                      \n",
      "epoch 35 [38.54s]: training loss=151.0960693359375  validation ndcg@10=0.017958547331465756 [1.01s]\n",
      "epoch 36 [38.29s]:  training loss=149.05345153808594                                      \n",
      "epoch 37 [38.27s]:  training loss=148.8363800048828                                       \n",
      "epoch 38 [38.13s]:  training loss=146.7684783935547                                       \n",
      "epoch 39 [38.67s]:  training loss=163.37017822265625                                      \n",
      "epoch 40 [38.24s]: training loss=158.04745483398438  validation ndcg@10=0.01850496251735544 [0.92s]\n",
      "epoch 41 [38.99s]:  training loss=148.56930541992188                                      \n",
      "epoch 42 [39.31s]:  training loss=153.2954559326172                                       \n",
      "epoch 43 [39.1s]:  training loss=156.90802001953125                                       \n",
      "epoch 44 [37.99s]:  training loss=173.14613342285156                                      \n",
      "epoch 45 [38.52s]: training loss=167.72061157226562  validation ndcg@10=0.01870765563321675 [0.95s]\n",
      "epoch 46 [37.88s]:  training loss=168.87307739257812                                      \n",
      "epoch 47 [38.85s]:  training loss=157.28787231445312                                      \n",
      "epoch 48 [38.31s]:  training loss=165.17942810058594                                      \n",
      "epoch 49 [38.32s]:  training loss=165.33876037597656                                      \n",
      "epoch 50 [37.96s]: training loss=164.11050415039062  validation ndcg@10=0.019470997905692553 [0.92s]\n",
      "epoch 1 [18.24s]:  training loss=0.7924864888191223                                       \n",
      "epoch 2 [17.97s]:  training loss=0.7108455300331116                                       \n",
      "epoch 3 [18.14s]:  training loss=0.608873188495636                                        \n",
      "epoch 4 [18.13s]:  training loss=0.5462743639945984                                       \n",
      "epoch 5 [17.85s]: training loss=0.49028390645980835  validation ndcg@10=0.013769213921739201 [0.49s]\n",
      "epoch 6 [18.64s]:  training loss=0.44486865401268005                                      \n",
      "epoch 7 [17.6s]:  training loss=0.40982869267463684                                       \n",
      "epoch 8 [17.83s]:  training loss=0.3863423764705658                                       \n",
      "epoch 9 [18.11s]:  training loss=0.3636176586151123                                       \n",
      "epoch 10 [18.26s]: training loss=0.3386498987674713  validation ndcg@10=0.0228437219386076 [0.48s]\n",
      "epoch 11 [18.51s]:  training loss=0.32255393266677856                                     \n",
      "epoch 12 [19.17s]:  training loss=0.3110213875770569                                      \n",
      "epoch 13 [18.29s]:  training loss=0.29305270314216614                                     \n",
      "epoch 14 [17.95s]:  training loss=0.28197821974754333                                     \n",
      "epoch 15 [17.83s]: training loss=0.26936033368110657  validation ndcg@10=0.025287681729205487 [0.47s]\n",
      "epoch 16 [18.27s]:  training loss=0.2623238265514374                                      \n",
      "epoch 17 [17.57s]:  training loss=0.2531663477420807                                      \n",
      "epoch 18 [18.15s]:  training loss=0.24469216167926788                                     \n",
      "epoch 19 [19.1s]:  training loss=0.23529331386089325                                      \n",
      "epoch 20 [17.56s]: training loss=0.23345501720905304  validation ndcg@10=0.026506781721613738 [0.49s]\n",
      "epoch 21 [17.93s]:  training loss=0.2222144454717636                                      \n",
      "epoch 22 [17.54s]:  training loss=0.22272397577762604                                     \n",
      "epoch 23 [17.41s]:  training loss=0.2117597609758377                                      \n",
      "epoch 24 [17.84s]:  training loss=0.20690444111824036                                     \n",
      "epoch 25 [17.76s]: training loss=0.20315569639205933  validation ndcg@10=0.027325483583165316 [0.47s]\n",
      "epoch 26 [17.86s]:  training loss=0.19850565493106842                                     \n",
      "epoch 27 [17.67s]:  training loss=0.19315530359745026                                     \n",
      "epoch 28 [17.93s]:  training loss=0.1909167617559433                                      \n",
      "epoch 29 [17.13s]:  training loss=0.18895064294338226                                     \n",
      "epoch 30 [18.02s]: training loss=0.18560339510440826  validation ndcg@10=0.02807087086443642 [0.49s]\n",
      "epoch 31 [18.15s]:  training loss=0.18265357613563538                                     \n",
      "epoch 32 [17.83s]:  training loss=0.17414481937885284                                     \n",
      "epoch 33 [17.97s]:  training loss=0.1718778759241104                                      \n",
      "epoch 34 [17.52s]:  training loss=0.17070218920707703                                     \n",
      "epoch 35 [17.56s]: training loss=0.16882403194904327  validation ndcg@10=0.028273421044303196 [0.51s]\n",
      "epoch 36 [17.65s]:  training loss=0.1663454920053482                                      \n",
      "epoch 37 [17.44s]:  training loss=0.16136832535266876                                     \n",
      "epoch 38 [17.8s]:  training loss=0.16085635125637054                                      \n",
      "epoch 39 [17.92s]:  training loss=0.15522226691246033                                     \n",
      "epoch 40 [17.82s]: training loss=0.15479439496994019  validation ndcg@10=0.02926656280562605 [0.48s]\n",
      "epoch 41 [18.35s]:  training loss=0.1498158872127533                                      \n",
      "epoch 42 [18.92s]:  training loss=0.15240253508090973                                     \n",
      "epoch 43 [17.37s]:  training loss=0.14530302584171295                                     \n",
      "epoch 44 [17.61s]:  training loss=0.1488933265209198                                      \n",
      "epoch 45 [18.11s]: training loss=0.1425972729921341  validation ndcg@10=0.02882036627056398 [0.53s]\n",
      "epoch 46 [17.73s]:  training loss=0.1407763510942459                                      \n",
      "epoch 47 [18.93s]:  training loss=0.13881143927574158                                     \n",
      "epoch 48 [17.71s]:  training loss=0.13829228281974792                                     \n",
      "epoch 49 [17.73s]:  training loss=0.1390323042869568                                      \n",
      "epoch 50 [18.02s]: training loss=0.1323801726102829  validation ndcg@10=0.029370091190907582 [0.48s]\n",
      "epoch 51 [17.97s]:  training loss=0.13534818589687347                                     \n",
      "epoch 52 [17.61s]:  training loss=0.12894171476364136                                     \n",
      "epoch 53 [17.77s]:  training loss=0.130989208817482                                       \n",
      "epoch 54 [17.82s]:  training loss=0.12946800887584686                                     \n",
      "epoch 55 [18.49s]: training loss=0.12679147720336914  validation ndcg@10=0.03003570316957415 [0.57s]\n",
      "epoch 56 [17.61s]:  training loss=0.12361089140176773                                     \n",
      "epoch 57 [17.23s]:  training loss=0.12347210943698883                                     \n",
      "epoch 58 [17.92s]:  training loss=0.12152709811925888                                     \n",
      "epoch 59 [17.45s]:  training loss=0.11913944035768509                                     \n",
      "epoch 60 [18.12s]: training loss=0.11958558112382889  validation ndcg@10=0.029472255485210493 [0.46s]\n",
      "epoch 61 [18.12s]:  training loss=0.11966262012720108                                     \n",
      "epoch 62 [17.97s]:  training loss=0.11562318354845047                                     \n",
      "epoch 63 [17.53s]:  training loss=0.11627457290887833                                     \n",
      "epoch 64 [17.31s]:  training loss=0.11341351270675659                                     \n",
      "epoch 65 [18.26s]: training loss=0.11279670894145966  validation ndcg@10=0.029895366016407212 [0.48s]\n",
      "epoch 66 [17.67s]:  training loss=0.1102164089679718                                      \n",
      "epoch 67 [17.81s]:  training loss=0.11169692128896713                                     \n",
      "epoch 68 [17.48s]:  training loss=0.10709516704082489                                     \n",
      "epoch 69 [17.88s]:  training loss=0.10831142216920853                                     \n",
      "epoch 70 [17.81s]: training loss=0.10928082466125488  validation ndcg@10=0.030155874814815988 [0.49s]\n",
      "epoch 71 [18.17s]:  training loss=0.10741304606199265                                     \n",
      "epoch 72 [18.85s]:  training loss=0.10339873284101486                                     \n",
      "epoch 73 [17.56s]:  training loss=0.10701640695333481                                     \n",
      "epoch 74 [17.71s]:  training loss=0.10722167789936066                                     \n",
      "epoch 75 [18.12s]: training loss=0.10533737391233444  validation ndcg@10=0.029789214243844186 [0.52s]\n",
      "epoch 76 [17.7s]:  training loss=0.10322459787130356                                      \n",
      "epoch 77 [17.31s]:  training loss=0.10154411941766739                                     \n",
      "epoch 78 [17.9s]:  training loss=0.0984068289399147                                       \n",
      "epoch 79 [17.98s]:  training loss=0.10143665224313736                                     \n",
      "epoch 80 [17.31s]: training loss=0.1003655269742012  validation ndcg@10=0.03045445028364071 [0.51s]\n",
      "epoch 81 [17.91s]:  training loss=0.09996266663074493                                     \n",
      "epoch 82 [17.44s]:  training loss=0.10038983821868896                                     \n",
      "epoch 83 [17.57s]:  training loss=0.09553932398557663                                     \n",
      "epoch 84 [17.51s]:  training loss=0.0983823761343956                                      \n",
      "epoch 85 [18.02s]: training loss=0.09394342452287674  validation ndcg@10=0.030147325285693596 [0.53s]\n",
      "epoch 86 [17.99s]:  training loss=0.09535650163888931                                     \n",
      "epoch 87 [17.35s]:  training loss=0.09356273710727692                                     \n",
      "epoch 88 [17.71s]:  training loss=0.09592009335756302                                     \n",
      "epoch 89 [17.93s]:  training loss=0.09205875545740128                                     \n",
      "epoch 90 [17.56s]: training loss=0.09297119081020355  validation ndcg@10=0.02964921254212226 [0.54s]\n",
      "epoch 91 [19.49s]:  training loss=0.09008636325597763                                     \n",
      "epoch 92 [17.69s]:  training loss=0.0898435115814209                                      \n",
      "epoch 93 [17.95s]:  training loss=0.0872863307595253                                      \n",
      "epoch 94 [17.73s]:  training loss=0.08961512893438339                                     \n",
      "epoch 95 [17.18s]: training loss=0.09143798053264618  validation ndcg@10=0.029699663247014976 [0.48s]\n",
      "epoch 96 [17.8s]:  training loss=0.0909818783402443                                       \n",
      "epoch 97 [17.77s]:  training loss=0.08855785429477692                                     \n",
      "epoch 98 [17.29s]:  training loss=0.08686868846416473                                     \n",
      "epoch 99 [17.64s]:  training loss=0.08684682101011276                                     \n",
      "epoch 100 [17.98s]: training loss=0.08373906463384628  validation ndcg@10=0.029612905848999612 [0.52s]\n",
      "epoch 101 [17.89s]:  training loss=0.08395025879144669                                    \n",
      "epoch 102 [17.83s]:  training loss=0.084530770778656                                      \n",
      "epoch 103 [18.5s]:  training loss=0.08215855807065964                                     \n",
      "epoch 104 [17.92s]:  training loss=0.08429624140262604                                    \n",
      "epoch 105 [17.72s]: training loss=0.08251875638961792  validation ndcg@10=0.030268263889798103 [0.5s]\n",
      "epoch 1 [42.12s]:  training loss=0.2917206585407257                                       \n",
      "epoch 2 [43.82s]:  training loss=0.21075870096683502                                      \n",
      "epoch 3 [43.12s]:  training loss=0.21058814227581024                                      \n",
      "epoch 4 [36.94s]:  training loss=0.2091420292854309                                       \n",
      "epoch 5 [36.37s]: training loss=0.21206779778003693  validation ndcg@10=0.017277118813979427 [0.92s]\n",
      "epoch 6 [36.23s]:  training loss=0.2098972201347351                                       \n",
      "epoch 7 [35.86s]:  training loss=0.21581348776817322                                      \n",
      "epoch 8 [35.26s]:  training loss=0.21872417628765106                                      \n",
      "epoch 9 [35.36s]:  training loss=0.21888355910778046                                      \n",
      "epoch 10 [36.38s]: training loss=0.2161005586385727  validation ndcg@10=0.019498263483133493 [0.88s]\n",
      "epoch 11 [35.54s]:  training loss=0.22135435044765472                                     \n",
      "epoch 12 [35.68s]:  training loss=0.23023051023483276                                     \n",
      "epoch 13 [35.67s]:  training loss=0.2299526482820511                                      \n",
      "epoch 14 [35.84s]:  training loss=0.23100152611732483                                     \n",
      "epoch 15 [35.3s]: training loss=0.22145147621631622  validation ndcg@10=0.01854556119963659 [0.86s]\n",
      "epoch 16 [35.26s]:  training loss=0.23778323829174042                                     \n",
      "epoch 17 [35.15s]:  training loss=0.24031953513622284                                     \n",
      "epoch 18 [35.12s]:  training loss=0.23195190727710724                                     \n",
      "epoch 19 [35.15s]:  training loss=0.24284742772579193                                     \n",
      "epoch 20 [35.17s]: training loss=0.25414368510246277  validation ndcg@10=0.020875045418474646 [0.83s]\n",
      "epoch 21 [35.22s]:  training loss=0.24512407183647156                                     \n",
      "epoch 22 [35.37s]:  training loss=0.24750347435474396                                     \n",
      "epoch 23 [35.09s]:  training loss=0.23620352149009705                                     \n",
      "epoch 24 [35.37s]:  training loss=0.25415292382240295                                     \n",
      "epoch 25 [35.39s]: training loss=0.2457886040210724  validation ndcg@10=0.019898991880822062 [0.92s]\n",
      "epoch 26 [35.35s]:  training loss=0.24994297325611115                                     \n",
      "epoch 27 [35.59s]:  training loss=0.2615106701850891                                      \n",
      "epoch 28 [36.51s]:  training loss=0.2719632685184479                                      \n",
      "epoch 29 [35.96s]:  training loss=0.2553502321243286                                      \n",
      "epoch 30 [35.55s]: training loss=0.27464887499809265  validation ndcg@10=0.018511370383543183 [0.83s]\n",
      "epoch 31 [35.53s]:  training loss=0.2693302631378174                                      \n",
      "epoch 32 [35.39s]:  training loss=0.28078535199165344                                     \n",
      "epoch 33 [35.44s]:  training loss=0.2650948166847229                                      \n",
      "epoch 34 [35.16s]:  training loss=0.27725258469581604                                     \n",
      "epoch 35 [35.25s]: training loss=0.27650412917137146  validation ndcg@10=0.02010120505129615 [0.83s]\n",
      "epoch 36 [35.25s]:  training loss=0.2687893807888031                                      \n",
      "epoch 37 [35.28s]:  training loss=0.2902633249759674                                      \n",
      "epoch 38 [35.27s]:  training loss=0.2845764756202698                                      \n",
      "epoch 39 [35.26s]:  training loss=0.2804339826107025                                      \n",
      "epoch 40 [34.98s]: training loss=0.2876480519771576  validation ndcg@10=0.019087172908639982 [0.86s]\n",
      "epoch 41 [35.09s]:  training loss=0.2674792408943176                                      \n",
      "epoch 42 [35.19s]:  training loss=0.28299829363822937                                     \n",
      "epoch 43 [35.51s]:  training loss=0.2877291738986969                                      \n",
      "epoch 44 [35.51s]:  training loss=0.29587385058403015                                     \n",
      "epoch 45 [36.04s]: training loss=0.30528053641319275  validation ndcg@10=0.019140577426126223 [0.89s]\n",
      "epoch 1 [31.19s]:  training loss=0.831264853477478                                        \n",
      "epoch 2 [32.39s]:  training loss=0.791597843170166                                        \n",
      "epoch 3 [33.0s]:  training loss=0.7573869228363037                                        \n",
      "epoch 4 [32.84s]:  training loss=0.711426854133606                                        \n",
      "epoch 5 [33.3s]: training loss=0.6564604640007019  validation ndcg@10=0.007697464225664166 [0.87s]\n",
      "epoch 6 [32.89s]:  training loss=0.6128718852996826                                       \n",
      "epoch 7 [32.5s]:  training loss=0.5833637714385986                                        \n",
      "epoch 8 [32.56s]:  training loss=0.5584714412689209                                       \n",
      "epoch 9 [32.55s]:  training loss=0.5426236391067505                                       \n",
      "epoch 10 [32.28s]: training loss=0.5264667868614197  validation ndcg@10=0.012180636659796086 [0.81s]\n",
      "epoch 11 [33.16s]:  training loss=0.5102227926254272                                      \n",
      "epoch 12 [31.78s]:  training loss=0.4887690246105194                                      \n",
      "epoch 13 [32.28s]:  training loss=0.47131508588790894                                     \n",
      "epoch 14 [33.12s]:  training loss=0.4581409692764282                                      \n",
      "epoch 15 [32.71s]: training loss=0.44059425592422485  validation ndcg@10=0.021678734081415334 [0.82s]\n",
      "epoch 16 [33.47s]:  training loss=0.42243659496307373                                     \n",
      "epoch 17 [31.88s]:  training loss=0.4081754684448242                                      \n",
      "epoch 18 [32.44s]:  training loss=0.3967946171760559                                      \n",
      "epoch 19 [34.6s]:  training loss=0.37942415475845337                                      \n",
      "epoch 20 [30.47s]: training loss=0.3703305721282959  validation ndcg@10=0.024662233749399718 [0.75s]\n",
      "epoch 21 [31.9s]:  training loss=0.3577895760536194                                       \n",
      "epoch 22 [31.73s]:  training loss=0.34825316071510315                                     \n",
      "epoch 23 [31.98s]:  training loss=0.3383662700653076                                      \n",
      "epoch 24 [31.66s]:  training loss=0.33054253458976746                                     \n",
      "epoch 25 [31.39s]: training loss=0.32356077432632446  validation ndcg@10=0.025414170838050375 [0.79s]\n",
      "epoch 26 [31.66s]:  training loss=0.3107089400291443                                      \n",
      "epoch 27 [31.96s]:  training loss=0.30494424700737                                        \n",
      "epoch 28 [32.29s]:  training loss=0.29501575231552124                                     \n",
      "epoch 29 [33.2s]:  training loss=0.29209816455841064                                      \n",
      "epoch 30 [31.53s]: training loss=0.2849394679069519  validation ndcg@10=0.02508516337893604 [0.75s]\n",
      "epoch 31 [31.32s]:  training loss=0.276296466588974                                       \n",
      "epoch 32 [31.84s]:  training loss=0.2709198594093323                                      \n",
      "epoch 33 [32.24s]:  training loss=0.2709773778915405                                      \n",
      "epoch 34 [33.1s]:  training loss=0.2680054008960724                                       \n",
      "epoch 35 [32.57s]: training loss=0.2616207003593445  validation ndcg@10=0.025731491968329336 [0.82s]\n",
      "epoch 36 [32.22s]:  training loss=0.2540769875049591                                      \n",
      "epoch 37 [32.34s]:  training loss=0.2505546510219574                                      \n",
      "epoch 38 [32.24s]:  training loss=0.24186158180236816                                     \n",
      "epoch 39 [32.01s]:  training loss=0.2423940747976303                                      \n",
      "epoch 40 [32.24s]: training loss=0.23648977279663086  validation ndcg@10=0.025387352434213622 [0.86s]\n",
      "epoch 41 [31.95s]:  training loss=0.2392592579126358                                      \n",
      "epoch 42 [32.46s]:  training loss=0.23036716878414154                                     \n",
      "epoch 43 [31.12s]:  training loss=0.2293912023305893                                      \n",
      "epoch 44 [31.79s]:  training loss=0.22550605237483978                                     \n",
      "epoch 45 [32.23s]: training loss=0.22152797877788544  validation ndcg@10=0.025975598470815518 [0.81s]\n",
      "epoch 46 [31.9s]:  training loss=0.21530365943908691                                      \n",
      "epoch 47 [32.89s]:  training loss=0.21786916255950928                                     \n",
      "epoch 48 [32.01s]:  training loss=0.21623539924621582                                     \n",
      "epoch 49 [31.36s]:  training loss=0.2115650475025177                                      \n",
      "epoch 50 [32.64s]: training loss=0.20532359182834625  validation ndcg@10=0.026512763687038938 [0.85s]\n",
      "epoch 51 [32.24s]:  training loss=0.20925727486610413                                     \n",
      "epoch 52 [31.95s]:  training loss=0.20562207698822021                                     \n",
      "epoch 53 [32.14s]:  training loss=0.2001650184392929                                      \n",
      "epoch 54 [32.07s]:  training loss=0.19774161279201508                                     \n",
      "epoch 55 [31.78s]: training loss=0.19621415436267853  validation ndcg@10=0.026385918660552583 [0.76s]\n",
      "epoch 56 [32.11s]:  training loss=0.19286568462848663                                     \n",
      "epoch 57 [31.97s]:  training loss=0.19345907866954803                                     \n",
      "epoch 58 [31.73s]:  training loss=0.19445092976093292                                     \n",
      "epoch 59 [32.06s]:  training loss=0.1920212358236313                                      \n",
      "epoch 60 [32.1s]: training loss=0.18971866369247437  validation ndcg@10=0.026524371649276766 [0.76s]\n",
      "epoch 61 [34.64s]:  training loss=0.18591322004795074                                     \n",
      "epoch 62 [31.53s]:  training loss=0.18367889523506165                                     \n",
      "epoch 63 [32.13s]:  training loss=0.1811906397342682                                      \n",
      "epoch 64 [31.85s]:  training loss=0.18091820180416107                                     \n",
      "epoch 65 [31.92s]: training loss=0.17935632169246674  validation ndcg@10=0.02671728093779226 [0.81s]\n",
      "epoch 66 [32.15s]:  training loss=0.17839878797531128                                     \n",
      "epoch 67 [32.53s]:  training loss=0.17486758530139923                                     \n",
      "epoch 68 [32.32s]:  training loss=0.17874988913536072                                     \n",
      "epoch 69 [32.11s]:  training loss=0.17461346089839935                                     \n",
      "epoch 70 [32.45s]: training loss=0.1752966046333313  validation ndcg@10=0.02728908552747133 [0.84s]\n",
      "epoch 71 [31.85s]:  training loss=0.17033441364765167                                     \n",
      "epoch 72 [32.19s]:  training loss=0.1676950305700302                                      \n",
      "epoch 73 [31.32s]:  training loss=0.16901801526546478                                     \n",
      "epoch 74 [33.43s]:  training loss=0.16472667455673218                                     \n",
      "epoch 75 [31.8s]: training loss=0.16443872451782227  validation ndcg@10=0.027761757117017855 [0.79s]\n",
      "epoch 76 [32.16s]:  training loss=0.1640561819076538                                      \n",
      "epoch 77 [31.19s]:  training loss=0.16556861996650696                                     \n",
      "epoch 78 [32.39s]:  training loss=0.16240762174129486                                     \n",
      "epoch 79 [32.41s]:  training loss=0.1634688377380371                                      \n",
      "epoch 80 [32.46s]: training loss=0.15897241234779358  validation ndcg@10=0.027537053361309485 [0.75s]\n",
      "epoch 81 [32.38s]:  training loss=0.1582929790019989                                      \n",
      "epoch 82 [32.48s]:  training loss=0.15672864019870758                                     \n",
      "epoch 83 [32.54s]:  training loss=0.15330998599529266                                     \n",
      "epoch 84 [32.59s]:  training loss=0.1550195813179016                                      \n",
      "epoch 85 [31.81s]: training loss=0.15274587273597717  validation ndcg@10=0.027412258572670087 [0.85s]\n",
      "epoch 86 [31.74s]:  training loss=0.15241014957427979                                     \n",
      "epoch 87 [31.3s]:  training loss=0.15196868777275085                                      \n",
      "epoch 88 [33.13s]:  training loss=0.15127497911453247                                     \n",
      "epoch 89 [32.19s]:  training loss=0.1487264186143875                                      \n",
      "epoch 90 [32.25s]: training loss=0.1464153379201889  validation ndcg@10=0.027738759249333333 [0.87s]\n",
      "epoch 91 [32.6s]:  training loss=0.14551220834255219                                      \n",
      "epoch 92 [32.03s]:  training loss=0.1488940566778183                                      \n",
      "epoch 93 [31.87s]:  training loss=0.14464426040649414                                     \n",
      "epoch 94 [32.19s]:  training loss=0.14590570330619812                                     \n",
      "epoch 95 [31.9s]: training loss=0.14357270300388336  validation ndcg@10=0.028171792778143943 [0.78s]\n",
      "epoch 96 [31.76s]:  training loss=0.14337274432182312                                     \n",
      "epoch 97 [32.38s]:  training loss=0.14509402215480804                                     \n",
      "epoch 98 [32.5s]:  training loss=0.14345191419124603                                      \n",
      "epoch 99 [31.81s]:  training loss=0.14109715819358826                                     \n",
      "epoch 100 [32.29s]: training loss=0.14082685112953186  validation ndcg@10=0.028794959277358218 [0.88s]\n",
      "epoch 101 [34.11s]:  training loss=0.14159242808818817                                    \n",
      "epoch 102 [32.3s]:  training loss=0.13785481452941895                                     \n",
      "epoch 103 [31.67s]:  training loss=0.1339828222990036                                     \n",
      "epoch 104 [31.64s]:  training loss=0.13838963210582733                                    \n",
      "epoch 105 [31.22s]: training loss=0.13347475230693817  validation ndcg@10=0.02882548954267116 [0.77s]\n",
      "epoch 106 [31.67s]:  training loss=0.13539570569992065                                    \n",
      "epoch 107 [32.26s]:  training loss=0.13354095816612244                                    \n",
      "epoch 108 [31.85s]:  training loss=0.1320064663887024                                     \n",
      "epoch 109 [31.72s]:  training loss=0.1316143274307251                                     \n",
      "epoch 110 [31.36s]: training loss=0.13008275628089905  validation ndcg@10=0.02948935562185272 [0.82s]\n",
      "epoch 111 [31.9s]:  training loss=0.12918104231357574                                     \n",
      "epoch 112 [31.74s]:  training loss=0.1270069181919098                                     \n",
      "epoch 113 [31.82s]:  training loss=0.1324317306280136                                     \n",
      "epoch 114 [32.35s]:  training loss=0.1281881183385849                                     \n",
      "epoch 115 [33.32s]: training loss=0.12467489391565323  validation ndcg@10=0.028400020972355652 [0.84s]\n",
      "epoch 116 [31.81s]:  training loss=0.1253564953804016                                     \n",
      "epoch 117 [32.73s]:  training loss=0.12887804210186005                                    \n",
      "epoch 118 [32.07s]:  training loss=0.12435923516750336                                    \n",
      "epoch 119 [31.5s]:  training loss=0.12366929650306702                                     \n",
      "epoch 120 [32.25s]: training loss=0.12611953914165497  validation ndcg@10=0.028894632920219623 [0.77s]\n",
      "epoch 121 [31.94s]:  training loss=0.12479227036237717                                    \n",
      "epoch 122 [32.21s]:  training loss=0.12485059350728989                                    \n",
      "epoch 123 [31.75s]:  training loss=0.12349382787942886                                    \n",
      "epoch 124 [31.97s]:  training loss=0.12017905712127686                                    \n",
      "epoch 125 [31.88s]: training loss=0.12513235211372375  validation ndcg@10=0.028869688583063643 [0.84s]\n",
      "epoch 126 [32.6s]:  training loss=0.12048923969268799                                     \n",
      "epoch 127 [31.87s]:  training loss=0.12230654060840607                                    \n",
      "epoch 128 [33.37s]:  training loss=0.12019488215446472                                    \n",
      "epoch 129 [32.01s]:  training loss=0.11739519983530045                                    \n",
      "epoch 130 [31.86s]: training loss=0.12240316718816757  validation ndcg@10=0.029123394977291477 [0.78s]\n",
      "epoch 131 [32.09s]:  training loss=0.11676955968141556                                    \n",
      "epoch 132 [31.53s]:  training loss=0.11568248271942139                                    \n",
      "epoch 133 [31.7s]:  training loss=0.11810959130525589                                     \n",
      "epoch 134 [32.06s]:  training loss=0.1162114366889                                        \n",
      "epoch 135 [31.93s]: training loss=0.11450812220573425  validation ndcg@10=0.029160323430739345 [0.85s]\n",
      "epoch 1 [24.06s]:  training loss=0.34440672397613525                                      \n",
      "epoch 2 [23.92s]:  training loss=0.3525993824005127                                       \n",
      "epoch 3 [24.56s]:  training loss=0.38050249218940735                                      \n",
      "epoch 4 [24.16s]:  training loss=0.415228009223938                                        \n",
      "epoch 5 [23.89s]: training loss=0.42463064193725586  validation ndcg@10=0.018881218837925845 [0.64s]\n",
      "epoch 6 [24.27s]:  training loss=0.4396297335624695                                       \n",
      "epoch 7 [24.56s]:  training loss=0.4611942172050476                                       \n",
      "epoch 8 [25.15s]:  training loss=0.4983839988708496                                       \n",
      "epoch 9 [23.85s]:  training loss=0.45418277382850647                                      \n",
      "epoch 10 [24.11s]: training loss=0.4778411090373993  validation ndcg@10=0.016864985474105268 [0.73s]\n",
      "epoch 11 [23.88s]:  training loss=0.481764554977417                                       \n",
      "epoch 12 [24.45s]:  training loss=0.4987508952617645                                      \n",
      "epoch 13 [24.4s]:  training loss=0.5250635147094727                                       \n",
      "epoch 14 [24.55s]:  training loss=0.5149183869361877                                      \n",
      "epoch 15 [24.26s]: training loss=0.5215542912483215  validation ndcg@10=0.01818099893359404 [0.63s]\n",
      "epoch 16 [24.55s]:  training loss=0.5420783162117004                                      \n",
      "epoch 17 [23.8s]:  training loss=0.5473916530609131                                       \n",
      "epoch 18 [23.89s]:  training loss=0.5680351257324219                                      \n",
      "epoch 19 [24.05s]:  training loss=0.5845062136650085                                      \n",
      "epoch 20 [23.79s]: training loss=0.5940449237823486  validation ndcg@10=0.02046460028899881 [0.64s]\n",
      "epoch 21 [24.72s]:  training loss=0.5603008270263672                                      \n",
      "epoch 22 [24.14s]:  training loss=0.5799999833106995                                      \n",
      "epoch 23 [24.27s]:  training loss=0.5760258436203003                                      \n",
      "epoch 24 [23.72s]:  training loss=0.5519572496414185                                      \n",
      "epoch 25 [23.83s]: training loss=0.5734441876411438  validation ndcg@10=0.02026373135644714 [0.65s]\n",
      "epoch 26 [23.87s]:  training loss=0.6187103390693665                                      \n",
      "epoch 27 [25.08s]:  training loss=0.6749173402786255                                      \n",
      "epoch 28 [23.42s]:  training loss=0.6266403794288635                                      \n",
      "epoch 29 [23.68s]:  training loss=0.6333727240562439                                      \n",
      "epoch 30 [24.3s]: training loss=0.6192558407783508  validation ndcg@10=0.020549739508250026 [0.62s]\n",
      "epoch 31 [24.14s]:  training loss=0.6197779178619385                                      \n",
      "epoch 32 [23.99s]:  training loss=0.620795488357544                                       \n",
      "epoch 33 [23.94s]:  training loss=0.5942713022232056                                      \n",
      "epoch 34 [23.64s]:  training loss=0.6666845083236694                                      \n",
      "epoch 35 [23.86s]: training loss=0.6313373446464539  validation ndcg@10=0.02055345225326339 [0.7s]\n",
      "epoch 36 [23.93s]:  training loss=0.6471617817878723                                      \n",
      "epoch 37 [24.0s]:  training loss=0.6527323722839355                                       \n",
      "epoch 38 [24.05s]:  training loss=0.6445939540863037                                      \n",
      "epoch 39 [24.19s]:  training loss=0.7231820821762085                                      \n",
      "epoch 40 [24.33s]: training loss=0.6692304611206055  validation ndcg@10=0.018895744823981174 [0.63s]\n",
      "epoch 41 [23.82s]:  training loss=0.6688845753669739                                      \n",
      "epoch 42 [23.93s]:  training loss=0.7131595611572266                                      \n",
      "epoch 43 [24.18s]:  training loss=0.6907284259796143                                      \n",
      "epoch 44 [24.75s]:  training loss=0.6947179436683655                                      \n",
      "epoch 45 [25.3s]: training loss=0.6782656311988831  validation ndcg@10=0.019069412895804487 [0.63s]\n",
      "epoch 46 [23.97s]:  training loss=0.6710898876190186                                      \n",
      "epoch 47 [23.39s]:  training loss=0.6883805394172668                                      \n",
      "epoch 48 [23.97s]:  training loss=0.7509380578994751                                      \n",
      "epoch 49 [23.9s]:  training loss=0.7153769135475159                                       \n",
      "epoch 50 [23.8s]: training loss=0.7287427186965942  validation ndcg@10=0.018978919911932967 [0.68s]\n",
      "epoch 51 [23.85s]:  training loss=0.7524601817131042                                      \n",
      "epoch 52 [23.81s]:  training loss=0.7650770545005798                                      \n",
      "epoch 53 [23.8s]:  training loss=0.6874285936355591                                       \n",
      "epoch 54 [23.79s]:  training loss=0.7375745177268982                                      \n",
      "epoch 55 [24.11s]: training loss=0.7325459122657776  validation ndcg@10=0.02007174373702273 [0.67s]\n",
      "epoch 56 [24.16s]:  training loss=0.7724899649620056                                      \n",
      "epoch 57 [23.77s]:  training loss=0.7504363656044006                                      \n",
      "epoch 58 [24.37s]:  training loss=0.7548737525939941                                      \n",
      "epoch 59 [24.1s]:  training loss=0.7372576594352722                                       \n",
      "epoch 60 [24.47s]: training loss=0.74237060546875  validation ndcg@10=0.02061529753143535 [0.67s]\n",
      "epoch 61 [24.36s]:  training loss=0.7214710116386414                                      \n",
      "epoch 62 [24.05s]:  training loss=0.7671564221382141                                      \n",
      "epoch 63 [26.25s]:  training loss=0.7520496249198914                                      \n",
      "epoch 64 [24.09s]:  training loss=0.7955951690673828                                      \n",
      "epoch 65 [24.14s]: training loss=0.7755473852157593  validation ndcg@10=0.019503288541616697 [0.63s]\n",
      "epoch 66 [24.28s]:  training loss=0.8069619536399841                                      \n",
      "epoch 67 [23.94s]:  training loss=0.7714746594429016                                      \n",
      "epoch 68 [23.76s]:  training loss=0.760529100894928                                       \n",
      "epoch 69 [23.83s]:  training loss=0.7761935591697693                                      \n",
      "epoch 70 [23.82s]: training loss=0.7936452627182007  validation ndcg@10=0.020308407035550924 [0.72s]\n",
      "epoch 71 [24.13s]:  training loss=0.8296186923980713                                      \n",
      "epoch 72 [23.79s]:  training loss=0.7557848691940308                                      \n",
      "epoch 73 [24.02s]:  training loss=0.7597920894622803                                      \n",
      "epoch 74 [24.3s]:  training loss=0.8225417137145996                                       \n",
      "epoch 75 [23.88s]: training loss=0.82357257604599  validation ndcg@10=0.01873580287737197 [0.63s]\n",
      "epoch 76 [24.27s]:  training loss=0.8001416325569153                                      \n",
      "epoch 77 [23.76s]:  training loss=0.7941203713417053                                      \n",
      "epoch 78 [23.95s]:  training loss=0.8176993727684021                                      \n",
      "epoch 79 [24.11s]:  training loss=0.8129894733428955                                      \n",
      "epoch 80 [23.91s]: training loss=0.8279314637184143  validation ndcg@10=0.019372442367127617 [0.63s]\n",
      "epoch 81 [25.32s]:  training loss=0.7536802291870117                                      \n",
      "epoch 82 [24.0s]:  training loss=0.837206244468689                                        \n",
      "epoch 83 [24.54s]:  training loss=0.7967433929443359                                      \n",
      "epoch 84 [23.99s]:  training loss=0.8690474629402161                                      \n",
      "epoch 85 [23.99s]: training loss=0.7794651985168457  validation ndcg@10=0.02203571342487284 [0.65s]\n",
      "epoch 86 [24.07s]:  training loss=0.843788743019104                                       \n",
      "epoch 87 [24.16s]:  training loss=0.8333565592765808                                      \n",
      "epoch 88 [24.35s]:  training loss=0.8560164570808411                                      \n",
      "epoch 89 [24.9s]:  training loss=0.8691964149475098                                       \n",
      "epoch 90 [23.64s]: training loss=0.8596459627151489  validation ndcg@10=0.02020739218134003 [0.68s]\n",
      "epoch 91 [24.09s]:  training loss=0.8551930785179138                                      \n",
      "epoch 92 [23.78s]:  training loss=0.7930445671081543                                      \n",
      "epoch 93 [24.17s]:  training loss=0.8660424947738647                                      \n",
      "epoch 94 [23.54s]:  training loss=0.8225455284118652                                      \n",
      "epoch 95 [24.04s]: training loss=0.7807901501655579  validation ndcg@10=0.02011224735591542 [0.65s]\n",
      "epoch 96 [24.23s]:  training loss=0.827998161315918                                       \n",
      "epoch 97 [23.92s]:  training loss=0.8356427550315857                                      \n",
      "epoch 98 [24.0s]:  training loss=0.8780413866043091                                       \n",
      "epoch 99 [25.25s]:  training loss=0.8166857361793518                                      \n",
      "epoch 100 [24.24s]: training loss=0.8414347171783447  validation ndcg@10=0.020686331034920024 [0.62s]\n",
      "epoch 101 [23.8s]:  training loss=0.8581339716911316                                      \n",
      "epoch 102 [23.67s]:  training loss=0.8593029379844666                                     \n",
      "epoch 103 [24.04s]:  training loss=0.8512224555015564                                     \n",
      "epoch 104 [23.74s]:  training loss=0.9071328043937683                                     \n",
      "epoch 105 [23.96s]: training loss=0.9221877455711365  validation ndcg@10=0.020938961009654082 [0.7s]\n",
      "epoch 106 [24.1s]:  training loss=0.8482384085655212                                      \n",
      "epoch 107 [24.03s]:  training loss=0.8926187753677368                                     \n",
      "epoch 108 [24.23s]:  training loss=0.8370335698127747                                     \n",
      "epoch 109 [23.92s]:  training loss=0.8850969672203064                                     \n",
      "epoch 110 [24.24s]: training loss=0.8761246204376221  validation ndcg@10=0.021709353734432106 [0.69s]\n",
      "epoch 1 [53.51s]:  training loss=0.7590519785881042                                       \n",
      "epoch 2 [53.34s]:  training loss=0.5711853504180908                                       \n",
      "epoch 3 [53.66s]:  training loss=0.4831025302410126                                       \n",
      "epoch 4 [54.27s]:  training loss=0.41899850964546204                                      \n",
      "epoch 5 [53.03s]: training loss=0.36225736141204834  validation ndcg@10=0.024626766914837752 [1.36s]\n",
      "epoch 6 [53.13s]:  training loss=0.32151684165000916                                      \n",
      "epoch 7 [54.07s]:  training loss=0.28433364629745483                                      \n",
      "epoch 8 [53.96s]:  training loss=0.2694244682788849                                       \n",
      "epoch 9 [53.78s]:  training loss=0.24973753094673157                                      \n",
      "epoch 10 [52.9s]: training loss=0.2336409091949463  validation ndcg@10=0.026959082354031876 [1.35s]\n",
      "epoch 11 [54.77s]:  training loss=0.22241812944412231                                     \n",
      "epoch 12 [54.51s]:  training loss=0.2109946757555008                                      \n",
      "epoch 13 [53.61s]:  training loss=0.204563170671463                                       \n",
      "epoch 14 [53.95s]:  training loss=0.19470787048339844                                     \n",
      "epoch 15 [54.55s]: training loss=0.1874329000711441  validation ndcg@10=0.02818884067324145 [1.37s]\n",
      "epoch 16 [54.5s]:  training loss=0.17861007153987885                                      \n",
      "epoch 17 [54.11s]:  training loss=0.1712554544210434                                      \n",
      "epoch 18 [54.65s]:  training loss=0.16705453395843506                                     \n",
      "epoch 19 [55.36s]:  training loss=0.16357234120368958                                     \n",
      "epoch 20 [53.03s]: training loss=0.1565469652414322  validation ndcg@10=0.028581929832101532 [1.36s]\n",
      "epoch 21 [55.1s]:  training loss=0.14962582290172577                                      \n",
      "epoch 22 [54.11s]:  training loss=0.14484700560569763                                     \n",
      "epoch 23 [52.85s]:  training loss=0.14089922606945038                                     \n",
      "epoch 24 [53.4s]:  training loss=0.1382548213005066                                       \n",
      "epoch 25 [53.52s]: training loss=0.1372360736131668  validation ndcg@10=0.028072064362032616 [1.34s]\n",
      "epoch 26 [54.27s]:  training loss=0.13121554255485535                                     \n",
      "epoch 27 [54.55s]:  training loss=0.12713870406150818                                     \n",
      "epoch 28 [52.44s]:  training loss=0.1208004280924797                                      \n",
      "epoch 29 [52.07s]:  training loss=0.11919251084327698                                     \n",
      "epoch 30 [52.81s]: training loss=0.11704517155885696  validation ndcg@10=0.026830262929879937 [1.34s]\n",
      "epoch 31 [54.73s]:  training loss=0.11294783651828766                                     \n",
      "epoch 32 [53.18s]:  training loss=0.11342793703079224                                     \n",
      "epoch 33 [53.13s]:  training loss=0.10851053893566132                                     \n",
      "epoch 34 [52.52s]:  training loss=0.10712769627571106                                     \n",
      "epoch 35 [53.36s]: training loss=0.10648991912603378  validation ndcg@10=0.028129983086161457 [1.39s]\n",
      "epoch 36 [53.24s]:  training loss=0.10589434951543808                                     \n",
      "epoch 37 [53.64s]:  training loss=0.10226284712553024                                     \n",
      "epoch 38 [52.98s]:  training loss=0.0983908399939537                                      \n",
      "epoch 39 [53.14s]:  training loss=0.0976332277059555                                      \n",
      "epoch 40 [53.1s]: training loss=0.09456706047058105  validation ndcg@10=0.028241392792923176 [1.32s]\n",
      "epoch 41 [53.61s]:  training loss=0.09504037350416183                                     \n",
      "epoch 42 [52.76s]:  training loss=0.09062878787517548                                     \n",
      "epoch 43 [52.79s]:  training loss=0.09084883332252502                                     \n",
      "epoch 44 [52.99s]:  training loss=0.09046458452939987                                     \n",
      "epoch 45 [53.06s]: training loss=0.08739802986383438  validation ndcg@10=0.028993475664226928 [1.39s]\n",
      "epoch 46 [52.88s]:  training loss=0.08712301403284073                                     \n",
      "epoch 47 [52.36s]:  training loss=0.08600535988807678                                     \n",
      "epoch 48 [53.13s]:  training loss=0.08460347354412079                                     \n",
      "epoch 49 [53.3s]:  training loss=0.08226428180932999                                      \n",
      "epoch 50 [53.25s]: training loss=0.08448919653892517  validation ndcg@10=0.029307874216120897 [1.3s]\n",
      "epoch 51 [53.7s]:  training loss=0.08093031495809555                                      \n",
      "epoch 52 [53.35s]:  training loss=0.07845564931631088                                     \n",
      "epoch 53 [53.37s]:  training loss=0.07912033796310425                                     \n",
      "epoch 54 [53.29s]:  training loss=0.07644271105527878                                     \n",
      "epoch 55 [52.75s]: training loss=0.07510935515165329  validation ndcg@10=0.02958460113717408 [1.32s]\n",
      "epoch 56 [52.37s]:  training loss=0.0770740658044815                                      \n",
      "epoch 57 [53.11s]:  training loss=0.07515532523393631                                     \n",
      "epoch 58 [53.25s]:  training loss=0.07359335571527481                                     \n",
      "epoch 59 [52.63s]:  training loss=0.07240576297044754                                     \n",
      "epoch 60 [52.75s]: training loss=0.07295609265565872  validation ndcg@10=0.028870067256233478 [1.39s]\n",
      "epoch 61 [52.95s]:  training loss=0.0743122547864914                                      \n",
      "epoch 62 [53.07s]:  training loss=0.07214470952749252                                     \n",
      "epoch 63 [53.39s]:  training loss=0.07066778093576431                                     \n",
      "epoch 64 [52.79s]:  training loss=0.07239679992198944                                     \n",
      "epoch 65 [52.95s]: training loss=0.06983693689107895  validation ndcg@10=0.029191582857283904 [1.35s]\n",
      "epoch 66 [54.01s]:  training loss=0.06722884625196457                                     \n",
      "epoch 67 [53.31s]:  training loss=0.0664542019367218                                      \n",
      "epoch 68 [53.73s]:  training loss=0.06663760542869568                                     \n",
      "epoch 69 [52.28s]:  training loss=0.06679175049066544                                     \n",
      "epoch 70 [52.86s]: training loss=0.0653655081987381  validation ndcg@10=0.029795367850143414 [1.28s]\n",
      "epoch 71 [53.44s]:  training loss=0.06627015024423599                                     \n",
      "epoch 72 [53.86s]:  training loss=0.0646417960524559                                      \n",
      "epoch 73 [52.75s]:  training loss=0.06349531561136246                                     \n",
      "epoch 74 [52.27s]:  training loss=0.06428498029708862                                     \n",
      "epoch 75 [53.32s]: training loss=0.06234166398644447  validation ndcg@10=0.02798864566139695 [1.3s]\n",
      "epoch 76 [53.23s]:  training loss=0.06129029765725136                                     \n",
      "epoch 77 [52.66s]:  training loss=0.06509438902139664                                     \n",
      "epoch 78 [52.83s]:  training loss=0.06022416800260544                                     \n",
      "epoch 79 [53.31s]:  training loss=0.06040726974606514                                     \n",
      "epoch 80 [52.22s]: training loss=0.0592283271253109  validation ndcg@10=0.028641627027567545 [1.45s]\n",
      "epoch 81 [53.16s]:  training loss=0.061227742582559586                                    \n",
      "epoch 82 [54.41s]:  training loss=0.06131438910961151                                     \n",
      "epoch 83 [52.5s]:  training loss=0.05869647115468979                                      \n",
      "epoch 84 [53.19s]:  training loss=0.06147030368447304                                     \n",
      "epoch 85 [52.42s]: training loss=0.060313187539577484  validation ndcg@10=0.029654765037135433 [1.41s]\n",
      "epoch 86 [53.35s]:  training loss=0.05852542072534561                                     \n",
      "epoch 87 [52.1s]:  training loss=0.057691894471645355                                     \n",
      "epoch 88 [53.92s]:  training loss=0.05960845574736595                                     \n",
      "epoch 89 [52.64s]:  training loss=0.05660489574074745                                     \n",
      "epoch 90 [52.78s]: training loss=0.05691676586866379  validation ndcg@10=0.028814324363641244 [1.36s]\n",
      "epoch 91 [52.54s]:  training loss=0.057022470980882645                                    \n",
      "epoch 92 [54.6s]:  training loss=0.05389874428510666                                      \n",
      "epoch 93 [52.4s]:  training loss=0.05555487051606178                                      \n",
      "epoch 94 [53.47s]:  training loss=0.05483713373541832                                     \n",
      "epoch 95 [53.38s]: training loss=0.05424518138170242  validation ndcg@10=0.028564094891669806 [1.35s]\n",
      "epoch 1 [17.27s]:  training loss=0.6873971819877625                                       \n",
      "epoch 2 [16.93s]:  training loss=0.47301626205444336                                      \n",
      "epoch 3 [17.01s]:  training loss=0.35323044657707214                                      \n",
      "epoch 4 [16.23s]:  training loss=0.2979309856891632                                       \n",
      "epoch 5 [16.27s]: training loss=0.2679843604564667  validation ndcg@10=0.026104092044010895 [0.47s]\n",
      "epoch 6 [16.48s]:  training loss=0.23825649917125702                                      \n",
      "epoch 7 [16.24s]:  training loss=0.2202761024236679                                       \n",
      "epoch 8 [16.5s]:  training loss=0.2008519172668457                                        \n",
      "epoch 9 [16.47s]:  training loss=0.19069339334964752                                      \n",
      "epoch 10 [16.58s]: training loss=0.17573781311511993  validation ndcg@10=0.028459140393791913 [0.49s]\n",
      "epoch 11 [16.52s]:  training loss=0.166965052485466                                       \n",
      "epoch 12 [16.44s]:  training loss=0.1574762910604477                                      \n",
      "epoch 13 [16.17s]:  training loss=0.15063099563121796                                     \n",
      "epoch 14 [16.36s]:  training loss=0.1430269032716751                                      \n",
      "epoch 15 [16.81s]: training loss=0.13737346231937408  validation ndcg@10=0.028364493706442045 [0.49s]\n",
      "epoch 16 [16.59s]:  training loss=0.13346295058727264                                     \n",
      "epoch 17 [16.54s]:  training loss=0.12814584374427795                                     \n",
      "epoch 18 [16.47s]:  training loss=0.1224757581949234                                      \n",
      "epoch 19 [16.37s]:  training loss=0.11831503361463547                                     \n",
      "epoch 20 [16.55s]: training loss=0.11661649495363235  validation ndcg@10=0.027071547188795377 [0.47s]\n",
      "epoch 21 [16.97s]:  training loss=0.10991429537534714                                     \n",
      "epoch 22 [16.56s]:  training loss=0.10728512704372406                                     \n",
      "epoch 23 [17.16s]:  training loss=0.10537316650152206                                     \n",
      "epoch 24 [16.36s]:  training loss=0.10332468152046204                                     \n",
      "epoch 25 [16.4s]: training loss=0.09870584309101105  validation ndcg@10=0.028726744342975043 [0.48s]\n",
      "epoch 26 [16.51s]:  training loss=0.09595184773206711                                     \n",
      "epoch 27 [16.32s]:  training loss=0.09788761287927628                                     \n",
      "epoch 28 [16.3s]:  training loss=0.09272407740354538                                      \n",
      "epoch 29 [16.9s]:  training loss=0.08903118222951889                                      \n",
      "epoch 30 [16.47s]: training loss=0.08712029457092285  validation ndcg@10=0.029260380616543 [0.46s]\n",
      "epoch 31 [16.77s]:  training loss=0.08515075594186783                                     \n",
      "epoch 32 [16.25s]:  training loss=0.08364509791135788                                     \n",
      "epoch 33 [16.73s]:  training loss=0.08175917714834213                                     \n",
      "epoch 34 [16.37s]:  training loss=0.08008290827274323                                     \n",
      "epoch 35 [16.66s]: training loss=0.07977806031703949  validation ndcg@10=0.02923554409873688 [0.48s]\n",
      "epoch 36 [16.34s]:  training loss=0.07763945311307907                                     \n",
      "epoch 37 [16.02s]:  training loss=0.07758550345897675                                     \n",
      "epoch 38 [16.37s]:  training loss=0.07639005035161972                                     \n",
      "epoch 39 [17.7s]:  training loss=0.07436736673116684                                      \n",
      "epoch 40 [17.51s]: training loss=0.07323989272117615  validation ndcg@10=0.029542723359053117 [0.44s]\n",
      "epoch 41 [14.93s]:  training loss=0.07192838191986084                                     \n",
      "epoch 42 [15.32s]:  training loss=0.06816300004720688                                     \n",
      "epoch 43 [15.22s]:  training loss=0.06942058354616165                                     \n",
      "epoch 44 [15.31s]:  training loss=0.06677420437335968                                     \n",
      "epoch 45 [14.83s]: training loss=0.06862486153841019  validation ndcg@10=0.028320557714091702 [0.46s]\n",
      "epoch 46 [15.42s]:  training loss=0.06608649343252182                                     \n",
      "epoch 47 [15.18s]:  training loss=0.06414784491062164                                     \n",
      "epoch 48 [15.39s]:  training loss=0.06588057428598404                                     \n",
      "epoch 49 [15.34s]:  training loss=0.06198747456073761                                     \n",
      "epoch 50 [15.19s]: training loss=0.06383085995912552  validation ndcg@10=0.028094865218657058 [0.45s]\n",
      "epoch 51 [15.18s]:  training loss=0.06043415516614914                                     \n",
      "epoch 52 [15.11s]:  training loss=0.06096365302801132                                     \n",
      "epoch 53 [15.29s]:  training loss=0.06118097901344299                                     \n",
      "epoch 54 [15.34s]:  training loss=0.05826641991734505                                     \n",
      "epoch 55 [15.33s]: training loss=0.05807654559612274  validation ndcg@10=0.028624214896723975 [0.46s]\n",
      "epoch 56 [15.71s]:  training loss=0.057680174708366394                                    \n",
      "epoch 57 [16.26s]:  training loss=0.05441293120384216                                     \n",
      "epoch 58 [15.68s]:  training loss=0.0555085614323616                                      \n",
      "epoch 59 [15.16s]:  training loss=0.055495984852313995                                    \n",
      "epoch 60 [15.52s]: training loss=0.05401402711868286  validation ndcg@10=0.029158974812997375 [0.47s]\n",
      "epoch 61 [16.1s]:  training loss=0.05255932733416557                                      \n",
      "epoch 62 [15.33s]:  training loss=0.052905142307281494                                    \n",
      "epoch 63 [15.19s]:  training loss=0.0511716865003109                                      \n",
      "epoch 64 [15.25s]:  training loss=0.05317830666899681                                     \n",
      "epoch 65 [15.41s]: training loss=0.050691135227680206  validation ndcg@10=0.028098380825881904 [0.45s]\n",
      "epoch 1 [11.01s]:  training loss=0.8453364372253418                                       \n",
      "epoch 2 [11.05s]:  training loss=0.8306087851524353                                       \n",
      "epoch 3 [11.17s]:  training loss=0.8187726140022278                                       \n",
      "epoch 4 [10.76s]:  training loss=0.8050533533096313                                       \n",
      "epoch 5 [10.7s]: training loss=0.7987052202224731  validation ndcg@10=0.004841177038385663 [0.43s]\n",
      "epoch 6 [10.55s]:  training loss=0.7855490446090698                                       \n",
      "epoch 7 [10.77s]:  training loss=0.7824591398239136                                       \n",
      "epoch 8 [10.96s]:  training loss=0.7736292481422424                                       \n",
      "epoch 9 [10.77s]:  training loss=0.7697177529335022                                       \n",
      "epoch 10 [10.89s]: training loss=0.7599814534187317  validation ndcg@10=0.0076185271905848785 [0.39s]\n",
      "epoch 11 [11.07s]:  training loss=0.7542136311531067                                      \n",
      "epoch 12 [10.8s]:  training loss=0.7444806694984436                                       \n",
      "epoch 13 [11.0s]:  training loss=0.7388629913330078                                       \n",
      "epoch 14 [10.93s]:  training loss=0.7270268201828003                                      \n",
      "epoch 15 [11.07s]: training loss=0.7198355793952942  validation ndcg@10=0.007932672265099283 [0.4s]\n",
      "epoch 16 [11.17s]:  training loss=0.7062543630599976                                      \n",
      "epoch 17 [10.93s]:  training loss=0.6945611238479614                                      \n",
      "epoch 18 [11.1s]:  training loss=0.6862375140190125                                       \n",
      "epoch 19 [10.84s]:  training loss=0.6720285415649414                                      \n",
      "epoch 20 [10.98s]: training loss=0.6591246128082275  validation ndcg@10=0.0061119400734402085 [0.4s]\n",
      "epoch 21 [11.16s]:  training loss=0.6489850878715515                                      \n",
      "epoch 22 [11.09s]:  training loss=0.6366456151008606                                      \n",
      "epoch 23 [10.69s]:  training loss=0.624779224395752                                       \n",
      "epoch 24 [10.73s]:  training loss=0.6151671409606934                                      \n",
      "epoch 25 [10.82s]: training loss=0.6084019541740417  validation ndcg@10=0.007340761896927593 [0.39s]\n",
      "epoch 26 [11.04s]:  training loss=0.5983055233955383                                      \n",
      "epoch 27 [10.88s]:  training loss=0.5908696055412292                                      \n",
      "epoch 28 [11.09s]:  training loss=0.5841934680938721                                      \n",
      "epoch 29 [10.88s]:  training loss=0.5776963233947754                                      \n",
      "epoch 30 [11.11s]: training loss=0.5728017687797546  validation ndcg@10=0.008566602856561115 [0.42s]\n",
      "epoch 31 [10.8s]:  training loss=0.568665087223053                                        \n",
      "epoch 32 [10.79s]:  training loss=0.5579909682273865                                      \n",
      "epoch 33 [10.88s]:  training loss=0.5534900426864624                                      \n",
      "epoch 34 [11.01s]:  training loss=0.5469785332679749                                      \n",
      "epoch 35 [10.78s]: training loss=0.5451666116714478  validation ndcg@10=0.010516247539244334 [0.39s]\n",
      "epoch 36 [10.85s]:  training loss=0.5420402884483337                                      \n",
      "epoch 37 [11.16s]:  training loss=0.5376849174499512                                      \n",
      "epoch 38 [11.06s]:  training loss=0.5288135409355164                                      \n",
      "epoch 39 [11.0s]:  training loss=0.5264158248901367                                       \n",
      "epoch 40 [10.89s]: training loss=0.518165647983551  validation ndcg@10=0.011301336486578108 [0.39s]\n",
      "epoch 41 [11.09s]:  training loss=0.5184229016304016                                      \n",
      "epoch 42 [10.86s]:  training loss=0.5143620371818542                                      \n",
      "epoch 43 [10.48s]:  training loss=0.509794294834137                                       \n",
      "epoch 44 [10.93s]:  training loss=0.5038285851478577                                      \n",
      "epoch 45 [10.81s]: training loss=0.49939557909965515  validation ndcg@10=0.01181243992261178 [0.38s]\n",
      "epoch 46 [10.86s]:  training loss=0.49082690477371216                                     \n",
      "epoch 47 [11.01s]:  training loss=0.4961872398853302                                      \n",
      "epoch 48 [11.01s]:  training loss=0.48963192105293274                                     \n",
      "epoch 49 [10.32s]:  training loss=0.4816071093082428                                      \n",
      "epoch 50 [10.95s]: training loss=0.4784708321094513  validation ndcg@10=0.012490325689750233 [0.39s]\n",
      "epoch 51 [10.83s]:  training loss=0.47463300824165344                                     \n",
      "epoch 52 [10.74s]:  training loss=0.4676814675331116                                      \n",
      "epoch 53 [10.44s]:  training loss=0.4652045965194702                                      \n",
      "epoch 54 [10.58s]:  training loss=0.4621680974960327                                      \n",
      "epoch 55 [10.71s]: training loss=0.4606282114982605  validation ndcg@10=0.013077534039871986 [0.38s]\n",
      "epoch 56 [10.54s]:  training loss=0.4546355605125427                                      \n",
      "epoch 57 [10.59s]:  training loss=0.4499339759349823                                      \n",
      "epoch 58 [10.78s]:  training loss=0.4460752308368683                                      \n",
      "epoch 59 [10.76s]:  training loss=0.4418729543685913                                      \n",
      "epoch 60 [10.88s]: training loss=0.43717288970947266  validation ndcg@10=0.016044439361700653 [0.39s]\n",
      "epoch 61 [10.82s]:  training loss=0.43996191024780273                                     \n",
      "epoch 62 [10.52s]:  training loss=0.43324795365333557                                     \n",
      "epoch 63 [10.44s]:  training loss=0.42980167269706726                                     \n",
      "epoch 64 [10.82s]:  training loss=0.42686590552330017                                     \n",
      "epoch 65 [11.04s]: training loss=0.42375561594963074  validation ndcg@10=0.01940994763094247 [0.4s]\n",
      "epoch 66 [10.85s]:  training loss=0.4189164936542511                                      \n",
      "epoch 67 [10.82s]:  training loss=0.4123634397983551                                      \n",
      "epoch 68 [10.61s]:  training loss=0.4102363884449005                                      \n",
      "epoch 69 [10.77s]:  training loss=0.40589553117752075                                     \n",
      "epoch 70 [10.74s]: training loss=0.4081075191497803  validation ndcg@10=0.021409492498505017 [0.4s]\n",
      "epoch 71 [10.8s]:  training loss=0.3985862135887146                                       \n",
      "epoch 72 [10.43s]:  training loss=0.39495015144348145                                     \n",
      "epoch 73 [10.55s]:  training loss=0.39304065704345703                                     \n",
      "epoch 74 [10.46s]:  training loss=0.39174893498420715                                     \n",
      "epoch 75 [10.44s]: training loss=0.3850691616535187  validation ndcg@10=0.023242769221711097 [0.38s]\n",
      "epoch 76 [10.65s]:  training loss=0.38336774706840515                                     \n",
      "epoch 77 [10.66s]:  training loss=0.37918153405189514                                     \n",
      "epoch 78 [10.43s]:  training loss=0.37501344084739685                                     \n",
      "epoch 79 [11.01s]:  training loss=0.37194496393203735                                     \n",
      "epoch 80 [10.72s]: training loss=0.37382593750953674  validation ndcg@10=0.02430371869638699 [0.39s]\n",
      "epoch 81 [10.7s]:  training loss=0.3690715432167053                                       \n",
      "epoch 82 [10.98s]:  training loss=0.36741724610328674                                     \n",
      "epoch 83 [10.62s]:  training loss=0.3601560890674591                                      \n",
      "epoch 84 [10.87s]:  training loss=0.3579573631286621                                      \n",
      "epoch 85 [10.58s]: training loss=0.36066681146621704  validation ndcg@10=0.024837672773045615 [0.39s]\n",
      "epoch 86 [11.02s]:  training loss=0.3527625501155853                                      \n",
      "epoch 87 [10.7s]:  training loss=0.35161811113357544                                      \n",
      "epoch 88 [10.92s]:  training loss=0.34753885865211487                                     \n",
      "epoch 89 [11.32s]:  training loss=0.3476182818412781                                      \n",
      "epoch 90 [10.91s]: training loss=0.3447541892528534  validation ndcg@10=0.025500467454158078 [0.4s]\n",
      "epoch 91 [10.49s]:  training loss=0.33964139223098755                                     \n",
      "epoch 92 [10.45s]:  training loss=0.33719903230667114                                     \n",
      "epoch 93 [10.77s]:  training loss=0.3380327522754669                                      \n",
      "epoch 94 [10.68s]:  training loss=0.3350640833377838                                      \n",
      "epoch 95 [10.77s]: training loss=0.32924777269363403  validation ndcg@10=0.025638122566046945 [0.39s]\n",
      "epoch 96 [10.89s]:  training loss=0.3336349427700043                                      \n",
      "epoch 97 [10.67s]:  training loss=0.32551875710487366                                     \n",
      "epoch 98 [10.67s]:  training loss=0.3249565064907074                                      \n",
      "epoch 99 [10.53s]:  training loss=0.3241855502128601                                      \n",
      "epoch 100 [11.14s]: training loss=0.3222239315509796  validation ndcg@10=0.025970518737751257 [0.4s]\n",
      "epoch 101 [10.66s]:  training loss=0.32229724526405334                                    \n",
      "epoch 102 [10.87s]:  training loss=0.32190999388694763                                    \n",
      "epoch 103 [10.9s]:  training loss=0.3177148997783661                                      \n",
      "epoch 104 [10.75s]:  training loss=0.315095990896225                                      \n",
      "epoch 105 [10.59s]: training loss=0.309317022562027  validation ndcg@10=0.025981003028241627 [0.39s]\n",
      "epoch 106 [10.83s]:  training loss=0.309558629989624                                      \n",
      "epoch 107 [10.26s]:  training loss=0.3153318166732788                                     \n",
      "epoch 108 [10.74s]:  training loss=0.3067326545715332                                     \n",
      "epoch 109 [10.89s]:  training loss=0.3009151816368103                                     \n",
      "epoch 110 [10.91s]: training loss=0.3013131022453308  validation ndcg@10=0.02625699155960829 [0.39s]\n",
      "epoch 111 [11.11s]:  training loss=0.301969051361084                                      \n",
      "epoch 112 [10.75s]:  training loss=0.29745352268218994                                    \n",
      "epoch 113 [10.49s]:  training loss=0.2946223318576813                                     \n",
      "epoch 114 [10.98s]:  training loss=0.29544439911842346                                    \n",
      "epoch 115 [10.76s]: training loss=0.29267704486846924  validation ndcg@10=0.025959799737688357 [0.38s]\n",
      "epoch 116 [11.09s]:  training loss=0.29086819291114807                                    \n",
      "epoch 117 [11.03s]:  training loss=0.292239785194397                                      \n",
      "epoch 118 [10.91s]:  training loss=0.2892412841320038                                     \n",
      "epoch 119 [10.68s]:  training loss=0.2896597683429718                                     \n",
      "epoch 120 [10.66s]: training loss=0.28525787591934204  validation ndcg@10=0.026028892404141035 [0.38s]\n",
      "epoch 121 [11.02s]:  training loss=0.2845725417137146                                     \n",
      "epoch 122 [10.45s]:  training loss=0.2824464738368988                                     \n",
      "epoch 123 [10.83s]:  training loss=0.2794533669948578                                     \n",
      "epoch 124 [10.91s]:  training loss=0.27900275588035583                                    \n",
      "epoch 125 [10.58s]: training loss=0.27832186222076416  validation ndcg@10=0.026833303390602102 [0.4s]\n",
      "epoch 126 [11.01s]:  training loss=0.27448347210884094                                    \n",
      "epoch 127 [10.78s]:  training loss=0.27619099617004395                                    \n",
      "epoch 128 [10.72s]:  training loss=0.2714358866214752                                     \n",
      "epoch 129 [10.8s]:  training loss=0.27090802788734436                                     \n",
      "epoch 130 [10.43s]: training loss=0.2704177498817444  validation ndcg@10=0.026481947520236374 [0.38s]\n",
      "epoch 131 [10.84s]:  training loss=0.27084073424339294                                    \n",
      "epoch 132 [10.61s]:  training loss=0.26787352561950684                                    \n",
      "epoch 133 [10.92s]:  training loss=0.2655031085014343                                     \n",
      "epoch 134 [10.94s]:  training loss=0.2641751170158386                                     \n",
      "epoch 135 [10.64s]: training loss=0.2665270268917084  validation ndcg@10=0.026368938774031618 [0.4s]\n",
      "epoch 136 [10.87s]:  training loss=0.26151084899902344                                    \n",
      "epoch 137 [10.6s]:  training loss=0.2609093487262726                                      \n",
      "epoch 138 [10.89s]:  training loss=0.25754067301750183                                    \n",
      "epoch 139 [11.18s]:  training loss=0.2610637843608856                                     \n",
      "epoch 140 [10.76s]: training loss=0.2586597800254822  validation ndcg@10=0.026683025697690283 [0.38s]\n",
      "epoch 141 [10.59s]:  training loss=0.25841888785362244                                    \n",
      "epoch 142 [10.59s]:  training loss=0.2549024820327759                                     \n",
      "epoch 143 [10.68s]:  training loss=0.2563755214214325                                     \n",
      "epoch 144 [10.73s]:  training loss=0.2553294003009796                                     \n",
      "epoch 145 [10.76s]: training loss=0.2518128752708435  validation ndcg@10=0.02637391918411547 [0.39s]\n",
      "epoch 146 [10.96s]:  training loss=0.24767717719078064                                    \n",
      "epoch 147 [10.64s]:  training loss=0.25009992718696594                                    \n",
      "epoch 148 [10.91s]:  training loss=0.24821920692920685                                    \n",
      "epoch 149 [10.29s]:  training loss=0.24775435030460358                                    \n",
      "epoch 150 [10.64s]: training loss=0.24966296553611755  validation ndcg@10=0.02657906305033074 [0.39s]\n",
      "epoch 1 [21.24s]:  training loss=0.7821264863014221                                       \n",
      "epoch 2 [21.22s]:  training loss=0.6309534907341003                                       \n",
      "epoch 3 [21.35s]:  training loss=0.5313384532928467                                       \n",
      "epoch 4 [21.24s]:  training loss=0.45713844895362854                                      \n",
      "epoch 5 [21.24s]: training loss=0.4071953296661377  validation ndcg@10=0.02157219776264961 [0.54s]\n",
      "epoch 6 [20.94s]:  training loss=0.36828678846359253                                      \n",
      "epoch 7 [21.07s]:  training loss=0.33465051651000977                                      \n",
      "epoch 8 [21.16s]:  training loss=0.30860528349876404                                      \n",
      "epoch 9 [21.24s]:  training loss=0.29068437218666077                                      \n",
      "epoch 10 [21.3s]: training loss=0.279056191444397  validation ndcg@10=0.0245020962239823 [0.55s]\n",
      "epoch 11 [21.05s]:  training loss=0.262885183095932                                       \n",
      "epoch 12 [21.17s]:  training loss=0.2532884478569031                                      \n",
      "epoch 13 [21.17s]:  training loss=0.23850321769714355                                     \n",
      "epoch 14 [21.24s]:  training loss=0.2312747985124588                                      \n",
      "epoch 15 [21.09s]: training loss=0.22334574162960052  validation ndcg@10=0.02680943160746925 [0.56s]\n",
      "epoch 16 [20.89s]:  training loss=0.21676412224769592                                     \n",
      "epoch 17 [20.77s]:  training loss=0.20797288417816162                                     \n",
      "epoch 18 [21.08s]:  training loss=0.20414574444293976                                     \n",
      "epoch 19 [21.23s]:  training loss=0.1930832415819168                                      \n",
      "epoch 20 [21.71s]: training loss=0.18870872259140015  validation ndcg@10=0.027801670032316452 [0.55s]\n",
      "epoch 21 [21.32s]:  training loss=0.18439407646656036                                     \n",
      "epoch 22 [21.18s]:  training loss=0.18204857409000397                                     \n",
      "epoch 23 [21.23s]:  training loss=0.17676405608654022                                     \n",
      "epoch 24 [21.15s]:  training loss=0.17360959947109222                                     \n",
      "epoch 25 [22.26s]: training loss=0.16822846233844757  validation ndcg@10=0.028605642353057664 [0.57s]\n",
      "epoch 26 [21.23s]:  training loss=0.16133283078670502                                     \n",
      "epoch 27 [21.26s]:  training loss=0.1606879085302353                                      \n",
      "epoch 28 [21.2s]:  training loss=0.1541837900876999                                       \n",
      "epoch 29 [21.0s]:  training loss=0.15216466784477234                                      \n",
      "epoch 30 [20.8s]: training loss=0.14900368452072144  validation ndcg@10=0.02886647248390596 [0.55s]\n",
      "epoch 31 [21.31s]:  training loss=0.14855952560901642                                     \n",
      "epoch 32 [21.31s]:  training loss=0.1404246836900711                                      \n",
      "epoch 33 [21.34s]:  training loss=0.13964833319187164                                     \n",
      "epoch 34 [21.21s]:  training loss=0.13820157945156097                                     \n",
      "epoch 35 [20.92s]: training loss=0.13333582878112793  validation ndcg@10=0.029377324958062 [0.55s]\n",
      "epoch 36 [21.29s]:  training loss=0.13068774342536926                                     \n",
      "epoch 37 [21.27s]:  training loss=0.12933512032032013                                     \n",
      "epoch 38 [21.21s]:  training loss=0.12785768508911133                                     \n",
      "epoch 39 [21.26s]:  training loss=0.12501713633537292                                     \n",
      "epoch 40 [21.27s]: training loss=0.1231675073504448  validation ndcg@10=0.02884318667666838 [0.57s]\n",
      "epoch 41 [21.29s]:  training loss=0.12363037467002869                                     \n",
      "epoch 42 [21.3s]:  training loss=0.12007719278335571                                      \n",
      "epoch 43 [21.16s]:  training loss=0.11814624071121216                                     \n",
      "epoch 44 [20.79s]:  training loss=0.11570631712675095                                     \n",
      "epoch 45 [21.2s]: training loss=0.11543062329292297  validation ndcg@10=0.03043979911799288 [0.58s]\n",
      "epoch 46 [21.47s]:  training loss=0.11212664842605591                                     \n",
      "epoch 47 [20.71s]:  training loss=0.10858917981386185                                     \n",
      "epoch 48 [21.06s]:  training loss=0.10839392989873886                                     \n",
      "epoch 49 [20.75s]:  training loss=0.11037922650575638                                     \n",
      "epoch 50 [20.83s]: training loss=0.10672929883003235  validation ndcg@10=0.029874868193531277 [0.54s]\n",
      "epoch 51 [20.95s]:  training loss=0.10715661942958832                                     \n",
      "epoch 52 [20.78s]:  training loss=0.10469140112400055                                     \n",
      "epoch 53 [20.84s]:  training loss=0.09952179342508316                                     \n",
      "epoch 54 [20.7s]:  training loss=0.10098468512296677                                      \n",
      "epoch 55 [20.41s]: training loss=0.09994950145483017  validation ndcg@10=0.03012185268257193 [0.54s]\n",
      "epoch 56 [20.51s]:  training loss=0.09879441559314728                                     \n",
      "epoch 57 [20.5s]:  training loss=0.09892849624156952                                      \n",
      "epoch 58 [21.03s]:  training loss=0.0960596427321434                                      \n",
      "epoch 59 [20.97s]:  training loss=0.09357916563749313                                     \n",
      "epoch 60 [20.92s]: training loss=0.09542817622423172  validation ndcg@10=0.02966932005826422 [0.56s]\n",
      "epoch 61 [20.74s]:  training loss=0.09214416891336441                                     \n",
      "epoch 62 [20.97s]:  training loss=0.092359758913517                                       \n",
      "epoch 63 [20.89s]:  training loss=0.0920799970626831                                      \n",
      "epoch 64 [20.9s]:  training loss=0.09125261753797531                                      \n",
      "epoch 65 [20.88s]: training loss=0.0915490910410881  validation ndcg@10=0.030240280925773216 [0.55s]\n",
      "epoch 66 [21.13s]:  training loss=0.09253370761871338                                     \n",
      "epoch 67 [20.61s]:  training loss=0.08687658607959747                                     \n",
      "epoch 68 [20.91s]:  training loss=0.087164506316185                                       \n",
      "epoch 69 [20.73s]:  training loss=0.08415955305099487                                     \n",
      "epoch 70 [21.03s]: training loss=0.08281482011079788  validation ndcg@10=0.03019941075153542 [0.55s]\n",
      "epoch 1 [61.32s]:  training loss=1.5843995809555054                                       \n",
      "epoch 2 [62.12s]:  training loss=2.5630853176116943                                       \n",
      "epoch 3 [63.01s]:  training loss=2.776587724685669                                        \n",
      "epoch 4 [62.8s]:  training loss=3.1940481662750244                                        \n",
      "epoch 5 [62.61s]: training loss=3.2900781631469727  validation ndcg@10=0.020111075844127104 [1.35s]\n",
      "epoch 6 [63.22s]:  training loss=3.5007920265197754                                       \n",
      "epoch 7 [63.84s]:  training loss=3.509742021560669                                        \n",
      "epoch 8 [63.7s]:  training loss=3.749221086502075                                         \n",
      "epoch 9 [64.93s]:  training loss=3.705395460128784                                        \n",
      "epoch 10 [63.31s]: training loss=3.8660292625427246  validation ndcg@10=0.018272342574023447 [1.38s]\n",
      "epoch 11 [63.75s]:  training loss=3.9447572231292725                                      \n",
      "epoch 12 [63.66s]:  training loss=3.8831911087036133                                      \n",
      "epoch 13 [63.85s]:  training loss=3.9331789016723633                                      \n",
      "epoch 14 [63.76s]:  training loss=4.099077224731445                                       \n",
      "epoch 15 [64.09s]: training loss=4.051535129547119  validation ndcg@10=0.01750380432920195 [1.34s]\n",
      "epoch 16 [64.87s]:  training loss=4.555551052093506                                       \n",
      "epoch 17 [64.2s]:  training loss=4.571340084075928                                        \n",
      "epoch 18 [64.9s]:  training loss=4.444335460662842                                        \n",
      "epoch 19 [64.0s]:  training loss=4.702714443206787                                        \n",
      "epoch 20 [64.03s]: training loss=4.670057773590088  validation ndcg@10=0.018547891995747648 [1.38s]\n",
      "epoch 21 [63.84s]:  training loss=4.782631874084473                                       \n",
      "epoch 22 [63.39s]:  training loss=4.695682048797607                                       \n",
      "epoch 23 [64.01s]:  training loss=4.412418365478516                                       \n",
      "epoch 24 [63.68s]:  training loss=4.819349765777588                                       \n",
      "epoch 25 [64.24s]: training loss=5.054627418518066  validation ndcg@10=0.020515216090175002 [1.37s]\n",
      "epoch 26 [65.04s]:  training loss=5.102201461791992                                       \n",
      "epoch 27 [63.58s]:  training loss=5.044239521026611                                       \n",
      "epoch 28 [63.89s]:  training loss=5.086034297943115                                       \n",
      "epoch 29 [64.28s]:  training loss=5.230903625488281                                       \n",
      "epoch 30 [63.78s]: training loss=5.258337497711182  validation ndcg@10=0.019755702832589997 [1.35s]\n",
      "epoch 31 [64.44s]:  training loss=5.113518238067627                                       \n",
      "epoch 32 [63.9s]:  training loss=5.10275411605835                                         \n",
      "epoch 33 [63.59s]:  training loss=5.521832466125488                                       \n",
      "epoch 34 [63.57s]:  training loss=5.492728233337402                                       \n",
      "epoch 35 [65.2s]: training loss=5.497147083282471  validation ndcg@10=0.018366940576047878 [1.39s]\n",
      "epoch 36 [64.17s]:  training loss=5.376582145690918                                       \n",
      "epoch 37 [63.27s]:  training loss=5.562962532043457                                       \n",
      "epoch 38 [63.73s]:  training loss=5.0624003410339355                                      \n",
      "epoch 39 [64.0s]:  training loss=5.467184543609619                                        \n",
      "epoch 40 [64.06s]: training loss=5.46136474609375  validation ndcg@10=0.01846364024537343 [1.35s]\n",
      "epoch 41 [63.93s]:  training loss=5.831265926361084                                       \n",
      "epoch 42 [64.58s]:  training loss=5.970226764678955                                       \n",
      "epoch 43 [64.28s]:  training loss=5.7129340171813965                                      \n",
      "epoch 44 [63.75s]:  training loss=5.470109939575195                                       \n",
      "epoch 45 [64.03s]: training loss=5.893600940704346  validation ndcg@10=0.019387623250299572 [1.33s]\n",
      "epoch 46 [63.91s]:  training loss=6.099972248077393                                       \n",
      "epoch 47 [63.81s]:  training loss=5.881803512573242                                       \n",
      "epoch 48 [63.66s]:  training loss=5.827075481414795                                       \n",
      "epoch 49 [64.13s]:  training loss=5.925447940826416                                       \n",
      "epoch 50 [64.15s]: training loss=5.800752639770508  validation ndcg@10=0.0191082642464698 [1.3s]\n",
      "epoch 1 [9.18s]:  training loss=0.45008745789527893                                       \n",
      "epoch 2 [8.86s]:  training loss=0.22669075429439545                                       \n",
      "epoch 3 [8.87s]:  training loss=0.17316897213459015                                       \n",
      "epoch 4 [8.79s]:  training loss=0.1470431387424469                                        \n",
      "epoch 5 [9.05s]: training loss=0.1318623274564743  validation ndcg@10=0.027021418803071004 [0.37s]\n",
      "epoch 6 [8.84s]:  training loss=0.11709954589605331                                       \n",
      "epoch 7 [8.99s]:  training loss=0.10451114177703857                                       \n",
      "epoch 8 [9.03s]:  training loss=0.09944342821836472                                       \n",
      "epoch 9 [9.4s]:  training loss=0.0918361023068428                                         \n",
      "epoch 10 [9.28s]: training loss=0.08648696541786194  validation ndcg@10=0.02587143166150583 [0.36s]\n",
      "epoch 11 [9.3s]:  training loss=0.08236026018857956                                       \n",
      "epoch 12 [8.85s]:  training loss=0.07924851775169373                                      \n",
      "epoch 13 [9.13s]:  training loss=0.0732472687959671                                       \n",
      "epoch 14 [8.99s]:  training loss=0.0727672427892685                                       \n",
      "epoch 15 [9.06s]: training loss=0.07090622186660767  validation ndcg@10=0.025768289008953196 [0.37s]\n",
      "epoch 16 [9.38s]:  training loss=0.0663260743021965                                       \n",
      "epoch 17 [9.09s]:  training loss=0.06455201655626297                                      \n",
      "epoch 18 [9.3s]:  training loss=0.06454410403966904                                       \n",
      "epoch 19 [8.87s]:  training loss=0.06166893243789673                                      \n",
      "epoch 20 [9.25s]: training loss=0.06221612170338631  validation ndcg@10=0.02659665260985129 [0.36s]\n",
      "epoch 21 [9.4s]:  training loss=0.059335678815841675                                      \n",
      "epoch 22 [9.21s]:  training loss=0.05962425470352173                                      \n",
      "epoch 23 [9.1s]:  training loss=0.05803177133202553                                       \n",
      "epoch 24 [9.28s]:  training loss=0.05817876756191254                                      \n",
      "epoch 25 [9.06s]: training loss=0.056893378496170044  validation ndcg@10=0.025493965761208492 [0.37s]\n",
      "epoch 26 [9.22s]:  training loss=0.055011868476867676                                     \n",
      "epoch 27 [9.16s]:  training loss=0.05277702584862709                                      \n",
      "epoch 28 [9.01s]:  training loss=0.05550552159547806                                      \n",
      "epoch 29 [9.25s]:  training loss=0.05698319897055626                                      \n",
      "epoch 30 [9.05s]: training loss=0.05278873071074486  validation ndcg@10=0.023880302238179673 [0.37s]\n",
      "epoch 1 [23.68s]:  training loss=0.8105266690254211                                       \n",
      "epoch 2 [23.5s]:  training loss=0.7498524188995361                                      \n",
      "epoch 3 [23.18s]:  training loss=0.6794672012329102                                     \n",
      "epoch 4 [23.23s]:  training loss=0.6078445911407471                                     \n",
      "epoch 5 [23.66s]: training loss=0.5609660148620605  validation ndcg@10=0.008762159570225915 [0.6s]\n",
      "epoch 6 [23.64s]:  training loss=0.5296014547348022                                     \n",
      "epoch 7 [23.42s]:  training loss=0.5084269642829895                                     \n",
      "epoch 8 [23.88s]:  training loss=0.47851890325546265                                    \n",
      "epoch 9 [23.01s]:  training loss=0.4576292037963867                                     \n",
      "epoch 10 [23.32s]: training loss=0.4329875707626343  validation ndcg@10=0.017205915019107686 [0.6s]\n",
      "epoch 11 [23.0s]:  training loss=0.4127863347530365                                     \n",
      "epoch 12 [23.21s]:  training loss=0.3896569311618805                                    \n",
      "epoch 13 [23.69s]:  training loss=0.3736744821071625                                    \n",
      "epoch 14 [22.93s]:  training loss=0.3584827184677124                                    \n",
      "epoch 15 [23.87s]: training loss=0.3427032232284546  validation ndcg@10=0.02534609195408764 [0.66s]\n",
      "epoch 16 [23.58s]:  training loss=0.3293646275997162                                    \n",
      "epoch 17 [23.67s]:  training loss=0.31211450695991516                                   \n",
      "epoch 18 [22.88s]:  training loss=0.30497485399246216                                   \n",
      "epoch 19 [23.47s]:  training loss=0.2925613820552826                                    \n",
      "epoch 20 [23.36s]: training loss=0.28314077854156494  validation ndcg@10=0.026150177680716723 [0.61s]\n",
      "epoch 21 [22.81s]:  training loss=0.27467310428619385                                   \n",
      "epoch 22 [23.24s]:  training loss=0.26604267954826355                                   \n",
      "epoch 23 [23.71s]:  training loss=0.25668349862098694                                   \n",
      "epoch 24 [23.57s]:  training loss=0.2527388036251068                                    \n",
      "epoch 25 [23.91s]: training loss=0.2416367083787918  validation ndcg@10=0.025477685974848265 [0.62s]\n",
      "epoch 26 [23.35s]:  training loss=0.23670846223831177                                   \n",
      "epoch 27 [23.46s]:  training loss=0.23108850419521332                                   \n",
      "epoch 28 [23.29s]:  training loss=0.22883400321006775                                   \n",
      "epoch 29 [22.84s]:  training loss=0.2219056934118271                                    \n",
      "epoch 30 [23.9s]: training loss=0.21590737998485565  validation ndcg@10=0.02607591115026238 [0.61s]\n",
      "epoch 31 [23.96s]:  training loss=0.21242544054985046                                   \n",
      "epoch 32 [23.37s]:  training loss=0.20749269425868988                                   \n",
      "epoch 33 [23.28s]:  training loss=0.20809406042099                                      \n",
      "epoch 34 [23.85s]:  training loss=0.19874902069568634                                   \n",
      "epoch 35 [23.23s]: training loss=0.19905538856983185  validation ndcg@10=0.02667169358305581 [0.62s]\n",
      "epoch 36 [24.36s]:  training loss=0.19597996771335602                                   \n",
      "epoch 37 [23.02s]:  training loss=0.19210398197174072                                   \n",
      "epoch 38 [24.0s]:  training loss=0.1864204704761505                                     \n",
      "epoch 39 [23.79s]:  training loss=0.18508920073509216                                   \n",
      "epoch 40 [23.49s]: training loss=0.18165768682956696  validation ndcg@10=0.02606003208788159 [0.62s]\n",
      "epoch 41 [23.55s]:  training loss=0.1835302710533142                                    \n",
      "epoch 42 [23.34s]:  training loss=0.1780768483877182                                    \n",
      "epoch 43 [23.15s]:  training loss=0.17671513557434082                                   \n",
      "epoch 44 [23.56s]:  training loss=0.17266038060188293                                   \n",
      "epoch 45 [23.01s]: training loss=0.1708381026983261  validation ndcg@10=0.028094414578423637 [0.61s]\n",
      "epoch 46 [23.44s]:  training loss=0.1684827357530594                                    \n",
      "epoch 47 [24.11s]:  training loss=0.16718560457229614                                   \n",
      "epoch 48 [23.11s]:  training loss=0.16462518274784088                                   \n",
      "epoch 49 [23.85s]:  training loss=0.1620352566242218                                    \n",
      "epoch 50 [23.15s]: training loss=0.158164843916893  validation ndcg@10=0.02833127368659981 [0.66s]\n",
      "epoch 51 [24.04s]:  training loss=0.1548241823911667                                    \n",
      "epoch 52 [23.48s]:  training loss=0.1519140601158142                                    \n",
      "epoch 53 [23.06s]:  training loss=0.15508833527565002                                   \n",
      "epoch 54 [23.18s]:  training loss=0.15120890736579895                                   \n",
      "epoch 55 [23.85s]: training loss=0.14692002534866333  validation ndcg@10=0.02858337239815612 [0.63s]\n",
      "epoch 56 [24.09s]:  training loss=0.14712579548358917                                   \n",
      "epoch 57 [23.29s]:  training loss=0.14674289524555206                                   \n",
      "epoch 58 [22.63s]:  training loss=0.14393708109855652                                   \n",
      "epoch 59 [23.5s]:  training loss=0.14392895996570587                                    \n",
      "epoch 60 [23.83s]: training loss=0.14256344735622406  validation ndcg@10=0.02804401290064816 [0.62s]\n",
      "epoch 61 [24.17s]:  training loss=0.1420794427394867                                    \n",
      "epoch 62 [23.89s]:  training loss=0.13929948210716248                                   \n",
      "epoch 63 [23.82s]:  training loss=0.13747362792491913                                   \n",
      "epoch 64 [24.0s]:  training loss=0.13591685891151428                                    \n",
      "epoch 65 [23.22s]: training loss=0.1361156553030014  validation ndcg@10=0.02828761665030521 [0.62s]\n",
      "epoch 66 [23.6s]:  training loss=0.13644064962863922                                    \n",
      "epoch 67 [23.43s]:  training loss=0.13310086727142334                                   \n",
      "epoch 68 [24.08s]:  training loss=0.13025234639644623                                   \n",
      "epoch 69 [23.33s]:  training loss=0.13199570775032043                                   \n",
      "epoch 70 [23.16s]: training loss=0.12718930840492249  validation ndcg@10=0.02865706910335904 [0.61s]\n",
      "epoch 71 [23.42s]:  training loss=0.12594787776470184                                   \n",
      "epoch 72 [23.34s]:  training loss=0.1275104284286499                                    \n",
      "epoch 73 [23.56s]:  training loss=0.12474536150693893                                   \n",
      "epoch 74 [23.4s]:  training loss=0.12207366526126862                                    \n",
      "epoch 75 [23.23s]: training loss=0.12298297137022018  validation ndcg@10=0.029442149621084466 [0.6s]\n",
      "epoch 76 [24.13s]:  training loss=0.12249711155891418                                   \n",
      "epoch 77 [22.73s]:  training loss=0.12153463810682297                                   \n",
      "epoch 78 [23.06s]:  training loss=0.12204182893037796                                   \n",
      "epoch 79 [23.78s]:  training loss=0.11916496604681015                                   \n",
      "epoch 80 [23.01s]: training loss=0.11803985387086868  validation ndcg@10=0.029047301436964382 [0.6s]\n",
      "epoch 81 [23.75s]:  training loss=0.11648469418287277                                   \n",
      "epoch 82 [23.05s]:  training loss=0.11363936960697174                                   \n",
      "epoch 83 [22.65s]:  training loss=0.11592799425125122                                   \n",
      "epoch 84 [24.62s]:  training loss=0.11402422189712524                                   \n",
      "epoch 85 [23.81s]: training loss=0.11213384568691254  validation ndcg@10=0.02899353349294982 [0.61s]\n",
      "epoch 86 [23.94s]:  training loss=0.11119955778121948                                   \n",
      "epoch 87 [23.55s]:  training loss=0.11044573783874512                                   \n",
      "epoch 88 [24.0s]:  training loss=0.10766251385211945                                    \n",
      "epoch 89 [23.15s]:  training loss=0.10934814810752869                                   \n",
      "epoch 90 [23.31s]: training loss=0.10761997103691101  validation ndcg@10=0.02979227113906573 [0.62s]\n",
      "epoch 91 [23.07s]:  training loss=0.10842692852020264                                   \n",
      "epoch 92 [24.02s]:  training loss=0.10627133399248123                                   \n",
      "epoch 93 [24.2s]:  training loss=0.10691844671964645                                    \n",
      "epoch 94 [23.99s]:  training loss=0.10795649141073227                                   \n",
      "epoch 95 [23.96s]: training loss=0.10258613526821136  validation ndcg@10=0.02965244549139546 [0.62s]\n",
      "epoch 96 [24.01s]:  training loss=0.10331591218709946                                   \n",
      "epoch 97 [23.82s]:  training loss=0.10499128699302673                                   \n",
      "epoch 98 [23.57s]:  training loss=0.1033448651432991                                    \n",
      "epoch 99 [23.45s]:  training loss=0.0994904562830925                                    \n",
      "epoch 100 [23.45s]: training loss=0.09726954251527786  validation ndcg@10=0.029729246210023933 [0.6s]\n",
      "epoch 101 [23.5s]:  training loss=0.09756850451231003                                   \n",
      "epoch 102 [23.4s]:  training loss=0.09987765550613403                                   \n",
      "epoch 103 [23.56s]:  training loss=0.0982380136847496                                   \n",
      "epoch 104 [23.08s]:  training loss=0.09606630355119705                                  \n",
      "epoch 105 [22.65s]: training loss=0.09698925912380219  validation ndcg@10=0.029900594669024744 [0.62s]\n",
      "epoch 106 [23.99s]:  training loss=0.09539958089590073                                  \n",
      "epoch 107 [23.41s]:  training loss=0.09622421115636826                                  \n",
      "epoch 108 [24.73s]:  training loss=0.09506706893444061                                  \n",
      "epoch 109 [23.78s]:  training loss=0.09845967590808868                                  \n",
      "epoch 110 [22.65s]: training loss=0.09431738406419754  validation ndcg@10=0.02967548728383904 [0.58s]\n",
      "epoch 111 [23.28s]:  training loss=0.09327900409698486                                  \n",
      "epoch 112 [22.36s]:  training loss=0.09098304063081741                                  \n",
      "epoch 113 [23.99s]:  training loss=0.09030454605817795                                  \n",
      "epoch 114 [23.61s]:  training loss=0.091999851167202                                    \n",
      "epoch 115 [23.5s]: training loss=0.09060288965702057  validation ndcg@10=0.02984895914223539 [0.61s]\n",
      "epoch 116 [23.63s]:  training loss=0.08821992576122284                                  \n",
      "epoch 117 [24.17s]:  training loss=0.09141834080219269                                  \n",
      "epoch 118 [23.54s]:  training loss=0.08886231482028961                                  \n",
      "epoch 119 [22.59s]:  training loss=0.08812955766916275                                  \n",
      "epoch 120 [22.71s]: training loss=0.08615057170391083  validation ndcg@10=0.03039076082232913 [0.61s]\n",
      "epoch 121 [23.92s]:  training loss=0.08633539825677872                                  \n",
      "epoch 122 [23.18s]:  training loss=0.08627335727214813                                  \n",
      "epoch 123 [23.57s]:  training loss=0.08808226138353348                                  \n",
      "epoch 124 [23.58s]:  training loss=0.08388844132423401                                  \n",
      "epoch 125 [23.43s]: training loss=0.08574832230806351  validation ndcg@10=0.029906019811101414 [0.63s]\n",
      "epoch 126 [23.38s]:  training loss=0.0823199525475502                                   \n",
      "epoch 127 [23.07s]:  training loss=0.08579114079475403                                  \n",
      "epoch 128 [23.83s]:  training loss=0.0822792500257492                                   \n",
      "epoch 129 [23.95s]:  training loss=0.080476775765419                                    \n",
      "epoch 130 [23.94s]: training loss=0.08364512771368027  validation ndcg@10=0.03070703689813431 [0.59s]\n",
      "epoch 131 [24.26s]:  training loss=0.08152608573436737                                  \n",
      "epoch 132 [23.54s]:  training loss=0.07922153919935226                                  \n",
      "epoch 133 [23.7s]:  training loss=0.07776053249835968                                   \n",
      "epoch 134 [23.0s]:  training loss=0.08087741583585739                                   \n",
      "epoch 135 [23.93s]: training loss=0.07985489070415497  validation ndcg@10=0.030515609760240443 [0.6s]\n",
      "epoch 136 [24.1s]:  training loss=0.08005357533693314                                   \n",
      "epoch 137 [23.18s]:  training loss=0.07918083667755127                                  \n",
      "epoch 138 [23.43s]:  training loss=0.07830702513456345                                  \n",
      "epoch 139 [23.08s]:  training loss=0.07855342328548431                                  \n",
      "epoch 140 [23.74s]: training loss=0.0777740329504013  validation ndcg@10=0.030749115358587296 [0.6s]\n",
      "epoch 141 [23.15s]:  training loss=0.07815629988908768                                  \n",
      "epoch 142 [23.67s]:  training loss=0.07741910219192505                                  \n",
      "epoch 143 [23.34s]:  training loss=0.07790543884038925                                  \n",
      "epoch 144 [23.03s]:  training loss=0.07686133682727814                                  \n",
      "epoch 145 [24.25s]: training loss=0.0748126283288002  validation ndcg@10=0.030356172475166397 [0.61s]\n",
      "epoch 146 [23.81s]:  training loss=0.07706379145383835                                  \n",
      "epoch 147 [23.27s]:  training loss=0.07711175084114075                                  \n",
      "epoch 148 [24.06s]:  training loss=0.07550287991762161                                  \n",
      "epoch 149 [23.25s]:  training loss=0.07245037704706192                                  \n",
      "epoch 150 [23.12s]: training loss=0.07172510772943497  validation ndcg@10=0.030638988432440832 [0.61s]\n",
      "epoch 151 [23.59s]:  training loss=0.0740470215678215                                   \n",
      "epoch 152 [23.43s]:  training loss=0.07339077442884445                                  \n",
      "epoch 153 [23.56s]:  training loss=0.07288141548633575                                  \n",
      "epoch 154 [24.55s]:  training loss=0.07187381386756897                                  \n",
      "epoch 155 [24.03s]: training loss=0.07154186815023422  validation ndcg@10=0.030507781701806902 [0.61s]\n",
      "epoch 156 [23.48s]:  training loss=0.07072655111551285                                  \n",
      "epoch 157 [24.28s]:  training loss=0.07080487161874771                                  \n",
      "epoch 158 [23.68s]:  training loss=0.07219234108924866                                  \n",
      "epoch 159 [23.18s]:  training loss=0.06901172548532486                                  \n",
      "epoch 160 [23.05s]: training loss=0.07062243670225143  validation ndcg@10=0.03039602367564637 [0.63s]\n",
      "epoch 161 [23.73s]:  training loss=0.06927862018346786                                  \n",
      "epoch 162 [23.08s]:  training loss=0.06966590136289597                                  \n",
      "epoch 163 [23.63s]:  training loss=0.06902981549501419                                  \n",
      "epoch 164 [24.05s]:  training loss=0.06726708263158798                                  \n",
      "epoch 165 [22.77s]: training loss=0.06765542179346085  validation ndcg@10=0.030115954585334073 [0.59s]\n",
      "100%|██████████| 50/50 [29:57:20<00:00, 2156.81s/trial, best loss: -0.03085043538861164]\n"
     ]
    }
   ],
   "source": [
    "tune_pinsage('adobe_core5', use_text_feature=True, use_no_feature=False, use_only_text=True)\n",
    "# only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
